<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[数仓工作中常用的优化及函数]]></title>
    <url>%2F2020%2F05%2F02%2F%E6%95%B0%E4%BB%93%E5%B7%A5%E4%BD%9C%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E4%BC%98%E5%8C%96%E5%8F%8A%E5%87%BD%E6%95%B0.html</url>
    <content type="text"><![CDATA[数仓中常用的优化及函数]]></content>
  </entry>
  <entry>
    <title><![CDATA[hive基于python编写udf]]></title>
    <url>%2F2020%2F05%2F02%2Fhive%E5%9F%BA%E4%BA%8Epython%E7%BC%96%E5%86%99udf.html</url>
    <content type="text"><![CDATA[hive基于Python编写udf]]></content>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python命令行Click]]></title>
    <url>%2F2020%2F05%2F02%2Fpython%E5%91%BD%E4%BB%A4%E8%A1%8CClick.html</url>
    <content type="text"><![CDATA[Python的命令行Click]]></content>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python]]></title>
    <url>%2F2020%2F05%2F02%2Fpython.html</url>
    <content type="text"><![CDATA[Python的基础]]></content>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SparkSql的内置函数]]></title>
    <url>%2F2020%2F05%2F02%2FSparkSql%E7%9A%84%E5%86%85%E7%BD%AE%E5%87%BD%E6%95%B0.html</url>
    <content type="text"><![CDATA[1 SparkSql的内置函数• 1 !• 2 %• 3 &amp;• 4 *• 5 +• 6 -• 7 /• 8 &lt;• 9 &lt;=• 10 &lt;=&gt;• 11 =• 12 ==• 13 &gt;• 14 &gt;=• 15 ^• 16 abs• 17 acos• 18 add_months• 19 and• 20 approx_count_distinct• 21 approx_percentile• 22 array• 23 array_contains• 24 ascii• 25 asin• 26 assert_true• 27 atan• 28 atan2• 29 avg• 30 base64• 31 bigint• 32 bin• 33 binary• 34 bit_length• 35 boolean• 36 bround• 37 cast• 38 cbrt• 39 ceil• 40 ceiling• 41 char• 42 char_length• 43 character_length• 44 chr• 45 coalesce• 46 collect_list• 47 collect_set• 48 concat• 49 concat_ws• 50 conv• 51 corr• 52 cos• 53 cosh• 54 cot• 55 count• 56 count_min_sketch• 57 covar_pop• 58 covar_samp• 59 crc32• 60 cube• 61 cume_dist• 62 current_database• 63 current_date• 64 current_timestamp• 65 date• 66 date_add• 67 date_format• 68 date_sub• 69 date_trunc• 70 datediff• 71 day• 72 dayofmonth• 73 dayofweek• 74 dayofyear• 75 decimal• 76 decode• 77 degrees• 78 dense_rank• 79 double• 80 e• 81 elt• 82 encode• 83 exp• 84 explode• 85 explode_outer• 86 expm1• 87 factorial• 88 find_in_set• 89 first• 90 first_value• 91 float• 92 floor• 93 format_number• 94 format_string• 95 from_json• 96 from_unixtime• 97 from_utc_timestamp• 98 get_json_object• 99 greatest• 100 grouping• 101 grouping_id• 102 hash• 103 hex• 104 hour• 105 hypot• 106 if• 107 ifnull• 108 in• 109 initcap• 110 inline• 111 inline_outer• 112 input_file_block_length• 113 input_file_block_start• 114 input_file_name• 115 instr• 116 int• 117 isnan• 118 isnotnull• 119 isnull• 120 java_method• 121 json_tuple• 122 kurtosis• 123 lag• 124 last• 125 last_day• 126 last_value• 127 lcase• 128 lead• 129 least• 130 left• 131 length• 132 levenshtein• 133 like• 134 ln• 135 locate• 136 log• 137 log10• 138 log1p• 139 log2• 140 lower• 141 lpad• 142 ltrim• 143 map• 144 map_keys• 145 map_values• 146 max• 147 md5• 148 mean• 149 min• 150 minute• 151 mod• 152 monotonically_increasing_id• 153 month• 154 months_between• 155 named_struct• 156 nanvl• 157 negative• 158 next_day• 159 not• 160 now• 161 ntile• 162 nullif• 163 nvl• 164 nvl2• 165 octet_length• 166 or• 167 parse_url• 168 percent_rank• 169 percentile• 170 percentile_approx• 171 pi• 172 pmod• 173 posexplode• 174 posexplode_outer• 175 position• 176 positive• 177 pow• 178 power• 179 printf• 180 quarter• 181 radians• 182 rand• 183 randn• 184 rank• 185 reflect• 186 regexp_extract• 187 regexp_replace• 188 repeat• 189 replace• 190 reverse• 191 right• 192 rint• 193 rlike• 194 rollup• 195 round• 196 row_number• 197 rpad• 198 rtrim• 199 second• 200 sentences• 201 sha• 202 sha1• 203 sha2• 204 shiftleft• 205 shiftright• 206 shiftrightunsigned• 207 sign• 208 signum• 209 sin• 210 sinh• 211 size• 212 skewness• 213 smallint• 214 sort_array• 215 soundex• 216 space• 217 spark_partition_id• 218 split• 219 sqrt• 220 stack• 221 std• 222 stddev• 223 stddev_pop• 224 stddev_samp• 225 str_to_map• 226 string• 227 struct• 228 substr• 229 substring• 230 substring_index• 231 sum• 232 tan• 233 tanh• 234 timestamp• 235 tinyint• 236 to_date• 237 to_json• 238 to_timestamp• 239 to_unix_timestamp• 240 to_utc_timestamp• 241 translate• 242 trim• 243 trunc• 244 ucase• 245 unbase64• 246 unhex• 247 unix_timestamp• 248 upper• 249 uuid• 250 var_pop• 251 var_samp• 252 variance• 253 weekofyear• 254 when• 255 window• 256 xpath• 257 xpath_boolean• 258 xpath_double• 259 xpath_float• 260 xpath_int• 261 xpath_long• 262 xpath_number• 263 xpath_short• 264 xpath_string• 265 year• 266 |• 267 ~ !! expr ：逻辑非。 %expr1 % expr2 - 返回 expr1/expr2 的余数. 例子： SELECT 2 % 1.8; 0.2 SELECT MOD(2, 1.8); 0.2 &amp;expr1 &amp; expr2 - 返回 expr1 和 expr2 的按位AND的结果。 例子： SELECT 3 &amp; 5; 1 expr1 expr2 - 返回 expr1*expr2. 例子： SELECT 2 * 3; 6 +expr1 + expr2 - 返回 expr1+expr2. 例子： SELECT 1 + 2; 3 -expr1 - expr2 - 返回 expr1-expr2. 例子： SELECT 2 - 1; 1 /expr1 / expr2 - 返回 expr1/expr2，返回结果总是浮点数。 例子： SELECT 3 / 2; 1.5 SELECT 2L / 2L; 1.0 &lt;expr1 &lt; expr2 - 如果 expr1 小于 expr2 则返回 true. 参数:expr1, expr2 - 比较的两个参数类型必须一致，或者可以转换成一样的类型，而且这个类型支持排序。比如 map 类型就是不支持比较的，所以这个操作符不支持 map 类型的参数。 例子： SELECT 1 &lt; 2; true SELECT 1.1 &lt; ‘1’; false SELECT to_date(‘2009-07-30 04:17:52’) &lt; to_date(‘2009-07-30 04:17:52’); false SELECT to_date(‘2009-07-30 04:17:52’) &lt; to_date(‘2009-08-01 04:17:52’); true SELECT 1 &lt; NULL; NULL &lt;=expr1 &lt;= expr2 - 如果 expr1 小于等于 expr2。 例子：expr1, expr2 - 比较的两个参数类型必须一致，或者可以转换成一样的类型，而且这个类型支持排序。比如 map 类型就是不支持比较的，所以这个操作符不支持 map 类型的参数。 例子： SELECT 2 &lt;= 2; true SELECT 1.0 &lt;= ‘1’; true SELECT to_date(‘2009-07-30 04:17:52’) &lt;= to_date(‘2009-07-30 04:17:52’); true SELECT to_date(‘2009-07-30 04:17:52’) &lt;= to_date(‘2009-08-01 04:17:52’); true SELECT 1 &lt;= NULL; NULL &lt;=&gt;expr1 &lt;=&gt; expr2 - 返回的结果和 EQUAL(=) 一样。如果操作符两边都是 null，该操作符返回 true；仅一边为null则返回false。 参数:expr1, expr2 - 比较的两个参数类型必须一致，或者可以转换成一样的类型，而且这个类型支持排序。比如 map 类型就是不支持比较的，所以这个操作符不支持 map 类型的参数。例子： SELECT 2 &lt;=&gt; 2; true SELECT 1 &lt;=&gt; ‘1’; true SELECT true &lt;=&gt; NULL; false SELECT NULL &lt;=&gt; NULL; true =expr1 = expr2 - 如果 expr1 等于 expr2 则返回true，否则返回false。 参数:expr1, expr2 - 比较的两个参数类型必须一致，或者可以转换成一样的类型，而且这个类型支持排序。比如 map 类型就是不支持比较的，所以这个操作符不支持 map 类型的参数。例子： SELECT 2 = 2; true SELECT 1 = ‘1’; true SELECT true = NULL; NULL SELECT NULL = NULL; NULL ==expr1 == expr2 - 如果 expr1 等于 expr2 则返回true，否则返回false。 参数:expr1, expr2 - 比较的两个参数类型必须一致，或者可以转换成一样的类型，而且这个类型支持排序。比如 map 类型就是不支持比较的，所以这个操作符不支持 map 类型的参数。例子： SELECT 2 == 2; true SELECT 1 == ‘1’; true SELECT true == NULL; NULL SELECT NULL == NULL; NULL expr1 &gt; expr2 - 如果 expr1 大于 expr2 则返回 true。 参数:expr1, expr2 - 比较的两个参数类型必须一致，或者可以转换成一样的类型，而且这个类型支持排序。比如 map 类型就是不支持比较的，所以这个操作符不支持 map 类型的参数。例子： SELECT 2 &gt; 1; true SELECT 2 &gt; ‘1.1’; true SELECT to_date(‘2009-07-30 04:17:52’) &gt; to_date(‘2009-07-30 04:17:52’); false SELECT to_date(‘2009-07-30 04:17:52’) &gt; to_date(‘2009-08-01 04:17:52’); false SELECT 1 &gt; NULL; NULL =expr1 &gt;= expr2 - 如果 expr1 大于等于 expr2 则返回 true。 参数:expr1, expr2 - 比较的两个参数类型必须一致，或者可以转换成一样的类型，而且这个类型支持排序。比如 map 类型就是不支持比较的，所以这个操作符不支持 map 类型的参数。例子： SELECT 2 &gt;= 1; true SELECT 2.0 &gt;= ‘2.1’; false SELECT to_date(‘2009-07-30 04:17:52’) &gt;= to_date(‘2009-07-30 04:17:52’); true SELECT to_date(‘2009-07-30 04:17:52’) &gt;= to_date(‘2009-08-01 04:17:52’); false SELECT 1 &gt;= NULL; NULL ^expr1 ^ expr2 - 返回 expr1 和 expr2 的按位异或的结果。 例子： SELECT 3 ^ 5; 2 absabs(expr) - 返回数值的绝对值。例子： SELECT abs(-1); 1 acosacos(expr) - 如果 -1 &lt;= expr &lt;= 1，则返回 expr 的反余弦，否则返回 NaN。例子： SELECT acos(1); 0.0 SELECT acos(2); NaN add_monthsadd_months(start_date, num_months) 例子： SELECT add_months(‘2016-08-31’, 1); 2016-09-30 Since: 1.5.0 andexpr1 and expr2 - 逻辑 AND. approx_count_distinctapprox_count_distinct(expr[, relativeSD]) - 通过 HyperLogLog ++ 返回估计的基数. relativeSD 定义允许的最大估计误差。 approx_percentileapprox_percentile(col, percentage [, accuracy]) - 返回给定百分比处数值列 col 的近似百分位数值。百分比的值必须是 0.0 到 1.0 之间。 例子: SELECT approx_percentile(10.0, array(0.5, 0.4, 0.1), 100); [10.0,10.0,10.0] SELECT approx_percentile(10.0, 0.5, 100); 10.0 arrayarray(expr, …) - 返回给定值组成的数组。 例子: SELECT array(1, 2, 3); [1,2,3] array_containsarray_contains(array, value) - 如果数组包含了 value，则返回 true。 例子: SELECT array_contains(array(1, 2, 3), 2); true asciiascii(str) - 返回 str 的第一个字符的 ascii 数值。 例子: SELECT ascii(‘222’); 50 SELECT ascii(2); 50 asinasin(expr) - 如果 -1 &lt;= expr &lt;= 1，则返回 expr 的反正弦，否则返回 NaN。 例子: SELECT asin(0); 0.0 SELECT asin(2); NaN assert_trueassert_true(expr) - 如果 expr 表达式的返回值不是 true 则抛出异常。 例子: SELECT assert_true(0 &lt; 1); NULL atanatan(expr) - 返回 expr 的反正切。 例子: SELECT atan(0); 0.0 atan2atan2(expr1, expr2) - 返回平面的正 x 轴与由坐标（expr1，expr2）点之间的弧度角度。 例子: SELECT atan2(0, 0); 0.0 avgavg(expr) - 返回 expr 表达式的平均值。 base64base64(bin) - 将参数从二进制文件转换为 base64 的字符串。 例子: SELECT base64(‘Spark SQL’); U3BhcmsgU1FM bigintbigint(expr) - 将值 expr 转换为 bigint 数据类型。 binbin(expr) - 返回 long 类型的参数 expr 的二进制字符串表示形式。 例子: SELECT bin(13); 1101 SELECT bin(-13); 1111111111111111111111111111111111111111111111111111111111110011 SELECT bin(13.3); 1101 binarybinary(expr) - 将值 expr 转换为 binary 数据类型。 bit_lengthbit_length(expr) - 返回字符串数据的位长度或二进制数据的位数。 例子: SELECT bit_length(‘Spark SQL’); 72 booleanboolean(expr) - 将值 expr 转换为 boolean 数据类型。 broundbround(expr, d) - 使用 HALF_EVEN 舍入模式返回 expr 四舍五入至 d 位小数点的数据。 例子: SELECT bround(2.5, 0); 2.0 castcast(expr AS type) - 将 expr 转换成 type 类型的数据。 例子: SELECT cast(‘10’ as int); 10 cbrtcbrt(expr) - 返回 expr 的立方根。 例子: SELECT cbrt(27.0); 3.0 ceilceil(expr) - 返回不小于 expr 的最小整数。 例子: SELECT ceil(-0.1); 0 SELECT ceil(5); 5 ceilingceiling(expr) - 返回不小于 expr 的最小整数。 例子: SELECT ceiling(-0.1); 0 SELECT ceiling(5); 5 charchar(expr) - 返回二进制等效于 expr 的 ASCII 字符。 如果 n 大于256，则结果等于 chr(n%256) 例子: SELECT char(65); A char_lengthchar_length(expr) - 返回字符串数据的字符长度或二进制数据的字节数。 字符串数据的长度包括尾随空格，二进制数据的长度包括二进制零。 例子: SELECT char_length(‘Spark SQL ‘); 10 SELECT CHAR_LENGTH(‘Spark SQL ‘); 10 SELECT CHARACTER_LENGTH(‘Spark SQL ‘); 10 character_lengthcharacter_length(expr) - 返回字符串数据的字符长度或二进制数据的字节数。 字符串数据的长度包括尾随空格，二进制数据的长度包括二进制零。 例子: SELECT character_length(‘Spark SQL ‘); 10 SELECT CHAR_LENGTH(‘Spark SQL ‘); 10 SELECT CHARACTER_LENGTH(‘Spark SQL ‘); 10 chrchr(expr) - 返回二进制等效于 expr 的 ASCII 字符。 如果 n 大于256，则结果等于 chr(n%256) 例子: SELECT chr(65); A coalescecoalesce(expr1, expr2, …) - 返回第一个非空参数（如果存在）。 否则，返回 null。 例子: SELECT coalesce(NULL, 1, NULL); 1 collect_listcollect_list(expr) - 收集并返回非唯一元素列表。 collect_setcollect_set(expr) - 收集并返回唯一元素列表。 concatconcat(str1, str2, …, strN) - 返回由 str1, str2, …, strN 组成的字符串。 例子: SELECT concat(‘Spark’, ‘SQL’); SparkSQL concat_wsconcat_ws(sep, [str | array(str)]+) - 返回由 sep 分隔组成的字符串连接。 例子: SELECT concat_ws(‘ ‘, ‘Spark’, ‘SQL’); Spark SQL convconv(num, from_base, to_base) - 将 num 从 from_base 进制转换为 to_base 进制。 例子: SELECT conv(‘100’, 2, 10); 4 SELECT conv(-10, 16, -10); -16 corrcorr(expr1, expr2) - Returns Pearson coefficient of correlation between a set of number pairs. coscos(expr) - 返回 expr 的余弦。 例子: SELECT cos(0); 1.0 coshcosh(expr) - 返回 expr 的双曲余弦。 例子: SELECT cosh(0); 1.0 cotcot(expr) - 返回 expr 的余切值。 例子: SELECT cot(1); 0.6420926159343306 countcount(*) - Returns the total number of retrieved rows, including rows containing null. count(expr) - Returns the number of rows for which the supplied expression is non-null. count(DISTINCT expr[, expr…]) - Returns the number of rows for which the supplied expression(s) are unique and non-null. count_min_sketchcount_min_sketch(col, eps, confidence, seed) - Returns a count-min sketch of a column with the given esp,confidence and seed. The result is an array of bytes, which can be deserialized to aCountMinSketch before usage. Count-min sketch is a probabilistic data structure used forcardinality estimation using sub-linear space. covar_popcovar_pop(expr1, expr2) - Returns the population covariance of a set of number pairs. covar_sampcovar_samp(expr1, expr2) - Returns the sample covariance of a set of number pairs. crc32crc32(expr) - Returns a cyclic redundancy check value of the expr as a bigint. Examples: SELECT crc32(‘Spark’); 1557323817 cubecume_distcume_dist() - Computes the position of a value relative to all values in the partition. current_databasecurrent_database() - Returns the current database. Examples: SELECT current_database(); default current_datecurrent_date() - Returns the current date at the start of query evaluation. Since: 1.5.0 current_timestampcurrent_timestamp() - Returns the current timestamp at the start of query evaluation. Since: 1.5.0 datedate(expr) - Casts the value expr to the target data type date. date_adddate_add(start_date, num_days) - Returns the date that is num_days after start_date. Examples: SELECT date_add(‘2016-07-30’, 1); 2016-07-31 Since: 1.5.0 date_formatdate_format(timestamp, fmt) - Converts timestamp to a value of string in the format specified by the date format fmt. Examples: SELECT date_format(‘2016-04-08’, ‘y’); 2016 Since: 1.5.0 date_subdate_sub(start_date, num_days) - Returns the date that is num_days before start_date. Examples: SELECT date_sub(‘2016-07-30’, 1); 2016-07-29 Since: 1.5.0 date_truncdate_trunc(fmt, ts) - Returns timestamp ts truncated to the unit specified by the format model fmt.fmt should be one of [“YEAR”, “YYYY”, “YY”, “MON”, “MONTH”, “MM”, “DAY”, “DD”, “HOUR”, “MINUTE”, “SECOND”, “WEEK”, “QUARTER”] Examples: SELECT date_trunc(‘YEAR’, ‘2015-03-05T09:32:05.359’); 2015-01-01 00:00:00 SELECT date_trunc(‘MM’, ‘2015-03-05T09:32:05.359’); 2015-03-01 00:00:00 SELECT date_trunc(‘DD’, ‘2015-03-05T09:32:05.359’); 2015-03-05 00:00:00 SELECT date_trunc(‘HOUR’, ‘2015-03-05T09:32:05.359’); 2015-03-05 09:00:00 Since: 2.3.0 datediffdatediff(endDate, startDate) - Returns the number of days from startDate to endDate. Examples: SELECT datediff(‘2009-07-31’, ‘2009-07-30’); 1 SELECT datediff(‘2009-07-30’, ‘2009-07-31’); -1 Since: 1.5.0 dayday(date) - Returns the day of month of the date/timestamp. Examples: SELECT day(‘2009-07-30’); 30 Since: 1.5.0 dayofmonthdayofmonth(date) - Returns the day of month of the date/timestamp. Examples: SELECT dayofmonth(‘2009-07-30’); 30 Since: 1.5.0 dayofweekdayofweek(date) - Returns the day of the week for date/timestamp (1 = Sunday, 2 = Monday, …, 7 = Saturday). Examples: SELECT dayofweek(‘2009-07-30’); 5 Since: 2.3.0 dayofyeardayofyear(date) - Returns the day of year of the date/timestamp. Examples: SELECT dayofyear(‘2016-04-09’); 100 Since: 1.5.0 decimaldecimal(expr) - Casts the value expr to the target data type decimal. decodedecode(bin, charset) - Decodes the first argument using the second argument character set. Examples: SELECT decode(encode(‘abc’, ‘utf-8’), ‘utf-8’); abc degreesdegrees(expr) - Converts radians to degrees. Arguments: expr - angle in radiansExamples: SELECT degrees(3.141592653589793); 180.0 dense_rankdense_rank() - Computes the rank of a value in a group of values. The result is one plus thepreviously assigned rank value. Unlike the function rank, dense_rank will not produce gapsin the ranking sequence. doubledouble(expr) - Casts the value expr to the target data type double. ee() - Returns Euler’s number, e. Examples: SELECT e(); 2.718281828459045 eltelt(n, input1, input2, …) - Returns the n-th input, e.g., returns input2 when n is 2. Examples: SELECT elt(1, ‘scala’, ‘java’); scala encodeencode(str, charset) - Encodes the first argument using the second argument character set. Examples: SELECT encode(‘abc’, ‘utf-8’); abc expexp(expr) - Returns e to the power of expr. Examples: SELECT exp(0); 1.0 explodeexplode(expr) - Separates the elements of array expr into multiple rows, or the elements of map expr into multiple rows and columns. Examples: SELECT explode(array(10, 20)); 10 20 explode_outerexplode_outer(expr) - Separates the elements of array expr into multiple rows, or the elements of map exprinto multiple rows and columns. Examples: SELECT explode_outer(array(10, 20)); 10 20 expm1expm1(expr) - Returns exp(expr) - 1. Examples: SELECT expm1(0); 0.0 factorialfactorial(expr) - Returns the factorial of expr. expr is [0..20]. Otherwise, null. Examples: SELECT factorial(5); 120 find_in_setfind_in_set(str, str_array) - Returns the index (1-based) of the given string (str) in the comma-delimited list (str_array).Returns 0, if the string was not found or if the given string (str) contains a comma. Examples: SELECT find_in_set(‘ab’,’abc,b,ab,c,def’); 3 firstfirst(expr[, isIgnoreNull]) - Returns the first value of expr for a group of rows.If isIgnoreNull is true, returns only non-null values. first_valuefirst_value(expr[, isIgnoreNull]) - Returns the first value of expr for a group of rows.If isIgnoreNull is true, returns only non-null values. floatfloat(expr) - Casts the value expr to the target data type float. floorfloor(expr) - Returns the largest integer not greater than expr. Examples: SELECT floor(-0.1); -1 SELECT floor(5); 5 format_numberformat_number(expr1, expr2) - Formats the number expr1 like ‘#,###,###.##’, rounded to expr2decimal places. If expr2 is 0, the result has no decimal point or fractional part.This is supposed to function like MySQL’s FORMAT. Examples: SELECT format_number(12332.123456, 4); 12,332.1235 format_stringformat_string(strfmt, obj, …) - Returns a formatted string from printf-style format strings. Examples: SELECT format_string(“Hello World %d %s”, 100, “days”); Hello World 100 days from_jsonfrom_json(jsonStr, schema[, options]) - Returns a struct value with the given jsonStr and schema. Examples: SELECT from_json(‘{“a”:1, “b”:0.8}’, ‘a INT, b DOUBLE’); {“a”:1, “b”:0.8} SELECT from_json(‘{“time”:”26/08/2015”}’, ‘time Timestamp’, map(‘timestampFormat’, ‘dd/MM/yyyy’)); {“time”:”2015-08-26 00:00:00.0”} Since: 2.2.0 from_unixtimefrom_unixtime(unix_time, format) - Returns unix_time in the specified format. Examples: SELECT from_unixtime(0, ‘yyyy-MM-dd HH:mm:ss’); 1970-01-01 00:00:00 Since: 1.5.0 from_utc_timestampfrom_utc_timestamp(timestamp, timezone) - Given a timestamp like ‘2017-07-14 02:40:00.0’, interprets it as a time in UTC, and renders that time as a timestamp in the given time zone. For example, ‘GMT+1’ would yield ‘2017-07-14 03:40:00.0’. Examples: SELECT from_utc_timestamp(‘2016-08-31’, ‘Asia/Seoul’); 2016-08-31 09:00:00 Since: 1.5.0 get_json_objectget_json_object(json_txt, path) - Extracts a json object from path. Examples: SELECT get_json_object(‘{“a”:”b”}’, ‘$.a’); b greatestgreatest(expr, …) - Returns the greatest value of all parameters, skipping null values. Examples: SELECT greatest(10, 9, 2, 4, 3); 10 groupinggrouping_idhashhash(expr1, expr2, …) - Returns a hash value of the arguments. Examples: SELECT hash(‘Spark’, array(123), 2); -1321691492 hexhex(expr) - Converts expr to hexadecimal. Examples: SELECT hex(17); 11 SELECT hex(‘Spark SQL’); 537061726B2053514C hourhour(timestamp) - Returns the hour component of the string/timestamp. Examples: SELECT hour(‘2009-07-30 12:58:59’); 12 Since: 1.5.0 hypothypot(expr1, expr2) - Returns sqrt(expr12 + expr22). Examples: SELECT hypot(3, 4); 5.0 ifif(expr1, expr2, expr3) - If expr1 evaluates to true, then returns expr2; otherwise returns expr3. Examples: SELECT if(1 &lt; 2, ‘a’, ‘b’); a ifnullifnull(expr1, expr2) - Returns expr2 if expr1 is null, or expr1 otherwise. Examples: SELECT ifnull(NULL, array(‘2’)); [“2”] inexpr1 in(expr2, expr3, …) - Returns true if expr equals to any valN. Arguments: expr1, expr2, expr3, … - the arguments must be same type.Examples: SELECT 1 in(1, 2, 3); true SELECT 1 in(2, 3, 4); false SELECT named_struct(‘a’, 1, ‘b’, 2) in(named_struct(‘a’, 1, ‘b’, 1), named_struct(‘a’, 1, ‘b’, 3)); false SELECT named_struct(‘a’, 1, ‘b’, 2) in(named_struct(‘a’, 1, ‘b’, 2), named_struct(‘a’, 1, ‘b’, 3)); true initcapinitcap(str) - Returns str with the first letter of each word in uppercase.All other letters are in lowercase. Words are delimited by white space. Examples: SELECT initcap(‘sPark sql’); Spark Sql inlineinline(expr) - Explodes an array of structs into a table. Examples: SELECT inline(array(struct(1, ‘a’), struct(2, ‘b’))); 1 a 2 b inline_outerinline_outer(expr) - Explodes an array of structs into a table. Examples: SELECT inline_outer(array(struct(1, ‘a’), struct(2, ‘b’))); 1 a 2 b input_file_block_lengthinput_file_block_length() - Returns the length of the block being read, or -1 if not available. input_file_block_startinput_file_block_start() - Returns the start offset of the block being read, or -1 if not available. input_file_nameinput_file_name() - Returns the name of the file being read, or empty string if not available. instrinstr(str, substr) - Returns the (1-based) index of the first occurrence of substr in str. Examples: SELECT instr(‘SparkSQL’, ‘SQL’); 6 intint(expr) - Casts the value expr to the target data type int. isnanisnan(expr) - Returns true if expr is NaN, or false otherwise. Examples: SELECT isnan(cast(‘NaN’ as double)); true isnotnullisnotnull(expr) - Returns true if expr is not null, or false otherwise. Examples: SELECT isnotnull(1); true isnullisnull(expr) - Returns true if expr is null, or false otherwise. Examples: SELECT isnull(1); false java_methodjava_method(class, method[, arg1[, arg2 ..]]) - Calls a method with reflection. Examples: SELECT java_method(‘java.util.UUID’, ‘randomUUID’); c33fb387-8500-4bfa-81d2-6e0e3e930df2 SELECT java_method(‘java.util.UUID’, ‘fromString’, ‘a5cf6c42-0c85-418f-af6c-3e4e5b1328f2’); a5cf6c42-0c85-418f-af6c-3e4e5b1328f2 json_tuplejson_tuple(jsonStr, p1, p2, …, pn) - Returns a tuple like the function get_json_object, but it takes multiple names. All the input parameters and output column types are string. Examples: SELECT json_tuple(‘{“a”:1, “b”:2}’, ‘a’, ‘b’); 1 2 kurtosiskurtosis(expr) - Returns the kurtosis value calculated from values of a group. laglag(input[, offset[, default]]) - Returns the value of input at the offsetth rowbefore the current row in the window. The default value of offset is 1 and the defaultvalue of default is null. If the value of input at the offsetth row is null,null is returned. If there is no such offset row (e.g., when the offset is 1, the firstrow of the window does not have any previous row), default is returned. lastlast(expr[, isIgnoreNull]) - Returns the last value of expr for a group of rows.If isIgnoreNull is true, returns only non-null values. last_daylast_day(date) - Returns the last day of the month which the date belongs to. Examples: SELECT last_day(‘2009-01-12’); 2009-01-31 Since: 1.5.0 last_valuelast_value(expr[, isIgnoreNull]) - Returns the last value of expr for a group of rows.If isIgnoreNull is true, returns only non-null values. lcaselcase(str) - Returns str with all characters changed to lowercase. Examples: SELECT lcase(‘SparkSql’); sparksql leadlead(input[, offset[, default]]) - Returns the value of input at the offsetth rowafter the current row in the window. The default value of offset is 1 and the defaultvalue of default is null. If the value of input at the offsetth row is null,null is returned. If there is no such an offset row (e.g., when the offset is 1, the lastrow of the window does not have any subsequent row), default is returned. leastleast(expr, …) - Returns the least value of all parameters, skipping null values. Examples: SELECT least(10, 9, 2, 4, 3); 2 leftleft(str, len) - Returns the leftmost len(len can be string type) characters from the string str,if len is less or equal than 0 the result is an empty string. Examples: SELECT left(‘Spark SQL’, 3); Spa lengthlength(expr) - Returns the character length of string data or number of bytes of binary data. The length of string data includes the trailing spaces. The length of binary data includes binary zeros. Examples: SELECT length(‘Spark SQL ‘); 10 SELECT CHAR_LENGTH(‘Spark SQL ‘); 10 SELECT CHARACTER_LENGTH(‘Spark SQL ‘); 10 levenshteinlevenshtein(str1, str2) - Returns the Levenshtein distance between the two given strings. Examples: SELECT levenshtein(‘kitten’, ‘sitting’); 3 likestr like pattern - Returns true if str matches pattern, null if any arguments are null, false otherwise. Arguments: str - a string expressionpattern - a string expression. The pattern is a string which is matched literally, withexception to the following special symbols: _ matches any one character in the input (similar to . in posix regular expressions) % matches zero or more characters in the input (similar to .* in posix regularexpressions) The escape character is ‘\’. If an escape character precedes a special symbol or anotherescape character, the following character is matched literally. It is invalid to escapeany other character. Since Spark 2.0, string literals are unescaped in our SQL parser. For example, in orderto match “\abc”, the pattern should be “\abc”. When SQL config ‘spark.sql.parser.escapedStringLiterals’ is enabled, it fallbacksto Spark 1.6 behavior regarding string literal parsing. For example, if the config isenabled, the pattern to match “\abc” should be “\abc”. Examples: SELECT ‘%SystemDrive%\Users\John’ like ‘\%SystemDrive\%\Users%’ true Note: Use RLIKE to match with standard regular expressions. lnln(expr) - Returns the natural logarithm (base e) of expr. Examples: SELECT ln(1); 0.0 locatelocate(substr, str[, pos]) - Returns the position of the first occurrence of substr in str after position pos.The given pos and return value are 1-based. Examples: SELECT locate(‘bar’, ‘foobarbar’); 4 SELECT locate(‘bar’, ‘foobarbar’, 5); 7 SELECT POSITION(‘bar’ IN ‘foobarbar’); 4 loglog(base, expr) - Returns the logarithm of expr with base. Examples: SELECT log(10, 100); 2.0 log10log10(expr) - Returns the logarithm of expr with base 10. Examples: SELECT log10(10); 1.0 log1plog1p(expr) - Returns log(1 + expr). Examples: SELECT log1p(0); 0.0 log2log2(expr) - Returns the logarithm of expr with base 2. Examples: SELECT log2(2); 1.0 lowerlower(str) - Returns str with all characters changed to lowercase. Examples: SELECT lower(‘SparkSql’); sparksql lpadlpad(str, len, pad) - Returns str, left-padded with pad to a length of len.If str is longer than len, the return value is shortened to len characters. Examples: SELECT lpad(‘hi’, 5, ‘??’); ???hi SELECT lpad(‘hi’, 1, ‘??’); h ltrimltrim(str) - Removes the leading space characters from str. ltrim(trimStr, str) - Removes the leading string contains the characters from the trim string Arguments: str - a string expressiontrimStr - the trim string characters to trim, the default value is a single spaceExamples: SELECT ltrim(‘ SparkSQL ‘); SparkSQL SELECT ltrim(‘Sp’, ‘SSparkSQLS’); arkSQLS mapmap(key0, value0, key1, value1, …) - Creates a map with the given key/value pairs. Examples: SELECT map(1.0, ‘2’, 3.0, ‘4’); {1.0:”2”,3.0:”4”} map_keysmap_keys(map) - Returns an unordered array containing the keys of the map. Examples: SELECT map_keys(map(1, ‘a’, 2, ‘b’)); [1,2] map_valuesmap_values(map) - Returns an unordered array containing the values of the map. Examples: SELECT map_values(map(1, ‘a’, 2, ‘b’)); [“a”,”b”] maxmax(expr) - Returns the maximum value of expr. md5md5(expr) - Returns an MD5 128-bit checksum as a hex string of expr. Examples: SELECT md5(‘Spark’); 8cde774d6f7333752ed72cacddb05126 meanmean(expr) - Returns the mean calculated from values of a group. minmin(expr) - Returns the minimum value of expr. minuteminute(timestamp) - Returns the minute component of the string/timestamp. Examples: SELECT minute(‘2009-07-30 12:58:59’); 58 Since: 1.5.0 modexpr1 mod expr2 - Returns the remainder after expr1/expr2. Examples: SELECT 2 mod 1.8; 0.2 SELECT MOD(2, 1.8); 0.2 monotonically_increasing_idmonotonically_increasing_id() - Returns monotonically increasing 64-bit integers. The generated ID is guaranteedto be monotonically increasing and unique, but not consecutive. The current implementationputs the partition ID in the upper 31 bits, and the lower 33 bits represent the record numberwithin each partition. The assumption is that the data frame has less than 1 billionpartitions, and each partition has less than 8 billion records. monthmonth(date) - Returns the month component of the date/timestamp. Examples: SELECT month(‘2016-07-30’); 7 Since: 1.5.0 months_betweenmonths_between(timestamp1, timestamp2) - Returns number of months between timestamp1 and timestamp2. Examples: SELECT months_between(‘1997-02-28 10:30:00’, ‘1996-10-30’); 3.94959677 Since: 1.5.0 named_structnamed_struct(name1, val1, name2, val2, …) - Creates a struct with the given field names and values. Examples: SELECT named_struct(“a”, 1, “b”, 2, “c”, 3); {“a”:1,”b”:2,”c”:3} nanvlnanvl(expr1, expr2) - Returns expr1 if it’s not NaN, or expr2 otherwise. Examples: SELECT nanvl(cast(‘NaN’ as double), 123); 123.0 negativenegative(expr) - Returns the negated value of expr. Examples: SELECT negative(1); -1 next_daynext_day(start_date, day_of_week) - Returns the first date which is later than start_date and named as indicated. Examples: SELECT next_day(‘2015-01-14’, ‘TU’); 2015-01-20 Since: 1.5.0 notnot expr - Logical not. nownow() - Returns the current timestamp at the start of query evaluation. Since: 1.5.0 ntilentile(n) - Divides the rows for each window partition into n buckets rangingfrom 1 to at most n. nullifnullif(expr1, expr2) - Returns null if expr1 equals to expr2, or expr1 otherwise. Examples: SELECT nullif(2, 2); NULL nvlnvl(expr1, expr2) - Returns expr2 if expr1 is null, or expr1 otherwise. Examples: SELECT nvl(NULL, array(‘2’)); [“2”] nvl2nvl2(expr1, expr2, expr3) - Returns expr2 if expr1 is not null, or expr3 otherwise. Examples: SELECT nvl2(NULL, 2, 1); 1 octet_lengthoctet_length(expr) - Returns the byte length of string data or number of bytes of binary data. Examples: SELECT octet_length(‘Spark SQL’); 9 orexpr1 or expr2 - Logical OR. parse_urlparse_url(url, partToExtract[, key]) - Extracts a part from a URL. Examples: SELECT parse_url(‘http://spark.apache.org/path?query=1&#39;, ‘HOST’) spark.apache.org SELECT parse_url(‘http://spark.apache.org/path?query=1&#39;, ‘QUERY’) query=1 SELECT parse_url(‘http://spark.apache.org/path?query=1&#39;, ‘QUERY’, ‘query’) 1 percent_rankpercent_rank() - Computes the percentage ranking of a value in a group of values. percentilepercentile(col, percentage [, frequency]) - Returns the exact percentile value of numeric columncol at the given percentage. The value of percentage must be between 0.0 and 1.0. Thevalue of frequency should be positive integral percentile(col, array(percentage1 [, percentage2]…) [, frequency]) - Returns the exactpercentile value array of numeric column col at the given percentage(s). Each valueof the percentage array must be between 0.0 and 1.0. The value of frequency should bepositive integral percentile_approxpercentile_approx(col, percentage [, accuracy]) - Returns the approximate percentile value of numericcolumn col at the given percentage. The value of percentage must be between 0.0and 1.0. The accuracy parameter (default: 10000) is a positive numeric literal whichcontrols approximation accuracy at the cost of memory. Higher value of accuracy yieldsbetter accuracy, 1.0/accuracy is the relative error of the approximation.When percentage is an array, each value of the percentage array must be between 0.0 and 1.0.In this case, returns the approximate percentile array of column col at the givenpercentage array. Examples: SELECT percentile_approx(10.0, array(0.5, 0.4, 0.1), 100); [10.0,10.0,10.0] SELECT percentile_approx(10.0, 0.5, 100); 10.0 pipi() - Returns pi. Examples: SELECT pi(); 3.141592653589793 pmodpmod(expr1, expr2) - Returns the positive value of expr1 mod expr2. Examples: SELECT pmod(10, 3); 1 SELECT pmod(-10, 3); 2 posexplodeposexplode(expr) - Separates the elements of array expr into multiple rows with positions, or the elements of map expr into multiple rows and columns with positions. Examples: SELECT posexplode(array(10,20)); 0 10 1 20 posexplode_outerposexplode_outer(expr) - Separates the elements of array expr into multiple rows with positions, or the elements of map expr into multiple rows and columns with positions. Examples: SELECT posexplode_outer(array(10,20)); 0 10 1 20 positionposition(substr, str[, pos]) - Returns the position of the first occurrence of substr in str after position pos.The given pos and return value are 1-based. Examples: SELECT position(‘bar’, ‘foobarbar’); 4 SELECT position(‘bar’, ‘foobarbar’, 5); 7 SELECT POSITION(‘bar’ IN ‘foobarbar’); 4 positivepositive(expr) - Returns the value of expr. powpow(expr1, expr2) - Raises expr1 to the power of expr2. Examples: SELECT pow(2, 3); 8.0 powerpower(expr1, expr2) - Raises expr1 to the power of expr2. Examples: SELECT power(2, 3); 8.0 printfprintf(strfmt, obj, …) - Returns a formatted string from printf-style format strings. Examples: SELECT printf(“Hello World %d %s”, 100, “days”); Hello World 100 days quarterquarter(date) - Returns the quarter of the year for date, in the range 1 to 4. Examples: SELECT quarter(‘2016-08-31’); 3 Since: 1.5.0 radiansradians(expr) - Converts degrees to radians. Arguments: expr - angle in degreesExamples: SELECT radians(180); 3.141592653589793 randrand([seed]) - Returns a random value with independent and identically distributed (i.i.d.) uniformly distributed values in [0, 1). Examples: SELECT rand(); 0.9629742951434543 SELECT rand(0); 0.8446490682263027 SELECT rand(null); 0.8446490682263027 randnrandn([seed]) - Returns a random value with independent and identically distributed (i.i.d.) values drawn from the standard normal distribution. Examples: SELECT randn(); -0.3254147983080288 SELECT randn(0); 1.1164209726833079 SELECT randn(null); 1.1164209726833079 rankrank() - Computes the rank of a value in a group of values. The result is one plus the numberof rows preceding or equal to the current row in the ordering of the partition. The valueswill produce gaps in the sequence. reflectreflect(class, method[, arg1[, arg2 ..]]) - Calls a method with reflection. Examples: SELECT reflect(‘java.util.UUID’, ‘randomUUID’); c33fb387-8500-4bfa-81d2-6e0e3e930df2 SELECT reflect(‘java.util.UUID’, ‘fromString’, ‘a5cf6c42-0c85-418f-af6c-3e4e5b1328f2’); a5cf6c42-0c85-418f-af6c-3e4e5b1328f2 regexp_extractregexp_extract(str, regexp[, idx]) - Extracts a group that matches regexp. Examples: SELECT regexp_extract(‘100-200’, ‘(\d+)-(\d+)’, 1); 100 regexp_replaceregexp_replace(str, regexp, rep) - Replaces all substrings of str that match regexp with rep. Examples: SELECT regexp_replace(‘100-200’, ‘(\d+)’, ‘num’); num-num repeatrepeat(str, n) - Returns the string which repeats the given string value n times. Examples: SELECT repeat(‘123’, 2); 123123 replacereplace(str, search[, replace]) - Replaces all occurrences of search with replace. Arguments: str - a string expressionsearch - a string expression. If search is not found in str, str is returned unchanged.replace - a string expression. If replace is not specified or is an empty string, nothing replacesthe string that is removed from str.Examples: SELECT replace(‘ABCabc’, ‘abc’, ‘DEF’); ABCDEF reversereverse(str) - Returns the reversed given string. Examples: SELECT reverse(‘Spark SQL’); LQS krapS rightright(str, len) - Returns the rightmost len(len can be string type) characters from the string str,if len is less or equal than 0 the result is an empty string. Examples: SELECT right(‘Spark SQL’, 3); SQL rintrint(expr) - Returns the double value that is closest in value to the argument and is equal to a mathematical integer. Examples: SELECT rint(12.3456); 12.0 rlikestr rlike regexp - Returns true if str matches regexp, or false otherwise. Arguments: str - a string expressionregexp - a string expression. The pattern string should be a Java regular expression. Since Spark 2.0, string literals (including regex patterns) are unescaped in our SQLparser. For example, to match “\abc”, a regular expression for regexp can be“^\abc$”. There is a SQL config ‘spark.sql.parser.escapedStringLiterals’ that can be used tofallback to the Spark 1.6 behavior regarding string literal parsing. For example,if the config is enabled, the regexp that can match “\abc” is “^\abc$”. Examples: When spark.sql.parser.escapedStringLiterals is disabled (default). SELECT ‘%SystemDrive%\Users\John’ rlike ‘%SystemDrive%\Users.*’ true When spark.sql.parser.escapedStringLiterals is enabled. SELECT ‘%SystemDrive%\Users\John’ rlike ‘%SystemDrive%\Users.*’ true Note: Use LIKE to match with simple string pattern. rolluproundround(expr, d) - Returns expr rounded to d decimal places using HALF_UP rounding mode. Examples: SELECT round(2.5, 0); 3.0 row_numberrow_number() - Assigns a unique, sequential number to each row, starting with one,according to the ordering of rows within the window partition. rpadrpad(str, len, pad) - Returns str, right-padded with pad to a length of len.If str is longer than len, the return value is shortened to len characters. Examples: SELECT rpad(‘hi’, 5, ‘??’); hi??? SELECT rpad(‘hi’, 1, ‘??’); h rtrimrtrim(str) - Removes the trailing space characters from str. rtrim(trimStr, str) - Removes the trailing string which contains the characters from the trim string from the str Arguments: str - a string expressiontrimStr - the trim string characters to trim, the default value is a single spaceExamples: SELECT rtrim(‘ SparkSQL ‘); SparkSQL SELECT rtrim(‘LQSa’, ‘SSparkSQLS’); SSpark secondsecond(timestamp) - Returns the second component of the string/timestamp. Examples: SELECT second(‘2009-07-30 12:58:59’); 59 Since: 1.5.0 sentencessentences(str[, lang, country]) - Splits str into an array of array of words. Examples: SELECT sentences(‘Hi there! Good morning.’); [[“Hi”,”there”],[“Good”,”morning”]] shasha(expr) - Returns a sha1 hash value as a hex string of the expr. Examples: SELECT sha(‘Spark’); 85f5955f4b27a9a4c2aab6ffe5d7189fc298b92c sha1sha1(expr) - Returns a sha1 hash value as a hex string of the expr. Examples: SELECT sha1(‘Spark’); 85f5955f4b27a9a4c2aab6ffe5d7189fc298b92c sha2sha2(expr, bitLength) - Returns a checksum of SHA-2 family as a hex string of expr.SHA-224, SHA-256, SHA-384, and SHA-512 are supported. Bit length of 0 is equivalent to 256. Examples: SELECT sha2(‘Spark’, 256); 529bc3b07127ecb7e53a4dcf1991d9152c24537d919178022b2c42657f79a26b shiftleftshiftleft(base, expr) - Bitwise left shift. Examples: SELECT shiftleft(2, 1); 4 shiftrightshiftright(base, expr) - Bitwise (signed) right shift. Examples: SELECT shiftright(4, 1); 2 shiftrightunsignedshiftrightunsigned(base, expr) - Bitwise unsigned right shift. Examples: SELECT shiftrightunsigned(4, 1); 2 signsign(expr) - Returns -1.0, 0.0 or 1.0 as expr is negative, 0 or positive. Examples: SELECT sign(40); 1.0 signumsignum(expr) - Returns -1.0, 0.0 or 1.0 as expr is negative, 0 or positive. Examples: SELECT signum(40); 1.0 sinsin(expr) - Returns the sine of expr, as if computed by java.lang.Math.sin. Arguments: expr - angle in radiansExamples: SELECT sin(0); 0.0 sinhsinh(expr) - Returns hyperbolic sine of expr, as if computed by java.lang.Math.sinh. Arguments: expr - hyperbolic angleExamples: SELECT sinh(0); 0.0 sizesize(expr) - Returns the size of an array or a map. Returns -1 if null. Examples: SELECT size(array(‘b’, ‘d’, ‘c’, ‘a’)); 4 skewnessskewness(expr) - Returns the skewness value calculated from values of a group. smallintsmallint(expr) - Casts the value expr to the target data type smallint. sort_arraysort_array(array[, ascendingOrder]) - Sorts the input array in ascending or descending order according to the natural ordering of the array elements. Examples: SELECT sort_array(array(‘b’, ‘d’, ‘c’, ‘a’), true); [“a”,”b”,”c”,”d”] soundexsoundex(str) - Returns Soundex code of the string. Examples: SELECT soundex(‘Miller’); M460 spacespace(n) - Returns a string consisting of n spaces. Examples: SELECT concat(space(2), ‘1’); 1 spark_partition_idspark_partition_id() - Returns the current partition id. splitsplit(str, regex) - Splits str around occurrences that match regex. Examples: SELECT split(‘oneAtwoBthreeC’, ‘[ABC]’); [“one”,”two”,”three”,””] sqrtsqrt(expr) - Returns the square root of expr. Examples: SELECT sqrt(4); 2.0 stackstack(n, expr1, …, exprk) - Separates expr1, …, exprk into n rows. Examples: SELECT stack(2, 1, 2, 3); 1 2 3 NULL stdstd(expr) - Returns the sample standard deviation calculated from values of a group. stddevstddev(expr) - Returns the sample standard deviation calculated from values of a group. stddev_popstddev_pop(expr) - Returns the population standard deviation calculated from values of a group. stddev_sampstddev_samp(expr) - Returns the sample standard deviation calculated from values of a group. str_to_mapstr_to_map(text[, pairDelim[, keyValueDelim]]) - Creates a map after splitting the text into key/value pairs using delimiters. Default delimiters are ‘,’ for pairDelim and ‘:’ for keyValueDelim. Examples: SELECT str_to_map(‘a:1,b:2,c:3’, ‘,’, ‘:’); map(“a”:”1”,”b”:”2”,”c”:”3”) SELECT str_to_map(‘a’); map(“a”:null) stringstring(expr) - Casts the value expr to the target data type string. structstruct(col1, col2, col3, …) - Creates a struct with the given field values. substrsubstr(str, pos[, len]) - Returns the substring of str that starts at pos and is of length len, or the slice of byte array that starts at pos and is of length len. Examples: SELECT substr(‘Spark SQL’, 5); k SQL SELECT substr(‘Spark SQL’, -3); SQL SELECT substr(‘Spark SQL’, 5, 1); k substringsubstring(str, pos[, len]) - Returns the substring of str that starts at pos and is of length len, or the slice of byte array that starts at pos and is of length len. Examples: SELECT substring(‘Spark SQL’, 5); k SQL SELECT substring(‘Spark SQL’, -3); SQL SELECT substring(‘Spark SQL’, 5, 1); k substring_indexsubstring_index(str, delim, count) - Returns the substring from str before count occurrences of the delimiter delim.If count is positive, everything to the left of the final delimiter (counting from theleft) is returned. If count is negative, everything to the right of the final delimiter(counting from the right) is returned. The function substring_index performs a case-sensitive matchwhen searching for delim. Examples: SELECT substring_index(‘www.apache.org&#39;, ‘.’, 2); www.apache sumsum(expr) - Returns the sum calculated from values of a group. tantan(expr) - Returns the tangent of expr, as if computed by java.lang.Math.tan. Arguments: expr - angle in radiansExamples: SELECT tan(0); 0.0 tanhtanh(expr) - Returns the hyperbolic tangent of expr, as if computed byjava.lang.Math.tanh. Arguments: expr - hyperbolic angleExamples: SELECT tanh(0); 0.0 timestamptimestamp(expr) - Casts the value expr to the target data type timestamp. tinyinttinyint(expr) - Casts the value expr to the target data type tinyint. to_dateto_date(date_str[, fmt]) - Parses the date_str expression with the fmt expression toa date. Returns null with invalid input. By default, it follows casting rules to a date ifthe fmt is omitted. Examples: SELECT to_date(‘2009-07-30 04:17:52’); 2009-07-30 SELECT to_date(‘2016-12-31’, ‘yyyy-MM-dd’); 2016-12-31 Since: 1.5.0 to_jsonto_json(expr[, options]) - Returns a json string with a given struct value Examples: SELECT to_json(named_struct(‘a’, 1, ‘b’, 2)); {“a”:1,”b”:2} SELECT to_json(named_struct(‘time’, to_timestamp(‘2015-08-26’, ‘yyyy-MM-dd’)), map(‘timestampFormat’, ‘dd/MM/yyyy’)); {“time”:”26/08/2015”} SELECT to_json(array(named_struct(‘a’, 1, ‘b’, 2)); [{“a”:1,”b”:2}] SELECT to_json(map(‘a’, named_struct(‘b’, 1))); {“a”:{“b”:1}} SELECT to_json(map(named_struct(‘a’, 1),named_struct(‘b’, 2))); {“[1]”:{“b”:2}} SELECT to_json(map(‘a’, 1)); {“a”:1} SELECT to_json(array((map(‘a’, 1)))); [{“a”:1}] Since: 2.2.0 to_timestampto_timestamp(timestamp[, fmt]) - Parses the timestamp expression with the fmt expression toa timestamp. Returns null with invalid input. By default, it follows casting rules toa timestamp if the fmt is omitted. Examples: SELECT to_timestamp(‘2016-12-31 00:12:00’); 2016-12-31 00:12:00 SELECT to_timestamp(‘2016-12-31’, ‘yyyy-MM-dd’); 2016-12-31 00:00:00 Since: 2.2.0 to_unix_timestampto_unix_timestamp(expr[, pattern]) - Returns the UNIX timestamp of the given time. Examples: SELECT to_unix_timestamp(‘2016-04-08’, ‘yyyy-MM-dd’); 1460041200 Since: 1.6.0 to_utc_timestampto_utc_timestamp(timestamp, timezone) - Given a timestamp like ‘2017-07-14 02:40:00.0’, interprets it as a time in the given time zone, and renders that time as a timestamp in UTC. For example, ‘GMT+1’ would yield ‘2017-07-14 01:40:00.0’. Examples: SELECT to_utc_timestamp(‘2016-08-31’, ‘Asia/Seoul’); 2016-08-30 15:00:00 Since: 1.5.0 translatetranslate(input, from, to) - Translates the input string by replacing the characters present in the from string with the corresponding characters in the to string. Examples: SELECT translate(‘AaBbCc’, ‘abc’, ‘123’); A1B2C3 trimtrim(str) - Removes the leading and trailing space characters from str. trim(BOTH trimStr FROM str) - Remove the leading and trailing trimStr characters from str trim(LEADING trimStr FROM str) - Remove the leading trimStr characters from str trim(TRAILING trimStr FROM str) - Remove the trailing trimStr characters from str Arguments: str - a string expressiontrimStr - the trim string characters to trim, the default value is a single spaceBOTH, FROM - these are keywords to specify trimming string characters from both ends ofthe stringLEADING, FROM - these are keywords to specify trimming string characters from the leftend of the stringTRAILING, FROM - these are keywords to specify trimming string characters from the rightend of the stringExamples: SELECT trim(‘ SparkSQL ‘); SparkSQL SELECT trim(‘SL’, ‘SSparkSQLS’); parkSQ SELECT trim(BOTH ‘SL’ FROM ‘SSparkSQLS’); parkSQ SELECT trim(LEADING ‘SL’ FROM ‘SSparkSQLS’); parkSQLS SELECT trim(TRAILING ‘SL’ FROM ‘SSparkSQLS’); SSparkSQ trunctrunc(date, fmt) - Returns date with the time portion of the day truncated to the unit specified by the format model fmt.fmt should be one of [“year”, “yyyy”, “yy”, “mon”, “month”, “mm”] Examples: SELECT trunc(‘2009-02-12’, ‘MM’); 2009-02-01 SELECT trunc(‘2015-10-27’, ‘YEAR’); 2015-01-01 Since: 1.5.0 ucaseucase(str) - Returns str with all characters changed to uppercase. Examples: SELECT ucase(‘SparkSql’); SPARKSQL unbase64unbase64(str) - Converts the argument from a base 64 string str to a binary. Examples: SELECT unbase64(‘U3BhcmsgU1FM’); Spark SQL unhexunhex(expr) - Converts hexadecimal expr to binary. Examples: SELECT decode(unhex(‘537061726B2053514C’), ‘UTF-8’); Spark SQL unix_timestampunix_timestamp([expr[, pattern]]) - Returns the UNIX timestamp of current or specified time. Examples: SELECT unix_timestamp(); 1476884637 SELECT unix_timestamp(‘2016-04-08’, ‘yyyy-MM-dd’); 1460041200 Since: 1.5.0 upperupper(str) - Returns str with all characters changed to uppercase. Examples: SELECT upper(‘SparkSql’); SPARKSQL uuiduuid() - Returns an universally unique identifier (UUID) string. The value is returned as a canonical UUID 36-character string. Examples: SELECT uuid(); 46707d92-02f4-4817-8116-a4c3b23e6266 var_popvar_pop(expr) - Returns the population variance calculated from values of a group. var_sampvar_samp(expr) - Returns the sample variance calculated from values of a group. variancevariance(expr) - Returns the sample variance calculated from values of a group. weekofyearweekofyear(date) - Returns the week of the year of the given date. A week is considered to start on a Monday and week 1 is the first week with &gt;3 days. Examples: SELECT weekofyear(‘2008-02-20’); 8 Since: 1.5.0 whenCASE WHEN expr1 THEN expr2 [WHEN expr3 THEN expr4]* [ELSE expr5] END - When expr1 = true, returns expr2; else when expr3 = true, returns expr4; else returns expr5. Arguments: expr1, expr3 - the branch condition expressions should all be boolean type.expr2, expr4, expr5 - the branch value expressions and else value expression should all besame type or coercible to a common type.Examples: SELECT CASE WHEN 1 &gt; 0 THEN 1 WHEN 2 &gt; 0 THEN 2.0 ELSE 1.2 END; 1 SELECT CASE WHEN 1 &lt; 0 THEN 1 WHEN 2 &gt; 0 THEN 2.0 ELSE 1.2 END; 2 SELECT CASE WHEN 1 &lt; 0 THEN 1 WHEN 2 &lt; 0 THEN 2.0 END; NULL windowxpathxpath(xml, xpath) - Returns a string array of values within the nodes of xml that match the XPath expression. Examples: SELECT xpath(‘b1b2b3c1c2‘,’a/b/text()’); [‘b1’,’b2’,’b3’] xpath_booleanxpath_boolean(xml, xpath) - Returns true if the XPath expression evaluates to true, or if a matching node is found. Examples: SELECT xpath_boolean(‘1‘,’a/b’); true xpath_doublexpath_double(xml, xpath) - Returns a double value, the value zero if no match is found, or NaN if a match is found but the value is non-numeric. Examples: SELECT xpath_double(‘12‘, ‘sum(a/b)’); 3.0 xpath_floatxpath_float(xml, xpath) - Returns a float value, the value zero if no match is found, or NaN if a match is found but the value is non-numeric. Examples: SELECT xpath_float(‘12‘, ‘sum(a/b)’); 3.0 xpath_intxpath_int(xml, xpath) - Returns an integer value, or the value zero if no match is found, or a match is found but the value is non-numeric. Examples: SELECT xpath_int(‘12‘, ‘sum(a/b)’); 3 xpath_longxpath_long(xml, xpath) - Returns a long integer value, or the value zero if no match is found, or a match is found but the value is non-numeric. Examples: SELECT xpath_long(‘12‘, ‘sum(a/b)’); 3 xpath_numberxpath_number(xml, xpath) - Returns a double value, the value zero if no match is found, or NaN if a match is found but the value is non-numeric. Examples: SELECT xpath_number(‘12‘, ‘sum(a/b)’); 3.0 xpath_shortxpath_short(xml, xpath) - Returns a short integer value, or the value zero if no match is found, or a match is found but the value is non-numeric. Examples: SELECT xpath_short(‘12‘, ‘sum(a/b)’); 3 xpath_stringxpath_string(xml, xpath) - Returns the text contents of the first xml node that matches the XPath expression. Examples: SELECT xpath_string(‘bcc‘,’a/c’); cc yearyear(date) - Returns the year component of the date/timestamp. Examples: SELECT year(‘2016-07-30’); 2016 Since: 1.5.0 |expr1 | expr2 - Returns the result of bitwise OR of expr1 and expr2. Examples: SELECT 3 | 5; 7 ~~ expr - Returns the result of bitwise NOT of expr. Examples: SELECT ~ 0; -1]]></content>
      <tags>
        <tag>SparkSql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SparkSql随笔]]></title>
    <url>%2F2020%2F04%2F12%2FSparkSql%E9%9A%8F%E7%AC%94.html</url>
    <content type="text"><![CDATA[一 spark的Rdd,DF,DS的转换及用法1、三者的区别与联系三者发展历程: RDD(spark1.0) ===&gt; DataFrame(spark1.3) ===&gt; DataSet(spark1.6) 大概可以这么说: rdd + 表结构 = df rdd + 表结构 + 数据类型 = ds df + 数据类型 = ds 共性: 1）都是spark中得弹性分布式数据集，轻量级 2）都是惰性机制，延迟计算 3）根据内存情况，自动缓存，加快计算速度 4）都有partition分区概念 5）众多相同得算子：map flatmap 等等 区别： 1）RDD不支持SQL 2）DF每一行都是Row类型，不能直接访问字段，必须解析才行 3）DS每一行是什么类型是不一定的，在自定义了case class之后可以很自由的获 得每一行的信息 4）DataFrame与Dataset均支持spark sql的操作，比如select，group by之类，还 能注册临时表/视窗，进行sql语句操作 5）可以看出，Dataset在需要访问列中的某个字段时是非常方便的，然而，如果要 写一些适配性很强的函数时，如果使用Dataset，行的类型又不确定，可能是 各种case class，无法实现适配，这时候用DataFrame即Dataset[Row]就能比较 好的解决问题。 6) rdd的优缺点: - 优点: 编译时类型安全 编译时就能检查出类型错误 面向对象的编程风格 直接通过类名点的方式来操作数据- 缺点: 序列化和反序列化的性能开销 无论是集群间的通信, 还是IO操作都需要对对象的结构和数据进行序列化和反序列化 GC的性能开销 频繁的创建和销毁对象, 势必会增加GC 7) DF和DS的优缺点 DataFrame引入了schema和off-heap schema : RDD每一行的数据, 结构都是一样的.这个结构就存储在schema中. Spark通过schame就能够读懂数据, 因此在通信和IO时就只需要序列化和反序列化数据,而结构的部分就可以省略了. off-heap : 意味着JVM堆以外的内存,这些内存直接受操作系统管理（而不是JVM）。Spark能够以二进制的形式序列化数据(不包括结构)到off-heap中, 当要操作数据时,就直接操作off-heap内存. 由于Spark理解schema, 所以知道该如何操作 其API不是面向对象的这里我们就可以看出spark为了解决RDD的问题进行的取舍 RDD是分布式的Java对象的集合。DataFrame是分布式的Row对象的集合DataFrame除了提供了比RDD更丰富的算子以外，更重要的特点是提升执行效率、减少数据读取以及执行计划的优化，比如filter下推、裁剪等Dataset和DataFrame拥有完全相同的成员函数，区别只是每一行的数据类型不同DataFrame也可以叫Dataset[Row],每一行的类型是Row，不解析，每一行究竟有哪些字段，各个字段又是什么类型都无从得知，只能用getAS方法或者共性中的第七条提到的模式匹配拿出特定字段而Dataset中，每一行是什么类型是不一定的，在自定义了case class之后可以很自由的获得每一行的信息 2三者的转化1）DF/DS转RDD Val Rdd = DF/DS.rdd 2) DS/RDD转DF import spark.implicits._ 调用 toDF（就是把一行数据封装成row类型） sparkSession.createDataFrame(rdd,schema) toDF报错 可以吧rdd转df 3）RDD转DS 将RDD的每一行封装成样例类，再调用toDS方法 4）DF转DS 根据row字段定义样例类，再调用asDS方法[样例类] 或者df.as[case class] 特别注意： 在使用一些特殊的操作时，一定要加上 import spark.implicits._ 不然toDF、toDS无法使用 3 三者的性能比较计数时 DS&gt;RDD&gt;DF (其他计算不一定 具体看使用的) 123456789101112131415161718192021222324252627282930313233343536package com.huawei.spark.areaRoadFlowimport java.util.UUIDimport org.apache.spark.rdd.RDDimport org.apache.spark.sql.&#123;Dataset, SparkSession&#125;object Test_DF_DS_RDD_Speed &#123; def main(args: Array[String]): Unit = &#123; val spark: SparkSession = SparkSession.builder().appName("无测试").master("local").getOrCreate() spark.sparkContext.setLogLevel("ERROR") val firstRdd: RDD[(String, Int)] = spark.sparkContext.parallelize(0 to 400000).map(num =&gt; &#123; (UUID.randomUUID().toString, num) &#125;) firstRdd firstRdd.cache() val beginTimeRdd: Long = System.currentTimeMillis() firstRdd.map(tp=&gt;&#123;tp._1+"-"+tp._2&#125;).collect() val endTimeRdd: Long = System.currentTimeMillis() import spark.implicits._ val beginTimeDF: Long = System.currentTimeMillis() firstRdd.toDF().map(row=&gt;&#123;row.get(0)+"-"+row.get(1)&#125;).collect() val endTimeDF: Long = System.currentTimeMillis() val beginTimeDS: Long = System.currentTimeMillis() firstRdd.toDS().map(tp=&gt;&#123;tp._1+"-"+tp._2&#125;).collect() val endTimeDS: Long = System.currentTimeMillis() println(s"RDD算子耗时$&#123;endTimeRdd-beginTimeRdd&#125;") println(s"DF算子耗时$&#123;endTimeDF-beginTimeDF&#125;") println(s"DS算子耗时$&#123;endTimeDS-beginTimeDS&#125;") &#125;&#125; 结果: 123RDD算子耗时1782DF算子耗时3071DS算子耗时460 4 sparkSession读取不同的数据文件的返回值1234561 val ds:DataSet[String] =sparkSession.read.textFile(s"path")//可以是压缩文件 jdbc: DataFrame text: DataFrame load: DataFrame csv: DataFrame json,parquet,orc,table:DataFrame 二 SparkSql 解析josn1 常用方法1.1 读取json文件 12sparkSession.read.json(path).show() 直接就可以打印表&#123;json为单层&#125;json为多层的解析 SparkSql 读取配置文件 解析json并把数据按照类型插入hive也可解析来的json每一条的字段个数不一致,如:有的有android有的无,但配置文件 中必须每个都有 1 配置文件12插入数据库的name,json中的name,类型username,u,String 2 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106import java.text.SimpleDateFormatimport java.util.Localeimport com.google.gson.&#123;JsonObject, JsonParser&#125;import org.apache.spark.sql.&#123;DataFrame, Dataset, SparkSession&#125;import scala.collection.JavaConversions._import scala.collection.mutableimport scala.collection.mutable.&#123;ArrayBuffer, ListBuffer, Map&#125;object SparkParseFlowJsonTest2 &#123; val spark = new SparkSession.Builder() .appName("sql") .master("local[3]") .getOrCreate() import spark.implicits._ val mapType: mutable.Map[String, String] = Map() def main(args: Array[String]): Unit = &#123; val CONFIG_MAP = readConfig() val dataset = spark.read.textFile(s"json文件地址") val CONFIG_LIST:List[String] = CONFIG_MAP.keySet.toList //print(CONFIG_MAP) val dataset2 = dataset.map(re =&gt; &#123; val mapJson: mutable.Map[String, String] = handleMessage2CaseClass(re) var arrayData:ArrayBuffer[String] = ArrayBuffer() for ( key &lt;- CONFIG_LIST )&#123; var valueOption = mapJson.get(key) var value = "" if (valueOption != None)&#123; value=valueOption.get if (key.equalsIgnoreCase("_timestamp"))&#123; val formatter = new SimpleDateFormat("dd/MMM/yyyy:hh:mm:ss Z", Locale.ENGLISH) val formatStr = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss"); value = formatStr.format(formatter.parse(value)) &#125; &#125; arrayData += value &#125; arrayData &#125;) spark.udf.register("strToMap", (field: String) =&gt; strToMapUDF(field)) var lb: ListBuffer[String] = ListBuffer() for( i &lt;- 0 to CONFIG_LIST.length-1)&#123; var keyname=CONFIG_MAP.get(CONFIG_LIST.get(i)).get val str = mapType.get(keyname).toString.replace("Some(","").replace(")","") if (str.equalsIgnoreCase("Map"))&#123; lb = lb :+ f"strToMap(value[$i]) as $keyname" &#125;else&#123; lb = lb :+ f"cast(value[$i] as $&#123;str&#125;) as $keyname" &#125; &#125; val frame: DataFrame = dataset2.selectExpr(lb: _*) frame.show() frame.printSchema() &#125; //自定义udf string to map def strToMapUDF(field: String): Map[String,String] = &#123; val mapJson2: mutable.Map[String, String] = Map() val strings = field.split(",") for (i &lt;- 0 until strings.length)&#123; mapJson2.put(strings(i).split(":")(0),strings(i).split(":")(1)) &#125; mapJson2 &#125; //josnToMap def handleMessage2CaseClass(jsonStr: String): Map[String,String] = &#123; val mapJson: mutable.Map[String, String] = Map() val parser = new JsonParser val element = parser.parse(jsonStr) if(element.isJsonObject) &#123; val jsonObject: JsonObject = element.getAsJsonObject() val set = jsonObject.entrySet() val ite= set.iterator() while (ite.hasNext)&#123; val el = ite.next() val key: String = el.getKey val value = el.getValue.toString.replace("\"","") mapJson.put(key,value) &#125; &#125; mapJson &#125; //readConfig def readConfig(): Map[String,String] =&#123; val mapAll: mutable.Map[String, String] = Map() val data: Dataset[String] = spark.read.textFile("配置文件地址") val stringses: Array[Array[String]] = data.map(x =&gt; x.split(",") ).collect() val iterator: Iterator[Array[String]] = stringses.iterator while(iterator.hasNext)&#123; val arr: Array[String] = iterator.next() mapAll.put(arr(1),arr(0)) mapType.put(arr(0),arr(2)) &#125; mapAll &#125;&#125;]]></content>
  </entry>
  <entry>
    <title><![CDATA[基于python的数据分析]]></title>
    <url>%2F2020%2F03%2F15%2F%E5%9F%BA%E4%BA%8Epython%E7%9A%84%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90.html</url>
    <content type="text"><![CDATA[一 简介常用的组件：numpy,pandas,matplotlib,jupyter 环境搭建： 1.conda: 2.jupyter: 安装第三方库 pip install numpy或pandas或matplotlib或jupyter windows出错：1 权限，可以搜索cmd 按下ctrl+shift+enter 进入管理员 2.超时 1.1 设置超时pip install –default-timeout=10000 cryptography 或pip –default-timeout=100 install -U Pillow或numpy 再次下载 或者可以选则国内镜像地址： 使用PyPi的国内镜像站点，这里以清华大学的为例： pip install -i https://pypi.tuna.tsinghua.edu.cn/simple jupyterlab/numpy/matplotlib 二使用1 jupyter]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink]]></title>
    <url>%2F2020%2F01%2F29%2FFlink.html</url>
    <content type="text"><![CDATA[Flink目标: 批数据处理编程ExecutionEnviroment 流数据处理编程StreamExecutionEnviroment Flink原理 checkpoint、watermark Flink是什么 Flink是什么 Flink是一个分布式计算引擎 MapReduce Tez Spark Storm 同时支持流计算和批处理,Spark也能做批和流 和Spark不同, Flink是使用流的思想做批, Spark是采用做批的思想做流 Flink的优势 和Hadoop相比, Flink使用内存进行计算, 速度明显更优 和同样使用内存的Spark相比, Flink对于流的计算是实时的, 延迟更低 和同样使用实时流的Storm相比, Flink明显具有更优秀的API, 以及更多的支持, 并且支持批量计算 速度 测试环境：1.CPU：7000个； 2.内存：单机128GB； 3.版本：Hadoop 2.3.0，Spark 1.4，Flink 0.9 4.数据：800MB，8GB，8TB； 5.算法：K-means：以空间中K个点为中心进行聚类，对最靠近它们的对象归类。通过迭代的方法，逐次更新各聚类中心的值，直至得到最好的聚类结果。 6.迭代：K=10，3组数据 纵坐标是秒，横坐标是次数 结论: Spark和Flink全部都运行在Hadoop YARN上，性能为Flink &gt; Spark &gt; Hadoop(MR)，迭代次数越多越明显,性能上，Flink优于Spark和Hadoop最主要的原因是Flink支持增量迭代，具有对迭代自动优化的功能 在单机上, Storm大概能达到30万条/秒的吞吐量, Flink的吞吐量大概是Storm得3-5倍.在阿里中,Flink集群能达到每秒能处理17亿数据量,一天可处理上万亿条数据 在单机上, Flink消息处理的延迟大概在50毫秒左右, 这个数据大概是Spark的3-5倍 Flink的发展现状 08年Flink在德国柏林大学 14年Apache立为顶级项目.阿里15年开始使用 Flink在很多公司的生产环境中得到了使用, 例如: ebay, 腾讯, 阿里, 亚马逊, 华为等 Blink Flink的母公司被阿里全资收购, 阿里一直致力于Flink在国内的推广使用 Flink的适用场景 零售业和市场营销(运营) 物联网,5G 300M/s 延迟低 50ms 100ms 无人驾驶 华人运通:hiphi1 10万辆 560个 没200ms采集一次数据 2800条 电信业 银行和金融业 对比Flink、Spark、Storm Flink、Spark Streaming、Storm都可以进行实时计算，但各有特点 | 计算框架 | 处理模型 | 保证次数 | 容错机制 | 延时 | 吞吐量 || ————— | ——————— | ———————– | ——————- | —- | —- || Storm | native（数据进入立即处理） | At-least-once至少一次 | ACK机制 | 低 | 低 || Spark Streaming | micro-batching | Exactly-once | 基于RDD和 checkpoint | 中 | 高 || Flink | native、micro-batching | Exactly-once | checkpoint（Flink快照） | 低 | 高 | Flink的体系架构 有界流和无界流无界流：意思很明显，只有开始没有结束。必须连续的处理无界流数据，也即是在事件注入之后立即要对其进行处理。不能等待数据到达了再去全部处理，因为数据是无界的并且永远不会结束数据注入。处理无界流数据往往要求事件注入的时候有一定的顺序性，例如可以以事件产生的顺序注入，这样会使得处理结果完整。 有界流：也即是有明确的开始和结束的定义。有界流可以等待数据全部注入完成了再开始处理。注入的顺序不是必须的了，因为对于一个静态的数据集，我们是可以对其进行排序的。有界流的处理也可以称为批处理。 Data Streams ，Flink认为有界数据集是无界数据流的一种特例，所以说有界数据集也是一种数据流，事件流也是一种数据流。Everything is streams ，即Flink可以用来处理任何的数据，可以支持批处理、流处理、AI、MachineLearning等等。Stateful Computations，即有状态计算。有状态计算是最近几年来越来越被用户需求的一个功能。比如说一个网站一天内访问UV数，那么这个UV数便为状态。Flink提供了内置的对状态的一致性的处理，即如果任务发生了Failover，其状态不会丢失、不会被多算少算，同时提供了非常高的性能。 其它特点:​ 性能优秀(尤其在流计算领域)​ 高可扩展性​ 支持容错​ 纯内存式的计算引擎，做了内存管理方面的大量优化​ 支持eventime 的处理​ 支持超大状态的Job(在阿里巴巴中作业的state大小超过TB的是非常常见的)​ 支持exactly-once 的处理。 Flink安装及任务提交三种: 1 local（本地）——单机模式，一般不使用2 standalone——独立模式，Flink自带集群，开发测试环境使用3 yarn——计算资源统一由Hadoop YARN管理，生产测试环境使用 Standalone单机模式 Standalone集群模式 Standalone的高可用HA模式 Standalone方式安装 将Flink解压到指定目录， 进入到Flink目录，使用以下命令启动Flink 1./bin/start-cluster.sh 打开浏览器，使用http://服务器地址:8081，进入到Flink的Web UI中 standalone集群方式安装 下载Flink，并解压到指定目录 配置conf/flink-conf.yaml 123456789101112131415161718# 配置Master的机器名（IP地址）jobmanager.rpc.address: node01# 配置Master的端口号jobmanager.rpc.port: 6123# 配置Master的堆大小（默认MB）jobmanager.heap.size: 1024m# 配置每个TaskManager的堆大小（默认MB）taskmanager.heap.size: 1024m# 配置每个TaskManager可以运行的槽taskmanager.numberOfTaskSlots: 4# 配置每个taskmanager生成的临时文件夹taskmanager.tmp.dirs: /export/data/flink# 配置webui启动的机器名（IP地址）web.address: node01# 配置webui启动的端口号rest.port: 8081# 是否支持通过web ui提交Flink作业web.submit.enable: true 配置masters 1node01:8081 配置slaves文件 123node01node02node03 分发Flink到集群中的其他节点 12scp -r flink-1.7.2 node02:$PWDscp -r flink-1.7.2 node03:$PWD 启动集群 1./bin/start-cluster.sh 浏览Flink UI界面 1http://node01:8081 Flink主界面：通过主界面可以查看到当前的TaskManager和多少个Slots TaskManager界面：可以查看到当前Flink集群中有多少个TaskManager，每个TaskManager的slots、内存、CPU Core是多少。 ​ HA集群搭建Flink的JobManager存在单点故障，在生产环境中，需要对JobManager进行高可用部署。JobManager高可用基于ZooKeeper实现，同时HA的信息需要存储在HDFS中，故也需要HDFS集群。 前提：启动ZooKeeper—&gt;zkServer.sh start 前提：启动HDFS —&gt;start-dfs.sh 修改node02的conf/flink-conf.yaml配置文件 web.address: node02 rest.port: 8081 1234567#node01/02/03的每个flink-conf.yaml配置文件开启HAstate.backend: filesystemstate.backend.fs.checkpointdir: hdfs://node01:8020/flink-checkpointshigh-availability: zookeeperhigh-availability.storageDir: hdfs://node01:8020/flink/ha/high-availability.zookeeper.quorum: node01:2181,node02:2181,node03:2181high-availability.zookeeper.client.acl: open 修改3台机器的conf/masters配置文件 12node01:8081node02:8081 启动Zookeeper集群 启动HDFS集群 启动Flink集群 Flink程序提交方式​ 在企业生产中,为了最大化利用资源,一般都会在一个集群中同时运行多种类型的任务,我们Flink也是支持在Yarn/Mesos等平台运行.Flink的任务提交有两种方式,分别是Session和Job 首先需要配置相关Hadoop的环境 修改yarn-site.xml 1vim $HADOOP_HOME/etc/hadoop/yarn-site.xml 添加如下配置,并拷贝到node02/node03: 1234&lt;property&gt; &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt; 是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认是true。在这里面我们需要关闭，因为对于flink使用yarn模式下，很容易内存超标，这个时候yarn会自动杀掉job 添加HADOOP_CONF_DIR环境变量 12345678vim /etc/profile# 此处的路径为服务器上的hadoop路径export HADOOP_CONF_DIR=/export/servers/hadoop路径/etc/hadoopexport HADOOP_CONF_DIR=/export/servers/hadoop-2.7.5/etc/hadoopexport HADOOP_CONF_DIR=/export/servers/hadoop-2.7.5/etc/hadoop修改后记得:将node02和node03的环境变量都做修改.source /etc/profle 方式1:session 在yarn上启动一个Flink Job，执行以下命令 12345678#启动Yarn集群start-yarn.sh#通过-h参数可以查看yarn-session的参数功能bin/yarn-session.sh -h#使用Flink自带yarn-session.sh脚本开启Yarn会话bin/yarn-session.sh -n 2 -tm 800 -s 2#可以事先关闭 否则会出现程序跑不完./bin/stop-cluster.sh -n 表示分配多少个container，这里指的就是多少个taskmanager -tm 表示每个TaskManager的内存大小 -s 表示每个TaskManager的slots数量 上面的命令的意思是，同时向Yarn申请3个container（即便只申请了两个，因为ApplicationMaster和Job Manager有一个额外的容器。一旦将Flink部署到YARN群集中，它就会显示Job Manager的连接详细信息。），其中 2 个 Container 启动 TaskManager（-n 2），每个 TaskManager 拥有两个 Task Slot（-s 2），并且向每个 TaskManager 的 Container 申请 800M 的内存，以及一个ApplicationMaster（Job Manager）。 如果不想让Flink YARN客户端始终运行，那么也可以启动分离的 YARN会话。该参数被称为-d或–detached。在这种情况下，Flink YARN客户端只会将Flink提交给群集，然后关闭它自己 然后使用flink提交任务： 1bin/flink run examples/batch/WordCount.jar 通过上方的ApplicationMaster可以进入Flink的管理界面: 停止当前任务: yarn application -kill application_1562034096080_0001 方式2:job上面的YARN session是在Hadoop YARN环境下启动一个Flink cluster集群，里面的资源是可以共享给其他的Flink作业。我们还可以在YARN上启动一个Flink作业，这里我们还是使用./bin/flink，但是不需要事先启动YARN session： 1bin/flink run -m yarn-cluster -yn 2 ./examples/batch/WordCount.jar 以上命令在参数前加上y前缀，-yn表示TaskManager个数 停止yarn-cluster 1yarn application -kill application的ID 这种方式一般适用于长时间工作的任务,如果任务比较小,或者工作时间短,建议适用session方式,减少资源创建的时间.实际生产环境中,job方式适用较多. 区别: Session方式适合提交小任务,因为资源的开辟需要的时间比较长,session方式资源是共享的, Job适合提交长时间运行的任务,大作业,资源是独有的. 入门案例创建工程导入pom文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155&lt;properties&gt; &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;scala.version&gt;2.11.2&lt;/scala.version&gt; &lt;scala.compat.version&gt;2.11&lt;/scala.compat.version&gt; &lt;hadoop.version&gt;2.6.0&lt;/hadoop.version&gt; &lt;flink.version&gt;1.6.0&lt;/flink.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-library&lt;/artifactId&gt; &lt;version&gt;$&#123;scala.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-streaming-scala_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-scala_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-clients_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-table_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;$&#123;hadoop.version&#125;&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;artifactId&gt;xml-apis&lt;/artifactId&gt; &lt;groupId&gt;xml-apis&lt;/groupId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.38&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.2.22&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-kafka-0.9_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;sourceDirectory&gt;src/main/scala&lt;/sourceDirectory&gt; &lt;testSourceDirectory&gt;src/test/scala&lt;/testSourceDirectory&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;2.5.1&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;$&#123;maven.compiler.source&#125;&lt;/source&gt; &lt;target&gt;$&#123;maven.compiler.target&#125;&lt;/target&gt; &lt;!--&lt;encoding&gt;$&#123;project.build.sourceEncoding&#125;&lt;/encoding&gt;--&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;net.alchim31.maven&lt;/groupId&gt; &lt;artifactId&gt;scala-maven-plugin&lt;/artifactId&gt; &lt;version&gt;3.2.0&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;compile&lt;/goal&gt; &lt;goal&gt;testCompile&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;args&gt; &lt;!--&lt;arg&gt;-make:transitive&lt;/arg&gt;--&gt; &lt;arg&gt;-dependencyfile&lt;/arg&gt; &lt;arg&gt;$&#123;project.build.directory&#125;/.scala_dependencies&lt;/arg&gt; &lt;/args&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-surefire-plugin&lt;/artifactId&gt; &lt;version&gt;2.18.1&lt;/version&gt; &lt;configuration&gt; &lt;useFile&gt;false&lt;/useFile&gt; &lt;disableXmlReport&gt;true&lt;/disableXmlReport&gt; &lt;includes&gt; &lt;include&gt;**/*Test.*&lt;/include&gt; &lt;include&gt;**/*Suite.*&lt;/include&gt; &lt;/includes&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt; &lt;version&gt;2.3&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;shade&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;filters&gt; &lt;filter&gt; &lt;artifact&gt;*:*&lt;/artifact&gt; &lt;excludes&gt; &lt;!-- zip -d learn_spark.jar META-INF/*.RSA META-INF/*.DSA META-INF/*.SF --&gt; &lt;exclude&gt;META-INF/*.SF&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.DSA&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.RSA&lt;/exclude&gt; &lt;/excludes&gt; &lt;/filter&gt; &lt;/filters&gt; &lt;transformers&gt; &lt;transformer implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer"&gt; &lt;mainClass&gt;com.itheima.batch.WordCount&lt;/mainClass&gt; &lt;/transformer&gt; &lt;/transformers&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; 编写代码 WordCount 12345678def main(args: Array[String]): Unit = &#123; val env: ExecutionEnvironment = ExecutionEnvironment.getExecutionEnvironment val dataSet: DataSet[String] = env.fromElements("hello", "world", "java", "hello", "java") val mapData: DataSet[(String, Int)] = dataSet.map(line =&gt; (line, 1)) val groupData: GroupedDataSet[(String, Int)] = mapData.groupBy(0) val sumData: AggregateDataSet[(String, Int)] = groupData.sum(1) sumData.print()&#125; 在Yarn上运行WordCount12345678def main(args: Array[String]): Unit = &#123; val env: ExecutionEnvironment = ExecutionEnvironment.getExecutionEnvironment val dataSet: DataSet[String] = env.fromElements("hello", "world", "java", "hello", "java") val result: AggregateDataSet[(String, Int)] = dataSet.map(line =&gt; (line, 1)).groupBy(0).sum(1) result.setParallelism(1) result.writeAsText("hdfs://node01:8020/wordcount") env.execute() &#125; 修改pom.xml中的主类名 12345&lt;transformers&gt; &lt;transformer implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer"&gt; &lt;mainClass&gt;cn.itcast.flink.WorldCount_02&lt;/mainClass&gt; &lt;/transformer&gt;&lt;/transformers&gt; 打包 提交执行 将打出的jar放入服务器 使用session方式提交 1231.先启动yarnsession bin/yarn-session.sh -n 2 -tm 800 -s 22.去web界面提交jar包.或者 bin/flink run /home/elasticsearch/flinkjar/itcast_learn_flink-1.0-SNAPSHOT.jar com.itcast.DEMO.WordCount 使用job方式提交 1bin/flink run -m yarn-cluster -yn 2 /home/elasticsearch/flinkjar/itcast_learn_flink-1.0-SNAPSHOT.jar com.itcast.DEMO.WordCount 任务调度与执行Flink的所有操作都称之为Operator,客户端在提交任务的时候会对Operator进行优化操作,能进行合并的Operator会被合并为一个Operator,合并后的Operator称为Operator chain 客户端 主要职责是提交任务, 提交后可以结束进程, 也可以等待结果返回 JobManager 主要职责是调度工作并协调任务做检查点 JobManager从客户端接收到任务以后, 首先生成优化过的执行计划, 再调度到TaskManager中执行 TaskManager 主要职责是从JobManager处接收任务, 并部署和启动任务, 接收上游的数据并处理 TaskManager在创建之初就设置好了Slot, 每个Slot可以执行一个任务 Flink的APIDataSet的转换操作 Transformation Description Map 在算子中得到一个元素并生成一个新元素data.map { x =&gt; x.toInt } FlatMap 在算子中获取一个元素, 并生成任意个数的元素data.flatMap { str =&gt; str.split(&quot; &quot;) } MapPartition 类似Map, 但是一次Map一整个并行分区data.mapPartition { in =&gt; in map { (_, 1) } } Filter 如果算子返回true则包含进数据集, 如果不是则被过滤掉data.filter { _ &gt; 100 } Reduce 通过将两个元素合并为一个元素, 从而将一组元素合并为一个元素data.reduce { _ + _ } ReduceGroup 将一组元素合并为一个或者多个元素data.reduceGroup { elements =&gt; elements.sum } Aggregate 讲一组值聚合为一个值, 聚合函数可以看作是内置的Reduce函数data.aggregate(SUM, 0).aggregate(MIN, 2)data.sum(0).min(2) Distinct 去重 Join 按照相同的Key合并两个数据集input1.join(input2).where(0).equalTo(1)同时也可以选择进行合并的时候的策略, 是分区还是广播, 是基于排序的算法还是基于哈希的算法input1.join(input2, JoinHint.BROADCAST_HASH_FIRST).where(0).equalTo(1) OuterJoin 外连接, 包括左外, 右外, 完全外连接等left.leftOuterJoin(right).where(0).equalTo(1) { (left, right) =&gt; ... } CoGroup 二维变量的Reduce运算, 对每个输入数据集中的字段进行分组, 然后join这些组input1.coGroup(input2).where(0).equalTo(1) Cross 笛卡尔积input1.cross(input2) Union 并集input1.union(input2) Rebalance 分区重新平衡, 以消除数据倾斜input.rebalance() Hash-Partition 按照Hash分区input.partitionByHash(0) Range-Partition 按照Range分区input.partitionByRange(0) CustomParititioning 自定义分区input.partitionCustom(partitioner: Partitioner[K], key) First-n 返回数据集中的前n个元素input.first(3) flatmapmap =&gt; 1 =&gt; 1 flatmap =&gt; 1 =&gt; 多个数据 1234567def main(args: Array[String]): Unit = &#123; val env: ExecutionEnvironment = ExecutionEnvironment.getExecutionEnvironment val dataSet: DataSet[(String, Int)] = env.fromElements(("A" , 1) , ("B" , 1) , ("C" , 1)) dataSet.map(line =&gt; line._1 + "=" + line._2).print() println("------------------") dataSet.flatMap(line =&gt; line._1 + "=" + line._2).print()&#125; filter123//TODO fileter=&gt;val filter:DataSet[String] = elements.filter(line =&gt; line.contains("java"))//过滤出带java的数据filter.print() reduce12345678910111213141516171819202122232425//默认并行度为8,全局并行度设为1// env.setParallelism(1) //加载数据 val sourceData: DataSet[String] = env.readTextFile("access.log") val mapData: DataSet[Array[String]] = sourceData.map(line =&gt; line.split(" ")) val flatMapData: DataSet[String] = mapData.flatMap(line =&gt; line) val mData: DataSet[(String, Int)] = flatMapData.map(line =&gt; (line, 1)) //对数据进行分组操作,groupBy()可以指定要按照哪个来进行分组 val groupData: GroupedDataSet[(String, Int)] = mData.groupBy(0) //reduce((之前的数据,最新的数据) =&gt; ) val reduceData: DataSet[(String, Int)] = groupData.reduce((x, y) =&gt; (x._1, x._2 + y._2)) reduceData //将结果输出到本地文件 .writeAsText("result") //设置输出的并行度为1 .setParallelism(1) env.execute() reduceGroup reduceGroup是reduce的一种优化方案； 它会先分组reduce，然后在做整体的reduce；这样做的好处就是可以减少网络IO； join 求每个班级最高分 12345678910111213141516171819202122232425262728293031def main(args: Array[String]): Unit = &#123; //TODO join val data1 = new mutable.MutableList[(Int, String, Double)] //学生学号---学科---分数 data1.+=((1, "yuwen", 90.0)) data1.+=((2, "shuxue", 20.0)) data1.+=((3, "yingyu", 30.0)) data1.+=((4, "yuwen", 40.0)) data1.+=((5, "shuxue", 50.0)) data1.+=((6, "yingyu", 60.0)) data1.+=((7, "yuwen", 70.0)) data1.+=((8, "yuwen", 20.0)) val data2 = new mutable.MutableList[(Int, String)] //学号 ---班级 data2.+=((1,"class_1")) data2.+=((2,"class_1")) data2.+=((3,"class_2")) data2.+=((4,"class_2")) data2.+=((5,"class_3")) data2.+=((6,"class_3")) data2.+=((7,"class_4")) data2.+=((8,"class_1")) val env: ExecutionEnvironment = ExecutionEnvironment.getExecutionEnvironment val input1: DataSet[(Int, String, Double)] = env.fromCollection(Random.shuffle(data1)) val input2: DataSet[(Int, String)] = env.fromCollection(Random.shuffle(data2)) val data = input2.join(input1).where(0).equalTo(0)&#123; (input2, input1) =&gt; (input2._1,input2._2, input1._2, input1._3) &#125; data.groupBy(1).aggregate(Aggregations.MAX, 3).print()&#125; distinct去重123456789101112val data = new mutable.MutableList[(Int, String, Double)]data.+=((1, "yuwen", 90.0))data.+=((2, "shuxue", 20.0))data.+=((3, "yingyu", 30.0))data.+=((4, "wuli", 40.0))data.+=((5, "yuwen", 50.0))data.+=((6, "wuli", 60.0))data.+=((7, "yuwen", 70.0))// //fromCollection将数据转化成DataSetval input: DataSet[(Int, String, Double)] = env.fromCollection(Random.shuffle(data))val distinct = input.distinct(1)distinct.print() DataStream的转换操作Flink中的DataStream程序是实现数据流转换（例如，过滤，更新状态，定义窗口，聚合）的常规程序。数据流最初由各种来源（例如，消息队列，套接字流，文件）创建。结果通过接收器返回，例如可以将数据写入文件，或者写入标准输出（例如命令行终端）。Flink程序可以在各种情况下运行，可以独立运行，也可以嵌入其他程序中。执行可以发生在本地JVM或许多机器的集群中。 Transformation Description MapDataStream → DataStream dataStream.map { x =&gt; x * 2 } FlatMapDataStream → DataStream dataStream.flatMap { x =&gt; x.split(&quot;,&quot;) } FilterDataStream → DataStream dataStream.filter { _ != 0 } KeyByDataStream → KeyedStream 将一个流分为不相交的区, 可以按照名称指定Key, 也可以按照角标来指定`dataStream.keyBy(“key” ReduceKeyedStream → DataStream 滚动Reduce, 合并当前值和历史结果, 并发出新的结果值keyedStream.reduce { _ + _ } FoldKeyedStream → DataStream 按照初始值进行滚动折叠keyedStream.fold(&quot;start&quot;)((str, i) =&gt; { str + &quot;-&quot; + i }) AggregationsKeyedStream → DataStream 滚动聚合, sum, min, max等keyedStream.sum(0) WindowKeyedStream → DataStream 窗口函数, 根据一些特点对数据进行分组, 注意: 有可能是非并行的, 所有记录可能在一个任务中收集.window(TumblingEventTimeWindows.of(Time.seconds(5))) WindowAllDataStream → AllWindowedStream 窗口函数, 根据一些特点对数据进行分组, 和window函数的主要区别在于可以不按照Key分组dataStream.windowAll (TumblingEventTimeWindows.of(Time.seconds(5))) WindowApplyWindowedStream → DataStream 将一个函数作用于整个窗口windowedStream.apply { WindowFunction } WindowReduceWindowedStream → DataStream 在整个窗口上做一次reducewindowedStream.reduce { _ + _ } WindowFoldWindowedStream → DataStream 在整个窗口上做一次foldwindowedStream.fold(&quot;start&quot;, (str, i) =&gt; { str + &quot;-&quot; + i }) Aggregations on windowsWindowedStream → DataStream 在窗口上统计, sub, max, minwindowedStream.sum(10) UnionDataStream* → DataStream 合并多个流dataStream.union(dataStream1, dataStream2, ...) Window JoinDataStream → DataStream dataStream.join(otherStream).where(...).equalTo(...) .window(TumblingEventTimeWindows.of(Time.seconds(3))).apply{..} Window CoGroupDataStream, DataStream → DataStream dataStream.coGroup(otherStream).where(0).equalTo(1).window(...).apply{...} ConnectDataStream, DataStream → DataStream 连接两个流, 并且保留各自的数据类型, 在这个连接中可以共享状态someStream.connect(otherStream) SplitDataStream → SplitStream 将一个流切割为多个流someDataStream.split((x: Int) =&gt; x match ...) 入门案例编写代码: 1234567def main(args: Array[String]): Unit = &#123; val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment val data: DataStream[String] = env.socketTextStream("node01", 9999)//netcat val mapData: DataStream[(String, Int)] = data.map(line =&gt; (line, 1)) mapData.keyBy(0).sum(1).print() env.execute()&#125; 在Linux窗口中发送消息: 1nc -lk 9999 keyby1234567891011def main(args: Array[String]): Unit = &#123; val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment env.setParallelism(3) val textStream: DataStream[String] = env.socketTextStream("localhost" , 12345) val flatMap_data: DataStream[String] = textStream.flatMap(line =&gt; line.split("\t")) val map_data: DataStream[(String, Int)] = flatMap_data.map(line =&gt; (line , 1)) //TODO 逻辑上将一个流分成不相交的分区，每个分区包含相同键的元素。在内部，这是通过散列分区来实现的 val keyByData: KeyedStream[(String, Int), String] = map_data.keyBy(line =&gt; line._1) keyByData.writeAsText("keyByData") env.execute() &#125; Flink SQLFlink SQL可以让我们通过基于Table API和SQL来进行数据处理。Flink的批处理和流处理都支持Table API。Flink SQL完全遵循ANSI SQL标准。 批数据SQL 用法 构建Table运行环境 将DataSet注册为一张表 使用Table运行环境的sqlQuery方法来执行SQL语句 示例 使用Flink SQL统计用户消费订单的总金额、最大金额、最小金额、订单总数。 订单id 用户名 订单日期 消费基恩 1 zhangsan 2018-10-20 15:30 358.5 测试数据（订单ID、用户名、订单日期、订单金额） 123456789(1,"zhangsan","2018-10-20 15:30",358.5),(2,"zhangsan","2018-10-20 16:30",131.5),(3,"lisi","2018-10-20 16:30",127.5),(4,"lisi","2018-10-20 16:30",328.5),(5,"lisi","2018-10-20 16:30",432.5),(6,"zhaoliu","2018-10-20 22:30",451.0),(7,"zhaoliu","2018-10-20 22:30",362.0),(8,"zhaoliu","2018-10-20 22:30",364.0),(9,"zhaoliu","2018-10-20 22:30",341.0) 步骤 获取一个批处理运行环境 获取一个Table运行环境 创建一个样例类Order用来映射数据（订单名、用户名、订单日期、订单金额） 基于本地Order集合创建一个DataSet source 使用Table运行环境将DataSet注册为一张表 使用SQL语句来操作数据（统计用户消费订单的总金额、最大金额、最小金额、订单总数） 使用TableEnv.toDataSet将Table转换为DataSet 打印测试 参考代码 1234567891011121314151617181920212223242526272829303132val env = ExecutionEnvironment.getExecutionEnvironmentval tableEnv = TableEnvironment.getTableEnvironment(env)val orderDataSet = env.fromElements( Order(1, "zhangsan", "2018-10-20 15:30", 358.5), Order(2, "zhangsan", "2018-10-20 16:30", 131.5), Order(3, "lisi", "2018-10-20 16:30", 127.5), Order(4, "lisi", "2018-10-20 16:30", 328.5), Order(5, "lisi", "2018-10-20 16:30", 432.5), Order(6, "zhaoliu", "2018-10-20 22:30", 451.0), Order(7, "zhaoliu", "2018-10-20 22:30", 362.0), Order(8, "zhaoliu", "2018-10-20 22:30", 364.0), Order(9, "zhaoliu", "2018-10-20 22:30", 341.0))tableEnv.registerDataSet("t_order", orderDataSet)val allOrderTable: Table = tableEnv.sqlQuery&#123; """ |select | userName, | count(1) as totalCount, -- 订单总数 | max(money) as maxMoney, -- 最大订单金额 | min(money) as minMoney -- 最小订单金额 |from | t_order |group by | userName """.stripMargin&#125;allOrderTable.printSchema()tableEnv.toDataSet[Row](allOrderTable).print() 流数据SQL 流处理中也可以支持SQL。但是需要注意以下几点： 要使用流处理的SQL，必须要添加水印时间 使用registerDataStream注册表的时候，使用&#39;来指定字段 注册表的时候，必须要指定一个rowtime，否则无法在SQL中使用窗口 必须要导入import org.apache.flink.table.api.scala._隐式参数 SQL中使用tumble(时间列名, interval &#39;时间&#39; sencond)来进行定义窗口 TUMBLE(time_attr, interval)固定时间窗口 HOP(time_attr, interval, interval)滑动窗口, SESSION(time_attr, interval)会话窗口 示例 使用Flink SQL来统计5秒内用户的订单总数、订单的最大金额、订单的最小金额。 步骤 获取流处理运行环境 获取Table运行环境 设置处理时间为EventTime 创建一个订单样例类Order，包含四个字段（订单ID、用户ID、订单金额、时间戳） 创建一个自定义数据源 使用for循环生成1000个订单 随机生成订单ID（UUID） 随机生成用户ID（0-2） 随机生成订单金额（0-100） 时间戳为当前系统时间 每隔1秒生成一个订单 添加水印，允许延迟2秒 导入import org.apache.flink.table.api.scala._隐式参数 使用registerDataStream注册表，并分别指定字段，还要指定rowtime字段 编写SQL语句统计用户订单总数、最大金额、最小金额 分组时要使用tumble(时间列, interval &#39;窗口时间&#39; second)来创建窗口 使用tableEnv.sqlQuery执行sql语句 将SQL的执行结果转换成DataStream再打印出来 启动流处理程序 参考代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677// 3. 创建一个订单样例类`Order`，包含四个字段（订单ID、用户ID、订单金额、时间戳）case class Order(orderId:String, userId:Int, money:Long, timestamp:Long)def main(args: Array[String]): Unit = &#123; // 1. 创建流处理运行环境 val env = StreamExecutionEnvironment.getExecutionEnvironment // 2. 设置处理时间为`EventTime` env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime) val tableEnv = TableEnvironment.getTableEnvironment(env) // 4. 创建一个自定义数据源 val orderDataStream = env.addSource(new RichSourceFunction[Order] &#123; override def run(ctx: SourceFunction.SourceContext[Order]): Unit = &#123; // - 随机生成订单ID（UUID） // - 随机生成用户ID（0-2） // - 随机生成订单金额（0-100） // - 时间戳为当前系统时间 // - 每隔1秒生成一个订单 for (i &lt;- 0 until 1000) &#123; val order = Order(UUID.randomUUID().toString, Random.nextInt(3), Random.nextInt(101), System.currentTimeMillis()) TimeUnit.SECONDS.sleep(1) ctx.collect(order) &#125; &#125; override def cancel(): Unit = &#123;&#125; &#125;) // 5. 添加水印，允许延迟2秒 val watermarkDataStream: DataStream[Order] = orderDataStream.assignTimestampsAndWatermarks(new AssignerWithPeriodicWatermarks[Order] &#123; var currentTimestamp = 0L val delayTime = 2000 override def getCurrentWatermark: Watermark = &#123; new Watermark(currentTimestamp - delayTime) &#125; override def extractTimestamp(element: Order, previousElementTimestamp: Long): Long = &#123; val timestamp = element.timestamp currentTimestamp = Math.max(currentTimestamp, timestamp) currentTimestamp &#125; &#125;) // 6. 导入`import org.apache.flink.table.api.scala._`隐式参数 // 7. 使用`registerDataStream`注册表，并分别指定字段，还要指定rowtime字段 tableEnv.registerDataStream("t_order", watermarkDataStream, 'orderId, 'userId, 'money, 'orderDate.rowtime) // 8. 编写SQL语句统计用户订单总数、最大金额、最小金额 // - 分组时要使用`tumble(时间列, interval '窗口时间' second)`来创建窗口 val sql = """ |select | userId, | count(1) as totalCount, | max(money) as maxMoney, | min(money) as minMoney |from | t_order |group by | tumble(orderDate, interval '5' second), | userId """.stripMargin // 9. 使用`tableEnv.sqlQuery`执行sql语句 val table: Table = tableEnv.sqlQuery(sql) // 10. 将SQL的执行结果转换成DataStream再打印出来 table.toRetractStream[Row].print() env.execute("StreamSQLApp")&#125; 在SQL语句中，不要将名字取成SQL中的关键字，例如：timestamp。 Table转换为DataStream或DataSet 转换为DataSet 直接使用tableEnv.toDataSet方法就可以将Table转换为DataSet 转换的时候，需要指定泛型，可以是一个样例类，也可以是指定为Row类型 转换为DataStream 使用tableEnv.toAppendStream，将表直接附加在流上 使用tableEnv.toRetractStream，返回一个元组（Boolean, DataStream），Boolean表示数据是否被成功获取 转换的时候，需要指定泛型，可以是一个样例类，也可以是指定为Row类型 窗口/水印 源源不断地数据是无法进行统计工作的，因为数据流没有边界，无法统计到底有多少数据经过了这个流 window操作就是在数据流上，截取固定大小的一部分，这个部分是可以统计的 截取方式有两种 按照时间截取，例如：10秒钟、10分钟统计一次 按照消息数量截取，例如：每5个数据、或者50个数据统计一次 窗口Flink的窗口划分方式分为2种:time/count,即按时间划分和数量划分 tumbling-time-window (无重叠数据) 1.红绿灯路口会有汽车通过，一共会有多少汽车通过，无法计算。因为车流源源不断，计算没有边界。 2.统计每15秒钟通过红路灯的汽车数量，第一个15秒为2辆，第二个15秒为3辆，第三个15秒为1辆。。。 12345678910发送内容9,39,29,74,92,61,52,35,75,4 编码: 12345678910111213141516171819202122232425262728293031def main(args: Array[String]): Unit = &#123; //TODO time-window //1.创建运行环境 val env = StreamExecutionEnvironment.getExecutionEnvironment //2.定义数据流来源 val text = env.socketTextStream("node01", 9000) //3.转换数据格式，text-&gt;CarWc case class CarWc(sensorId: Int, carCnt: Int) val ds1: DataStream[CarWc] = text.map &#123; line =&gt; &#123; val tokens = line.split(",") CarWc(tokens(0).trim.toInt, tokens(1).trim.toInt) &#125; &#125; //4.执行统计操作，每个sensorId一个tumbling窗口，窗口的大小为5秒 //也就是说，每5秒钟统计一次，在这过去的5秒钟内，各个路口通过红绿灯汽车的数量。 val ds2: DataStream[CarWc] = ds1 .keyBy("sensorId") .timeWindow(Time.seconds(5)) .sum("carCnt") //5.显示统计结果 ds2.print() //6.触发流计算 env.execute(this.getClass.getName)&#125; sliding-time-window (有重叠数据) 编码: 12345678910111213141516171819202122232425val env = StreamExecutionEnvironment.getExecutionEnvironment//2.定义数据流来源val text = env.socketTextStream("localhost", 9999)//3.转换数据格式，text-&gt;CarWccase class CarWc(sensorId: Int, carCnt: Int)val ds1: DataStream[CarWc] = text.map &#123; line =&gt; &#123; val tokens = line.split(",") CarWc(tokens(0).trim.toInt, tokens(1).trim.toInt) &#125;&#125;//4.执行统计操作，每个sensorId一个sliding窗口，窗口时间10秒,滑动时间5秒//也就是说，每5秒钟统计一次，在这过去的10秒钟内，各个路口通过红绿灯汽车的数量。val ds2: DataStream[CarWc] = ds1 .keyBy("sensorId") .timeWindow(Time.seconds(10), Time.seconds(5)) .sum("carCnt")//5.显示统计结果ds2.print()//6.触发流计算env.execute(this.getClass.getName) tumbling-count-window (无重叠数据) 按照个数进行统计，比如： 每个路口分别统计，收到关于它的5条消息时统计在最近5条消息中，各自路口通过的汽车数量 代码: 12345678910111213141516171819202122232425val env = StreamExecutionEnvironment.getExecutionEnvironment//2.定义数据流来源val text = env.socketTextStream("localhost", 9999)//3.转换数据格式，text-&gt;CarWccase class CarWc(sensorId: Int, carCnt: Int)val ds1: DataStream[CarWc] = text.map &#123; (f) =&gt; &#123; val tokens = f.split(",") CarWc(tokens(0).trim.toInt, tokens(1).trim.toInt) &#125;&#125;//4.执行统计操作，每个sensorId一个tumbling窗口，窗口的大小为5//按照key进行收集，对应的key出现的次数达到5次作为一个结果val ds2: DataStream[CarWc] = ds1 .keyBy("sensorId") .countWindow(5) .sum("carCnt")//5.显示统计结果ds2.print()//6.触发流计算env.execute(this.getClass.getName) sliding-count-window (有重叠数据) 同样也是窗口长度和滑动窗口的操作：窗口长度是5，滑动长度是3 编码: 12345678910111213141516171819202122232425val env = StreamExecutionEnvironment.getExecutionEnvironment//2.定义数据流来源val text = env.socketTextStream("localhost", 9999)//3.转换数据格式，text-&gt;CarWccase class CarWc(sensorId: Int, carCnt: Int)val ds1: DataStream[CarWc] = text.map &#123; (f) =&gt; &#123; val tokens = f.split(",") CarWc(tokens(0).trim.toInt, tokens(1).trim.toInt) &#125;&#125;//4.执行统计操作，每个sensorId一个sliding窗口，窗口大小3条数据,窗口滑动为3条数据//也就是说，每个路口分别统计，收到关于它的3条消息时统计在最近5条消息中，各自路口通过的汽车数量val ds2: DataStream[CarWc] = ds1 .keyBy("sensorId") .countWindow(5, 3) .sum("carCnt")//5.显示统计结果ds2.print()//6.触发流计算env.execute(this.getClass.getName) 问题: Flink中的窗口分为两类，一类是按时间来分，一类是按照事件的种类来分。对吗？ 如果窗口滑动时间 &gt; 窗口时间，会出现数据丢失 如果窗口滑动时间 &lt; 窗口时间，会出现数据重复计算,比较适合实时排行榜 如果窗口滑动时间 = 窗口时间，数据不会被重复计算 水印Flink的时间划分方式 事件时间：事件时间是每条事件在它产生的时候记录的时间，该时间记录在事件中，在处理的时候可以被提取出来。小时的时间窗处理将会包含事件时间在该小时内的所有事件，而忽略事件到达的时间和到达的顺序 摄入时间：摄入时间是事件进入flink的时间，在source operator中，每个事件拿到当前时间作为时间戳，后续的时间窗口基于该时间。 处理时间：当前机器处理该条事件的时间 问题: ProcessingTime是指的进入到Flink数据流处理系统的时间，对吗？ 如何处理水印 需求： 以EventTime划分窗口，计算3秒钟内出价最高的产品 1234567891011121314151617181527911155000,boos1,pc1,100.01527911156000,boos2,pc1,200.01527911157000,boos1,pc1,300.01527911158000,boos2,pc1,500.01527911159000,boos1,pc1,600.01527911160000,boos1,pc1,700.01527911161000,boos2,pc2,700.01527911162000,boos2,pc2,900.01527911163000,boos2,pc2,1000.01527911164000,boos2,pc2,1100.01527911165000,boos1,pc2,1100.01527911166000,boos2,pc2,1300.01527911167000,boos2,pc2,1400.01527911168000,boos2,pc2,1600.01527911170000,boos1,pc2,1300.01527911171000,boos2,pc2,1700.01527911172000,boos2,pc2,1800.01527911173000,boos1,pc2,1500.0 代码: 123456789101112131415161718192021222324252627282930def main(args: Array[String]) &#123; //1.创建执行环境，并设置为使用EventTime val env = StreamExecutionEnvironment.getExecutionEnvironment //置为使用EventTime env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime) //2.创建数据流，并进行数据转化 val source = env.socketTextStream("localhost", 9999) case class SalePrice(time: Long, boosName: String, productName: String, price: Double) val dst1: DataStream[SalePrice] = source.map(value =&gt; &#123; val columns = value.split(",") SalePrice(columns(0).toLong, columns(1), columns(2), columns(3).toDouble) &#125;) //3.使用EventTime进行求最值操作 val dst2: DataStream[SalePrice] = dst1 //提取消息中的时间戳属性 .assignAscendingTimestamps(_.time) .keyBy(_.productName) .timeWindow(Time.seconds(3))//设置window方法一 .max("price") //4.显示结果 dst2.print() //5.触发流计算 env.execute() &#125;&#125; 当前代码理论上看没有任何问题，在实际使用的时候就会出现很多问题,甚至接收不到数据或者接收到的数据是不准确的；这是因为对于flink最初设计的时候，就考虑到了网络延迟，网络乱序等问题，所以提出了一个抽象概念水印（WaterMark） 水印分成两种形式： 代码中就需要添加水印操作: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647def main(args: Array[String]): Unit = &#123; //创建执行环境，并设置为使用EventTime val env = StreamExecutionEnvironment.getExecutionEnvironment env.setParallelism(1)//注意控制并发数 //置为使用EventTime env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime) val source = env.socketTextStream("node01", 9999) val dst1: DataStream[SalePrice] = source.map(value =&gt; &#123; val columns = value.split(",") SalePrice(columns(0).toLong, columns(1), columns(2), columns(3).toDouble) &#125;) //todo 水印时间 assignTimestampsAndWatermarks val timestamps_data = dst1.assignTimestampsAndWatermarks(new AssignerWithPeriodicWatermarks[SalePrice]&#123; var currentMaxTimestamp:Long = 0 val maxOutOfOrderness = 2000L //最大允许的乱序时间是2s var wm : Watermark = null val format = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss.SSS") override def getCurrentWatermark: Watermark = &#123; wm = new Watermark(currentMaxTimestamp - maxOutOfOrderness) wm &#125; override def extractTimestamp(element: SalePrice, previousElementTimestamp: Long): Long = &#123; val timestamp = element.time currentMaxTimestamp = Math.max(timestamp, currentMaxTimestamp) currentMaxTimestamp &#125; &#125;) val data: KeyedStream[SalePrice, String] = timestamps_data.keyBy(line =&gt; line.productName) val window_data: WindowedStream[SalePrice, String, TimeWindow] = data.timeWindow(Time.seconds(3)) val apply: DataStream[SalePrice] = window_data.apply(new MyWindowFunc) apply.print() env.execute() &#125;&#125;case class SalePrice(time: Long, boosName: String, productName: String, price: Double)class MyWindowFunc extends WindowFunction[SalePrice , SalePrice , String, TimeWindow]&#123; override def apply(key: String, window: TimeWindow, input: Iterable[SalePrice], out: Collector[SalePrice]): Unit = &#123; val seq = input.toArray val take: Array[SalePrice] = seq.sortBy(line =&gt; line.price).reverse.take(1) for(info &lt;- take)&#123; out.collect(info) &#125; &#125;&#125; 容错批处理系统比较容易实现容错机制，由于文件可以重复访问，当某个任务失败后，重启该任务即可。但是在流处理系统中，由于数据源是无限的数据流，一个流处理任务甚至可能会执行几个月，将所有数据缓存或是持久化，留待以后重复访问基本上是不可行的。 Checkpoint是Flink实现容错机制最核心的功能，它能够根据配置周期性地基于Stream中各个Operator的状态来生成Snapshot，从而将这些状态数据定期持久化存储下来，当Flink程序一旦意外崩溃时，重新运行程序时可以有选择地从这些Snapshot进行恢复，从而修正因为故障带来的程序数据状态中断。 Checkpoint Checkpoint流程 CheckpointCoordinator周期性的向该流应用的所有source算子发送barrier。 当某个source算子收到一个barrier时，会向自身所有下游算子广播该barrier，同时将自己的当前状态制作成快照(异步)，并保存到指定的持久化存储中，最后向CheckpointCoordinator报告自己快照制作情况 下游算子收到barrier之后，会向自身所有下游算子广播该barrier，同时将自身的相关状态制作成快照(异步)，并保存到指定的持久化存储中，最后向CheckpointCoordinator报告自身快照情况 每个算子按照步骤3不断制作快照并向下游广播，直到最后barrier传递到sink算子，快照制作完成。 当CheckpointCoordinator收到所有算子的报告之后，认为该周期的快照制作成功; 否则，如果在规定的时间内没有收到所有算子的报告，则认为本周期快照制作失败 单流的barrier 屏障作为数据流的一部分随着记录被注入到数据流中。屏障永远不会赶超通常的流记录，它会严格遵循顺序。 屏障将数据流中的记录隔离成一系列的记录集合，并将一些集合中的数据加入到当前的快照中，而另一些数据加入到下一个快照中。 每一个屏障携带着快照的ID，快照记录着ID并且将其放在快照数据的前面。 屏障不会中断流处理，因此非常轻量级。 并行barrier 不止一个输入流的时的operator，需要在快照屏障上对齐(align)输入流，才会发射出去。 可以看到1,2,3会一直放在Input buffer，直到另一个输入流的快照到达Operator。 问题: Flink中Barrier的对齐指的是Flink处理数据流的时候，会加入barrier，某一个operator接收到一个barriern，会等到接收到所有数据流的barrier，才继续往下处理。这样可以实现数据Exatly Once语义。对吗？ 一个Operator处理完数据流后，会将数据流中的barrier删除，这样可以减少处理的数据量，提高运行效率。对吗？ 持久化存储MemoryStateBackendstate数据保存在java堆内存中，执行checkpoint的时候，会把state的快照数据保存到jobmanager的内存中 基于内存的state backend在生产环境下不建议使用。 FsStateBackendstate数据保存在taskmanager的内存中，执行checkpoint的时候，会把state的快照数据保存到配置的文件系统中，可以使用hdfs等分布式文件系统。 RocksDBStateBackend基于RocksDB + FS RocksDB跟上面的都略有不同，它会在本地文件系统中维护状态，state会直接写入本地rocksdb中。同时RocksDB需要配置一个远端的filesystem。 代码: 123456789101112131415val env = StreamExecutionEnvironment.getExecutionEnvironment// start a checkpoint every 1000 ms//开启Flink的Checkpointenv.enableCheckpointing(5000)// advanced options:// 设置checkpoint的执行模式，最多执行一次或者至少执行一次env.getCheckpointConfig.setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE)// 设置checkpoint的超时时间env.getCheckpointConfig.setCheckpointTimeout(60000)// 如果在只做快照过程中出现错误，是否让整体任务失败：true是 false不是env.getCheckpointConfig.setFailTasksOnCheckpointingErrors(false)//设置同一时间有多少 个checkpoint可以同时执行 env.getCheckpointConfig.setMaxConcurrentCheckpoints(1)//设置checkpoint路径 env.setStateBackend(new FsStateBackend("hdfs://node01:8020/flink_checkpoint0000000"))]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>BigData</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis]]></title>
    <url>%2F2018%2F10%2F02%2FRedis.html</url>
    <content type="text"><![CDATA[一 概述开源的(基于BSD协议),使用ANSI C 编写 ,基于内存的且可以持久化,高性能k-v的nosql数据库 支持数据结构类型丰富 (k只有String 类型 ,v 有很多类型) 1特性Ø Redis支持数据的持久化，可以将内存中的数据保存在磁盘中，重启的时候可以再次加载进行使用。 Ø Redis不仅仅支持简单的key-value类型的数据，同时还提供list，set，zset，hash等数据结构的存储。 Ø Redis支持数据的备份，即master-slave模式的数据备份。 2 优势Ø 性能极高 – Redis能读的速度是110000次/s,写的速度是81000次/s 。 Ø 丰富的数据类型 – Redis支持二进制案例的 Strings, Lists, Hashes, Sets 及 Ordered Sets 数据类型操作。 Ø 原子 – Redis的所有操作都是原子性的，同时Redis还支持对几个操作全并后的原子性执行。 Ø 丰富的特性 – Redis还支持 publish/subscribe, 通知, key 过期等等特性。 二安装1 单机安装1下载 2 上传linux系统 3 解压 4 进入Redis解压目录 5 对Redis编译 12make MALLOC=libc若是出错 yum install gcc 6 安装到指定目录 1make PREFIX=/usr/local/redis install 安装完成 进入bin目录: 12345redis-server redis服务器redis-cli redis命令行客户端redis-benchmark redis性能测试工具redis-check-aof AOF文件修复工具redis-check-rdb RDB文件检索工具 将原Redis目录下make只有会有一个redis.conf文件，赋值到编译后的/redis目录下即可。 启动Redis服务: 1./bin/redis-server ./redis.conf 克隆回话 : 1打开客户端 bin/redis-cli 通过ip访问: 1bin/redis-cli -h 127.0.0.1 -p 6379 默认为localhost 2 集群安装2.1 伪分布主从环境搭建\1.创建redis目录： mkdir redis 2.在redis目录下分别创建3个端口目录： 6380,6381,6382（不在配置文件中写他的目录指定关系，直接在当前目录下执行，持久化目录） 3.当前目录下分别启动３个实例： redis-server –port 6380 redis-server –port 6381 –slaveof 127.0.0.1 6380 redis-server –port 6382 –slaveof 127.0.0.1 6380 主从演示 crud权限，高可用 伪分布哨兵集群搭建： 1 拷贝src下的redis-sentinel至bin目录下：2 启动三个主从redis实例 3 创建哨兵配置文件目录： mkdir sent4 目录下创建启动配置文件病拷贝： vi s1.conf cp s2.conf s3.conf 配置文件内容： port 26380,1,2 sentinel monitor sxt 127.0.0.1 6380 2 5 启动sentinel读取配置文件： redis-sentinel s1.conf s2.conf s3.conf 6 测试：演示自动提备 2.2 主从全分布集群(自)单节点多实例1 删除2.8 bin目录及文件: # cd /opt/sxt/redis # rm -fr bin 2 ftp 上传redis-cluster 目录到根目录 2 redis-cluster目录下解压redis 3.0 : # tar xf redis.....gz 3 redis目录下make命令编译拷贝bin至 /opt/sxt/redis/下 # make &amp;&amp; make PREFIX=/opt/sxt/redis install 成功后会有哨兵显示 4 测试 是否成功 # re+table 5 安装rubby编译环境 # yum -y install ruby rubygems 6 redis-cluster 目录下安装 redis gem 模块: # gem install --local redis-3.3.0.gem 8 创建文件目录、主从节点并匹配端口（已完成）: redis集群 3.x版本 物理节点1个 指定3个主节点端口为7000、7001、7002 对应的3个从节点端口为7003、7004、7005 mkdir cluster-test cd cluster-test mkdir 7000 7001 7002 7003 7004 7005 9 创建配置文件redis.conf（启集群模式: 3.0 支持单机集群，但必须在配置文件中说明) （已完成） 指定不冲突的端口 及 &lt;对应端口号&gt; 文件内容： 声明支持集群模式 指定端口 在7000-7005每个目录中均放入redis.conf redis.conf内容如下： cluster-enabled yes port 700X 11 创建集群，槽位认领 在安装目录下的src中,找到 redis-trib.rb 这是rubby脚本执行程序，完成redis3.0集群创建 12-- 将下方的192.168.80.81替换为自己的IP地址bin/redis-cli --cluster create --cluster-replicas 1 192.168.80.100:7000 192.168.80.100:7001 192.168.80.100:7003 192.168.80.100:7004 192.168.80.100:7005 192.168.80.100:7006 自动分配了主从，自动分配了slots，所有槽都有节点处理，集群上线 启动集群 123456bin/redis-server cluster/7001/7001.confbin/redis-server cluster/7002/7002.confbin/redis-server cluster/7003/7003.conf bin/redis-server cluster/7004/7004.conf bin/redis-server cluster/7005/7005.conf bin/redis-server cluster/7005/7006.conf 登录客户端: 123bin/redis-cli -c -h 192.168.80.100 -p 7001set hello worldget hello 配置Redis集群开机自启动: 1vim /etc/init.d/redisc 将下方脚本写入redisc文件中 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465#!/bin/sh# chkconfig: 2345 80 90## Simple Redis init.d script conceived to work on Linux systems# as it does use of the /proc filesystem.REDISPORT1=7001REDISPORT2=7002REDISPORT3=7003REDISPORT4=7004REDISPORT5=7005REDISPORT6=7006EXEC=/export/servers/redis-5.0.4/bin/redis-serverCLIEXEC=/export/servers/redis-5.0.4/bin/redis-cliPIDFILE=/var/run/redis_$&#123;REDISPORT1&#125;.pidCONF1="/export/servers/redis-5.0.4/cluster/$&#123;REDISPORT1&#125;/$&#123;REDISPORT1&#125;.conf"CONF2="/export/servers/redis-5.0.4/cluster/$&#123;REDISPORT2&#125;/$&#123;REDISPORT2&#125;.conf"CONF3="/export/servers/redis-5.0.4/cluster/$&#123;REDISPORT3&#125;/$&#123;REDISPORT3&#125;.conf"CONF4="/export/servers/redis-5.0.4/cluster/$&#123;REDISPORT4&#125;/$&#123;REDISPORT4&#125;.conf"CONF5="/export/servers/redis-5.0.4/cluster/$&#123;REDISPORT5&#125;/$&#123;REDISPORT5&#125;.conf"CONF6="/export/servers/redis-5.0.4/cluster/$&#123;REDISPORT6&#125;/$&#123;REDISPORT6&#125;.conf"case "$1" in start) if [ -f $PIDFILE ] then echo "$PIDFILE exists, process is already running or crashed" else echo "Starting Redis cluster server..." $EXEC $CONF1 &amp; $EXEC $CONF2 &amp; $EXEC $CONF3 &amp; $EXEC $CONF4 &amp; $EXEC $CONF5 &amp; $EXEC $CONF6 &amp; echo "启动成功..." fi ;; stop) if [ ! -f $PIDFILE ] then echo "$PIDFILE does not exist, process is not running" else PID=$(cat $PIDFILE) echo "Stopping ..." $CLIEXEC -p $REDISPORT1 shutdown $CLIEXEC -p $REDISPORT2 shutdown $CLIEXEC -p $REDISPORT3 shutdown $CLIEXEC -p $REDISPORT4 shutdown $CLIEXEC -p $REDISPORT5 shutdown $CLIEXEC -p $REDISPORT6 shutdown while [ -x /proc/$&#123;PID&#125; ] do echo "Waiting for Redis cluster to shutdown ..." sleep 1 done echo "Redis cluster stopped" fi ;; *) echo "Please use start or stop as first argument" ;;esac 注册为系统服务 1chkconfig --add redisc 以后可以使用命令来控制Redis的启动和关闭 启动：service redisc start 关闭：service redisc stop 配置为开机自启动 1chkconfig redisc on 三操作1 支持类型键 ——————&gt; 值 —-&gt; (字符串,列表,散列(hash),集合,有序集合,HyperLogLog) 2 String类型字符串类型是Redis中最为基础的数据存储类型，它在Redis中是二进制安全的，这便意味着该类型可以接受任何格式的数据，如JPEG图像数据或Json对象描述信息等。在Redis中字符串类型的Value最多可以容纳的数据长度是512M。 2.1 操作l SET key value 设定该Key持有指定的字符串Value，如果该Key已经存在，则覆盖其原有值。返回值：总是返回”OK” l GET key 获取指定Key的Value。如果与该Key关联的Value不是string类型，Redis将返回错误信息，因为GET命令只能用于获取string Value。 返回值：与该Key相关的Value，如果该Key不存在，则返回nil。 l MSET key value [key value …] 该命令原子性的完成参数中所有key/value的设置操作，其具体行为可以看成是多次迭代执行SET命令。 返回值：该命令不会失败，始终返回OK。 l MGET key [key …] 返回所有指定Keys的Values，如果其中某个Key不存在，或者其值不为string类型，该Key的Value将返回nil。 返回值：返回一组指定Keys的Values的列表。 l SETNX key value 如果指定的Key不存在，则设定该Key持有指定字符串Value，此时其效果等价于SET命令。相反，如果该Key已经存在，该命令将不做任何操作并返回。 返回值：1表示设置成功，否则0。 （not exists） l MSETNX key value [key value …] 该命令原子性的完成参数中所有key/value的设置操作，其具体行为可以看成是多次迭代执行SETNX命令。然而这里需要明确说明的是，如果在这一批Keys中有任意一个Key已经存在了，那么该操作将全部回滚，即所有的修改都不会生效。返回值：1表示所有Keys都设置成功，0则表示没有任何Key被修改。 ü APPEND key value 如果该Key已经存在，APPEND命令将参数Value的数据追加到已存在Value的末尾。如果该Key不存在，APPEND命令将会创建一个新的Key/Value。 返回值：追加后Value的长度。 ü DECR key 将指定Key的Value原子性的递减1。如果该Key不存在，其初始值为0，在decr之后其值为-1。如果Value的值不能转换为整型值，如Hello，该操作将执行失败并返回相应的错误信息。。 返回值：递减后的Value值。 l INCR key 将指定Key的Value原子性的递增1。如果该Key不存在，其初始值为0，在incr之后其值为1。如果Value的值不能转换为整型值，如Hello，该操作将执行失败并返回相应的错误信息。 返回值：递增后的Value值。 l DECRBY key decrement 将指定Key的Value原子性的减少decrement。如果该Key不存在，其初始值为0，在decrby之后其值为-decrement。如果Value的值不能转换为整型值，如Hello，该操作将执行失败并返回相应的错误信息。注意：该操作的取值范围是64位有符号整型。 返回值：减少后的Value值。 l INCRBY key increment 将指定Key的Value原子性的增加increment。如果该Key不存在，其初始值为0，在incrby之后其值为increment。如果Value的值不能转换为整型值，如Hello，该操作将执行失败并返回相应的错误信息。注意：该操作的取值范围是64位有符号整型。 返回值：增加后的Value值。 l GETSET key value 原子性的设置该Key为指定的Value，同时返回该Key的原有值。和GET命令一样，该命令也只能处理string Value，否则Redis将给出相关的错误信息。 返回值：返回该Key的原有值，如果该Key之前并不存在，则返回nil。 l STRLEN key 返回指定Key的字符值长度，如果Value不是string类型，Redis将执行失败并给出相关的错误信息。 返回值：指定Key的Value字符长度，如果该Key不存在，返回0。 l SETEX key seconds value 原子性完成两个操作，一是设置该Key的值为指定字符串，同时设置该Key在Redis服务器中的存活时间(秒数)。该命令主要应用于Redis被当做Cache服务器使用时。 （expire） l SETRANGE key offset value 替换指定Key的部分字符串值。从offset开始，替换的长度为该命令第三个参数value的字符串长度，其中如果offset的值大于该Key的原有值Value的字符串长度，Redis将会在Value的后面补齐(offset - strlen(value))数量的0x00，之后再追加新值。如果该键不存在，该命令会将其原值的长度假设为0，并在其后添补offset个0x00后再追加新值。鉴于字符串Value的最大长度为512M，因此offset的最大值为536870911。最后需要注意的是，如果该命令在执行时致使指定Key的原有值长度增加，这将会导致Redis重新分配足够的内存以容纳替换后的全部字符串，因此就会带来一定的性能折损。 返回值：修改后的字符串Value长度。 l GETRANGE key start end 截取字符串。该命令在截取子字符串时，将以闭区间的方式同时包含start(0表示第一个字符)和end所在的字符，如果end值超过Value的字符长度，该命令将只是截取从start开始之后所有的字符数据。 返回值：子字符串； l SETBIT key offset value 设置在指定Offset上BIT的值，该值只能为1或0，在设定后该命令返回该Offset上原有的BIT值。如果指定Key不存在，该命令将创建一个新值，并在指定的Offset上设定参数中的BIT值。如果Offset大于Value的字符长度，Redis将拉长Value值并在指定Offset上设置参数中的BIT值，中间添加的BIT值为0。最后需要说明的是Offset值必须大于0。 返回值：在指定Offset上的BIT原有值。 l GETBIT key offset 返回在指定Offset上BIT的值，0或1。如果Offset超过string value的长度，该命令将返回0，所以对于空字符串始终返回0。 返回值：在指定Offset上的BIT值。 (布隆过滤器) 3 list类型在Redis中，List类型是按照插入顺序排序的字符串链表。和数据结构中的普通链表一样，我们可以在其头部(left)和尾部(right)添加新的元素。在插入时，如果该键并不存在，Redis将为该键创建一个新的链表。与此相反，如果链表中所有的元素均被移除，那么该键也将会被从数据库中删除。List中可以包含的最大元素数量是4294967295。 ​ 从元素插入和删除的效率视角来看，如果我们是在链表的两头插入或删除元素，这将会是非常高效的操作，即使链表中已经存储了百万条记录，该操作也可以在常量时间内完成。然而需要说明的是，如果元素插入或删除操作是作用于链表中间，那将会是非常低效的。 3.1 操作l LPUSH key value [value …] 在指定Key所关联的List Value的头部插入参数中给出的所有Values。如果该Key不存在，该命令将在插入之前创建一个与该Key关联的空链表，之后再将数据从链表的头部插入。如果该键的Value不是链表类型，该命令将返回相关的错误信息。 返回值：插入后链表中元素的数量。 l LPUSHX key value 仅有当参数中指定的Key存在时，该命令才会在其所关联的List Value的头部插入参数中给出的Value，否则将不会有任何操作发生。 返回值：插入后链表中元素的数量。 l LRANGE key start stop 该命令的参数start和end都是0-based。即0表示链表头部(leftmost)的第一个元素。其中start的值也可以为负值，-1将表示链表中的最后一个元素，即尾部元素，-2表示倒数第二个并以此类推。该命令在获取元素时，start和end位置上的元素也会被取出。如果start的值大于链表中元素的数量，空链表将会被返回。如果end的值大于元素的数量，该命令则获取从start(包括start)开始，链表中剩余的所有元素。 返回值：返回指定范围内元素的列表。 l LPOP key 返回并弹出指定Key关联的链表中的第一个元素，即头部元素。如果该Key不存，返回nil。 返回值：链表头部的元素。 l LLEN key 返回指定Key关联的链表中元素的数量，如果该Key不存在，则返回0。如果与该Key关联的Value的类型不是链表，则返回相关的错误信息。 返回值：链表中元素的数量。 l LREM key count value 在指定Key关联的链表中，删除前count个值等于value的元素。如果count大于0，从头向尾遍历并删除，如果count小于0，则从尾向头遍历并删除。 如果count等于0，则删除链表中所有等于value的元素。如果指定的Key不存在，则直接返回0。 返回值：返回被删除的元素数量。 l LSET key index value 设定链表中指定位置的值为新值，其中0表示第一个元素，即头部元素，-1表示尾部元素。如果索引值Index超出了链表中元素的数量范围，该命令将返回相关的错误信息。 (对指定脚标的值进行设置) l LINDEX key index 该命令将返回链表中指定位置(index)的元素，index是0-based，表示头部元素，如果index为-1，表示尾部元素。如果与该Key关联的不是链表，该命令将返回相关的错误信息。 返回值：返回请求的元素，如果index超出范围，则返回nil。 (读出指定脚标的值) l LTRIM key start stop 该命令将仅保留指定范围内的元素，从而保证链接中的元素数量相对恒定。start和stop参数都是0-based，0表示头部元素。和其他命令一样，start和stop也可以为负值，-1表示尾部元素。如果start大于链表的尾部，或start大于stop，该命令不错报错，而是返回一个空的链表，与此同时该Key也将被删除。如果stop大于元素的数量，则保留从start开始剩余的所有元素。 l LINSERT key BEFORE|AFTER pivot value 该命令的功能是在pivot元素的前面或后面插入参数中的元素value。如果Key不存在，该命令将不执行任何操作。如果与Key关联的Value类型不是链表，相关的错误信息将被返回。 返回值：成功插入后链表中元素的数量，如果没有找到pivot，返回-1，如果key不存在，返回0。 (在指定的某个value前或后插入一个新的value) l RPUSH key value [value …] 在指定Key所关联的List Value的尾部插入参数中给出的所有Values。如果该Key不存在，该命令将在插入之前创建一个与该Key关联的空链表，之后再将数据从链表的尾部插入。如果该键的Value不是链表类型，该命令将返回相关的错误信息。 返回值：插入后链表中元素的数量。 l RPUSHX key value 仅有当参数中指定的Key存在时，该命令才会在其所关联的List Value的尾部插入参数中给出的Value，否则将不会有任何操作发生。 返回值：插入后链表中元素的数量。 l RPOP key 返回并弹出指定Key关联的链表中的最后一个元素，即尾部元素，。如果该Key不存，返回nil。 返回值：链表尾部的元素。 l RPOPLPUSH source destination 原子性的从与source键关联的链表尾部弹出一个元素，同时再将弹出的元素插入到与destination键关联的链表的头部。如果source键不存在，该命令将返回nil，同时不再做任何其它的操作了。如果source和destination是同一个键，则相当于原子性的将其关联链表中的尾部元素移到该链表的头部。 返回值：返回弹出和插入的元素。 4 hash类型Redis中的Hashes类型可以看成具有String Key和String Value的map容器。所以该类型非常适合于存储值对象的信息。如用户信息：Username、Password和Age等。每一个Hash可以存储4294967295个键值对。 4.1 操作l HSET key field value 为指定的Key设定Field/Value对，如果Key不存在，该命令将创建新Key以参数中的Field/Value对，如果参数中的Field在该Key中已经存在，则用新值覆盖其原有值。 返回值：1表示新的Field被设置了新值，0表示Field已经存在，用新值覆盖原有值。 l HGET key field 返回指定Key中指定Field的关联值。 返回值：返回参数中Field的关联值，如果参数中的Key或Field不存，返回nil。 l HSETNX key field value 只有当参数中的Key或Field不存在的情况下，为指定的Key设定Field/Value对，否则该命令不会进行任何操作。 返回值：1表示新的Field被设置了新值，0表示Key或Field已经存在，该命令没有进行任何操作。 l HEXISTS key field 判断指定Key中的指定Field是否存在。 返回值：1表示存在，0表示参数中的Field或Key不存在。 l HLEN key 获取该Key所包含的Field的数量。 返回Key包含的Field数量，如果Key不存在，返回0。 l HDEL key field [field …] 从指定Key的Hashes Value中删除参数中指定的多个字段，如果不存在的字段将被忽略。 返回值：如果Key不存在，则将其视为空Hashes，并返回0，否则返回实际删除的Field数量。 l HINCRBY key field increment 增加指定Key中指定Field关联的Value的值。如果Key或Field不存在，该命令将会创建一个新Key或新Field，并将其关联的Value初始化为0，之后再指定数字增加的操作。该命令支持的数字是64位有符号整型，即increment可以负数。返回值：运算后的值。 l HGETALL key 获取该键包含的所有Field/Value。其返回格式为一个Field、一个Value，并以此类推。 返回值：Field/Value的列表。 l HKEYS key 返回指定Key的所有Fields名。 返回值：Field的列表。 l HVALS key 返回指定Key的所有Values名。 返回值：Value的列表。 l HMGET key field [field …] 获取和参数中指定Fields关联的一组Values。如果请求的Field不存在，其值返回nil。如果Key不存在，该命令将其视为空Hash，因此返回一组nil。 返回值：返回和请求Fields关联的一组Values，其返回顺序等同于Fields的请求顺序。 l HMSET key field value [field value …] 逐对依次设置参数中给出的Field/Value对。如果其中某个Field已经存在，则用新值覆盖原有值。如果Key不存在，则创建新Key，同时设定参数中的Field/Value。 5 set类型常见操作在Redis中，我们可以将Set类型看作为没有排序的字符串集合。Set可包含的最大元素数量是4294967295。 ​ Set类型在功能上还存在着一个非常重要的特性，即在服务器端完成多个Sets之间的聚合计算操作，如unions、intersections和differences。由于这些操作均在服务端完成，因此效率极高，而且也节省了大量的网络IO开销。 5.1常见操作l SADD key member [member …] 如果在插入的过程用，参数中有的成员在Set中已经存在，该成员将被忽略，而其它成员仍将会被正常插入。如果执行该命令之前，该Key并不存在，该命令将会创建一个新的Set，此后再将参数中的成员陆续插入。 返回值：本次操作实际插入的成员数量。 l SCARD key 获取Set中成员的数量。 返回值：返回Set中成员的数量，如果该Key并不存在，返回0。 l SISMEMBER key member 判断参数中指定成员是否已经存在于与Key相关联的Set集合中。 返回值：1表示已经存在，0表示不存在，或该Key本身并不存在。 l SMEMBERS key 获取与该Key关联的Set中所有的成员。 返回值：返回Set中所有的成员。 l SPOP key 随机的移除并返回Set中的某一成员。 由于Set中元素的布局不受外部控制，因此无法像List那样确定哪个元素位于Set的头部或者尾部。 返回值：返回移除的成员，如果该Key并不存在，则返回nil。 l SRANDMEMBER key 和SPOP一样，随机的返回Set中的一个成员，不同的是该命令并不会删除返回的成员。 返回值：返回随机位置的成员，如果Key不存在则返回nil。 l SREM key member [member …] 从与Key关联的Set中删除参数中指定的成员，不存在的参数成员将被忽略，如果该Key并不存在，将视为空Set处理。 返回值：从Set中实际移除的成员数量，如果没有则返回0。 l SMOVE source destination member 原子性的将参数中的成员从source键移入到destination键所关联的Set中。如果该成员在source中并不存在，该命令将不会再执行任何操作并返回0，否则，该成员将从source移入到destination。如果此时该成员已经在destination中存在，那么该命令仅是将该成员从source中移出。 返回值：1表示正常移动，0表示source中并不包含参数成员。 l SDIFF key [key …] 返回参数中第一个Key所关联的Set和其后所有Keys所关联的Sets中成员的差异。如果Key不存在，则视为空Set。 返回值：差异结果成员的集合。 l SDIFFSTORE destination key [key …] 该命令和SDIFF命令在功能上完全相同，两者之间唯一的差别是SDIFF返回差异的结果成员，而该命令将差异成员存储在destination关联的Set中。如果destination键已经存在，该操作将覆盖它的成员。 返回值：返回差异成员的数量。 l SINTER key [key …] 该命令将返回参数中所有Keys关联的Sets中成员的交集。因此如果参数中任何一个Key关联的Set为空，或某一Key不存在，那么该命令的结果将为空集。 返回值：交集结果成员的集合。 l SINTERSTORE destination key [key …] 该命令和SINTER命令在功能上完全相同，两者之间唯一的差别是SINTER返回交集的结果成员，而该命令将交集成员存储在destination关联的Set中。如果destination键已经存在，该操作将覆盖它的成员。 返回值：返回交集成员的数量。 l SUNION key [key …] 该命令将返回参数中所有Keys关联的Sets中成员的并集。 返回值：并集结果成员的集合。 l SUNIONSTORE destination key [key …] 该命令和SUNION命令在功能上完全相同，两者之间唯一的差别是SUNION返回并集的结果成员，而该命令将并集成员存储在destination关联的Set中。如果destination键已经存在，该操作将覆盖它的成员。 返回值：返回并集成员的数量。 6 zset (sortedSet)Sorted-Sets和Sets类型极为相似，它们都是字符串的集合，都不允许重复的成员出现在一个Set中。它们之间的主要差别是Sorted-Sets中的每一个成员都会有一个分数(score)与之关联，Redis正是通过分数来为集合中的成员进行从小到大的排序。然而需要额外指出的是，尽管Sorted-Sets中的成员必须是唯一的，但是分数**(score)**却是可以重复的。 ​ 在Sorted-Set中添加、删除或更新一个成员都是非常快速的操作，由于Sorted-Sets中的成员在集合中的位置是有序的，因此，即便是访问位于集合中部的成员也仍然是非常高效的。事实上，Redis所具有的这一特征在很多其它类型的数据库中是很难实现的，换句话说，在该点上要想达到和Redis同样的高效，在其它数据库中进行建模是非常困难的。 6.1 操作l ZADD key score member [score] [member] 添加参数中指定的所有成员及其分数到指定key的Sorted-Set中，在该命令中我们可以指定多组score/member作为参数。如果在添加时参数中的某一成员已经存在，该命令将更新此成员的分数为新值，同时再将该成员基于新值重新排序。如果键不存在，该命令将为该键创建一个新的Sorted-Sets Value，并将score/member对插入其中。 返回值：本次操作实际插入的成员数量。 l ZINCRBY key increment member 该命令将为指定Key中的指定成员增加指定的分数。如果成员不存在，该命令将添加该成员并假设其初始分数为0，此后再将其分数加上increment。如果Key不存，该命令将创建该Key及其关联的Sorted-Sets，并包含参数指定的成员，其分数为increment参数。 返回值：以字符串形式表示的新分数。 l ZCARD key 获取与该Key相关联的Sorted-Sets中包含的成员总数量。 返回值：返回Sorted-Sets中的成员数量，如果该Key不存在，返回0。 l ZCOUNT key min max 该命令用于获取分数**(score)**在min和max之间的成员数量。缺省情况下，min和max表示的范围是闭区间范围，即min &lt;= score &lt;= max内的成员将被返回。然而我们可以通过在min和max的前面添加”(“字符来表示开区间，如(min max表示min &lt; score &lt;= max，而(min (max表示min &lt; score &lt; max。 返回值：分数指定范围内成员的数量。 l ZRANGE key start stop [WITHSCORES] 该命令返回排名在参数start和stop指定范围内的成员，这里start和stop参数都是0-based，即0表示第一个成员，-1表示最后一个成员。如果start大于该Sorted-Set中的最大索引值，或start &gt; stop，此时一个空集合将被返回。如果stop大于最大索引值，该命令将返回从start到集合的最后一个成员。如果命令中带有可选参数WITHSCORES选项，该命令在返回的结果中将包含每个成员的分数值，如value1,score1,value2,score2…。 返回值：返回索引在start和stop之间的成员列表。 l ZREVRANGE key start stop [WITHSCORES] 该命令的功能和ZRANGE基本相同，唯一的差别在于顺序相反。 返回值：返回指定的成员列表。 l ZRANGEBYSCORE key min max [WITHSCORES] [LIMIT offset count] 该命令将返回分数在min和max范围内的成员，即满足表达式min &lt;= score &lt;= max的成员，其中返回的成员是按照其分数从低到高的顺序返回，如果成员具有相同的分数，则按成员的字典顺序返回。可选参数LIMIT用于限制返回成员的数量范围。可选参数offset表示从符合条件的第offset个成员开始返回，同时返回count个成员。 返回值：返回分数在指定范围内的成员列表。 l ZREVRANGEBYSCORE key max min [WITHSCORES] [LIMIT offset count] 该命令除了排序方式是基于从高到低的分数排序之外，其它功能和参数含义均与ZRANGEBYSCORE相同。 返回值：返回分数在指定范围内的成员列表。 l ZRANK key member 该命令将返回参数中指定成员的位置值（按分数由低到高的顺序），其中0表示第一个成员，它是Sorted-Set中分数最低的成员。 返回值：如果该成员存在，则返回它的位置索引值。否则返回nil。 l ZREVRANK key member 该命令的功能和ZRANK基本相同，唯一的差别在于顺序相反 返回值：如果该成员存在，则返回它的位置索引值。否则返回nil。 l ZSCORE key member 获取指定成员的分数。 返回值：如果该成员存在，以字符串的形式返回其分数，否则返回nil。 l ZREM key member [member …] 该命令将移除参数中指定的成员，其中不存在的成员将被忽略。如果与该Key关联的Value不是Sorted-Set，相应的错误信息将被返回。 返回值：实际被删除的成员数量。 l ZREMRANGEBYRANK key start stop 删除索引位置位于start和stop之间的成员，start和stop都是0-based，即0表示分数最低的成员，-1表示最后一个成员，即分数最高的成员。 返回值：被删除的成员数量。 l ZREMRANGEBYSCORE key min max 删除分数在min和max之间的所有成员，即满足表达式min &lt;= score &lt;= max的所有成员。对于min和max参数，可以采用开区间的方式表示，具体规则参照ZCOUNT。 返回值：被删除的成员数量。 7 其他操作7.1 key通用操作l KEYS pattern 获取所有匹配pattern参数的Keys。需要说明的是，在我们的正常操作中应该尽量避免对该命令的调用，因为对于大型数据库而言，该命令是非常耗时的，对Redis服务器的性能打击也是比较大的。pattern支持glob-style的通配符格式，如星号(shift+8)表示任意一个或多个字符，?表示任意字符，[abc] 表示方括号中任意一个字母.匹配模式的键列表 l DEL key [key …] 从数据库删除中参数中指定的keys，如果指定键不存在，则直接忽略。还需要另行指出的是，如果指定的Key关联的数据类型不是String类型，而是List、Set、Hashes和Sorted Set等容器类型，该命令删除每个键的时间复杂度为O(M)，其中M表示容器中元素的数量。而对于String类型的Key，其时间复杂度为O(1)。 返回值：实际被删除的Key数量。 l EXISTS key 判断指定键是否存在。 返回值：1表示存在，0表示不存在。 l MOVE key db 将当前数据库中指定的键Key移动到参数中指定的数据库中。如果该Key在目标数据库中已经存在，或者在当前数据库中并不存在，该命令将不做任何操作并返回0。 返回值：移动成功返回1，否则0。 在redis.conf文件中定义了redis的默认库的数据 我们可以使用select 数值 来进行库的切换: select 10. l RENAME key newkey 为指定的键重新命名，如果参数中的两个Keys的名字相同，或者是源Key不存在，该命令都会返回相关的错误信息。如果newKey已经存在，则直接覆盖。 l RENAMENX key newkey 如果新值不存在，则将参数中的原值修改为新值。其它条件和RENAME一致。 返回值：1表示修改成功，否则0。 l PERSIST key 如果Key存在过期时间，该命令会将其过期时间消除，使该Key不再有超时，而是可以持久化存储。 返回值：1表示Key的过期时间被移除，0表示该Key不存在或没有过期时间。 l EXPIRE key seconds 该命令为参数中指定的Key设定超时的秒数，在超过该时间后，Key被自动的删除。如果该Key在超时之前被修改，与该键关联的超时将被移除。 返回值：1表示超时被设置，0则表示Key不存在，或不能被设置。 l EXPIREAT key timestamp 该命令的逻辑功能和EXPIRE完全相同，唯一的差别是该命令指定的超时时间是绝对时间，而不是相对时间。该时间参数是Unix timestamp格式的，即从1970年1月1日开始所流经的秒数。 返回值：1表示超时被设置，0则表示Key不存在，或不能被设置。 l TTL key 获取该键所剩的超时描述。 返回值：返回所剩描述，如果该键不存在或没有超时设置，则返回-1。 l RANDOMKEY 从当前打开的数据库中随机的返回一个Key。 返回值：返回的随机键，如果该数据库是空的则返回nil。 l TYPE key 获取与参数中指定键关联值的类型，该命令将以字符串的格式返回。 返回值：返回的字符串为string、list、set、hash和zset，如果key不存在返回none。 8 事务Redis 事务可以一次执行多个命令， 并且带有以下两个重要的保证： l 事务是一个单独的隔离操作：事务中的所有命令都会序列化、按顺序地执行。事务在执行的过程中，不会被其他客户端发送来的命令请求所打断。 l 事务是一个原子操作：事务中的命令要么全部被执行，要么全部都不执行。 一个事务从开始到执行会经历以下三个阶段： 开始事务。MULTI 命令入队。 执行事务。EXEC 8.1 常用命令l MULTI Redis Multi 命令用于标记一个事务块的开始。 事务块内的多条命令会按照先后顺序被放进一个队列当中，最后由 EXEC 命令原子性(atomic)地执行。 返回值:总是返回OK l EXEC Redis Exec 命令用于执行所有事务块内的命令 返回值: 事务块内所有命令的返回值，按命令执行的先后顺序排列。 当操作被打断时，返回空值 nil 。 l DISCARD Redis Discard 命令用于取消事务，放弃执行事务块内的所有命令。 返回值: 总是返回 OK 。 l WATCH Redis Watch 命令用于监视一个(或多个) key ，如果在事务执行之前这个(或这些) key 被其他命令所改动，那么事务将被打断 l UNWATCH Redis Unwatch 命令用于取消 WATCH 命令对所有 key 的监视 四redis 的java客户端Jedis使用java操作Redis ​ 在redis.conf配置文件中 bind 127.0.0.1 代表的是外部 不可以访问redis 做法：编辑redis.conf文件，注释掉bind 127.0.0.1 在redis.conf文件中设置密码 编辑redis.conf文件，找到requirepass 默认是被注释的。 打开注释,然后在requirepass后面写上我们要设置的密码。 linux 连接客户端 1bin/redis-cli -h ip -p 6379 -a admin maven依赖 12345678910&lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;2.7.1&lt;/version&gt;&lt;!--版本号可根据实际情况填写--&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-lang3&lt;/artifactId&gt; &lt;version&gt;3.3.2&lt;/version&gt;!--版本号可根据实际情况填写--&gt; &lt;/dependency&gt; 1 String 类型操作123456789101112131415161718192021222324252627282930313233343536373839404142//string操作public class JedisDemo2 &#123; Jedis jedis; @Before public void createJedis() &#123; jedis = new Jedis("192.168.19.128"); // 设置密码 jedis.auth("admin"); &#125; // 演示 set get @Test public void test1() &#123; jedis.set("username", "tom"); String value = jedis.get("username"); System.out.println(value); &#125; //演示mset mget @Test public void test2()&#123; jedis.mset("password","123","age","20"); List&lt;String&gt; values = jedis.mget("username","password","age"); System.out.println(values); &#125; //演示 append setrange getrange @Test public void test3()&#123; //jedis.append("username"," is boy"); //jedis.setrange("username", 7,"girl"); System.out.println(jedis.get("username")); System.out.println(jedis.getrange("username", 7, -1)); &#125;&#125; 2 list1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950//list操作public class JedisDemo3 &#123; Jedis jedis; @Before public void createJedis() &#123; jedis = new Jedis("192.168.19.128"); // 设置密码 jedis.auth("admin"); &#125; // 演示lpush lrange @Test public void test1() &#123; jedis.lpush("names", "tom", "james", "张三", "李四"); List&lt;String&gt; names = jedis.lrange("names", 0, -1); System.out.println(names); &#125; // lset @Test public void test2() &#123; // jedis.lset("names", 1, "王五"); // List&lt;String&gt; names = jedis.lrange("names", 0, -1); // System.out.println(names); String value = jedis.lindex("names", 1); System.out.println(value); &#125; // linsert @Test public void test3() &#123; jedis.linsert("names", LIST_POSITION.BEFORE, "james", "fox"); List&lt;String&gt; names = jedis.lrange("names", 0, -1); System.out.println(names); &#125; // lrem @Test public void test4()&#123; jedis.lrem("names", 1, "tom"); List&lt;String&gt; names = jedis.lrange("names", 0, -1); System.out.println(names); &#125;&#125; 3 hash12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758//hash操作public class JedisDemo4 &#123; Jedis jedis; @Before public void createJedis() &#123; jedis = new Jedis("192.168.19.128"); // 设置密码 jedis.auth("admin"); &#125; // 演示hset hget @Test public void test1() &#123; jedis.hset("user", "username", "tom"); String value = jedis.hget("user", "username"); System.out.println(value); &#125; // 演示hmset hmget @Test public void test2() &#123; Map&lt;String, String&gt; hash = new HashMap&lt;String, String&gt;(); hash.put("password", "123"); hash.put("sex", "male"); jedis.hmset("user", hash); List&lt;String&gt; values = jedis.hmget("user", "username", "password", "sex"); System.out.println(values); &#125; //演示 hgetall hkeys kvals @Test public void test3()&#123; Map&lt;String, String&gt; map = jedis.hgetAll("user"); for(String key:map.keySet())&#123; System.out.println(key+" "+map.get(key)); &#125; Set&lt;String&gt; keys = jedis.hkeys("user"); System.out.println(keys); List&lt;String&gt; values = jedis.hvals("user"); System.out.println(values); &#125; // 演示hdel @Test public void test4()&#123; jedis.hdel("user", "username","password"); Map&lt;String, String&gt; map = jedis.hgetAll("user"); for(String key:map.keySet())&#123; System.out.println(key+" "+map.get(key)); &#125; &#125;&#125; 4 set123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657//set操作public class JedisDemo5 &#123; Jedis jedis; @Before public void createJedis() &#123; jedis = new Jedis("192.168.19.128"); // 设置密码 jedis.auth("admin"); &#125; //演示sadd smembers @Test public void test1()&#123; jedis.sadd("language1","java","c++","ruby","python"); Set&lt;String&gt; smembers = jedis.smembers("language1"); System.out.println(smembers); &#125; //演示srem @Test public void test2()&#123; jedis.srem("language1", "java"); Set&lt;String&gt; smembers = jedis.smembers("language1"); System.out.println(smembers); &#125; //差集 sdiff @Test public void test3()&#123; jedis.sadd("language1","java","c++","ruby","python"); jedis.sadd("language2","ios","c++","c#","android"); Set&lt;String&gt; sdiff = jedis.sdiff("language1","language2"); System.out.println(sdiff); &#125; //交集 @Test public void test4()&#123; jedis.sadd("language1","java","c++","ruby","python"); jedis.sadd("language2","ios","c++","c#","android"); Set&lt;String&gt; sinter = jedis.sinter("language1","language2"); System.out.println(sinter); &#125; //并集 @Test public void test5()&#123; jedis.sadd("language1","java","c++","ruby","python"); jedis.sadd("language2","ios","c++","c#","android"); Set&lt;String&gt; sunion = jedis.sunion("language1","language2"); System.out.println(sunion); &#125; &#125; 5 sortedset123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990//sortedset操作public class JedisDemo6 &#123; Jedis jedis; @Before public void createJedis() &#123; jedis = new Jedis("192.168.19.128"); // 设置密码 jedis.auth("admin"); &#125; // 演示zadd zrange zrangeByScore @Test public void test1() &#123; Map&lt;String, Double&gt; sm = new HashMap&lt;String, Double&gt;(); sm.put("张三", 70.0); sm.put("李四", 80.0); sm.put("王五", 90.0); sm.put("赵六", 60.0); jedis.zadd("zkey", sm); Set&lt;String&gt; set = jedis.zrange("zkey", 0, -1); System.out.println(set); // 根据分数获取 Set&lt;String&gt; set1 = jedis.zrangeByScore("zkey", 70.0, 90.0); System.out.println(set1); &#125; // 获取分数元素 zrangeWithScores @Test public void test2() &#123; Map&lt;String, Double&gt; sm = new HashMap&lt;String, Double&gt;(); sm.put("张三", 70.0); sm.put("李四", 80.0); sm.put("王五", 90.0); sm.put("赵六", 60.0); jedis.zadd("zkey", sm); Set&lt;Tuple&gt; zws = jedis.zrangeWithScores("zkey", 0, -1); for (Tuple t : zws) &#123; System.out.println(t.getScore() + " " + t.getElement()); &#125; &#125; // zrank @Test public void test3() &#123; Map&lt;String, Double&gt; sm = new HashMap&lt;String, Double&gt;(); sm.put("张三", 70.0); sm.put("李四", 80.0); sm.put("王五", 90.0); sm.put("赵六", 60.0); jedis.zadd("zkey", sm); Long num = jedis.zrank("zkey", "赵六"); System.out.println(num); &#125; // zscore @Test public void test4() &#123; Map&lt;String, Double&gt; sm = new HashMap&lt;String, Double&gt;(); sm.put("张三", 70.0); sm.put("李四", 80.0); sm.put("王五", 90.0); sm.put("赵六", 60.0); jedis.zadd("zkey", sm); Double zscore = jedis.zscore("zkey", "张三"); System.out.println(zscore); &#125; // zrem @Test public void test5() &#123; Map&lt;String, Double&gt; sm = new HashMap&lt;String, Double&gt;(); sm.put("张三", 70.0); sm.put("李四", 80.0); sm.put("王五", 90.0); sm.put("赵六", 60.0); jedis.zadd("zkey", sm); jedis.zrem("zkey", "李四"); Set&lt;Tuple&gt; zws = jedis.zrangeWithScores("zkey", 0, -1); for (Tuple t : zws) &#123; System.out.println(t.getScore() + " " + t.getElement()); &#125; &#125;&#125; 6 key的常见操作123456789101112131415161718192021222324252627282930313233343536//key的通用操作public class JedisDemo7 &#123; Jedis jedis; @Before public void createJedis() &#123; jedis = new Jedis("192.168.19.128"); // 设置密码 jedis.auth("admin"); &#125; // keys patten @Test public void test1()&#123; Set&lt;String&gt; keys = jedis.keys("*"); System.out.println(keys); &#125; // del key @Test public void test2()&#123; Long del = jedis.del("user"); System.out.println(del); &#125; //关于key时间设置 @Test public void test3()&#123; //jedis.expire("username", 200); //设置生命周期为200秒 jedis.persist("username"); Long ttl = jedis.ttl("username"); //获取生命周期值 System.out.println(ttl); &#125;&#125; 五 Redis的持久化Redis将内存存储和持久化存储相结合，即可提供数据访问的高效性，又可保证数据存储的安全性 1 Redis数据持久化机制介绍1). RDB持久化： ​ 该机制是指在指定的时间间隔内将内存中的数据集快照写入磁盘。 类似于hdfs的fsimage 2). AOF(append only file)持久化: ​ 该机制将以日志的形式记录服务器所处理的每一个写操作，在Redis服务器启动之初会读取该文件来重新构建数据库，以保证启动后数据库中的数据是完整的。 3). 同时应用AOF和RDB。 4). 无持久化： ​ 可通过配置的方式禁用Redis服务器的持久化功能，这样我们就可以将Redis视为一个功能加强版的memcached了 2 Redis数据持久化配置与测试l RDB快照方式: 是将数据转为二进制进行存储 缺省情况下，Redis会将数据集的快照dump到dump.rdb文件中。此外，我们也可以通过配置文件来修改Redis服务器dump快照的频率，在打开redis.conf文件之后，我们搜索save，可以看到下面的配置信息： Ø save 900 1 #在900秒(15分钟)之后，如果至少有1个key发生变化，则dump内存快照。 Ø save 300 10 #在300秒(5分钟)之后，如果至少有10个key发生变化，则dump内存快照。 Ø save 60 10000 #在60秒(1分钟)之后，如果至少有10000个key发生变化，则dump内存快照。 注意:关于dump.rdb文件存储的位置,它是设置是在redis.conf文件中 dir ./ 这段配置指的是服务器启动时的当前路径。 产生一个RDB: 121 阻塞方式 客户端执行save命令2 非阻塞 bgsave 是一个异步操作 策略: 123自动 按照配置文件满足就执行bgsave 手动 客户端 发起save bgsave 命令 l AOF日志文件方式： 采用追加的方式保存(追加的是写操作的命令) 默认文件为 appendonly.aof 记录所有的写操作命令,在服务启动时使用这些命令可以还原数据库 可以调整aof的持久化策略 ,在服务器出现故障时,不丢失任何数据,也可以丢失一秒的数据,相对于rdb损失小的多 不能保证绝对不丢失数据 aof重写机制 aof文件过大 合并重复的操作,aof会使用尽可能少的命令来记录 重写过程 1 fork一个子进程负责重写aof文件 2 子进程创建一个临时文件写入aof信息 3 父进程会开辟一个内存换冲区 接收新的写明令 4 子进程重写完后,父进程获得一个信号.将父进程接受到的新的写操作有子进程写入临时文件中 5 新文件替代旧文件 注意 如果写操作出现故障导致命令写半截 可以使用Redis-check-aof工具修复 Ø AOF日志持久化机制的开启： 将appendonly no 改为 appendonly yes ​ Ø AOF同步方式的配置： ​ 在Redis的配置文件中存在三种同步方式，它们分别是： ​ appendfsync always #每次有数据修改发生时都会写入AOF文件。 ​ appendfsync everysec #每秒钟同步一次，该策略为AOF的缺省策略。 ​ appendfsync no #从不同步。高效但是数据不会被持久化。 3 RDB与AOF对比总结l RDB存在哪些优势呢？ ​ 1). 数据的备份和恢复非常方便，因为一个数据库只有一个持久化文件 ​ 2). 性能最大化。对于Redis的服务进程而言，在开始持久化时，它唯一需要做的只是fork出子进程，之后再由子进程完成这些持久化的工作，这样就可以极大的避免服务进程执行IO操作了。 ​ 3). 相比于AOF机制，如果数据集很大，RDB的启动效率会更高。 ​ l RDB又存在哪些劣势呢？ ​ 1).系统一旦在定时持久化之前出现宕机现象，此前没有来得及写入磁盘的数据都将丢失。 ​ 2). 由于RDB是通过fork子进程来协助完成数据持久化工作的，因此，如果当数据集较大时，可能会导致整个服务器停止服务几百毫秒，甚至是1秒钟。 l AOF的优势有哪些呢？ 1). 该机制可以带来更高的数据安全性，即数据持久性。Redis中提供了3 种同步策略，即每秒同步、每修改同步和不同步。 2).对日志文件的写入操作采用的是append模式，因此在写入过程中即使出现宕机现象，也不会破坏日志文件中已经存在的内容。 3). 如果日志过大，Redis可以自动启用rewrite机制迅速“瘦身”(也可手动触发aof的rewrite操作，命令： bgrewriteaof) ​ 4). AOF日志格式清晰、易于理解，很容易用AOF日志文件完成数据的重建。 ​ l AOF的劣势有哪些呢？ ​ 1). 对于相同数量的数据集而言，AOF文件通常要大于RDB文件。 ​ 2). 根据同步策略的不同，AOF在运行效率上往往会慢于RDB。总之，每秒同步策略的效率是比较高的，同步禁用策略的效率和RDB一样高效。]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark调优]]></title>
    <url>%2F2018%2F09%2F28%2FSpark%E8%B0%83%E4%BC%98.html</url>
    <content type="text"><![CDATA[一 sparkCore调优一 开发调优1 概述在大数据计算领域，Spark已经成为了越来越流行、越来越受欢迎的计算平台之一。Spark的功能涵盖了大数据领域的离线批处理、SQL类处理、流式/实时计算、机器学习、图计算等各种不同类型的计算操作，应用范围与前景非常广泛。在美团•大众点评，已经有很多同学在各种项目中尝试使用Spark。大多数同学（包括笔者在内），最初开始尝试使用Spark的原因很简单，主要就是为了让大数据计算作业的执行速度更快、性能更高。 然而，通过Spark开发出高性能的大数据计算作业，并不是那么简单的。如果没有对Spark作业进行合理的调优，Spark作业的执行速度可能会很慢，这样就完全体现不出Spark作为一种快速大数据计算引擎的优势来。因此，想要用好Spark，就必须对其进行合理的性能优化。 Spark的性能调优实际上是由很多部分组成的，不是调节几个参数就可以立竿见影提升作业性能的。我们需要根据不同的业务场景以及数据情况，对Spark作业进行综合性的分析，然后进行多个方面的调节和优化，才能获得最佳性能。 笔者根据之前的Spark作业开发经验以及实践积累，总结出了一套Spark作业的性能优化方案。整套方案主要分为开发调优、资源调优、数据倾斜调优、shuffle调优几个部分。开发调优和资源调优是所有Spark作业都需要注意和遵循的一些基本原则，是高性能Spark作业的基础；数据倾斜调优，主要讲解了一套完整的用来解决Spark作业数据倾斜的解决方案；shuffle调优，面向的是对Spark的原理有较深层次掌握和研究的同学，主要讲解了如何对Spark作业的shuffle运行过程以及细节进行调优。 本文作为Spark性能优化指南的基础篇，主要讲解开发调优以及资源调优。 2前言Spark性能优化的第一步，就是要在开发Spark作业的过程中注意和应用一些性能优化的基本原则。开发调优，就是要让大家了解以下一些Spark基本开发原则，包括：RDD lineage设计、算子的合理使用、特殊操作的优化等。在开发过程中，时时刻刻都应该注意以上原则，并将这些原则根据具体的业务以及实际的应用场景，灵活地运用到自己的Spark作业中。 3 调优原则一：避免创建重复的RDD通常来说，我们在开发一个Spark作业时，首先是基于某个数据源（比如Hive表或HDFS文件）创建一个初始的RDD；接着对这个RDD执行某个算子操作，然后得到下一个RDD；以此类推，循环往复，直到计算出最终我们需要的结果。在这个过程中，多个RDD会通过不同的算子操作（比如map、reduce等）串起来，这个“RDD串”，就是RDD lineage，也就是“RDD的血缘关系链”。 我们在开发过程中要注意：对于同一份数据，只应该创建一个RDD，不能创建多个RDD来代表同一份数据。 一些Spark初学者在刚开始开发Spark作业时，或者是有经验的工程师在开发RDD lineage极其冗长的Spark作业时，可能会忘了自己之前对于某一份数据已经创建过一个RDD了，从而导致对于同一份数据，创建了多个RDD。这就意味着，我们的Spark作业会进行多次重复计算来创建多个代表相同数据的RDD，进而增加了作业的性能开销。 一个简单的例子1234567891011121314151617// 需要对名为“hello.txt”的HDFS文件进行一次map操作，再进行一次reduce操作。也就是说，需要对一份数据执行两次算子操作。// 错误的做法：对于同一份数据执行多次算子操作时，创建多个RDD。// 这里执行了两次textFile方法，针对同一个HDFS文件，创建了两个RDD出来，然后分别对每个RDD都执行了一个算子操作。// 这种情况下，Spark需要从HDFS上两次加载hello.txt文件的内容，并创建两个单独的RDD；第二次加载HDFS文件以及创建RDD的性能开销，很明显是白白浪费掉的。val rdd1 = sc.textFile("hdfs://192.168.0.1:9000/hello.txt")rdd1.map(...)val rdd2 = sc.textFile("hdfs://192.168.0.1:9000/hello.txt")rdd2.reduce(...)// 正确的用法：对于一份数据执行多次算子操作时，只使用一个RDD。// 这种写法很明显比上一种写法要好多了，因为我们对于同一份数据只创建了一个RDD，然后对这一个RDD执行了多次算子操作。// 但是要注意到这里为止优化还没有结束，由于rdd1被执行了两次算子操作，第二次执行reduce操作的时候，还会再次从源头处重新计算一次rdd1的数据，因此还是会有重复计算的性能开销。// 要彻底解决这个问题，必须结合“原则三：对多次使用的RDD进行持久化”，才能保证一个RDD被多次使用时只被计算一次。val rdd1 = sc.textFile("hdfs://192.168.0.1:9000/hello.txt")rdd1.map(...)rdd1.reduce(...) 原则二：尽可能复用同一个RDD除了要避免在开发过程中对一份完全相同的数据创建多个RDD之外，在对不同的数据执行算子操作时还要尽可能地复用一个RDD。比如说，有一个RDD的数据格式是key-value类型的，另一个是单value类型的，这两个RDD的value数据是完全一样的。那么此时我们可以只使用key-value类型的那个RDD，因为其中已经包含了另一个的数据。对于类似这种多个RDD的数据有重叠或者包含的情况，我们应该尽量复用一个RDD，这样可以尽可能地减少RDD的数量，从而尽可能减少算子执行的次数。 一个简单的例子1234567891011121314151617181920212223242526// 错误的做法。// 有一个&lt;Long, String&gt;格式的RDD，即rdd1。// 接着由于业务需要，对rdd1执行了一个map操作，创建了一个rdd2，而rdd2中的数据仅仅是rdd1中的value值而已，也就是说，rdd2是rdd1的子集。JavaPairRDD&lt;Long, String&gt; rdd1 = ...JavaRDD&lt;String&gt; rdd2 = rdd1.map(...)// 分别对rdd1和rdd2执行了不同的算子操作。rdd1.reduceByKey(...)rdd2.map(...)// 正确的做法。// 上面这个case中，其实rdd1和rdd2的区别无非就是数据格式不同而已，rdd2的数据完全就是rdd1的子集而已，却创建了两个rdd，并对两个rdd都执行了一次算子操作。// 此时会因为对rdd1执行map算子来创建rdd2，而多执行一次算子操作，进而增加性能开销。// 其实在这种情况下完全可以复用同一个RDD。// 我们可以使用rdd1，既做reduceByKey操作，也做map操作。// 在进行第二个map操作时，只使用每个数据的tuple._2，也就是rdd1中的value值，即可。JavaPairRDD&lt;Long, String&gt; rdd1 = ...rdd1.reduceByKey(...)rdd1.map(tuple._2...)// 第二种方式相较于第一种方式而言，很明显减少了一次rdd2的计算开销。// 但是到这里为止，优化还没有结束，对rdd1我们还是执行了两次算子操作，rdd1实际上还是会被计算两次。// 因此还需要配合“原则三：对多次使用的RDD进行持久化”进行使用，才能保证一个RDD被多次使用时只被计算一次。 原则三：对多次使用的RDD进行持久化当你在Spark代码中多次对一个RDD做了算子操作后，恭喜，你已经实现Spark作业第一步的优化了，也就是尽可能复用RDD。此时就该在这个基础之上，进行第二步优化了，也就是要保证对一个RDD执行多次算子操作时，这个RDD本身仅仅被计算一次。 Spark中对于一个RDD执行多次算子的默认原理是这样的：每次你对一个RDD执行一个算子操作时，都会重新从源头处计算一遍，计算出那个RDD来，然后再对这个RDD执行你的算子操作。这种方式的性能是很差的。 因此对于这种情况，我们的建议是：对多次使用的RDD进行持久化。此时Spark就会根据你的持久化策略，将RDD中的数据保存到内存或者磁盘中。以后每次对这个RDD进行算子操作时，都会直接从内存或磁盘中提取持久化的RDD数据，然后执行算子，而不会从源头处重新计算一遍这个RDD，再执行算子操作。 对多次使用的RDD进行持久化的代码示例1234567891011121314151617// 如果要对一个RDD进行持久化，只要对这个RDD调用cache()和persist()即可。// 正确的做法。// cache()方法表示：使用非序列化的方式将RDD中的数据全部尝试持久化到内存中。// 此时再对rdd1执行两次算子操作时，只有在第一次执行map算子时，才会将这个rdd1从源头处计算一次。// 第二次执行reduce算子时，就会直接从内存中提取数据进行计算，不会重复计算一个rdd。val rdd1 = sc.textFile("hdfs://192.168.0.1:9000/hello.txt").cache()rdd1.map(...)rdd1.reduce(...)// persist()方法表示：手动选择持久化级别，并使用指定的方式进行持久化。// 比如说，StorageLevel.MEMORY_AND_DISK_SER表示，内存充足时优先持久化到内存中，内存不充足时持久化到磁盘文件中。// 而且其中的_SER后缀表示，使用序列化的方式来保存RDD数据，此时RDD中的每个partition都会序列化成一个大的字节数组，然后再持久化到内存或磁盘中。// 序列化的方式可以减少持久化的数据对内存/磁盘的占用量，进而避免内存被持久化数据占用过多，从而发生频繁GC。val rdd1 = sc.textFile("hdfs://192.168.0.1:9000/hello.txt").persist(StorageLevel.MEMORY_AND_DISK_SER)rdd1.map(...)rdd1.reduce(...) 对于persist()方法而言，我们可以根据不同的业务场景选择不同的持久化级别。 Spark的持久化级别 持久化级别 含义解释 MEMORY_ONLY 使用未序列化的Java对象格式，将数据保存在内存中。如果内存不够存放所有的数据，则数据可能就不会进行持久化。那么下次对这个RDD执行算子操作时，那些没有被持久化的数据，需要从源头处重新计算一遍。这是默认的持久化策略，使用cache()方法时，实际就是使用的这种持久化策略。 MEMORY_AND_DISK 使用未序列化的Java对象格式，优先尝试将数据保存在内存中。如果内存不够存放所有的数据，会将数据写入磁盘文件中，下次对这个RDD执行算子时，持久化在磁盘文件中的数据会被读取出来使用。 MEMORY_ONLY_SER 基本含义同MEMORY_ONLY。唯一的区别是，会将RDD中的数据进行序列化，RDD的每个partition会被序列化成一个字节数组。这种方式更加节省内存，从而可以避免持久化的数据占用过多内存导致频繁GC。 MEMORY_AND_DISK_SER 基本含义同MEMORY_AND_DISK。唯一的区别是，会将RDD中的数据进行序列化，RDD的每个partition会被序列化成一个字节数组。这种方式更加节省内存，从而可以避免持久化的数据占用过多内存导致频繁GC。 DISK_ONLY 使用未序列化的Java对象格式，将数据全部写入磁盘文件中。 MEMORY_ONLY_2, MEMORY_AND_DISK_2, 等等. 对于上述任意一种持久化策略，如果加上后缀_2，代表的是将每个持久化的数据，都复制一份副本，并将副本保存到其他节点上。这种基于副本的持久化机制主要用于进行容错。假如某个节点挂掉，节点的内存或磁盘中的持久化数据丢失了，那么后续对RDD计算时还可以使用该数据在其他节点上的副本。如果没有副本的话，就只能将这些数据从源头处重新计算一遍了。 如何选择一种最合适的持久化策略 默认情况下，性能最高的当然是MEMORY_ONLY，但前提是你的内存必须足够足够大，可以绰绰有余地存放下整个RDD的所有数据。因为不进行序列化与反序列化操作，就避免了这部分的性能开销；对这个RDD的后续算子操作，都是基于纯内存中的数据的操作，不需要从磁盘文件中读取数据，性能也很高；而且不需要复制一份数据副本，并远程传送到其他节点上。但是这里必须要注意的是，在实际的生产环境中，恐怕能够直接用这种策略的场景还是有限的，如果RDD中数据比较多时（比如几十亿），直接用这种持久化级别，会导致JVM的OOM内存溢出异常。 如果使用MEMORY_ONLY级别时发生了内存溢出，那么建议尝试使用MEMORY_ONLY_SER级别。该级别会将RDD数据序列化后再保存在内存中，此时每个partition仅仅是一个字节数组而已，大大减少了对象数量，并降低了内存占用。这种级别比MEMORY_ONLY多出来的性能开销，主要就是序列化与反序列化的开销。但是后续算子可以基于纯内存进行操作，因此性能总体还是比较高的。此外，可能发生的问题同上，如果RDD中的数据量过多的话，还是可能会导致OOM内存溢出的异常。 如果纯内存的级别都无法使用，那么建议使用MEMORY_AND_DISK_SER策略，而不是MEMORY_AND_DISK策略。因为既然到了这一步，就说明RDD的数据量很大，内存无法完全放下。序列化后的数据比较少，可以节省内存和磁盘的空间开销。同时该策略会优先尽量尝试将数据缓存在内存中，内存缓存不下才会写入磁盘。 通常不建议使用DISK_ONLY和后缀为_2的级别：因为完全基于磁盘文件进行数据的读写，会导致性能急剧降低，有时还不如重新计算一次所有RDD。后缀为_2的级别，必须将所有数据都复制一份副本，并发送到其他节点上，数据复制以及网络传输会导致较大的性能开销，除非是要求作业的高可用性，否则不建议使用。 原则四：尽量避免使用shuffle类算子如果有可能的话，要尽量避免使用shuffle类算子。因为Spark作业运行过程中，最消耗性能的地方就是shuffle过程。shuffle过程，简单来说，就是将分布在集群中多个节点上的同一个key，拉取到同一个节点上，进行聚合或join等操作。比如reduceByKey、join等算子，都会触发shuffle操作。 shuffle过程中，各个节点上的相同key都会先写入本地磁盘文件中，然后其他节点需要通过网络传输拉取各个节点上的磁盘文件中的相同key。而且相同key都拉取到同一个节点进行聚合操作时，还有可能会因为一个节点上处理的key过多，导致内存不够存放，进而溢写到磁盘文件中。因此在shuffle过程中，可能会发生大量的磁盘文件读写的IO操作，以及数据的网络传输操作。磁盘IO和网络数据传输也是shuffle性能较差的主要原因。 因此在我们的开发过程中，能避免则尽可能避免使用reduceByKey、join、distinct、repartition等会进行shuffle的算子，尽量使用map类的非shuffle算子。这样的话，没有shuffle操作或者仅有较少shuffle操作的Spark作业，可以大大减少性能开销。 Broadcast与map进行join代码示例12345678910111213141516// 传统的join操作会导致shuffle操作。// 因为两个RDD中，相同的key都需要通过网络拉取到一个节点上，由一个task进行join操作。val rdd3 = rdd1.join(rdd2)// Broadcast+map的join操作，不会导致shuffle操作。// 使用Broadcast将一个数据量较小的RDD作为广播变量。val rdd2Data = rdd2.collect()val rdd2DataBroadcast = sc.broadcast(rdd2Data)// 在rdd1.map算子中，可以从rdd2DataBroadcast中，获取rdd2的所有数据。// 然后进行遍历，如果发现rdd2中某条数据的key与rdd1的当前数据的key是相同的，那么就判定可以进行join。// 此时就可以根据自己需要的方式，将rdd1当前数据与rdd2中可以连接的数据，拼接在一起（String或Tuple）。val rdd3 = rdd1.map(rdd2DataBroadcast...)// 注意，以上操作，建议仅仅在rdd2的数据量比较少（比如几百M，或者一两G）的情况下使用。// 因为每个Executor的内存中，都会驻留一份rdd2的全量数据。 原则五：使用map-side预聚合的shuffle操作如果因为业务需要，一定要使用shuffle操作，无法用map类的算子来替代，那么尽量使用可以map-side预聚合的算子。 所谓的map-side预聚合，说的是在每个节点本地对相同的key进行一次聚合操作，类似于MapReduce中的本地combiner。map-side预聚合之后，每个节点本地就只会有一条相同的key，因为多条相同的key都被聚合起来了。其他节点在拉取所有节点上的相同key时，就会大大减少需要拉取的数据数量，从而也就减少了磁盘IO以及网络传输开销。通常来说，在可能的情况下，建议使用reduceByKey或者aggregateByKey算子来替代掉groupByKey算子。因为reduceByKey和aggregateByKey算子都会使用用户自定义的函数对每个节点本地的相同key进行预聚合。而groupByKey算子是不会进行预聚合的，全量的数据会在集群的各个节点之间分发和传输，性能相对来说比较差。 比如如下两幅图，就是典型的例子，分别基于reduceByKey和groupByKey进行单词计数。其中第一张图是groupByKey的原理图，可以看到，没有进行任何本地聚合时，所有数据都会在集群节点之间传输；第二张图是reduceByKey的原理图，可以看到，每个节点本地的相同key数据，都进行了预聚合，然后才传输到其他节点上进行全局聚合。 原则六：使用高性能的算子除了shuffle相关的算子有优化原则之外，其他的算子也都有着相应的优化原则。 使用reduceByKey/aggregateByKey替代groupByKey详情见“原则五：使用map-side预聚合的shuffle操作”。 使用mapPartitions替代普通mapmapPartitions类的算子，一次函数调用会处理一个partition所有的数据，而不是一次函数调用处理一条，性能相对来说会高一些。但是有的时候，使用mapPartitions会出现OOM（内存溢出）的问题。因为单次函数调用就要处理掉一个partition所有的数据，如果内存不够，垃圾回收时是无法回收掉太多对象的，很可能出现OOM异常。所以使用这类操作时要慎重！ 使用foreachPartitions替代foreach原理类似于“使用mapPartitions替代map”，也是一次函数调用处理一个partition的所有数据，而不是一次函数调用处理一条数据。在实践中发现，foreachPartitions类的算子，对性能的提升还是很有帮助的。比如在foreach函数中，将RDD中所有数据写MySQL，那么如果是普通的foreach算子，就会一条数据一条数据地写，每次函数调用可能就会创建一个数据库连接，此时就势必会频繁地创建和销毁数据库连接，性能是非常低下；但是如果用foreachPartitions算子一次性处理一个partition的数据，那么对于每个partition，只要创建一个数据库连接即可，然后执行批量插入操作，此时性能是比较高的。实践中发现，对于1万条左右的数据量写MySQL，性能可以提升30%以上。 使用filter之后进行coalesce操作通常对一个RDD执行filter算子过滤掉RDD中较多数据后（比如30%以上的数据），建议使用coalesce算子，手动减少RDD的partition数量，将RDD中的数据压缩到更少的partition中去。因为filter之后，RDD的每个partition中都会有很多数据被过滤掉，此时如果照常进行后续的计算，其实每个task处理的partition中的数据量并不是很多，有一点资源浪费，而且此时处理的task越多，可能速度反而越慢。因此用coalesce减少partition数量，将RDD中的数据压缩到更少的partition之后，只要使用更少的task即可处理完所有的partition。在某些场景下，对于性能的提升会有一定的帮助。 使用repartitionAndSortWithinPartitions替代repartition与sort类操作repartitionAndSortWithinPartitions是Spark官网推荐的一个算子，官方建议，如果需要在repartition重分区之后，还要进行排序，建议直接使用repartitionAndSortWithinPartitions算子。因为该算子可以一边进行重分区的shuffle操作，一边进行排序。shuffle与sort两个操作同时进行，比先shuffle再sort来说，性能可能是要高的。 原则七：广播大变量有时在开发过程中，会遇到需要在算子函数中使用外部变量的场景（尤其是大变量，比如100M以上的大集合），那么此时就应该使用Spark的广播（Broadcast）功能来提升性能。 在算子函数中使用到外部变量时，默认情况下，Spark会将该变量复制多个副本，通过网络传输到task中，此时每个task都有一个变量副本。如果变量本身比较大的话（比如100M，甚至1G），那么大量的变量副本在网络中传输的性能开销，以及在各个节点的Executor中占用过多内存导致的频繁GC，都会极大地影响性能。 因此对于上述情况，如果使用的外部变量比较大，建议使用Spark的广播功能，对该变量进行广播。广播后的变量，会保证每个Executor的内存中，只驻留一份变量副本，而Executor中的task执行时共享该Executor中的那份变量副本。这样的话，可以大大减少变量副本的数量，从而减少网络传输的性能开销，并减少对Executor内存的占用开销，降低GC的频率。 广播大变量的代码示例123456789101112// 以下代码在算子函数中，使用了外部的变量。// 此时没有做任何特殊操作，每个task都会有一份list1的副本。val list1 = ...rdd1.map(list1...)// 以下代码将list1封装成了Broadcast类型的广播变量。// 在算子函数中，使用广播变量时，首先会判断当前task所在Executor内存中，是否有变量副本。// 如果有则直接使用；如果没有则从Driver或者其他Executor节点上远程拉取一份放到本地Executor内存中。// 每个Executor内存中，就只会驻留一份广播变量副本。val list1 = ...val list1Broadcast = sc.broadcast(list1)rdd1.map(list1Broadcast...) 原则八：使用Kryo优化序列化性能在Spark中，主要有三个地方涉及到了序列化： 在算子函数中使用到外部变量时，该变量会被序列化后进行网络传输（见“原则七：广播大变量”中的讲解）。 将自定义的类型作为RDD的泛型类型时（比如JavaRDD，Student是自定义类型），所有自定义类型对象，都会进行序列化。因此这种情况下，也要求自定义的类必须实现Serializable接口。 使用可序列化的持久化策略时（比如MEMORY_ONLY_SER），Spark会将RDD中的每个partition都序列化成一个大的字节数组。 对于这三种出现序列化的地方，我们都可以通过使用Kryo序列化类库，来优化序列化和反序列化的性能。Spark默认使用的是Java的序列化机制，也就是ObjectOutputStream/ObjectInputStream API来进行序列化和反序列化。但是Spark同时支持使用Kryo序列化库，Kryo序列化类库的性能比Java序列化类库的性能要高很多。官方介绍，Kryo序列化机制比Java序列化机制，性能高10倍左右。Spark之所以默认没有使用Kryo作为序列化类库，是因为Kryo要求最好要注册所有需要进行序列化的自定义类型，因此对于开发者来说，这种方式比较麻烦。 以下是使用Kryo的代码示例，我们只要设置序列化类，再注册要序列化的自定义类型即可（比如算子函数中使用到的外部变量类型、作为RDD泛型类型的自定义类型等）： 123456// 创建SparkConf对象。val conf = new SparkConf().setMaster(...).setAppName(...)// 设置序列化器为KryoSerializer。conf.set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")// 注册要序列化的自定义类型。conf.registerKryoClasses(Array(classOf[MyClass1], classOf[MyClass2])) 原则九：优化数据结构Java中，有三种类型比较耗费内存： 对象，每个Java对象都有对象头、引用等额外的信息，因此比较占用内存空间。 字符串，每个字符串内部都有一个字符数组以及长度等额外信息。 集合类型，比如HashMap、LinkedList等，因为集合类型内部通常会使用一些内部类来封装集合元素，比如Map.Entry。 因此Spark官方建议，在Spark编码实现中，特别是对于算子函数中的代码，尽量不要使用上述三种数据结构，尽量使用字符串替代对象，使用原始类型（比如Int、Long）替代字符串，使用数组替代集合类型，这样尽可能地减少内存占用，从而降低GC频率，提升性能。 但是在笔者的编码实践中发现，要做到该原则其实并不容易。因为我们同时要考虑到代码的可维护性，如果一个代码中，完全没有任何对象抽象，全部是字符串拼接的方式，那么对于后续的代码维护和修改，无疑是一场巨大的灾难。同理，如果所有操作都基于数组实现，而不使用HashMap、LinkedList等集合类型，那么对于我们的编码难度以及代码可维护性，也是一个极大的挑战。因此笔者建议，在可能以及合适的情况下，使用占用内存较少的数据结构，但是前提是要保证代码的可维护性。 原则十：Data Locality本地化级别PROCESS_LOCAL：进程本地化，代码和数据在同一个进程中，也就是在同一个executor中；计算数据的task由executor执行，数据在executor的BlockManager中；性能最好 NODE_LOCAL：节点本地化，代码和数据在同一个节点中；比如说，数据作为一个HDFS block块，就在节点上，而task在节点上某个executor中运行；或者是，数据和task在一个节点上的不同executor中；数据需要在进程间进行传输NO_PREF：对于task来说，数据从哪里获取都一样，没有好坏之分RACK_LOCAL：机架本地化，数据和task在一个机架的两个节点上；数据需要通过网络在节点之间进行传输ANY：数据和task可能在集群中的任何地方，而且不在一个机架中，性能最差 spark.locality.wait，默认是3s Spark在Driver上，对Application的每一个stage的task，进行分配之前，都会计算出每个task要计算的是哪个分片数据，RDD的某个partition；Spark的task分配算法，优先，会希望每个task正好分配到它要计算的数据所在的节点，这样的话，就不用在网络间传输数据； 但是可能task没有机会分配到它的数据所在的节点，因为可能那个节点的计算资源和计算能力都满了；所以呢，这种时候，通常来说，Spark会等待一段时间，默认情况下是3s钟（不是绝对的，还有很多种情况，对不同的本地化级别，都会去等待），到最后，实在是等待不了了，就会选择一个比较差的本地化级别，比如说，将task分配到靠它要计算的数据所在节点，比较近的一个节点，然后进行计算。 但是对于第二种情况，通常来说，肯定是要发生数据传输，task会通过其所在节点的BlockManager来获取数据，BlockManager发现自己本地没有数据，会通过一个getRemote()方法，通过TransferService（网络数据传输组件）从数据所在节点的BlockManager中，获取数据，通过网络传输回task所在节点。 对于我们来说，当然不希望是类似于第二种情况的了。最好的，当然是task和数据在一个节点上，直接从本地executor的BlockManager中获取数据，纯内存，或者带一点磁盘IO；如果要通过网络传输数据的话，那么实在是，性能肯定会下降的，大量网络传输，以及磁盘IO，都是性能的杀手。 什么时候要调节这个参数？ 观察日志，spark作业的运行日志，推荐大家在测试的时候，先用client模式，在本地就直接可以看到比较全的日志。日志里面会显示，starting task。。。，PROCESS LOCAL、NODE LOCAL，观察大部分task的数据本地化级别。 如果大多都是PROCESS_LOCAL，那就不用调节了如果是发现，好多的级别都是NODE_LOCAL、ANY，那么最好就去调节一下数据本地化的等待时长调节完，应该是要反复调节，每次调节完以后，再来运行，观察日志看看大部分的task的本地化级别有没有提升；看看，整个spark作业的运行时间有没有缩短 但是注意别本末倒置，本地化级别倒是提升了，但是因为大量的等待时长，spark作业的运行时间反而增加了，那就还是不要调节了。 spark.locality.wait，默认是3s；可以改成6s，10s 默认情况下，下面3个的等待时长，都是跟上面那个是一样的，都是3s 123spark.locality.wait.process//建议60sspark.locality.wait.node//建议30sspark.locality.wait.rack//建议20s]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[负载均衡]]></title>
    <url>%2F2018%2F07%2F15%2F%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1.html</url>
    <content type="text"><![CDATA[一 网络浏览器端口数 65535 网页服务器端口固定80 linux 命令 netstat -natp 查看端口IP占用情况 n把ip转为逻辑名称 a 是所有 p 是打印 TCP/IP协议 OSI七层模型 1 应用层 7 NGINX七层 (是封装的协议 若做具体的事 一层一层向下 然后一层一层的阻塞) 分层意义解耦 http SMTP ssh 2 表示层6 3 会话层5 4 传输控制层4 (只是控制但不管 数据的发送) lvs 四层 内核 TCP 三次握手 &gt; 传输数据 &gt; 四次分手 这个过程是基于 IP :prot 到 IP: prot的 具有唯一性 1234567891011为什么有三次握手 A 客户端 B 服务器流程 A向B 发送连接包(sync) B 收到 后 向A 发送 我已收到连接(sync+ack)A 收到后 向B 发送确认包 (ack) 因为连接是双向的 若是两次 只有A 能确认 我发出去的B一定能接到 而 B 只是确定我发出去了 但不确定 A 收到没有 所以有了第三次四次分手客户端 向服务器发送 断开连接(fin) 服务器收到后 回复确认(只是一个确认 不是我也想断开) 然后服务器又再去向客户端发送 断开连接(fin) 客户端收到后 回复 确认因为断开 必须 双方都同意才能断开若是两台服务器b,c 三次握手 a向b 发送 连接 b回复 然后 a向c 发送确认 则B 服务器则会一直等待 会出问题 tcp (面向连接的可靠的协议) udp (不可靠的不是面向连接的协议) socket 5 网络层 3 IP ICmp linux上查看 路由表 route -n 这个表是 网络层的核心 123若是ping www.baidu.com 的包是哪个网卡发的 会拿着www.baidu.com解析的IP 与上表中的 子网掩码 一个一个按位与 余出来的值与Destination(网络号) 的值相同 则由这个 网卡发 最终基本都由 0.0.0.0 发 所以0.0.0.0 很特殊 是一个默认网关的子网掩码和网络号 1234tcp/ip 协议是基于一跳一跳(route)传输的 就是我只管发出 而不管 你下一个发给谁 只要最终到达目的地即可如 三次握手 第一次我发出信息 会通过路由表 解析出谁发 就是找出路由出口 如家里局域网上网 路由器就是下一跳 与路由器的距离就是吓一跳 所有发出的信息网关就是吓一跳 子网掩码 ip与子网掩码 按位与得到是网络号 网关 三次握手的每一次发送 都会被这一层 阻塞 6 链路层2 以太网 Ethernet : MAC (物理地址) ARP 协议 地址解析协议 有一个ARP表 linux 查看 arp -a 这个地址 是自己电脑的地址 at 物理地址 在ping www.baidu.com 之后 会多出一个 网关地址 at MAC 12当一个 计算机A想 请求另外一个节点的硬件地址的时候 他会封一个arp包 值是全F 发给路由器 路由器会把他的物理地址学习 并把这个包进行广播 则目标节点 会把自己的物理地址 传给路由 路由把地址转发给A 最后路由也学了 两台不同的地址 7 物理层1 总结 最终七层过后这个信封要填满: 12应用层发 请求 被阻塞 因为传输控制层要建立连接 产生第一个发送的包 但不能发出去 因为网络层 要知道下一跳是谁去路由表 找到下一跳 之后 但不知道 对方的MAC地址 所以来到链路层 1整个互联网是建立在下一跳的模式下 IP是逻辑的两个端点 Mac是物理上连接的两个节点 在每一跳的时候会修改源Mac地址 和目标Mac地址 2端点间 tcp 传输过程 确认机制 状态机制 不可分割 三次握手和 四次分手是不可分割的 3 解析数据包是需要成本的 交换机 两层 只关心Mac地址 ​ 学习机制 路由器 三层 只关心IP的路由表 LVS服务器 : 在四层 只关心PORT ,状态 并不需要和客户端进行握手连接 他只需要拿到四层的状态取查看状态 快速转发出去即可 四层可以很快的进行负载 效率高 但是瞎子负载 不知道客户端请求的是啥 因为是基于包的 不是基于面向连接的 不能根据资源进行很好的负载 NGINX : 七层 关心socket 对应关系 是必须先和客户端进行三次握手开辟资源之后才能做负载的 速度慢 但可以知道请求的是什么 可以做更好的资源转发 最终 可以四层 和七层联合使用 先使用 四层 承受大量的并发 在四层后面 放大量的七层 然后经七层 转发给不同的服务器 淘宝就是这样做的 二 LVS负载均衡1 四层拓补图 lvs不做三次握手和四次分手 他只是转发消息到其他的服务器 并且保证 三次握手和四次分手 都是针对同一个服务器 且当第一次 三四 和第二次三四 请求的资源是一样的 就是页面是一样的 (只针对静态资源) 就是lvs服务器后面的服务器都是镜像部署 上网流程分析 公网IP 如百度等 在互联网中是惟一的 私网IP 如局域网中的都是 私网IP发出的消息 是不会在公网IP中进行下一跳 12路由器会有一个公网IP 比如局域网中 192.168.80.12:21214 发送请求到百度 在路由器(18.11.18.22)中 会将对这个用户产生一个端口 最终(18.11.18.22:223 192.168.80.12:21214) 去发送消息给百度 这个过程是S_NET 地址转换 S_net 是修改源地址的IP D_NET 是修改目标的IP地址]]></content>
      <categories>
        <category>高并发与负载均衡</category>
      </categories>
      <tags>
        <tag>高并发与负载均衡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kettle]]></title>
    <url>%2F2018%2F07%2F01%2FKettle.html</url>
    <content type="text"><![CDATA[一 概述数据仓库模型]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>BigData</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[StructuredStreaming]]></title>
    <url>%2F2018%2F05%2F10%2FStructuredStreaming.html</url>
    <content type="text"><![CDATA[一 概述Structured Streaming 是 Spark Streaming 的进化版, 如果了解了 Spark 的各方面的进化过程, 有助于理解 Structured Streaming 的使命和作用 1 Spark 编程模型的进化过程过程 编程模型 RDD 的优点和缺陷 2011 编程模型 DataFrame 的优点和缺陷 2013 编程模型 Dataset 的优点和缺陷 2015 RDD 的优点 面向对象的操作方式 可以处理任何类型的数据 RDD 的缺点 运行速度比较慢, 执行过程没有优化 API 比较僵硬, 对结构化数据的访问和操作没有优化 DataFrame 的优点 针对结构化数据高度优化, 可以通过列名访问和转换数据 增加 Catalyst 优化器, 执行过程是优化的, 避免了因为开发者的原因影响效率 DataFrame 的缺点 只能操作结构化数据 只有无类型的 API, 也就是只能针对列和 SQL 操作数据, API 依然僵硬 Dataset 的优点 结合了 RDD 和 DataFrame 的 API, 既可以操作结构化数据, 也可以操作非结构化数据 既有有类型的 API 也有无类型的 API, 灵活选择 2 Spark 的 序列化 的进化过程Spark 中的序列化过程决定了数据如何存储, 是性能优化一个非常重要的着眼点, Spark 的进化并不只是针对编程模型提供的 API, 在大数据处理中, 也必须要考虑性能 过程 序列化和反序列化是什么 Spark 中什么地方用到序列化和反序列化 RDD 的序列化和反序列化如何实现 Dataset 的序列化和反序列化如何实现 1在java中序列化12345678910111213141516171819202122public class JavaSerializable implements Serializable &#123; NonSerializable ns = new NonSerializable();&#125;public class NonSerializable &#123;&#125;public static void main(String[] args) throws IOException &#123; // 序列化 JavaSerializable serializable = new JavaSerializable(); ObjectOutputStream objectOutputStream = new ObjectOutputStream(new FileOutputStream("/tmp/obj.ser")); // 这里会抛出一个 "java.io.NotSerializableException: cn.itcast.NonSerializable" 异常 objectOutputStream.writeObject(serializable); objectOutputStream.flush(); objectOutputStream.close(); // 反序列化 FileInputStream fileInputStream = new FileInputStream("/tmp/obj.ser"); ObjectInputStream objectOutputStream = new ObjectInputStream(fileInputStream); JavaSerializable serializable1 = objectOutputStream.readObject();&#125; 序列化是什么 序列化的作用就是可以将对象的内容变成二进制, 存入文件中保存 反序列化指的是将保存下来的二进制对象数据恢复成对象 序列化对对象的要求 对象必须实现 Serializable 接口 对象中的所有属性必须都要可以被序列化, 如果出现无法被序列化的属性, 则序列化失败 限制 对象被序列化后, 生成的二进制文件中, 包含了很多环境信息, 如对象头, 对象中的属性字段等, 所以内容相对较大 因为数据量大, 所以序列化和反序列化的过程比较慢 序列化的应用场景 持久化对象数据 网络中不能传输 Java 对象, 只能将其序列化后传输二进制数据 2 在 Spark 中的序列化和反序列化的应用场景1Task 分发 Task 是一个对象, 想在网络中传输对象就必须要先序列化 2RDD 缓存 RDD 中处理的是对象, 例如说字符串, Person 对象等 如果缓存 RDD 中的数据, 就需要缓存这些对象 对象是不能存在文件中的, 必须要将对象序列化后, 将二进制数据存入文件 3广播变量 广播变量会分发到不同的机器上, 这个过程中需要使用网络, 对象在网络中传输就必须先被序列化 4Shuffle 过程 Shuffle 过程是由 Reducer 从 Mapper 中拉取数据, 这里面涉及到两个需要序列化对象的原因 RDD 中的数据对象需要在 Mapper 端落盘缓存, 等待拉取 Mapper 和 Reducer 要传输数据对象 5Spark Streaming 的 Receiver Spark Streaming 中获取数据的组件叫做 Receiver, 获取到的数据也是对象形式, 在获取到以后需要落盘暂存, 就需要对数据对象进行序列化 6算子引用外部对象 12345class Unserializable(i: Int)rdd.map(i =&gt; new Unserializable(i)) .collect .foreach(println) 在 Map 算子的函数中, 传入了一个 Unserializable 的对象 Map 算子的函数是会在整个集群中运行的, 那 Unserializable 对象就需要跟随 Map 算子的函数被传输到不同的节点上 如果 Unserializable 不能被序列化, 则会报错 3 rdd的序列化RDD 的序列化 RDD 的序列化只能使用 Java 序列化器, 或者 Kryo 序列化器 为什么? RDD 中存放的是数据对象, 要保留所有的数据就必须要对对象的元信息进行保存, 例如对象头之类的 保存一整个对象, 内存占用和效率会比较低一些 Kryo 是什么 Kryo 是 Spark 引入的一个外部的序列化工具, 可以增快 RDD 的运行速度 因为 Kryo 序列化后的对象更小, 序列化和反序列化的速度非常快 在 RDD 中使用 Kryo 的过程如下 12345678910val conf = new SparkConf() .setMaster("local[2]") .setAppName("KyroTest")conf.set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")conf.registerKryoClasses(Array(classOf[Person]))val sc = new SparkContext(conf)rdd.map(arr =&gt; Person(arr(0), arr(1), arr(2))) 4 DataFrame 和 Dataset 中的序列化RDD 中无法感知数据的组成, 无法感知数据结构, 只能以对象的形式处理数据 1DataFrame 和 Dataset 的特点 DataFrame 和 Dataset 是为结构化数据优化的 在 DataFrame 和 Dataset 中, 数据和数据的 Schema 是分开存储的 DataFrame 中没有数据对象这个概念, 所有的数据都以行的形式存在于 Row 对象中, Row 中记录了每行数据的结构, 包括列名, 类型等 Dataset 中上层可以提供有类型的 API, 用以操作数据, 但是在内部, 无论是什么类型的数据对象 Dataset 都使用一个叫做 InternalRow 的类型的对象存储数据 5 优化1 元信息独立 RDD 不保存数据的元信息, 所以只能使用 Java Serializer 或者 Kyro Serializer 保存 整个对象 DataFrame 和 Dataset 中保存了数据的元信息, 所以可以把元信息独立出来分开保存 3 一个 DataFrame 或者一个 Dataset 中, 元信息只需要保存一份, 序列化的时候, 元信息不需要参与 4在反序列化 ( InternalRow → Object ) 时加入 Schema 信息即可 元信息不再参与序列化, 意味着数据存储量的减少, 和效率的增加 2 使用堆外内存 DataFrame 和 Dataset 不再序列化元信息, 所以内存使用大大减少. 同时新的序列化方式还将数据存入堆外内存中, 从而避免 GC 的开销. 堆外内存又叫做 Unsafe, 之所以叫不安全的, 因为不能使用 Java 的垃圾回收机制, 需要自己负责对象的创建和回收, 性能很好, 但是不建议普通开发者使用, 毕竟不安全 6总结 当需要将对象缓存下来的时候, 或者在网络中传输的时候, 要把对象转成二进制, 在使用的时候再将二进制转为对象, 这个过程叫做序列化和反序列化 在 Spark 中有很多场景需要存储对象, 或者在网络中传输对象 Task 分发的时候, 需要将任务序列化, 分发到不同的 Executor 中执行 缓存 RDD 的时候, 需要保存 RDD 中的数据 广播变量的时候, 需要将变量序列化, 在集群中广播 RDD 的 Shuffle 过程中 Map 和 Reducer 之间需要交换数据 算子中如果引入了外部的变量, 这个外部的变量也需要被序列化 RDD 因为不保留数据的元信息, 所以必须要序列化整个对象, 常见的方式是 Java 的序列化器, 和 Kyro 序列化器 Dataset 和 DataFrame 中保留数据的元信息, 所以可以不再使用 Java 的序列化器和 Kyro 序列化器, 使用 Spark 特有的序列化协议, 生成 UnsafeInternalRow 用以保存数据, 这样不仅能减少数据量, 也能减少序列化和反序列化的开销, 其速度大概能达到 RDD 的序列化的 20 倍左右 3 Spark Streaming 和 Structured Streaming1 Spark Streaming 时代Spark Streaming 其实就是 RDD 的 API 的流式工具, 其本质还是 RDD, 存储和执行过程依然类似 RDD 2 Structured Streaming 时代Structured Streaming 其实就是 Dataset 的 API 的流式工具, API 和 Dataset 保持高度一致 3 Spark Streaming 和 Structured Streaming Structured Streaming 相比于 Spark Streaming 的进步就类似于 Dataset 相比于 RDD 的进步 另外还有一点, Structured Streaming 已经支持了连续流模型, 也就是类似于 Flink 那样的实时流, 而不是小批量, 但在使用的时候仍然有限制, 大部分情况还是应该采用小批量模式 在 2.2.0 以后 Structured Streaming 被标注为稳定版本, 意味着以后的 Spark 流式开发不应该在采用 Spark Streaming了 二 小试牛刀需求 编写一个流式计算的应用, 不断的接收外部系统的消息 对消息中的单词进行词频统计 统计全局的结果 Socket Server(Netcat nc) 等待 Structured Streaming 程序连接 Structured Streaming 程序启动, 连接 Socket Server, 等待 Socket Server 发送数据 Socket Server 发送数据, Structured Streaming 程序接收数据 Structured Streaming 程序接收到数据后处理数据 数据处理后, 生成对应的结果集, 在控制台打印 pom 文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147&lt;properties&gt; &lt;scala.version&gt;2.11.8&lt;/scala.version&gt; &lt;spark.version&gt;2.2.0&lt;/spark.version&gt; &lt;slf4j.version&gt;1.7.16&lt;/slf4j.version&gt; &lt;log4j.version&gt;1.2.17&lt;/log4j.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-library&lt;/artifactId&gt; &lt;version&gt;$&#123;scala.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.47&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming_2.11&lt;/artifactId&gt; &lt;version&gt;2.2.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql-kafka-0-10_2.11&lt;/artifactId&gt; &lt;version&gt;2.2.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-hive_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;2.6.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;jcl-over-slf4j&lt;/artifactId&gt; &lt;version&gt;$&#123;slf4j.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;version&gt;$&#123;slf4j.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;version&gt;$&#123;slf4j.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;$&#123;log4j.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.10&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;sourceDirectory&gt;src/main/scala&lt;/sourceDirectory&gt; &lt;testSourceDirectory&gt;src/test/scala&lt;/testSourceDirectory&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.0&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;net.alchim31.maven&lt;/groupId&gt; &lt;artifactId&gt;scala-maven-plugin&lt;/artifactId&gt; &lt;version&gt;3.2.0&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;compile&lt;/goal&gt; &lt;goal&gt;testCompile&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;args&gt; &lt;arg&gt;-dependencyfile&lt;/arg&gt; &lt;arg&gt;$&#123;project.build.directory&#125;/.scala_dependencies&lt;/arg&gt; &lt;/args&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;!--打包插件 因为默认的打包不包含maven的依赖--&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt; &lt;version&gt;3.1.1&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;shade&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;filters&gt; &lt;filter&gt; &lt;artifact&gt;*:*&lt;/artifact&gt; &lt;excludes&gt; &lt;exclude&gt;META-INF/*.SF&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.DSA&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.RSA&lt;/exclude&gt; &lt;/excludes&gt; &lt;/filter&gt; &lt;/filters&gt; &lt;transformers&gt; &lt;transformer implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer"&gt; &lt;mainClass&gt;&lt;/mainClass&gt; &lt;/transformer&gt; &lt;/transformers&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; 123456789101112131415161718192021222324252627282930313233@Testdef rmDemo(): Unit = &#123; //创建SparkSession val session: SparkSession = SparkSession.builder() .master("local[3]") .appName("demo") .getOrCreate() //调整 Log 级别, 避免过多的 Log 影响视线 session.sparkContext.setLogLevel("error") import session.implicits._ //读取外部数据源 转为dataset val source: Dataset[String] = session.readStream .format("socket") .option("host", "node03") .option("port", 9999) .load() .as[String] //默认 readStream 会返回 DataFrame, 但是词频统计更适合使用 Dataset 的有类型 API //统计词频 val wc: Dataset[(String, Long)] = source.flatMap(_.split(" ")) .map((_, 1)) .groupByKey(_._1) .count() //输出结果 wc.writeStream .outputMode(OutputMode.Complete()) // 统计全局结果, 而不是一个批次 .format("console") // 将结果输出到控制台 .start() // 开始运行流式应用 .awaitTermination() // 阻塞主线程, 在子线程中不断获取数据&#125; 总结 Structured Streaming 中的编程步骤依然是先读, 后处理, 最后落地 Structured Streaming 中的编程模型依然是 DataFrame 和 Dataset Structured Streaming 中依然是有外部数据源读写框架的, 叫做 readStream 和 writeStream Structured Streaming 和 SparkSQL 几乎没有区别, 唯一的区别是, readStream 读出来的是流, writeStream 是将流输出, 而 SparkSQL 中的批处理使用 read 和 write 运行 在虚拟机 中运行 nc -lk 9999 程序本地运行 从结果集中可以观察到以下内容 Structured Streaming 依然是小批量的流处理 Structured Streaming 的输出是类似 DataFrame 的, 也具有 Schema, 所以也是针对结构化数据进行优化的 从输出的时间特点上来看, 是一个批次先开始, 然后收集数据, 再进行展示, 这一点和 Spark Streaming 不太一样 Structured Streaming 的 API 和运行也是针对结构化数据进行优化过的 三Stuctured Streaming 的体系和结构1 无限扩展的表格Structured Streaming 是一个复杂的体系, 由很多组件组成, 这些组件之间也会进行交互, 如果无法站在整体视角去观察这些组件之间的关系, 也无法理解 Structured Streaming 的全局 1Dataset 和流式计算可以理解为 Spark 中的 Dataset 有两种, 一种是处理静态批量数据的 Dataset, 一种是处理动态实时流的 Dataset, 这两种 Dataset 之间的区别如下 流式的 Dataset 使用 readStream 读取外部数据源创建, 使用 writeStream 写入外部存储 批式的 Dataset 使用 read 读取外部数据源创建, 使用 write 写入外部存储 2如何使用 Dataset 这个编程模型表示流式计算? 可以把流式的数据想象成一个不断增长, 无限无界的表 无论是否有界, 全都使用 Dataset 这一套 API 通过这样的做法, 就能完全保证流和批的处理使用完全相同的代码, 减少这两种处理方式的差异 3WordCount 的原理 整个计算过程大致上分为如下三个部分 Source, 读取数据源 Query, 在流式数据上的查询 Result, 结果集生成 整个的过程如下 随着时间段的流动, 对外部数据进行批次的划分 在逻辑上, 将缓存所有的数据, 生成一张无限扩展的表, 在这张表上进行查询 根据要生成的结果类型, 来选择是否生成基于整个数据集的结果 4总结 Dataset 不仅可以表达流式数据的处理, 也可以表达批量数据的处理 Dataset 之所以可以表达流式数据的处理, 因为 Dataset 可以模拟一张无限扩展的表, 外部的数据会不断的流入到其中 2 体系结构1 体系结构在 Structured Streaming 中负责整体流程和执行的驱动引擎叫做 StreamExecution StreamExecution 在流上进行基于 Dataset 的查询, 也就是说, Dataset 之所以能够在流上进行查询, 是因为 StreamExecution 的调度和管理 StreamExecution 如何工作? StreamExecution 分为三个重要的部分 Source, 从外部数据源读取数据 LogicalPlan, 逻辑计划, 在流上的查询计划 Sink, 对接外部系统, 写入结果 2 StreamExecution 的执行顺序 1 根据进度标记, 从 Source 获取到一个由 DataFrame 表示的批次, 这个 DataFrame 表示数据的源头 123456val source = spark.readStream .format("socket") .option("host", "127.0.0.1") .option("port", 9999) .load() .as[String] 这一点非常类似 val df = spark.read.csv() 所生成的 DataFrame, 同样都是表示源头 2 根据源头 DataFrame 生成逻辑计划 1234val words = source.flatMap(_.split(" ")) .map((_, 1)) .groupByKey(_._1) .count() 述代码表示的就是数据的查询, 这一个步骤将这样的查询步骤生成为逻辑执行计划 3 优化逻辑计划最终生成物理计划 这一步其实就是使用 Catalyst 对执行计划进行优化, 经历基于规则的优化和基于成本模型的优化 4 执行物理计划将表示执行结果的 DataFrame / Dataset 交给 Sink 整个物理执行计划会针对每一个批次的数据进行处理, 处理后每一个批次都会生成一个表示结果的 Dataset Sink 可以将每一个批次的结果 Dataset 落地到外部数据源 5 执行完毕后, 汇报 Source 这个批次已经处理结束, Source 提交并记录最新的进度 3 增量查询 上图中清晰的展示了最终的结果生成是全局的结果, 而不是一个批次的结果, 但是从 StreamExecution 中可以看到, 针对流的处理是按照一个批次一个批次来处理的 那么, 最终是如何生成全局的结果集呢? 状态记录 在 Structured Streaming 中有一个全局范围的高可用 StateStore, 这个时候针对增量的查询变为如下步骤 从 StateStore 中取出上次执行完成后的状态 把上次执行的结果加入本批次, 再进行计算, 得出全局结果 将当前批次的结果放入 StateStore 中, 留待下次使用 总结 StreamExecution 是整个 Structured Streaming 的核心, 负责在流上的查询 StreamExecution 中三个重要的组成部分, 分别是 Source 负责读取每个批量的数据, Sink 负责将结果写入外部数据源, Logical Plan 负责针对每个小批量生成执行计划 StreamExecution 中使用 StateStore 来进行状态的维护 四 Source 从 HDFS 中读取数据 从 Kafka 中读取数据 1 从 HDFS 中读取数据读取小文件 1编写小文件 编写 Python 小程序, 在某个目录生成大量小文件 Python 是解释型语言, 其程序可以直接使用命令运行无需编译, 所以适合编写快速使用的程序, 很多时候也使用 Python 代替 Shell 使用 Python 程序创建新的文件, 并且固定的生成一段 JSON 文本写入文件 在真实的环境中, 数据也是一样的不断产生并且被放入 HDFS 中, 但是在真实场景下, 可能是 Flume 把小文件不断上传到 HDFS 中, 也可能是 Sqoop 增量更新不断在某个目录中上传小文件 使用 Structured Streaming 汇总数据 HDFS 中的数据是不断的产生的, 所以也是流式的数据 数据集是 JSON 格式, 要有解析 JSON 的能力 因为数据是重复的, 要对全局的流数据进行汇总和去重, 其实真实场景下的数据清洗大部分情况下也是要去重的 使用控制台展示数据 最终的数据结果以表的形式呈现 使用控制台展示数据意味着不需要在修改展示数据的代码, 将 Sink 部分的内容放在下一个大章节去说明 真实的工作中, 可能数据是要落地到 MySQL, HBase, HDFS 这样的存储系统中 步骤 Step 1: 编写 Python 脚本不断的产生数据 使用 Python 创建字符串保存文件中要保存的数据 创建文件并写入文件内容 使用 Python 调用系统 HDFS 命令上传文件 Step 2: 编写 Structured Streaming 程序处理数据 创建 SparkSession 使用 SparkSession 的 readStream 读取数据源 使用 Dataset 操作数据, 只需要去重 使用 Dataset 的 writeStream 设置 Sink 将数据展示在控制台中 Step 3: 部署程序, 验证结果 上传脚本到服务器中, 使用 python 命令运行脚本 开启流计算应用, 读取 HDFS 中对应目录的数据 查看运行结果 注意点 在读取 HDFS 的文件时, Source 不仅对接数据源, 也负责反序列化数据源中传过来的数据 Source 可以从不同的数据源中读取数据, 如 Kafka, HDFS 数据源可能会传过来不同的数据格式, 如 JSON, Parquet 读取 HDFS 文件的这个 Source 叫做 FileStreamSource 从命名就可以看出来这个 Source 不仅支持 HDFS, 还支持本地文件读取, 亚马逊云, 阿里云 等文件系统的读取, 例如: file://, s3://, oss:// 基于流的 Dataset 操作和基于静态数据集的 Dataset 操作是一致的 整体流程 Python 程序产生数据到 HDFS 中 Structured Streaming 从 HDFS 中获取数据 Structured Streaming 处理数据 将数据展示在控制台 1编写Python文件 12345678910111213141516import osfor index in range(100): content = """ &#123;"name":"Michael"&#125; &#123;"name":"Andy", "age":30&#125; &#123;"name":"Justin", "age":19&#125; """ file_name = "/export/dataset/text&#123;0&#125;.json".format(index) with open(file_name, "w") as file: file.write(content) os.system("/export/servers/hadoop/bin/hdfs dfs -mkdir -p /dataset/dataset/") os.system("/export/servers/hadoop/bin/hdfs dfs -put &#123;0&#125; /dataset/dataset/".format(file_name)) 12345678910111213141516171819202122232425262728//读取hdfs上的文件 @Test def hdfsSource(): Unit = &#123; val spark = SparkSession.builder() .appName("hdfs_source") .master("local[3]") .getOrCreate() System.setProperty("path", "C:\\winutils") spark.sparkContext.setLogLevel("WARN") val userSchema = new StructType() .add("name", "string") .add("age", "integer") val source = spark .readStream // 指明读取的是一个流式的 Dataset .schema(userSchema) //指定读取到的数据的 Schema .json("hdfs://node03:8020/dataset/dataset") //指定目录位置, 以及数据格式 val result = source.distinct() //去重 result.writeStream .outputMode(OutputMode.Update()) .format("console") .start() .awaitTermination() &#125; 运行 运行 Python 脚本 12345678# 进入 Python 文件被上传的位置cd ~# 创建放置生成文件的目录mkdir -p /export/dataset# 运行程序python gen_files.py 运行spark 程序 1本地运行 2上传运行 1spark-submit --class cn.itcast.structured.HDFSSource ./original-streaming-0.0.1.jar 总结: Python 生成文件到 HDFS, 这一步在真实环境下, 可能是由 Flume 和 Sqoop 收集并上传至 HDFS Structured Streaming 从 HDFS 中读取数据并处理 Structured Streaming 讲结果表展示在控制台 2 从 Kafka 中读取数据kafka 回顾 Kafka 是一个 Pub / Sub 系统 1 一个pub 一个 sub 2多个 pub 一个sub 3 一个 pub 多个 sub 4 多个 pub 多个sub Kafka 有一个非常重要的应用场景就是对接业务系统和数据系统, 作为一个数据管道, 其需要流通的数据量惊人, 所以 Kafka 如果要满足这种场景的话, 就一定具有以下两个特点 高吞吐量 高可靠性 topic 与partition 消息和事件经常是不同类型的, 例如用户注册是一种消息, 订单创建也是一种消息 Kafka 中使用 Topic 来组织不同类型的消息 Kafka 中的 Topic 要承受非常大的吞吐量, 所以 Topic 应该是可以分片的, 应该是分布式的 Kafka 的应用场景 一般的系统中, 业务系统会不止一个, 数据系统也会比较复杂 为了减少业务系统和数据系统之间的耦合, 要将其分开, 使用一个中间件来流转数据 Kafka 因为其吞吐量超高, 所以适用于这种场景 Kafka 如何保证高吞吐量 因为消息会有很多种类, Kafka 中可以创建多个队列, 每一个队列就是一个 Topic, 可以理解为是一个主题, 存放相关的消息 因为 Topic 直接存放消息, 所以 Topic 必须要能够承受非常大的通量, 所以 Topic 是分布式的, 是可以分片的, 使用分布式的并行处理能力来解决高通量的问题 1 Kafka 和 Structured Streaming 整合的结构Topic 的 Offset Topic 是分区的, 每一个 Topic 的分区分布在不同的 Broker 上 每个分区都对应一系列的 Log 文件, 消息存在于 Log 中, 消息的 ID 就是这条消息在本分区的 Offset 偏移量 Offset 又称作为偏移量, 其实就是一个东西距离另外一个东西的距离 Kafka 中使用 Offset 命名消息, 而不是指定 ID 的原因是想表示永远自增, ID 是可以指定的, 但是 Offset只能是一个距离值, 它只会越来越大, 所以, 叫做 Offset 而不叫 ID 也是这个考虑, 消息只能追加到 Log 末尾, 只能增长不能减少 分析 Structured Streaming 中使用 Source 对接外部系统, 对接 Kafka 的 Source 叫做 KafkaSource KafkaSource 中会使用 KafkaSourceRDD 来映射外部 Kafka 的 Topic, 两者的 Partition 一一对应 结论 Structured Streaming 会并行的从 Kafka 中获取数据 Structured Streaming 读取 Kafka 消息的三种方式 Earliest 从每个 Kafka 分区最开始处开始获取 Assign 手动指定每个 Kafka 分区中的 Offset Latest 不再处理之前的消息, 只获取流计算启动后新产生的数据 总结 Kafka 中的消息存放在某个 Topic 的某个 Partition 中, 消息是不可变的, 只会在消息过期的时候从最早的消息开始删除, 消息的 ID 也叫做 Offset, 并且只能正增长 Structured Streaming 整合 Kafka 的时候, 会并行的通过 Offset 从所有 Topic 的 Partition 中获取数据 Structured Streaming 在从 Kafka 读取数据的时候, 可以选择从最早的地方开始读取, 也可以选择从任意位置读取, 也可以选择只读取最新的 1 使用生产者在 Kafka 的 Topic : streaming-test 中输入 JSON 数据 发送数据时 必须把json转换为一行发送 1234567891011121314&#123; "devices": &#123; "cameras": &#123; "device_id": "awJo6rH", "last_event": &#123; "has_sound": true, "has_motion": true, "has_person": true, "start_time": "2016-12-29T00:00:00.000Z", "end_time": "2016-12-29T18:42:00.000Z" &#125; &#125; &#125;&#125; JSON 数据本质上就是字符串, 只不过这个字符串是有结构的, 虽然有结构, 但是很难直接从字符串中取出某个值 而反序列化, 就是指把 JSON 数据转为对象, 或者转为 DataFrame, 可以直接使用某一个列或者某一个字段获取数据, 更加方便 而想要做到这件事, 必须要先根据数据格式, 编写 Schema 对象, 从而通过一些方式转为 DataFrame 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172 @Test def kafakaSource(): Unit = &#123; // 1. 创建 SparkSession val spark = SparkSession.builder() .appName("hdfs_source") .master("local[6]") .getOrCreate() import spark.implicits._ // 2. 读取 Kafka 数据 val source: DataFrame = spark.readStream //设置为 Kafka 指定使用 KafkaSource 读取数据 .format("kafka") //指定 Kafka 的 Server 地址 .option("kafka.bootstrap.servers", "node01:9092,node02:9092,node03:9092") //要监听的 Topic, 可以传入多个, 传入多个 Topic 则监听多个 Topic, 也可以使用 topic-* 这样的通配符写法 .option("subscribe", "streaming_test_1") //从什么位置开始获取数据, 可选值有 earliest, assign, latest .option("startingOffsets", "earliest") //从头开始读 .load() // 3. 定义 JSON 中的类型 /*&#123; "devices": &#123; "cameras": &#123; "device_id": "awJo6rH", "last_event": &#123; "has_sound": true, "has_motion": true, "has_person": true, "start_time": "2016-12-29T00:00:00.000Z", "end_time": "2016-12-29T18:42:00.000Z" &#125; &#125; &#125;&#125;*/ val eventType = new StructType() // 里层3 .add("has_sound", BooleanType) .add("has_motion", BooleanType) .add("has_person", BooleanType) .add("start_time", DateType) .add("end_time", DateType) val cameraType = new StructType() //里层2 .add("device_id", StringType) .add("last_event", eventType) val deviceType = new StructType() .add("cameras", cameraType) //里层1 val schema = new StructType() .add("devices", deviceType) //最外层 // 4. 解析 JSON // 需求: DataFrame(time, has_person) import org.apache.spark.sql.functions._ val jsonOptions = Map("timestampFormat" -&gt; "yyyy-MM-dd'T'HH:mm:ss.sss'Z'") val result = source.selectExpr("CAST(key AS STRING) as key", "CAST(value AS STRING) as value") .select(from_json('value, schema, jsonOptions).alias("parsed_value")) .selectExpr("parsed_value.devices.cameras.last_event.start_time", "parsed_value.devices.cameras.last_event.has_person") // 5. 打印数据 result.writeStream .format("console") .outputMode(OutputMode.Append()) .start() .awaitTermination() &#125; 使用 Structured Streaming 来对接 Kafka 并反序列化 Kafka 中的 JSON 格式的消息, 是一个非常重要的技能 无论使用什么方式, 如果想反序列化 JSON 数据, 就必须要先追踪 JSON 数据的结构 注意点 1 业务系统如何把数据给 Kafka ? 可以主动或者被动的把数据交给 Kafka, 但是无论使用什么方式, 都在使用 Kafka 的 Client 类库来完成这件事, Kafka 的类库调用方式如下 12Producer&lt;String, String&gt; producer = new KafkaProducer&lt;String, String&gt;(properties);producer.send(new ProducerRecord&lt;String, String&gt;("HelloWorld", msg)); 其中发给 Kafka 的消息是 KV 类型的 2 使用 Structured Streaming 访问 Kafka 获取数据的时候, 需要什么东西呢? 存储当前处理过的 Kafka 的 Offset 对接多个 Kafka Topic 的时候, 要知道这条数据属于哪个 Topi 总结 Kafka 中收到的消息是 KV 类型的, 有 Key, 有 Value Structured Streaming 对接 Kafka 的时候, 每一条 Kafka 消息不能只是 KV, 必须要有 Topic, Partition之类的信息 从 Kafka 获取的 DataFrame 格式 1source.printSchema() 123456789结果root |-- key: binary (nullable = true) |-- value: binary (nullable = true) |-- topic: string (nullable = true) |-- partition: integer (nullable = true) |-- offset: long (nullable = true) |-- timestamp: timestamp (nullable = true) |-- timestampType: integer (nullable = true) 从 Kafka 中读取到的并不是直接是数据, 而是一个包含各种信息的表格, 其中每个字段的含义如下 Key 类型 解释 key binary Kafka 消息的 Key value binary Kafka 消息的 Value topic string 本条消息所在的 Topic, 因为整合的时候一个 Dataset 可以对接多个 Topic, 所以有这样一个信息 partition integer 消息的分区号 offset long 消息在其分区的偏移量 timestamp timestamp 消息进入 Kafka 的时间戳 timestampType integer 时间戳类型 2 总结 一定要把 JSON 转为一行, 再使用 Producer 发送, 不然会出现获取多行的情况 使用 Structured Streaming 连接 Kafka 的时候, 需要配置如下三个参数 kafka.bootstrap.servers : 指定 Kafka 的 Server 地址 subscribe : 要监听的 Topic, 可以传入多个, 传入多个 Topic 则监听多个 Topic, 也可以使用 topic-* 这样的通配符写法 startingOffsets : 从什么位置开始获取数据, 可选值有 earliest, assign, latest 从 Kafka 获取到的 DataFrame 的 Schema 如下 12345678root |-- key: binary (nullable = true) |-- value: binary (nullable = true) |-- topic: string (nullable = true) |-- partition: integer (nullable = true) |-- offset: long (nullable = true) |-- timestamp: timestamp (nullable = true) |-- timestampType: integer (nullable = true) 3 JSON 解析 准备好 JSON 所在的列 问题 由 Dataset 的结构可以知道 key 和 value 列的类型都是 binary 二进制, 所以要将其转为字符串, 才可进行 JSON 解析 解决方式 source.selectExpr(&quot;CAST(key AS STRING) as key&quot;, &quot;CAST(value AS STRING) as value&quot;) 编写 Schema 对照 JSON 的格式 Key 要对应 JSON 中的 Key Value 的类型也要对应 JSON 中的 Value 类型 12345678910111213141516val eventType = new StructType() .add("has_sound", BooleanType, nullable = true) .add("has_motion", BooleanType, nullable = true) .add("has_person", BooleanType, nullable = true) .add("start_time", DateType, nullable = true) .add("end_time", DateType, nullable = true)val camerasType = new StructType() .add("device_id", StringType, nullable = true) .add("last_event", eventType, nullable = true)val devicesType = new StructType() .add("cameras", camerasType, nullable = true)val schema = new StructType() .add("devices", devicesType, nullable = true) 因为 JSON 中包含 Date 类型的数据, 所以要指定时间格式化方式 1val jsonOptions = Map("timestampFormat" -&gt; "yyyy-MM-dd'T'HH:mm:ss.sss'Z'") 使用 from_json 这个 UDF 格式化 JSON 1.select(from_json('value, schema, jsonOptions).alias("parsed_value")) 选择格式化过后的 JSON 中的字段 因为 JSON 被格式化过后, 已经变为了 StructType, 所以可以直接获取其中某些字段的值 12.selectExpr("parsed_value.devices.cameras.last_event.has_person as has_person", "parsed_value.devices.cameras.last_event.start_time as start_time") 数据处理 统计各个时段有人的数据 123.filter('has_person === true).groupBy('has_person, 'start_time).count() 将数据落地到控制台 12345result.writeStream .outputMode(OutputMode.Complete()) .format("console") .start() .awaitTermination() 全部代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253import org.apache.spark.sql.SparkSessionval spark = SparkSession.builder() .master("local[6]") .appName("kafka integration") .getOrCreate()import org.apache.spark.sql.streaming.OutputModeimport org.apache.spark.sql.types._val source = spark .readStream .format("kafka") .option("kafka.bootstrap.servers", "node01:9092,node02:9092,node03:9092") .option("subscribe", "streaming-test") .option("startingOffsets", "earliest") .load()val eventType = new StructType() .add("has_sound", BooleanType, nullable = true) .add("has_motion", BooleanType, nullable = true) .add("has_person", BooleanType, nullable = true) .add("start_time", DateType, nullable = true) .add("end_time", DateType, nullable = true)val camerasType = new StructType() .add("device_id", StringType, nullable = true) .add("last_event", eventType, nullable = true)val devicesType = new StructType() .add("cameras", camerasType, nullable = true)val schema = new StructType() .add("devices", devicesType, nullable = true)val jsonOptions = Map("timestampFormat" -&gt; "yyyy-MM-dd'T'HH:mm:ss.sss'Z'")import org.apache.spark.sql.functions._import spark.implicits._val result = source.selectExpr("CAST(key AS STRING) as key", "CAST(value AS STRING) as value") .select(from_json('value, schema, jsonOptions).alias("parsed_value")) .selectExpr("parsed_value.devices.cameras.last_event.has_person as has_person", "parsed_value.devices.cameras.last_event.start_time as start_time") .filter('has_person === true) .groupBy('has_person, 'start_time) .count()result.writeStream .outputMode(OutputMode.Complete()) .format("console") .start() .awaitTermination() 运行测试 进入服务器中, 启动 Kafka 启动 Kafka 的 Producer 1bin/kafka-console-producer.sh --broker-list node01:9092,node02:9092,node03:9092 --topic streaming-test 启动 Spark shell 并拷贝代码进行测试 1./bin/spark-shell --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.2.0 因为需要和 Kafka 整合, 所以在启动的时候需要加载和 Kafka 整合的包 spark-sql-kafka-0-10 五 sink1 HDFS Sink场景 Kafka 往往作为数据系统和业务系统之间的桥梁 数据系统一般由批量处理和流式处理两个部分组成 在 Kafka 作为整个数据平台入口的场景下, 需要使用 StructuredStreaming 接收 Kafka 的数据并放置于 HDFS上, 后续才可以进行批量处理 需求 从 Kafka 接收数据, 从给定的数据集中, 裁剪部分列, 落地于 HDFS 1234567891011121314151617181920212223242526272829303132333435363738@Test def hdfsSink(): Unit = &#123; System.setProperty("hadoop.home.dir", "C:\\winutil") // 1. 创建 SparkSession val spark = SparkSession.builder() .appName("hdfs_sink") .master("local[6]") .getOrCreate() import spark.implicits._ // 2. 读取 Kafka 数据 val source: Dataset[String] = spark.readStream .format("kafka") .option("kafka.bootstrap.servers", "node01:9092,node02:9092,node03:9092") .option("subscribe", "streaming_test_2") .option("startingOffsets", "earliest") .load() .selectExpr("CAST(value AS STRING) as value") .as[String] // 1::Toy Story (1995)::Animation|Children's|Comedy // 3. 处理 CSV, Dataset(String), Dataset(id, name, category) val result = source.map(item =&gt; &#123; val arr = item.split("::") (arr(0).toInt, arr(1).toString, arr(2).toString) &#125;).as[(Int, String, String)].toDF("id", "name", "category") // 4. 落地到 HDFS 中 result.writeStream .format("parquet") .option("path", "dataset/streaming/moives/") .option("checkpointLocation", "checkpoint") .start() .awaitTermination() &#125; 2 Kafka Sink场景 有很多时候, ETL 过后的数据, 需要再次放入 Kafka 在 Kafka 后, 可能会有流式程序统一将数据落地到 HDFS 或者 HBase 需求 从 Kafka 中获取数据, 简单处理, 再次放入 Kafka 12345678910111213141516171819202122232425262728293031323334353637383940@Test def kafkaSink(): Unit = &#123; System.setProperty("hadoop.home.dir", "C:\\winutil") // 1. 创建 SparkSession val spark = SparkSession.builder() .appName("hdfs_sink") .master("local[6]") .getOrCreate() import spark.implicits._ // 2. 读取 Kafka 数据 val source: Dataset[String] = spark.readStream .format("kafka") .option("kafka.bootstrap.servers", "node01:9092,node02:9092,node03:9092") .option("subscribe", "streaming_test_2") .option("startingOffsets", "earliest") .load() .selectExpr("CAST(value AS STRING) as value") .as[String] // 1::Toy Story (1995)::Animation|Children's|Comedy // 3. 处理 CSV, Dataset(String), Dataset(id, name, category) val result = source.map(item =&gt; &#123; val arr = item.split("::") (arr(0).toInt, arr(1).toString, arr(2).toString) &#125;).as[(Int, String, String)].toDF("id", "name", "category") // 4. 落地到 kafka 中 result.writeStream .format("kafka") .outputMode(OutputMode.Append()) .option("checkpointLocation", "checkpoint") .option("kafka.bootstrap.servers", "node01:9092,node02:9092,node03:9092") .option("topic", "streaming_test_3") .start() .awaitTermination() &#125; 3 Foreach Writer掌握 Foreach 模式理解如何扩展 Structured Streaming 的 Sink, 同时能够将数据落地到 MySQL 场景 收集业务系统数据 数据处理 放入 OLTP 数据 外部通过 ECharts 获取并处理数据 这个场景下, StructuredStreaming 就需要处理数据并放入 MySQL 或者 MongoDB, HBase 中以供 Web 程序可以获取数据, 图表的形式展示在前端 Foreach 模式:: 在 Structured Streaming 中, 并未提供完整的 MySQL/JDBC 整合工具 不止 MySQL 和 JDBC, 可能会有其它的目标端需要写入 很多时候 Structured Streaming 需要对接一些第三方的系统, 例如阿里云的云存储, 亚马逊云的云存储等, 但是 Spark 无法对所有第三方都提供支持, 有时候需要自己编写 既然无法满足所有的整合需求, StructuredStreaming 提供了 Foreach, 可以拿到每一个批次的数据 通过 Foreach 拿到数据后, 可以通过自定义写入方式, 从而将数据落地到其它的系统 需求 从 Kafka 中获取数据, 处理后放入 MySQL 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758@Test def mysqlSink(): Unit = &#123; System.setProperty("hadoop.home.dir", "C:\\winutil") // 1. 创建 SparkSession val spark = SparkSession.builder() .appName("hdfs_sink") .master("local[6]") .getOrCreate() import spark.implicits._ // 2. 读取 Kafka 数据 val source: Dataset[String] = spark.readStream .format("kafka") .option("kafka.bootstrap.servers", "node01:9092,node02:9092,node03:9092") .option("subscribe", "streaming_test_2") .option("startingOffsets", "earliest") .load() .selectExpr("CAST(value AS STRING) as value") .as[String] // 1::Toy Story (1995)::Animation|Children's|Comedy // 3. 处理 CSV, Dataset(String), Dataset(id, name, category) val result = source.map(item =&gt; &#123; val arr = item.split("::") (arr(0).toInt, arr(1).toString, arr(2).toString) &#125;).as[(Int, String, String)].toDF("id", "name", "category") // 4. 落地到 MySQL class MySQLWriter extends ForeachWriter[Row] &#123; private val driver = "com.mysql.jdbc.Driver" private var connection: sql.Connection = _ private val url = "jdbc::mysql://node01:3306/streaming-movies-result" private var statement: sql.Statement = _ override def open(partitionId: Long, version: Long): Boolean = &#123; Class.forName(driver) connection = DriverManager.getConnection(url) statement = connection.createStatement() true &#125; override def process(value: Row): Unit = &#123; statement.executeUpdate(s"insert into movies values($&#123;value.get(0)&#125;, $&#123;value.get(1)&#125;, $&#123;value.get(2)&#125;)") &#125; override def close(errorOrNull: Throwable): Unit = &#123; connection.close() &#125; &#125; result.writeStream .foreach(new MySQLWriter) .start() .awaitTermination() &#125; 4 自定义SinkForeach 倾向于一次处理一条数据, 如果想拿到 DataFrame 幂等的插入外部数据源, 则需要自定义 Sink Spark 加载 Sink 流程分析 writeStream 方法中会创建一个 DataStreamWriter 对象 1234567def writeStream: DataStreamWriter[T] = &#123; if (!isStreaming) &#123; logicalPlan.failAnalysis( "'writeStream' can be called only on streaming Dataset/DataFrame") &#125; new DataStreamWriter[T](this)&#125; 在 DataStreamWriter 对象上通过 format 方法指定 Sink 的短名并记录下来 1234def format(source: String): DataStreamWriter[T] = &#123; this.source = source this&#125; 3 .最终会通过 DataStreamWriter 对象上的 start 方法启动执行, 其中会通过短名创建 DataSource 123456val dataSource = DataSource( df.sparkSession, className = source, // 传入的 Sink 短名 options = extraOptions.toMap, partitionColumns = normalizedParCols.getOrElse(Nil)) 在创建 DataSource 的时候, 会通过一个复杂的流程创建出对应的 Source 和 Sink 1lazy val providingClass: Class[_] = DataSource.lookupDataSource(className) 在这个复杂的创建流程中, 有一行最关键的代码, 就是通过 Java 的类加载器加载所有的 DataSourceRegister 1val serviceLoader = ServiceLoader.load(classOf[DataSourceRegister], loader) 在 DataSourceRegister 中会创建对应的 Source 或者 Sink 123456789101112131415161718192021trait DataSourceRegister &#123; def shortName(): String // 提供短名&#125;trait StreamSourceProvider &#123; def createSource( //创建 Source sqlContext: SQLContext, metadataPath: String, schema: Option[StructType], providerName: String, parameters: Map[String, String]): Source&#125;trait StreamSinkProvider &#123; def createSink( //创建 Sink sqlContext: SQLContext, parameters: Map[String, String], partitionColumns: Seq[String], outputMode: OutputMode): Sink&#125; 自定义 Sink 的方式 根据前面的流程说明, 有两点非常重要 Spark 会自动加载所有 DataSourceRegister 的子类, 所以需要通过 DataSourceRegister 加载 Source 和 Sink Spark 提供了 StreamSinkProvider 用以创建 Sink, 提供必要的依赖 所以如果要创建自定义的 Sink, 需要做两件事 创建一个注册器, 继承 DataSourceRegister 提供注册功能, 继承 StreamSinkProvider 获取创建 Sink 的必备依赖 创建一个 Sink 子类 步骤 读取 Kafka 数据 简单处理数据 创建 Sink 创建 Sink 注册器 使用自定义 Sink 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566@Test def zdSink(): Unit = &#123; import org.apache.spark.sql.SparkSession val spark = SparkSession.builder() .master("local[6]") .appName("kafka integration") .getOrCreate() import spark.implicits._ val source = spark .readStream .format("kafka") .option("kafka.bootstrap.servers", "node01:9092,node02:9092,node03:9092") .option("subscribe", "streaming-bank") .option("startingOffsets", "earliest") .load() .selectExpr("CAST(value AS STRING)") .as[String] val result = source.map &#123; item =&gt; val arr = item.replace("\"", "").split(";") (arr(0).toInt, arr(1).toInt, arr(5).toInt) &#125; .as[(Int, Int, Int)] .toDF("age", "job", "balance") class MySQLSink(options: Map[String, String], outputMode: OutputMode) extends Sink &#123; override def addBatch(batchId: Long, data: DataFrame): Unit = &#123; val userName = options.get("userName").orNull val password = options.get("password").orNull val table = options.get("table").orNull val jdbcUrl = options.get("jdbcUrl").orNull val properties = new Properties properties.setProperty("user", userName) properties.setProperty("password", password) data.write.mode(outputMode.toString).jdbc(jdbcUrl, table, properties) &#125; &#125; class MySQLStreamSinkProvider extends StreamSinkProvider with DataSourceRegister &#123; override def createSink(sqlContext: SQLContext, parameters: Map[String, String], partitionColumns: Seq[String], outputMode: OutputMode): Sink = &#123; new MySQLSink(parameters, outputMode) &#125; override def shortName(): String = "mysql" &#125; result.writeStream .format("mysql") .option("username", "root") .option("password", "root") .option("table", "streaming-bank-result") .option("jdbcUrl", "jdbc:mysql://node01:3306/test") .start() .awaitTermination() &#125; 5 Tigger如何控制 StructuredStreaming 的处理时间 步骤 微批次处理 连续流处理 什么是微批次 并不是真正的流, 而是缓存一个批次周期的数据, 后处理这一批次的数据 通用流程 步骤 根据 Spark 提供的调试用的数据源 Rate 创建流式 DataFrame Rate 数据源会定期提供一个由两列 timestamp, value 组成的数据, value 是一个随机数 处理和聚合数据, 计算每个个位数和十位数各有多少条数据 对 value 求 log10 即可得出其位数 后按照位数进行分组, 最终就可以看到每个位数的数据有多少个 1234567891011121314151617181920val spark = SparkSession.builder() .master("local[6]") .appName("socket_processor") .getOrCreate()import org.apache.spark.sql.functions._import spark.implicits._spark.sparkContext.setLogLevel("ERROR")val source = spark.readStream .format("rate") .load()val result = source.select(log10('value) cast IntegerType as 'key, 'value) .groupBy('key) .agg(count('key) as 'count) .select('key, 'count) .where('key.isNotNull) .sort('key.asc) 默认方式划分批次 介绍 默认情况下的 Structured Streaming 程序会运行在微批次的模式下, 当一个批次结束后, 下一个批次会立即开始处理 步骤 指定落地到 Console 中, 不指定 Trigger 代码 12345result.writeStream .outputMode(OutputMode.Complete()) .format("console") .start() .awaitTermination() 按照固定时间间隔划分批次 介绍 使用微批次处理数据, 使用用户指定的时间间隔启动批次, 如果间隔指定为 0, 则尽可能快的去处理, 一个批次紧接着一个批次 如果前一批数据提前完成, 待到批次间隔达成的时候再启动下一个批次 如果前一批数据延后完成, 下一个批次会在前面批次结束后立即启动 如果没有数据可用, 则不启动处理 步骤 通过 Trigger.ProcessingTime() 指定处理间隔 代码 123456result.writeStream .outputMode(OutputMode.Complete()) .format("console") .trigger(Trigger.ProcessingTime("2 seconds")) .start() .awaitTermination() 一次性划分批次 介绍 只划分一个批次, 处理完成以后就停止 Spark 工作, 当需要启动一下 Spark 处理遗留任务的时候, 处理完就关闭集群的情况下, 这个划分方式非常实用 步骤 使用 Trigger.Once 一次性划分批次 代码 123456result.writeStream .outputMode(OutputMode.Complete()) .format("console") .trigger(Trigger.Once()) .start() .awaitTermination() 连续流处理 介绍 微批次会将收到的数据按照批次划分为不同的 DataFrame, 后执行 DataFrame, 所以其数据的处理延迟取决于每个 DataFrame 的处理速度, 最快也只能在一个 DataFrame 结束后立刻执行下一个, 最快可以达到 100ms 左右的端到端延迟 而连续流处理可以做到大约 1ms 的端到端数据处理延迟 连续流处理可以达到 at-least-once 的容错语义 从 Spark 2.3 版本开始支持连续流处理, 我们所采用的 2.2 版本还没有这个特性, 并且这个特性截止到 2.4 依然是实验性质, 不建议在生产环境中使用 操作 步骤 使用特殊的 Trigger 完成功能 代码 123456result.writeStream .outputMode(OutputMode.Complete()) .format("console") .trigger(Trigger.Continuous("1 second")) .start() .awaitTermination() 限制 只支持 Map 类的有类型操作 只支持普通的的 SQL 类操作, 不支持聚合 Source 只支持 Kafka Sink 只支持 Kafka, Console, Memory 6 从 Source 到 Sink 的流程 在每个 StreamExecution 的批次最开始, StreamExecution 会向 Source 询问当前 Source 的最新进度, 即最新的 offset StreamExecution 将 Offset 放到 WAL 里 StreamExecution 从 Source 获取 start offset, end offset 区间内的数据 StreamExecution 触发计算逻辑 logicalPlan 的优化与编译 计算结果写出给 Sink 调用 Sink.addBatch(batchId: Long, data: DataFrame) 完成 此时才会由 Sink 的写入操作开始触发实际的数据获取和计算过程 在数据完整写出到 Sink 后, StreamExecution 通知 Source 批次 id 写入到 batchCommitLog, 当前批次结束 7 错误恢复和容错语义1 端到端 Source 可能是 Kafka, HDFS Sink 也可能是 Kafka, HDFS, MySQL 等存储服务 消息从 Source 取出, 经过 Structured Streaming 处理, 最后落地到 Sink 的过程, 叫做端到端 2 三种容错语义1 at-most-once 在数据从 Source 到 Sink 的过程中, 出错了, Sink 可能没收到数据, 但是不会收到两次, 叫做 at-most-once 一般错误恢复的时候, 不重复计算, 则是 at-most-once 2 at-least-once 在数据从 Source 到 Sink 的过程中, 出错了, Sink 一定会收到数据, 但是可能收到两次, 叫做 at-least-once 一般错误恢复的时候, 重复计算可能完成也可能未完成的计算, 则是 at-least-once 3 exactly-once 在数据从 Source 到 Sink 的过程中, 虽然出错了, Sink 一定恰好收到应该收到的数据, 一条不重复也一条都不少, 即是 exactly-once 想做到 exactly-once 是非常困难的 3 Sink 的容错 1 故障恢复一般分为 Driver 的容错和 Task 的容错 Driver 的容错指的是整个系统都挂掉了 Task 的容错指的是一个任务没运行明白, 重新运行一次 2 因为 Spark 的 Executor 能够非常好的处理 Task 的容错, 所以我们主要讨论 Driver 的容错, 如果出错的时候 读取 WAL offsetlog 恢复出最新的 offsets 当 StreamExecution 找到 Source 获取数据的时候, 会将数据的起始放在 WAL offsetlog 中, 当出错要恢复的时候, 就可以从中获取当前处理批次的数据起始, 例如 Kafka 的 Offset 读取 batchCommitLog 决定是否需要重做最近一个批次 当 Sink 处理完批次的数据写入时, 会将当前的批次 ID 存入 batchCommitLog, 当出错的时候就可以从中取出进行到哪一个批次了, 和 WAL 对比即可得知当前批次是否处理完 如果有必要的话, 当前批次数据重做 如果上次执行在 (5) 结束前即失效, 那么本次执行里 Sink 应该完整写出计算结果 如果上次执行在 (5) 结束后才失效, 那么本次执行里 Sink 可以重新写出计算结果 (覆盖上次结果), 也可以跳过写出计算结果(因为上次执行已经完整写出过计算结果了) 这样即可保证每次执行的计算结果, 在 Sink 这个层面, 是 不重不丢 的, 即使中间发生过失效和恢复, 所以 Structured Streaming 可以做到 exactly-once 4 容错所需要的存储 存储 offsetlog 和 batchCommitLog 关乎于错误恢复 offsetlog 和 batchCommitLog 需要存储在可靠的空间里 offsetlog 和 batchCommitLog 存储在 Checkpoint 中 WAL 其实也存在于 Checkpoint 中 指定 Checkpoint 只有指定了 Checkpoint 路径的时候, 对应的容错功能才可以开启 123456aggDF .writeStream .outputMode("complete") .option("checkpointLocation", "path/to/HDFS/dir") // 指定 Checkpoint 的路径, 这个路径对应的目录必须是 HDFS 兼容的文件系统 .format("memory") .start() 需要的外部支持 如果要做到 exactly-once, 只是 Structured Streaming 能做到还不行, 还需要 Source 和 Sink 系统的支持 Source 需要支持数据重放 当有必要的时候, Structured Streaming 需要根据 start 和 end offset 从 Source 系统中再次获取数据, 这叫做重放 Sink 需要支持幂等写入 如果需要重做整个批次的时候, Sink 要支持给定的 ID 写入数据, 这叫幂等写入, 一个 ID 对应一条数据进行写入, 如果前面已经写入, 则替换或者丢弃, 不能重复 所以 Structured Streaming 想要做到 exactly-once, 则也需要外部系统的支持, 如下 Source Sources 是否可重放 原生内置支持 注解 HDFS 可以 已支持 包括但不限于 Text, JSON, CSV, Parquet, ORC Kafka 可以 已支持 Kafka 0.10.0+ RateStream 可以 已支持 以一定速率产生数据 RDBMS 可以 待支持 预计后续很快会支持 Socket 不可以 已支持 主要用途是在技术会议和讲座上做 Demo Sink Sinks 是否幂等写入 原生内置支持 注解 HDFS 可以 支持 包括但不限于 Text, JSON, CSV, Parquet, ORC ForeachSink 可以 支持 可定制度非常高的 Sink, 是否可以幂等取决于具体的实现 RDBMS 可以 待支持 预计后续很快会支持 Kafka 不可以 支持 Kafka 目前不支持幂等写入, 所以可能会有重复写入 六 . 有状态算子1状态1 无状态算子 2 有状态算子 有中间状态需要保存 增量查询 2 常规算子Structured Streaming 的常规数据处理方式 需求 给定电影评分数据集 ratings.dat 筛选评分超过三分的电影 以追加模式展示数据, 以流的方式来一批数据处理一批数据, 最终每一批次展示为如下效果 123456+------+-------+|Rating|MovieID|+------+-------+| 5| 1193|| 4| 3408|+------+-------+ 读取文件的时候只能读取一个文件夹, 因为是流的操作, 流的场景是源源不断有新的文件读取 12345678910val source = spark.readStream .textFile("dataset/ratings") .map(line =&gt; &#123; val columns = line.split("::") (columns(0).toInt, columns(1).toInt, columns(2).toInt, columns(3).toLong) &#125;) .toDF("UserID", "MovieID", "Rating", "Timestamp")val result = source.select('Rating, 'MovieID) .where('Rating &gt; 3) 针对静态数据集的很多转换算子, 都可以应用在流式的 Dataset 上, 例如 Map, FlatMap, Where, Select 等 3 分组算子能够使用分组完成常见需求, 并了解如何扩展行 需求 给定电影数据集 movies.dat, 其中三列 MovieID, Title, Genres 统计每个分类下的电影数量 123456789101112131415161718val source = spark.readStream .textFile("dataset/movies") .map(line =&gt; &#123; val columns = line.split("::") (columns(0).toInt, columns(1).toString, columns(2).toString.split("\\|")) &#125;) .toDF("MovieID", "Title", "Genres")val result = source.select(explode('Genres) as 'Genres) .groupBy('Genres) .agg(count('Genres) as 'Count)result.writeStream .outputMode(OutputMode.Complete()) .format("console") .queryName("genres_count") .start() .awaitTermination() Structured Streaming 不仅支持 groupBy, 还支持 groupByKey]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>BigData</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SparkStreaming]]></title>
    <url>%2F2018%2F05%2F01%2FSparkStreaming.html</url>
    <content type="text"><![CDATA[一概述1 流计算与批量计算批量计算 数据已经存在, 一次性读取所有的数据进行批量处理 hdfs &gt; spark sql &gt; hdfs 流计算 数据源源不断的进来, 经过处理后落地 设备&gt; kafka &gt; SparkStreaming &gt; hbase等 2 流和批 架构流和批都是有意义的, 有自己的应用场景, 那么如何结合流和批呢? 如何在同一个系统中使用这两种不同的解决方案呢? 混合架构 混合架构的名字叫做 Lambda 架构, 混合架构最大的特点就是将流式计算和批处理结合起来,后在进行查询的时候分别查询流系统和批系统, 最后将结果合并在一起 一般情况下 Lambda 架构分三层 批处理层: 批量写入, 批量读取 服务层: 分为两个部分, 一部分对应批处理层, 一部分对应速度层 速度层: 随机读取, 随即写入, 增量计算 优点 兼顾优点, 在批处理层可以全量查询和分析, 在速度层可以查询最新的数据 速度很快, 在大数据系统中, 想要快速的获取结果是非常困难的, 因为高吞吐量和快速返回结果往往很难兼得, 例如 Impala 和 Hive, Hive 能进行非常大规模的数据量的处理, Impala 能够快速的查询返回结果, 但是很少有一个系统能够兼得两点, Lambda 使用多种融合的手段从而实现 缺点 Lambda 是一个非常反人类的设计, 因为我们需要在系统中不仅维护多套数据层, 还需要维护批处理和流式处理两套框架, 这非常困难, 一套都很难搞定, 两套带来的运维问题是是指数级提升的 流失架构 流式架构常见的叫做 Kappa 结构, 是 Lambda 架构 的一个变种, 其实本质上就是删掉了批处理 优点 非常简单 效率很高, 在存储系统的发展下, 很多存储系统已经即能快速查询又能批量查询了, 所以 Kappa 架构 在新时代还是非常够用的 问题: 丧失了一些 Lambda 的优秀特点 3 SparkStreaming 特点 说明 Spark Streaming 是 Spark Core API 的扩展 Spark Streaming 具有类似 RDD 的 API, 易于使用, 并可和现有系统共用相似代码,,,,一个非常重要的特点是, Spark Streaming 可以在流上使用基于 Spark 的机器学习和流计算, 是一个一站式的平台 Spark Streaming 具有很好的整合性 Spark Streaming 可以从 Kafka, Flume, TCP 等流和队列中获取数据,,,,Spark Streaming 可以将处理过的数据写入文件系统, 常见数据库中 Spark Streaming 是微批次处理模型 微批次处理的方式不会有长时间运行的 Operator, 所以更易于容错设计,,,,,,,微批次模型能够避免运行过慢的服务, 实行推测执行 TCP三次握手回顾 1 Client 向 Server 发送 SYN(j), 进入 SYN_SEND 状态等待 Server 响应 2 Server 收到 Client 的 SYN(j) 并发送确认包 ACK(j + 1), 同时自己也发送一个请求连接的 SYN(k) 给 Client, 进入 SYN_RECV 状态等待 Client 确认 3 Client 收到 Server 的 ACK + SYN, 向 Server 发送连接确认 ACK(k + 1), 此时, Client 和 Server 都进入 ESTABLISHED 状态, 准备数据发送 netcat Netcat 简写 nc, 命令行中使用 nc 命令调用 Netcat 是一个非常常见的 Socket 工具, 可以使用 nc 建立 Socket server 也可以建立 Socket client nc -l 建立 Socket server, l 是 listen 监听的意思 nc host port 建立 Socket client, 并连接到某个 Socket server 一台虚拟机 nc -lk 1567 另一台复制的虚拟机 nc localhost 1567 两台可以通信 4 小试牛刀使用 Spark Streaming 程序和 Socket server 进行交互 创建maven工程 pom 依赖 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147&lt;properties&gt; &lt;scala.version&gt;2.11.8&lt;/scala.version&gt; &lt;spark.version&gt;2.2.0&lt;/spark.version&gt; &lt;slf4j.version&gt;1.7.16&lt;/slf4j.version&gt; &lt;log4j.version&gt;1.2.17&lt;/log4j.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-library&lt;/artifactId&gt; &lt;version&gt;$&#123;scala.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.47&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming_2.11&lt;/artifactId&gt; &lt;version&gt;2.2.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql-kafka-0-10_2.11&lt;/artifactId&gt; &lt;version&gt;2.2.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-hive_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;2.6.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;jcl-over-slf4j&lt;/artifactId&gt; &lt;version&gt;$&#123;slf4j.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;version&gt;$&#123;slf4j.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;version&gt;$&#123;slf4j.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;$&#123;log4j.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.10&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;sourceDirectory&gt;src/main/scala&lt;/sourceDirectory&gt; &lt;testSourceDirectory&gt;src/test/scala&lt;/testSourceDirectory&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.0&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;net.alchim31.maven&lt;/groupId&gt; &lt;artifactId&gt;scala-maven-plugin&lt;/artifactId&gt; &lt;version&gt;3.2.0&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;compile&lt;/goal&gt; &lt;goal&gt;testCompile&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;args&gt; &lt;arg&gt;-dependencyfile&lt;/arg&gt; &lt;arg&gt;$&#123;project.build.directory&#125;/.scala_dependencies&lt;/arg&gt; &lt;/args&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;!--打包插件 因为默认的打包不包含maven的依赖--&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt; &lt;version&gt;3.1.1&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;shade&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;filters&gt; &lt;filter&gt; &lt;artifact&gt;*:*&lt;/artifact&gt; &lt;excludes&gt; &lt;exclude&gt;META-INF/*.SF&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.DSA&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.RSA&lt;/exclude&gt; &lt;/excludes&gt; &lt;/filter&gt; &lt;/filters&gt; &lt;transformers&gt; &lt;transformer implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer"&gt; &lt;mainClass&gt;&lt;/mainClass&gt; &lt;/transformer&gt; &lt;/transformers&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; 123456789101112131415161718192021222324252627282930313233343536373839404142//Spark Streaming 是微小的批处理 并不是实时流, 而是按照时间切分小批量, 一个一个的小批量处理class StreamIngDemo extends Serializable &#123; @Test def stream(): Unit = &#123; // 1. 初始化环境 // 在 SparkCore 中的内存, 创建 SparkContext 的时候使用 // 在创建 Streaming Context 的时候也要用到 conf, 说明 Spark Streaming 是基于 Spark Core 的 // 在执行 master 的时候, 不能指定一个线程 Spark Streaming 中至少要有两个线程 // 因为在 Streaming 运行的时候, 需要开一个新的线程来去一直监听数据的获取 val sparkConf = new SparkConf().setAppName("streaming").setMaster("local[3]") // StreamingContext 其实就是 Spark Streaming 的入口 // 相当于 SparkContext 是 Spark Core 的入口一样 // 它们也都叫做 XXContext 第二个参数是控制多长时间生成一个rdd去统计 val context = new StreamingContext(sparkConf, Seconds(3)) // socketTextStream 这个方法用于创建一个 DStream, 监听 Socket 输入, 当做文本来处理 // sparkContext.textFile() 创建一个 rdd, 他们俩类似, 都是创建对应的数据集 // RDD -&gt; Spark Core DStream -&gt; Spark Streaming // DStream 可以理解为是一个流式的 RDD //调整 Log 级别, 避免过多的 Log 影响视线 context.sparkContext.setLogLevel("WARN") val lines = context.socketTextStream( hostname = "node03", port = 9999, //选择 Receiver 获取到数据后的保存方式, 此处是内存和磁盘都有, 并且序列化后保存 storageLevel = StorageLevel.MEMORY_AND_DISK_SER ) //数据处理 //把句子拆为单词 val dstr1 = lines.flatMap(it =&gt; it.split(" ")) //转换单词 val dstr2 = dstr1.map((_, 1)) //词频 val res = dstr2.reduceByKey(_ + _) //展示和启动 res.print() // 类似 RDD 中的 Action, 执行最后的数据输出和收集 // 启动流和 JobGenerator, 开始流式处理数据 context.start() // main 方法执行完毕后整个程序就会退出, 所以需要阻塞主线程 context.awaitTermination() &#125; 可以在本地运行 也可以在集群运行 12集群运行spark-submit --class cn.itcast.streaming.StreamingWordCount --master local[6] original-streaming-0.0.1.jar node02 9999 5 总结注意点: 1Spark Streaming 并不是真正的来一条数据处理一条 Spark Streaming 的处理机制叫做小批量, 英文叫做 mini-batch, 是收集了一定时间的数据后生成 RDD, 后针对 RDD 进行各种转换操作, 这个原理提现在如下两个地方 2Spark Streaming 中至少要有两个线程 在使用 spark-submit 启动程序的时候, 不能指定一个线程 主线程被阻塞了, 等待程序运行 需要开启后台线程获取数据 创建StreamingContext StreamingContext 是 Spark Streaming 程序的入口 在创建 StreamingContext 的时候, 必须要指定两个参数, 一个是 SparkConf, 一个是流中生成 RDD 的时间间隔 StreamingContext 提供了如下功能 创建 DStream, 可以通过读取 Kafka, 读取 Socket 消息, 读取本地文件等创建一个流, 并且作为整个 DAG 中的 InputDStream RDD 遇到 Action 才会执行, 但是 DStream 不是, DStream 只有在 StreamingContext.start() 后才会开始接收数据并处理数据 使用 StreamingContext.awaitTermination() 等待处理被终止 使用 StreamingContext.stop() 来手动的停止处理 在使用的时候有如下注意点 同一个 Streaming 程序中, 只能有一个 StreamingContext 一旦一个 Context 已经启动 (start), 则不能添加新的数据源 ** 算子这些算子类似 RDD, 也会生成新的 DStream 这些算子操作最终会落到每一个 DStream 生成的 RDD 中 reduceByKey 这个算子需要特别注意, 这个聚合并不是针对于整个流, 而是针对于某个批次的数据 二原理1Spark Streaming 的特点 Spark Streaming 会源源不断的处理数据, 称之为流计算 Spark Streaming 并不是实时流, 而是按照时间切分小批量, 一个一个的小批量处理 Spark Streaming 是流计算, 所以可以理解为数据会源源不断的来, 需要长时间运行 2Spark Streaming 是按照时间切分小批量1Spark Streaming` 中的编程模型叫做 `DStream`, 所有的 `API` 都从 `DStream` 开始, 其作用就类似于 `RDD` 之于 `Spark Core 可以理解为 DStream 是一个管道, 数据源源不断的从这个管道进去, 被处理, 再出去 但是需要注意的是, DStream 并不是严格意义上的实时流, 事实上, DStream 并不处理数据, 而是处理 RDD Spark Streaming 是小批量处理数据, 并不是实时流 Spark Streaming 对数据的处理是按照时间切分为一个又一个小的 RDD, 然后针对 RDD 进行处理 RDD 中针对数据的处理是使用算子, 在 DStream 中针对数据的操作也是算子 难道 DStream 会把算子的操作交给 RDD 去处理? 如何交? 3 Spark Streaming 是流计算, 流计算的数据是无限的无限的数据一般指的是数据不断的产生, 比如说运行中的系统, 无法判定什么时候公司会倒闭, 所以也无法断定数据什么时候会不再产生数据 那就会产生一个问题 如何不简单的读取数据, 如何应对数据量时大时小? ​ 如何数据是无限的, 意味着可能要一直运行下去 那就会又产生一个问题 Spark Streaming 不会出错吗? 数据出错了怎么办? 四个问题: DStream 如何对应 RDD? 如何切分 RDD? 如何读取数据? 如何容错? 4 DAG的定义RDD 和 DStream 的 DAG rdd的wordcount : 1234val textRDD = sc.textFile(...)val splitRDD = textRDD.flatMap(_.split(" "))val tupleRDD = splitRDD.map((_, 1))val reduceRDD = tupleRDD.reduceByKey(_ + _) DStream的wordcount 123val lines: DStream[String] = ssc.socketTextStream(...)val words: DStream[String] = lines.flatMap(_.split(" "))val wordCounts: DStream[(String, Int)] = words.map(x =&gt; (x, 1)).reduceByKey(_ + _) 1RDD 和 DStream 的区别 DStream 的数据是不断进入的, RDD 是针对一个数据的操作 像 RDD 一样, DStream 也有不同的子类, 通过不同的算子生成 一个 DStream 代表一个数据集, 其中包含了针对于上一个数据的操作 DStream 根据时间切片, 划分为多个 RDD, 针对 DStream 的计算函数, 会作用于每一个 DStream 中的 RDD 2DStream 如何形式 DAG 每个 DStream 都有一个关联的 DStreamGraph 对象 DStreamGraph 负责表示 DStream 之间的的依赖关系和运行步骤 DStreamGraph 中会单独记录 InputDStream 和 OutputDStream 3切分流, 生成小批量静态和动态 DStream 对应 RDD DStreamGraph 表示 DStream 之间的依赖关系和运行流程, 相当于 RDD 通过 DAGScheduler 所生成的 RDD DAG RDD 的运行分为逻辑计划和物理计划 逻辑计划就是 RDD 之间依赖关系所构成的一张有向无环图 后根据这张 DAG 生成对应的 TaskSet 调度到集群中运行, 但是在 DStream 中则不能这么简单的划分, 因为 DStream 中有一个非常重要的逻辑, 需要按照时间片划分小批量 在 Streaming 中, DStream 类似 RDD, 生成的是静态的数据处理过程, 例如一个 DStream 中的数据经过 map 转为其它模样 在 Streaming 中, DStreamGraph 类似 DAG, 保存了这种数据处理的过程 上述两点, 其实描述的是静态的一张 DAG, 数据处理过程, 但是 Streaming 是动态的, 数据是源源不断的来的 所以, 在 DStream 中, 静态和动态是两个概念, 有不同的流程 DStreamGraph 将 DStream 联合起来, 生成 DStream 之间的 DAG, 这些 DStream 之间的关系是相互依赖的关系, 例如一个 DStream 经过 map 转为另外一个 DStream 但是把视角移动到 DStream 中来看, DStream 代表了源源不断的 RDD 的生成和处理, 按照时间切片, 所以一个 DStream DAG 又对应了随着时间的推进所产生的无限个 RDD DAG 动态生成 RDD DAG 的过程 RDD DAG 的生成是按照时间来切片的, Streaming 会维护一个 Timer, 固定的时间到达后通过如下五个步骤生成一个 RDD DAG 后调度执行 通知 Receiver 将收到的数据暂存, 并汇报存储的元信息, 例如存在哪, 存了什么 通过 DStreamGraph 复制出一套新的 RDD DAG 将数据暂存的元信息和 RDD DAG 一同交由 JobScheduler 去调度执行 提交结束后, 对系统当前的状态 Checkpoint 4数据的产生和导入1Receiver在 Spark Streaming 中一个非常大的挑战是, 很多外部的队列和存储系统都是分块的, RDD 是分区的, 在读取外部数据源的时候, 会用不同的分区对照外部系统的分片, 例如 不仅 RDD, DStream 中也面临这种挑战 DStream 中是 RDD 流, 只是 RDD 的分区对应了 Kafka 的分区就可以了吗? 答案是不行, 因为需要一套单独的机制来保证并行的读取外部数据源, 这套机制叫做 Receiver Receiver 的结构 为了保证并行获取数据, 对应每一个外部数据源的分区, 所以 Receiver 也要是分布式的, 主要分为三个部分 Receiver 是一个对象, 是可以有用户自定义的获取逻辑对象, 表示了如何获取数据 Receiver Tracker 是 Receiver 的协调和调度者, 其运行在 Driver 上 Receiver Supervisor 被 Receiver Tracker 调度到不同的几点上分布式运行, 其会拿到用户自定义的 Receiver 对象, 使用这个对象来获取外部数据 Receiver 的执行过程 在 Spark Streaming 程序开启时候, Receiver Tracker 使用 JobScheduler 分发 Job 到不同的节点, 每个 Job 包含一个 Task , 这个 Task 就是 Receiver Supervisor, 这个部分的源码还挺精彩的, 其实是复用了通用的调度逻辑 ReceiverSupervisor 启动后运行 Receiver 实例 Receiver 启动后, 就将持续不断地接收外界数据, 并持续交给 ReceiverSupervisor 进行数据存储 ReceiverSupervisor 持续不断地接收到 Receiver 转来的数据, 并通过 BlockManager 来存储数据 获取的数据存储完成后发送元数据给 Driver 端的 ReceiverTracker, 包含数据块的 id, 位置, 数量, 大小 等信息 2 Directdirect 使用Kafka低层次API实现的. 特点: 会创建和Kafka分区数一样的RDD分区.提升性能 使用checkpoint将分区/偏移量信息进行保存 0:2342344 因为直接根据分区id+偏移量获取消息,也解决了偏移量不一致问题. 总结: reciver 使用Kafka高层次API实现, 特点: 启动一个Reciver进行消费,启动和Kafka分区数一样的线程数进行消费,所以性能提升不大 为了确保数据安全,需要启用WAL预写日志功能,数据会在HDFS备份一份,Kafka里面默认也有一份,所以数据会被重复保存 偏移量是在zookeeper上面存储的,因为信息不同步,有可能引起数据的重复消费问题. 5 容错因为要非常长时间的运行, 对于任何一个流计算系统来说, 容错都是非常致命也非常重要的一环, 在 Spark Streaming 中, 大致提供了如下的容错手段 1 热备这行代码中的 StorageLevel.MEMORY_AND_DISK_SER 的作用是什么? 其实就是热备份 当 Receiver 获取到数据要存储的时候, 是交给 BlockManager 存储的 如果设置了 StorageLevel.MEMORY_AND_DISK_SER, 则意味着 BlockManager 不仅会在本机存储, 也会发往其它的主机进行存储, 本质就是冗余备份 如果某一个计算失败了, 通过冗余的备份, 再次进行计算即可 这是默认的容错手段 2 冷备冷备在 Spark Streaming 中的手段叫做 WAL (预写日志) 当 Receiver 获取到数据后, 会交给 BlockManager 存储 在存储之前先写到 WAL 中, WAL 中保存了 Redo Log, 其实就是记录了数据怎么产生的, 以便于恢复的时候通过 Log恢复 当出错的时候, 通过 Redo Log 去重放数据 3 重放 有一些上游的外部系统是支持重放的, 比如说 Kafka Kafka 可以根据 Offset 来获取数据 当 SparkStreaming 处理过程中出错了, 只需要通过 Kafka 再次读取即可 三操作1 中间状态需求: 统计整个流中, 所有出现的单词数量, 而不是一个批中的数量 在小试牛刀中 只能统计某个时间段内的单词数量, 因为 reduceByKey 只能作用于某一个 RDD, 不能作用于整个流 如果想要求单词总数该怎么办? 以使用状态来记录中间结果, 从而每次来一批数据, 计算后和中间状态求和, 于是就完成了总数的统计 使用 updateStateByKey 可以做到这件事 updateStateByKey 会将中间状态存入 CheckPoint 中 123456789101112131415161718192021222324252627282930313233343536373839404142//使用中间状态(updateStateByKey)记录结果 可以统计整个流中出现的单词数量 等操作 @Test def zhongjianStream(): Unit = &#123; // 1. 创建 Context val conf = new SparkConf() .setAppName("updateStateBykey") .setMaster("local[3]") val ssc = new StreamingContext(conf, Seconds(2)) ssc.sparkContext.setLogLevel("WARN") // 2. 读取数据生成 DStream val source = ssc.socketTextStream( hostname = "node03", port = 9999, storageLevel = StorageLevel.MEMORY_AND_DISK_SER_2 ) // 3. 词频统计 val wordsTuple = source.flatMap(_.split(" ")) .map((_, 1)) // 4. 全局聚合 ssc.checkpoint("checkpoint") def updateFunc(newValue: Seq[Int], runningValue: Option[Int]): Option[Int] = &#123; // newValue : 对应当前批次中 Key 对应的所有 Value // runningValue : 当前的中间结果 val currBatchValue = newValue.sum val state = runningValue.getOrElse(0) + currBatchValue Some(state) &#125; val result = wordsTuple.updateStateByKey[Int](updateFunc _) // 5. 输出 result.print() ssc.start() ssc.awaitTermination() &#125; 2 window操作需求: 计算过 30s 的单词总数, 每 10s 更新一次 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051//使用窗口Window操作 需求 计算过 30s 的单词总数, 每 10s 更新一次 @Test def winStream(): Unit = &#123; val sparkConf = new SparkConf().setAppName("NetworkWordCount").setMaster("local[6]") val ssc = new StreamingContext(sparkConf, Seconds(1)) ssc.sparkContext.setLogLevel("WARN") val lines: DStream[String] = ssc.socketTextStream( hostname = "node03", port = 9999, storageLevel = StorageLevel.MEMORY_AND_DISK_SER) val words = lines.flatMap(_.split(" ")).map(x =&gt; (x, 1)) // 通过 window 操作, 会将流分为多个窗口 val wordsWindow = words.window(Seconds(30), Seconds(10)) // 此时是针对于窗口求聚合 val wordCounts = wordsWindow.reduceByKey((newValue, runningValue) =&gt; newValue + runningValue) wordCounts.print() ssc.start() ssc.awaitTermination() &#125; //使用窗口函数的另一种方式' 既然 window 操作经常配合 reduce 这种聚合, 所以 Spark Streaming 提供了较为方便的方法 @Test def win2(): Unit =&#123; val sparkConf = new SparkConf().setAppName("NetworkWordCount").setMaster("local[6]") val ssc = new StreamingContext(sparkConf, Seconds(1)) //调整 Log 级别, 避免过多的 Log 影响视线 ssc.sparkContext.setLogLevel("warn") val lines: DStream[String] = ssc.socketTextStream( hostname = "node03", port = 9999, storageLevel = StorageLevel.MEMORY_AND_DISK_SER) val words = lines.flatMap(_.split(" ")).map(x =&gt; (x, 1)) // 开启窗口并自动进行 reduceByKey 的聚合 val wordCounts = words.reduceByKeyAndWindow( reduceFunc = (n, r) =&gt; n + r, windowDuration = Seconds(30), slideDuration = Seconds(10)) wordCounts.print() ssc.start() ssc.awaitTermination() &#125; 窗口时间 在 window 函数中, 接收两个参数 windowDuration 窗口长度, window 函数会将多个 DStream 中的 RDD 按照时间合并为一个, 那么窗口长度配置的就是将多长时间内的 RDD 合并为一个 slideDuration 滑动间隔, 比较好理解的情况是直接按照某个时间来均匀的划分为多个 window, 但是往往需求可能是统计最近 xx分 内的所有数据, 一秒刷新一次, 那么就需要设置滑动窗口的时间间隔了, 每隔多久生成一个 window 滑动时间的问题 如果 windowDuration &gt; slideDuration, 则在每一个不同的窗口中, 可能计算了重复的数据 如果 windowDuration &lt; slideDuration, 则在每一个不同的窗口之间, 有一些数据为能计算进去 但是其实无论谁比谁大, 都不能算错, 例如, 我的需求有可能就是统计一小时内的数据, 一天刷新两次]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>BigData</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sparkSql高级]]></title>
    <url>%2F2018%2F04%2F12%2FsparkSql%E9%AB%98%E7%BA%A7.html</url>
    <content type="text"><![CDATA[一聚合操作 groupBy rollup cube pivot RelationalGroupedDataset 上的聚合操作 1自定义udf与udaf123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115package com.nicai.wwwimport org.apache.spark.sql.expressions.&#123;MutableAggregationBuffer, UserDefinedAggregateFunction&#125;import org.apache.spark.sql.types._import org.apache.spark.sql.&#123;Dataset, Row, SparkSession&#125;import org.junit.Test/** * Created by 春雨里洗过的太阳 */class Udf extends Serializable &#123; private val spark: SparkSession = new SparkSession.Builder() .master("local[3]") .appName("act") .getOrCreate() import spark.implicits._ //自定义udf 需求 增加一列 值为第二列中 (1995) @Test def udfDemo(): Unit = &#123; val ds: Dataset[String] = spark.read .text("G:\\develop\\data\\movies.dat") .as[String] val unit: Dataset[(Long, String, String)] = ds.map(it =&gt; &#123; val arr: Array[String] = it.split("::") (arr(0).toLong, arr(1), arr(2)) &#125;) val frame = unit.toDF("id", "title", "gen") import org.apache.spark.sql.functions._ //自定以udf def zudf(title: String): String = &#123; val pattern = "(?&lt;=\\s\\()\\d&#123;4&#125;(?=\\))".r pattern.findFirstIn("Sabrina (1995)").getOrElse(" ") &#125; //注册udf val udf2 = udf(zudf _) //这个_ 是方法和函数的转换 frame.withColumn("year", udf2('title)) .show() &#125; //自定义udaf 需求 /*给定数据集 Dataset(userId: String, tags: Map(Tag, Weight)) 求每个用户的标签聚合字符串 "tag1:weight1,tag2:weight2"*/ @Test def udafMain(): Unit = &#123; val source = Seq( ("user1", Map("tag1" -&gt; 1.0, "tag2" -&gt; 2.0)), ("user1", Map("tag1" -&gt; 1.5, "tag3" -&gt; 2.0, "tag4" -&gt; 2.1)), ("user2", Map("tag1" -&gt; 1.5, "tag4" -&gt; 2.1)) ).toDF("id", "tags") source.groupBy("id") .agg(udafDemo2('id, 'tags) as "tags") .show() &#125;&#125;//自定义udafobject udafDemo2 extends UserDefinedAggregateFunction &#123; //聚合函数的输入数据结构 override def inputSchema: StructType = &#123; new StructType() .add("id", StringType) //就是数据的ID .add("tags", MapType(StringType, DoubleType)) //就是数据的Map &#125; // 缓存区数据结构 override def bufferSchema: StructType = &#123; new StructType() .add("tags", MapType(StringType, DoubleType)) //要处理的 数据列 &#125; //聚合函数返回值数据结构 override def dataType: DataType = StringType //聚合函数是否是幂等的 即相同输入是否总能的到相同的输出 override def deterministic: Boolean = true //初始化缓冲区 override def initialize(buffer: MutableAggregationBuffer): Unit = &#123; buffer(0) = Map[String, Double]() &#125; //给聚合函数传入一条新数据进行处理 override def update(buffer: MutableAggregationBuffer, input: Row): Unit = &#123; buffer(0) = mergeTagMap(buffer.getAs[Map[String, Double]](0), input.getAs[Map[String, Double]](1)) &#125; def mergeTagMap(m1: Map[String, Double], m2: Map[String, Double]): Map[String, Double] = &#123; val merge: Map[String, Double] = m1.map &#123; case (key, value) =&gt; if (m2.contains(key)) (key, value + m2(key)) else (key, value) &#125; m2 ++ merge &#125; //合并聚合函数缓冲区 override def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = &#123; buffer1(0) = mergeTagMap(buffer1.getAs[Map[String, Double]](0), buffer2.getAs[Map[String, Double]](0)) &#125; //计算最终结果 override def evaluate(buffer: Row): Any = &#123; val result = buffer.getAs[Map[String, Double]](0) result.map &#123; case (key, value) =&gt; key + ":" + value &#125; .mkString(",") &#125;&#125; 2 聚合groupBy 算子会按照列将 Dataset 分组, 并返回一个 RelationalGroupedDataset 对象, 通过 RelationalGroupedDataset 可以对分组进行聚合 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475package com.nicai.wwwimport org.apache.spark.sql.SparkSessionimport org.apache.spark.sql.types.&#123;DoubleType, IntegerType, StructField, StructType&#125;import org.junit.Test/** * Created by 春雨里洗过的太阳 */class Juhe &#123; private val spark: SparkSession = new SparkSession.Builder() .master("local[3]") .appName("act") .getOrCreate() import spark.implicits._ import org.apache.spark.sql.functions._ //使用 functions 函数进行聚合 @Test def groupBy(): Unit = &#123; val schema = StructType( List( StructField("id", IntegerType), StructField("year", IntegerType), StructField("month", IntegerType), StructField("day", IntegerType), StructField("hour", IntegerType), StructField("season", IntegerType), StructField("pm", DoubleType) ) ) val frame = spark.read .schema(schema) .option("header", true) .csv("G:\\develop\\bigdatas\\BigData\\day29sparksql2\\data\\pm_final.csv") //分组 val dataset = frame.groupBy('year) //聚合 dataset.agg(avg('pm) as "avg_pm") .orderBy('avg_pm) .show() &#125; // 除了使用 functions 进行聚合, 还可以直接使用 RelationalGroupedDataset 的 API 进行聚合 @Test def groupBy2(): Unit = &#123; val schema = StructType( List( StructField("id", IntegerType), StructField("year", IntegerType), StructField("month", IntegerType), StructField("day", IntegerType), StructField("hour", IntegerType), StructField("season", IntegerType), StructField("pm", DoubleType) ) ) val frame = spark.read .schema(schema) .option("header", true) .csv("G:\\develop\\data\\pm_final.csv") //分组 val dataset = frame.groupBy('year) //聚合 dataset.avg("pm") .orderBy("avg(pm)") .show() dataset.max("pm") .show() &#125;&#125; 3 多维聚合我们可能经常需要针对数据进行多维的聚合, 也就是一次性统计小计, 总计等, 一般的思路如下 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293class DuoWeiJuHe &#123; private val spark: SparkSession = new SparkSession.Builder() .master("local[3]") .appName("act") .getOrCreate() import spark.implicits._ import org.apache.spark.sql.functions._ //使用groupBy 多维聚合 @Test def groupByDuo(): Unit = &#123; val schema = StructType( List( StructField("source", StringType), StructField("year", IntegerType), StructField("month", IntegerType), StructField("day", IntegerType), StructField("hour", IntegerType), StructField("season", IntegerType), StructField("pm", DoubleType) ) ) val frame = spark.read .schema(schema) .option("header", true) .csv("G:\\develop\\bigdatas\\BigData\\day29sparksql2\\data\\pm_final.csv") //需求一 按地区 年份 求平均值 val sourceandYear = frame.groupBy('source, 'year) val sY = sourceandYear.agg(avg('pm) as "avg_pm") //需求二 按 地区 求平均值 val sour = frame.groupBy('source) val res = sour.agg(avg('pm) as "avg_pm") .select('source, lit(null) as "year", 'avg_pm) //聚合 val result = sY.union(res) .sort('source, 'year asc_nulls_last, 'avg_pm) result.show() &#125; //使用rollup 多维聚合 @Test def groupByDuo2(): Unit = &#123; val schema = StructType( List( StructField("source", StringType), StructField("year", IntegerType), StructField("month", IntegerType), StructField("day", IntegerType), StructField("hour", IntegerType), StructField("season", IntegerType), StructField("pm", DoubleType) ) ) val frame = spark.read .schema(schema) .option("header", true) .csv("G:\\develop\\bigdatas\\BigData\\day29sparksql2\\data\\pm_final.csv") frame.rollup('source, 'year) .agg(avg("pm")) .sort('source, 'year.desc_nulls_last) .show() &#125; //多维聚合 cube @Test def groupByDuo3(): Unit = &#123; val schema = StructType( List( StructField("source", StringType), StructField("year", IntegerType), StructField("month", IntegerType), StructField("day", IntegerType), StructField("hour", IntegerType), StructField("season", IntegerType), StructField("pm", DoubleType) ) ) val frame = spark.read .schema(schema) .option("header", true) .csv("G:\\develop\\bigdatas\\BigData\\day29sparksql2\\data\\pm_final.csv") frame.cube('source, 'year) .agg(avg("pm")) .sort('source, 'year.desc_nulls_last) .show() &#125;&#125; 4 连接 与广播连接连接分为两种 无类型连接 join 连接类型 Join Types 123456781 cross 交叉连接 笛卡尔积 全连接* 交叉连接是一个非常重的操作, 在生产中, 尽量不要将两个大数据集交叉连接, 如果一定要交叉连接, 也需要在交叉连接后进行过滤, 优化器会进行优化2 inner 内连接 只存在有关联的数据3 outer full fullouter 全外连接4 leftouter left 左外连接 包含两侧连接上的数据和左侧没有连接上的数据5 leftanti 是一种特殊的连接形式和左外连接类似,但是其结果集中没有右侧的数据,只包含左边集合中没连接上的数据6 leftsemi 和 LeftAnti 恰好相反, LeftSemi 的结果集也没有右侧集合的数据, 但是只包含左侧集合中连接上的数据7 rightouter right 右外连接 与左连接相反 包含两侧连接上的数据 和右侧没有连接上的数据8 广播连接 解决join的性能问题 应为join是一个shuffle操作 是一个小文件的广播操作 12345678910111213@Test def leftanti(): Unit = &#123; val person = Seq((0, "Lucy", 0), (1, "Lily", 0), (2, "Tim", 2), (3, "Danial", 3)) .toDF("id", "name", "cityId") person.createOrReplaceTempView("person") //创建临时表 val cities = Seq((0, "Beijing"), (1, "Shanghai"), (2, "Guangzhou")) .toDF("id", "name") cities.createOrReplaceTempView("cities") //创建临时表 person.join(cities, person.col("cityId") === cities.col("id"), "leftanti") .show() &#125; 广播连接 Join 会在集群中分发两个数据集, 两个数据集都要复制到 Reducer 端, 是一个非常复杂和标准的 ShuffleDependency 优化: Map 端 Join 之所以说它效率很低, 原因是需要在集群中进行数据拷贝, 如果能减少数据拷贝, 就能减少开销 可以将小数据集收集起来, 分发给每一个 Executor, 然后在需要 Join 的时候, 让较大的数据集在 Map 端直接获取小数据集, 从而进行 Join, 这种方式是不需要进行 Shuffle 的, 所以称之为 Map 端 Join 正常join图: 开启mapjoin图 12345678910111213141516171819202122//广播连接 @Test def leftanti2(): Unit = &#123; val personRDD = spark.sparkContext.parallelize(Seq((0, "Lucy", 0), (1, "Lily", 0), (2, "Tim", 2), (3, "Danial", 3))) val citiesRDD = spark.sparkContext.parallelize(Seq((0, "Beijing"), (1, "Shanghai"), (2, "Guangzhou"))) //设置广播 val guangbo = spark.sparkContext.broadcast(citiesRDD.collectAsMap()) //进行操作 val tuples = personRDD.mapPartitions( iter =&gt; &#123; val value = guangbo.value val result = for (person &lt;- iter if (value.contains(person._3))) yield (person._1, person._2, value(person._3)) result &#125; ).collect() tuples.foreach(println(_)) &#125; 1 使用 Dataset 实现 Join 的时候会自动进行 Map 端 Join12345678910自动进行 Map 端 Join 需要依赖一个系统参数 spark.sql.autoBroadcastJoinThreshold, 当数据集小于这个参数的大小时, 会自动进行 Map 端 Join如下, 开启自动 Joinprintln(spark.conf.get("spark.sql.autoBroadcastJoinThreshold").toInt / 1024 / 1024)println(person.crossJoin(cities).queryExecution.sparkPlan.numberedTreeString)当关闭这个参数的时候, 则不会自动 Map 端 Join 了spark.conf.set("spark.sql.autoBroadcastJoinThreshold", -1)println(person.crossJoin(cities).queryExecution.sparkPlan.numberedTreeString) 2 也可以使用函数强制开启 Map 端 Join12345678910111213在使用 Dataset 的 join 时, 可以使用 broadcast 函数来实现 Map 端 Joinimport org.apache.spark.sql.functions._spark.conf.set("spark.sql.autoBroadcastJoinThreshold", -1)println(person.crossJoin(broadcast(cities)).queryExecution.sparkPlan.numberedTreeString)即使是使用 SQL 也可以使用特殊的语法开启spark.conf.set("spark.sql.autoBroadcastJoinThreshold", -1)val resultDF = spark.sql( """ |select /*+ MAPJOIN (rt) */ * from person cross join cities rt """.stripMargin)println(resultDF.queryExecution.sparkPlan.numberedTreeString) 5 窗口函数1234窗口函数 解决了 在一个大的数据集里 进行分组 求前几个等需求** 窗口操作分为两个部分** 1窗口定义, 定义时可以指定 Partition, Order, Frame** 2函数操作, 可以使用三大类函数, 排名函数, 分析函数, 聚合函数 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889package com.nicai.wwwimport org.apache.spark.sql.SparkSessionimport org.apache.spark.sql.expressions.Windowimport org.junit.Test/** * Created by 春雨里洗过的太阳 on 2019/8/20 * * 窗口函数 解决了 在一个大的数据集里 进行分组 求前几个等需求 * * 窗口操作分为两个部分 * * 窗口定义, 定义时可以指定 Partition, Order, Frame * * 函数操作, 可以使用三大类函数, 排名函数, 分析函数, 聚合函数 */class WindowsDemo &#123; val spark = SparkSession.builder() .appName("window") .master("local[6]") .getOrCreate() import spark.implicits._ import org.apache.spark.sql.functions._//需求 求每个类别(第二列)的其中的值与这个类别中最大值的差值 @Test def chazhi(): Unit = &#123; val source = Seq( ("Thin", "Cell phone", 6000), ("Normal", "Tablet", 1500), ("Mini", "Tablet", 5500), ("Ultra thin", "Cell phone", 5500), ("Very thin", "Cell phone", 6000), ("Big", "Tablet", 2500), ("Bendable", "Cell phone", 3000), ("Foldable", "Cell phone", 3000), ("Pro", "Tablet", 4500), ("Pro2", "Tablet", 6500) ).toDF("product", "category", "rev") val win = Window.partitionBy('category) .orderBy('rev.desc) source.select( 'product,'category,'rev,(max('rev) over win )-'rev as "rev_diff" ).show() /*可以作用于 窗口函数的 函数 (max的位置) * 排名函数: rank排名函数, 计算当前数据在其 Frame 中的位置 如果有重复, 则重复项后面的行号会有空挡 * dense_rank 和 rank 一样, 但是结果中没有空挡 * * row_number 和 rank 一样, 也是排名, 但是不同点是即使有重复想, 排名依然增长 * 分析函数: * * first_value 获得这个租的第一条数据 * last_value 获得这个租的最后 一条数据 * lag lag(field, n) 获取当前数据的 field 列向前 n 条数据 * lead lead(field, n) 获取当前数据的 field 列向后 n 条数据 *聚合函数 所有 * */ &#125; @Test def partTopN(): Unit =&#123; val source = Seq( ("Thin", "Cell phone", 6000), ("Normal", "Tablet", 1500), ("Mini", "Tablet", 5500), ("Ultra thin", "Cell phone", 5500), ("Very thin", "Cell phone", 6000), ("Big", "Tablet", 2500), ("Bendable", "Cell phone", 3000), ("Foldable", "Cell phone", 3000), ("Pro", "Tablet", 4500), ("Pro2", "Tablet", 6500) ).toDF("product", "category", "rev") val win = Window.partitionBy('category) .orderBy('rev.desc) source.select( 'product,'category,'rev,rank() over win as "rank" ) .where('rank &lt;= 2) .show() &#125;&#125; 问题1: Spark 和 Hive 这样的系统中, 有自增主键吗? 没有 问题2: 为什么分布式系统中很少见自增主键? 因为分布式环境下数据在不同的节点中, 很难保证顺序 解决方案: 按照某一列去排序, 取前两条数据 遗留问题: 不容易在分组中取每一组的前两个 窗口函数在sql中的语法 12345678910111213SELECT product, category, revenueFROM ( SELECT product, category, revenue, dense_rank() OVER (PARTITION BY category ORDER BY revenue DESC) as rank FROM productRevenue) tmpWHERE rank &lt;= 2 窗口函数在 SQL 中的完整语法如下 1function OVER (PARITION BY ... ORDER BY ... FRAME_TYPE BETWEEN ... AND ...) 在 Spark 中, 使用 SQL 或者 DataFrame 都可以操作窗口 窗口函数和 GroupBy 最大的区别, 就是 GroupBy 的聚合对每一个组只有一个结果, 而窗口函数可以对每一条数据都有一个结果 说白了, 窗口函数其实就是根据当前数据, 计算其在所在的组中的统计数据 窗口函数会针对 每一个组中的每一条数据 进行统计聚合或者 rank, 一个组又称为一个 Frame 分组由两个字段控制, Partition 在整体上进行分组和分区 而通过 Frame 可以通过 当前行 来更细粒度的分组控制]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>BigData</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SparkSQL]]></title>
    <url>%2F2018%2F03%2F20%2FSparkSQL.html</url>
    <content type="text"><![CDATA[一 概述1 数据分析的方式数据分析的方式大致上可以划分为 SQL 和 命令式两种 命令式 在前面的 RDD 部分, 非常明显可以感觉的到是命令式的, 主要特征是通过一个算子, 可以得到一个结果, 通过结果再进行后续计算. 在前面的 RDD 部分, 非常明显可以感觉的到是命令式的, 主要特征是通过一个算子, 可以得到一个结果, 通过结果再进行后续计算. 12345sc.textFile("...") .flatMap(_.split(" ")) .map((_, 1)) .reduceByKey(_ + _) .collect() 命令式的优点 操作粒度更细, 能够控制数据的每一个处理环节操作更明确, 步骤更清晰, 容易维护支持非结构化数据的操作 命令式的缺点 需要一定的代码功底写起来比较麻烦 SQL 对于一些数据科学家, 要求他们为了做一个非常简单的查询, 写一大堆代码, 明显是一件非常残忍的事情, 所以 SQL on Hadoop 是一个非常重要的方向. 123456SELECT name, age, schoolFROM studentsWHERE age &gt; 10 SQL 的优点 表达非常清晰, 比如说这段 SQL 明显就是为了查询三个字段, 又比如说这段 SQL 明显能看到是想查询年龄大于 10 岁的条目 SQL 的缺点 想想一下 3 层嵌套的 SQL, 维护起来应该挺力不从心的吧 试想一下, 如果使用 SQL 来实现机器学习算法, 也挺为难的吧 SQL 擅长数据分析和通过简单的语法表示查询, 命令式操作适合过程式处理和算法性的处理. 在 Spark 出现之前, 对于结构化数据的查询和处理, 一个工具一向只能支持 SQL 或者命令式, 使用者被迫要使用多个工具来适应两种场景, 并且多个工具配合起来比较费劲. 而 Spark 出现了以后, 统一了两种数据处理范式, 是一种革新性的进步. 过程 因为 SQL 是数据分析领域一个非常重要的范式, 所以 Spark 一直想要支持这种范式, 而伴随着一些决策失误, 这个过程其实还是非常曲折的 Hive 解决的问题 Hive 实现了 SQL on Hadoop, 使用 MapReduce 执行任务简化了 MapReduce 任务 新的问题 Hive 的查询延迟比较高, 原因是使用 MapReduce 做调度 Shark 解决的问题 Shark 改写 Hive 的物理执行计划, 使用 Spark 作业代替 MapReduce 执行物理计划使用列式内存存储以上两点使得 Shark 的查询效率很高 新的问题 Shark 重用了 Hive 的 SQL 解析, 逻辑计划生成以及优化, 所以其实可以认为 Shark 只是把 Hive 的物理执行替换为了 Spark 作业执行计划的生成严重依赖 Hive, 想要增加新的优化非常困难Hive 使用 MapReduce 执行作业, 所以 Hive 是进程级别的并行, 而 Spark 是线程级别的并行, 所以 Hive 中很多线程不安全的代码不适用于 Spark 由于以上问题, Shark 维护了 Hive 的一个分支, 并且无法合并进主线, 难以为继 SparkSQL 解决的问题 Spark SQL 使用 Hive 解析 SQL 生成 AST 语法树, 将其后的逻辑计划生成, 优化, 物理计划都自己完成, 而不依赖 Hive执行计划和优化交给优化器 Catalyst内建了一套简单的 SQL 解析器, 可以不使用 HQL, 此外, 还引入和 DataFrame 这样的 DSL API, 完全可以不依赖任何 Hive 的组件Shark 只能查询文件, Spark SQL 可以直接降查询作用于 RDD, 这一点是一个大进步 新的问题 对于初期版本的 SparkSQL, 依然有挺多问题, 例如只能支持 SQL 的使用, 不能很好的兼容命令式, 入口不够统一等 Dataset SparkSQL 在 2.0 时代, 增加了一个新的 API, 叫做 Dataset, Dataset 统一和结合了 SQL 的访问和命令式 API 的使用, 这是一个划时代的进步 在 Dataset 中可以轻易的做到使用 SQL 查询并且筛选数据, 然后使用命令式 API 进行探索式分析 注意 SparkSQL 不只是一个 SQL 引擎, SparkSQL 也包含了一套对 结构化数据的命令式 API, 事实上, 所有 Spark中常见的工具, 都是依赖和依照于 SparkSQL 的 API 设计的 总结 SparkSQL 是一个为了支持 SQL 而设计的工具, 但同时也支持命令式的 API 底层是rdd 2 sparkSql应用场景 定义 特点 举例 结构化数据 有固定的 Schema 有预定义的 Schema 关系型数据库的表 半结构化数据 没有固定的 Schema, 但是有结构 没有固定的 Schema, 有结构信息, 数据一般是自描述的 指一些有结构的文件格式, 例如 JSON 非结构化数据 没有固定 Schema, 也没有结构 没有固定 Schema, 也没有结构 指文档图片之类的格式 结构化数据 如关系型数据库 一般指数据有固定的 Schema, 例如在用户表中, name 字段是 String 型, 那么每一条数据的 name 字段值都可以当作 String 来使用 123456789+----+--------------+---------------------------+-------+---------+| id | name | url | alexa | country |+----+--------------+---------------------------+-------+---------+| 1 | Google | https://www.google.cm/ | 1 | USA || 2 | 淘宝 | https://www.taobao.com/ | 13 | CN || 3 | 菜鸟教程 | http://www.runoob.com/ | 4689 | CN || 4 | 微博 | http://weibo.com/ | 20 | CN || 5 | Facebook | https://www.facebook.com/ | 3 | USA |+----+--------------+---------------------------+-------+---------+ 半结构化数据 一般指的是数据没有固定的 Schema, 但是数据本身是有结构的 12345678910111213141516&#123; "firstName": "John", "lastName": "Smith", "age": 25, "phoneNumber": [ &#123; "type": "home", "number": "212 555-1234" &#125;, &#123; "type": "fax", "number": "646 555-4567" &#125; ] &#125; 没有固定 Schema 指的是半结构化数据是没有固定的 Schema 的, 可以理解为没有显式指定 Schema比如说一个用户信息的 JSON 文件, 第一条数据的 phone_num 有可能是 String, 第二条数据虽说应该也是 String, 但是如果硬要指定为 BigInt, 也是有可能的因为没有指定 Schema, 没有显式的强制的约束 有结构 虽说半结构化数据是没有显式指定 Schema 的, 也没有约束, 但是半结构化数据本身是有有隐式的结构的, 也就是数据自身可以描述自身例如 JSON 文件, 其中的某一条数据是有字段这个概念的, 每个字段也有类型的概念, 所以说 JSON 是可以描述自身的, 也就是数据本身携带有元信息 SparkSQL 处理什么数据的问题? Spark 的 RDD 主要用于处理 非结构化数据 和 半结构化数据 SparkSQL 主要用于处理 结构化数据 SparkSQL 相较于 RDD 的优势在哪? SparkSQL 提供了更好的外部数据源读写支持 因为大部分外部数据源是有结构化的, 需要在 RDD 之外有一个新的解决方案, 来整合这些结构化数据源 SparkSQL 提供了直接访问列的能力 因为 SparkSQL 主要用做于处理结构化数据, 所以其提供的 API 具有一些普通数据库的能力 总结 虽然Sparksql是基于rdd的但是sparkSql的速度比rdd快很多,sparksql可以针对结构化数据的API进行更好的操作 SparkSQL 适用于处理结构化数据的场景 SparkSQL 是一个即支持 SQL 又支持命令式数据处理的工具 SparkSQL 的主要适用场景是处理结构化数据 二 SparkSql 处理数据123456789101112131415161718192021case class Person(name: String, age: Int) @Test def sqlDemo(): Unit = &#123; val spark = new SparkSession.Builder() .appName("sql") .master("local[3]") .getOrCreate()//注意: spark 在此处不是包, 而是 SparkSession 对象 import spark.implicits._ val sourceRDD = spark.sparkContext.parallelize(Seq(Person("zhangsan", 10), Person("lisi", 15))) val personDS = sourceRDD.toDS() val resultDS = personDS.where('age &gt; 10) .where('age &lt; 20) .select('name) .as[String] resultDS.show() &#125; SparkSQL 中有一个新的入口点, 叫做 SparkSession SparkSQL 中有一个新的类型叫做 Dataset SparkSQL 有能力直接通过字段名访问数据集, 说明 SparkSQL 的 API 中是携带 Schema 信息的 SparkSession SparkContext 作为 RDD 的创建者和入口, 其主要作用有如下两点 创建 RDD, 主要是通过读取文件创建 RDD监控和调度任务, 包含了一系列组件, 例如 DAGScheduler, TaskSheduler 为什么无法使用 SparkContext 作为 SparkSQL 的入口? SparkContext 在读取文件的时候, 是不包含 Schema 信息的, 因为读取出来的是 RDD`SparkContext在整合数据源如Cassandra,JSON,Parquet等的时候是不灵活的, 而DataFrame和Dataset一开始的设计目标就是要支持更多的数据源SparkContext的调度方式是直接调度RDD`, 但是一般情况下针对结构化数据的访问, 会先通过优化器优化一下 所以 SparkContext 确实已经不适合作为 SparkSQL 的入口, 所以刚开始的时候 Spark 团队为 SparkSQL 设计了两个入口点, 一个是 SQLContext 对应 Spark 标准的 SQL 执行, 另外一个是 HiveContext 对应 HiveSQL 的执行和 Hive 的支持. 在 Spark 2.0 的时候, 为了解决入口点不统一的问题, 创建了一个新的入口点 SparkSession, 作为整个 Spark 生态工具的统一入口点, 包括了 SQLContext, HiveContext, SparkContext 等组件的功能 新的入口应该有什么特性? 能够整合 SQLContext, HiveContext, SparkContext, StreamingContext 等不同的入口点为了支持更多的数据源, 应该完善读取和写入体系同时对于原来的入口点也不能放弃, 要向下兼容 2.1 DataSet 和 DataFrame 1SparkSQL` 最大的特点就是它针对于结构化数据设计, 所以 `SparkSQL` 应该是能支持针对某一个字段的访问的, 而这种访问方式有一个前提, 就是 `SparkSQL` 的数据集中, 要 **包含结构化信息**, 也就是俗称的 `Schema 而 SparkSQL 对外提供的 API 有两类, 一类是直接执行 SQL, 另外一类就是命令式. SparkSQL 提供的命令式 API 就是 DataFrame 和 Dataset, 暂时也可以认为 DataFrame 就是 Dataset, 只是在不同的 API 中返回的是 Dataset 的不同表现形式 123456// RDDrdd.map &#123; case Person(id, name, age) =&gt; (age, 1) &#125; .reduceByKey &#123;case ((age, count), (totalAge, totalCount)) =&gt; (age, count + totalCount)&#125;// DataFramedf.groupBy("age").count("age") 例如: 123456789101112131415case class Person(name: String, age: Int)@Test def sqlDataSet(): Unit = &#123; val spark = new SparkSession.Builder() .appName("sql") .master("local[3]") .getOrCreate() import spark.implicits._ val sourRdd = spark.createDataset(Seq(new Person("你猜", 22), new Person("我你猜", 56))) val frame = sourRdd.toDF() frame.createOrReplaceTempView("per") val frame2 = spark.sql("select name from per where age &gt; 23") frame2.show() &#125; 以往使用 SQL 肯定是要有一个表的, 在 Spark 中, 并不存在表的概念, 但是有一个近似的概念, 叫做 DataFrame, 所以一般情况下要先通过 DataFrame 或者 Dataset 注册一张临时表, 然后使用 SQL 操作这张临时表 总结 SparkSQL 提供了 SQL 和 命令式 API 两种不同的访问结构化数据的形式, 并且它们之间可以无缝的衔接 命令式 API 由一个叫做 Dataset 的组件提供, 其还有一个变形, 叫做 DataFrame 三 Catalyst 优化器3.1 rdd与sparksql 的对比rdd运行流程 大致运行步骤 先将 RDD 解析为由 Stage 组成的 DAG, 后将 Stage 转为 Task 直接运行 问题 任务会按照代码所示运行, 依赖开发者的优化, 开发者的会在很大程度上影响运行效率 解决办法 创建一个组件, 帮助开发者修改和优化代码, 但是这在 RDD 上是无法实现的 为什么 RDD 无法自我优化? RDD 没有 Schema 信息 RDD 可以同时处理结构化和非结构化的数据 SparkSQL 和 RDD 不同, SparkSQL 的 Dataset 和 SQL 并不是直接生成计划交给集群执行, 而是经过了一个叫做 Catalyst 的优化器, 这个优化器能够自动帮助开发者优化代码 也就是说, 在 SparkSQL 中, 开发者的代码即使不够优化, 也会被优化为相对较好的形式去执行 为什么 SparkSQL 提供了这种能力? 首先, SparkSQL 大部分情况用于处理结构化数据和半结构化数据, 所以 SparkSQL 可以获知数据的 Schema, 从而根据其 Schema 来进行优化 3.2 Catalyst为了解决过多依赖 Hive 的问题, SparkSQL 使用了一个新的 SQL 优化器替代 Hive 中的优化器, 这个优化器就是 Catalyst, 整个 SparkSQL 的架构大致如下 API 层简单的说就是 Spark 会通过一些 API 接受 SQL 语句 收到 SQL 语句以后, 将其交给 Catalyst, Catalyst 负责解析 SQL, 生成执行计划等 Catalyst 的输出应该是 RDD 的执行计划 最终交由集群运行 Step 1 : 解析 SQL, 并且生成 AST (抽象语法树) Step 2 : 在 AST 中加入元数据信息, 做这一步主要是为了一些优化, 例如 col = col 这样的条件, Step 3 : 对已经加入元数据的 AST, 输入优化器, 进行优化, 从两种常见的优化开始, 简单介绍 列值裁剪 Column Pruning, 在谓词下推后, people 表之上的操作只用到了 id 列, 所以可以把其它列裁剪掉, 这样可以减少处理的数据量, 从而优化处理速度 谓词下推 Predicate Pushdown, 将 Filter 这种可以减小数据集的操作下推, 放在 Scan 的位置, 这样可以减少操作时候的数据量 还有其余很多优化点, 大概一共有一二百种, 随着 SparkSQL 的发展, 还会越来越多, 感兴趣的同学可以继续通过源码了解, 源码在 org.apache.spark.sql.catalyst.optimizer.Optimizer Step 4 : 上面的过程生成的 AST 其实最终还没办法直接运行, 这个 AST 叫做 逻辑计划, 结束后, 需要生成 物理计划, 从而生成 RDD 来运行 在生成物理计划的时候, 会经过成本模型对整棵树再次执行优化, 选择一个更好的计划 在生成物理计划以后, 因为考虑到性能, 所以会使用代码生成, 在机器中运行 总结 123SparkSQL 和 RDD 不同的主要点是在于其所操作的数据是结构化的, 提供了对数据更强的感知和分析能力, 能够对代码进行更深层的优化, 而这种能力是由一个叫做 Catalyst 的优化器所提供的Catalyst 的主要运作原理是分为三步, 先对 SQL 或者 Dataset 的代码解析, 生成逻辑计划, 后对逻辑计划进行优化, 再生成物理计划, 最后生成代码到集群中以 RDD 的形式运行 四 DataSet的特点12345678910111213141516class DataSetDemo &#123; @Test def dataSet(): Unit =&#123; val spark = new SparkSession.Builder() .master("local[3]") .appName("dataset") .getOrCreate() import spark.implicits._ val dataset: Dataset[People] = spark.createDataset(Seq(People("zhangsan", 9), People("lisi", 15))) dataset.filter(it =&gt; it.age&gt;5) dataset.filter('age &gt; 5) dataset.filter("age&gt;5").show &#125;&#125;case class People (name:String,age:Int) Dataset 是什么? 1Dataset` 是一个强类型, 并且类型安全的数据容器, 并且提供了结构化查询 `API` 和类似 `RDD` 一样的命令式 `API 即使使用 Dataset 的命令式 API, 执行计划也依然会被优化 Dataset 具有 RDD 的方便, 同时也具有 DataFrame 的性能优势, 并且 Dataset 还是强类型的, 能做到类型安全. 123456789101112131415161718scala&gt; spark.range(1).filter('id === 0).explain(true)== Parsed Logical Plan =='Filter ('id = 0)+- Range (0, 1, splits=8)== Analyzed Logical Plan ==id: bigintFilter (id#51L = cast(0 as bigint))+- Range (0, 1, splits=8)== Optimized Logical Plan ==Filter (id#51L = 0)+- Range (0, 1, splits=8)== Physical Plan ==*Filter (id#51L = 0)+- *Range (0, 1, splits=8) dataSet底层 Dataset 最底层处理的是对象的序列化形式, 通过查看 Dataset 生成的物理执行计划, 也就是最终所处理的 RDD, 就可以判定 Dataset 底层处理的是什么形式的数据 12val dataset: Dataset[People] = spark.createDataset(Seq(People("zhangsan", 9), People("lisi", 15)))val internalRDD: RDD[InternalRow] = dataset.queryExecution.toRdd 1dataset.queryExecution.toRdd` 这个 `API` 可以看到 `Dataset` 底层执行的 `RDD`, 这个 `RDD` 中的范型是 `InternalRow`, `InternalRow` 又称之为 `Catalyst Row`, 是 `Dataset` 底层的数据结构, 也就是说, 无论 `Dataset` 的范型是什么, 无论是 `Dataset[Person]` 还是其它的, 其最底层进行处理的数据结构都是 `InternalRow 所以, Dataset 的范型对象在执行之前, 需要通过 Encoder 转换为 InternalRow, 在输入之前, 需要把 InternalRow 通过 Decoder 转换为范型对象 可以获取 Dataset 对应的 RDD 表示 12345678910111213141516171819在 Dataset 中, 可以使用一个属性 rdd 来得到它的 RDD 表示, 例如 Dataset[T] → RDD[T]val dataset: Dataset[People] = spark.createDataset(Seq(People("zhangsan", 9), People("lisi", 15)))/*(2) MapPartitionsRDD[3] at rdd at Testing.scala:159 [] | MapPartitionsRDD[2] at rdd at Testing.scala:159 [] | MapPartitionsRDD[1] at rdd at Testing.scala:159 [] | ParallelCollectionRDD[0] at rdd at Testing.scala:159 [] */1 使用 Dataset.rdd 将 Dataset 转为 RDD 的形式println(dataset.rdd.toDebugString) // 这段代码的执行计划为什么多了两个步骤?/*(2) MapPartitionsRDD[5] at toRdd at Testing.scala:160 [] | ParallelCollectionRDD[4] at toRdd at Testing.scala:160 [] */2 Dataset 的执行计划底层的 RDDprintln(dataset.queryExecution.toRdd.toDebugString) 可以看到 (1) 对比 (2) 对了两个步骤, 这两个步骤的本质就是将 Dataset 底层的 InternalRow 转为 RDD 中的对象形式, 这个操作还是会有点重的, 所以慎重使用 rdd 属性来转换 Dataset 为 RDD 总结 Dataset 是一个新的 Spark 组件, 其底层还是 RDD Dataset 提供了访问对象中某个特定字段的能力, 不用像 RDD 一样每次都要针对整个对象做操作 **Dataset 和 RDD 不同, 如果想把 Dataset[T] 转为 RDD[T], 则需要对 Dataset 底层的 InternalRow 做转换, 是一个比较重量级的操作 一般不对dataset转换为rdd** 五 DataFrame的特点DataFrame 是 SparkSQL 中一个表示关系型数据库中 表 的函数式抽象, 其作用是让 Spark 处理大规模结构化数据的时候更加容易. 一般 DataFrame 可以处理结构化的数据, 或者是半结构化的数据, 因为这两类数据中都可以获取到 Schema 信息. 也就是说 DataFrame 中有 Schema 信息, 可以像操作表一样操作 DataFrame. DataFrame 由两部分构成, 一是 row 的集合, 每个 row 对象表示一个行, 二是描述 DataFrame 结构的 Schema DataFrame 支持 SQL 中常见的操作, 例如: select, filter, join, group, sort, join 等 1234567891011val spark: SparkSession = new sql.SparkSession.Builder() .appName("hello") .master("local[6]") .getOrCreate()import spark.implicits._val peopleDF: DataFrame = Seq(People("zhangsan", 15), People("lisi", 15)).toDF()peopleDF.groupBy('age) .count() .show() 通过隐式转换创建 DataFrame 这种方式本质上是使用 SparkSession 中的隐式转换来进行的 12345678910val spark: SparkSession = new sql.SparkSession.Builder() .appName("hello") .master("local[6]") .getOrCreate()// 必须要导入隐式转换// 注意: spark 在此处不是包, 而是 SparkSession 对象import spark.implicits._val peopleDF: DataFrame = Seq(People("zhangsan", 15), People("lisi", 15)).toDF() 根据源码可以知道, toDF 方法可以在 RDD 和 Seq 中使用 通过集合创建 DataFrame 的时候, 集合中不仅可以包含样例类, 也可以只有普通数据类型, 后通过指定列名来创建 123456789101112131415161718192021222324252627282930val spark: SparkSession = new sql.SparkSession.Builder() .appName("hello") .master("local[6]") .getOrCreate()import spark.implicits._val df1: DataFrame = Seq("nihao", "hello").toDF("text")/*+-----+| text|+-----+|nihao||hello|+-----+ */df1.show()val df2: DataFrame = Seq(("a", 1), ("b", 1)).toDF("word", "count")/*+----+-----+|word|count|+----+-----+| a| 1|| b| 1|+----+-----+ */df2.show() 通过外部集合创建 DataFrame 12345678910val spark: SparkSession = new sql.SparkSession.Builder() .appName("hello") .master("local[6]") .getOrCreate()val df = spark.read .option("header", true) .csv("dataset/BeijingPM20100101_20151231.csv")df.show(10)df.printSchema() 总结 123456789DataFrame 是一个类似于关系型数据库表的函数式组件DataFrame 一般处理结构化数据和半结构化数据DataFrame 具有数据对象的 Schema 信息可以使用命令式的 API 操作 DataFrame, 同时也可以使用 SQL 操作 DataFrameDataFrame 可以由一个已经存在的集合直接创建, 也可以读取外部的数据源来创建 dataset与dataframe的异同1DataFrame 就是 Dataset Dataset 中可以使用列来访问数据, DataFrame 也可以 Dataset 的执行是优化的, DataFrame 也是 Dataset 具有命令式 API, 同时也可以使用 SQL 来访问, DataFrame 也可以使用这两种不同的方式访问 1DataFrame 是 Dataset 的一种特殊情况, 也就是说 DataFrame 是 Dataset[Row] 的别名 2 语义不同第一点: DataFrame 表达的含义是一个支持函数式操作的 表, 而 Dataset 表达是是一个类似 RDD 的东西, Dataset 可以处理任何对象 第二点: DataFrame 中所存放的是 Row 对象, 而 Dataset 中可以存放任何类型的对象 第三点: DataFrame 的操作方式和 Dataset 是一样的, 但是对于强类型操作而言, 它们处理的类型不同 第三点: DataFrame 只能做到运行时类型检查, Dataset 能做到编译和运行时都有类型检查 row是什么 123Row 对象表示的是一个 行Row 的操作类似于 Scala 中的 Map 数据类型 3 DataFrame 和 Dataset 之间可以非常简单的相互转换123456789101112val spark: SparkSession = new sql.SparkSession.Builder() .appName("hello") .master("local[6]") .getOrCreate()import spark.implicits._val df: DataFrame = Seq(People("zhangsan", 15), People("lisi", 15)).toDF()val ds_fdf: Dataset[People] = df.as[People]val ds: Dataset[People] = Seq(People("zhangsan", 15), People("lisi", 15)).toDS()val df_fds: DataFrame = ds.toDF() 总结 12345DataFrame 就是 Dataset, 他们的方式是一样的, 也都支持 API 和 SQL 两种操作方式DataFrame 只能通过表达式的形式, 或者列的形式来访问数据, 只有 Dataset 支持针对于整个对象的操作DataFrame 中的数据表示为 Row, 是一个行的概念 六 读写1读文件: DataFrameReader123456789101112131415161718192021@Test def reader(): Unit =&#123; val builder = new SparkSession.Builder() .master("local[3]") .appName("read") .getOrCreate() val read = builder.read //read类型为DataFrameReader .format("csv") .option("header",true) .option("schema",true) .load("day28SparkSql/data/BeijingPM20100101_20151231.csv") //第二种 底层是format加load builder.read .option("header",true) .option("schema",true) .csv("day28SparkSql/data/BeijingPM20100101_20151231.csv") &#125;//如果使用 load 方法加载数据, 但是没有指定 format 的话, 默认是按照 Parquet 文件格式读取//也就是说, SparkSQL 默认的读取格式是 Parquet 组件: schema :结构信息, 因为 Dataset 是有结构的, 所以在读取数据的时候, 就需要有 Schema 信息, 有可能是从外部数据源获取的, 也有可能是指定的 option:连接外部数据源的参数, 例如 JDBC 的 URL, 或者读取 CSV 文件是否引入 Header 等 format:外部数据源的格式, 例如 csv, jdbc, json 等 总结 12345使用 spark.read 可以获取 SparkSQL 中的外部数据源访问框架 DataFrameReaderDataFrameReader 有三个组件 format, schema, optionDataFrameReader 有两种使用方式, 一种是使用 load 加 format 指定格式, 还有一种是使用封装方法 csv, json 等 2 写文件DataFrameWriter对于 ETL 来说, 数据保存和数据读取一样重要, 所以 SparkSQL 中增加了一个新的数据写入框架, 叫做 DataFrameWriter 1234567891011121314//写文件 @Test def writer(): Unit =&#123; val sparkSession = new spark.sql.SparkSession.Builder() .master("local[3]") .appName("writer") .getOrCreate() //System.setProperty("hadoop.home","c:\\winutils")//win特有 val reader = sparkSession.read.option("he",true).csv("G:\\develop\\data\\BeijingPM20100101_20151231.csv") //reader.write.json("G:\\develop\\bigdatas\\BigData\\day28SparkSql\\data\\beijin_json")//生成的文件夹 //第二种 reader.write.format("json").save("G:\\develop\\data\\beijin_json2") //生成的文件夹 &#125; 组件: source 写入目标, 文件格式等, 通过 format 方法设定 mode 写入模式, 例如一张表已经存在, 如果通过 DataFrameWriter 向这张表中写入数据, 是覆盖表呢, 还是向表中追加呢? 通过 mode方法设定 extraOptions 外部参数, 例如 JDBC 的 URL, 通过 options, option 设定 partitioningColumns 类似 Hive 的分区, 保存表的时候使用, 这个地方的分区不是 RDD的分区, 而是文件的分区, 或者表的分区, 通过 partitionBy 设定 bucketColumnNames 类似 Hive 的分桶, 保存表的时候使用, 通过 bucketBy 设定 sortColumnNames 用于排序的列, 通过 sortBy 设定 mode 指定了写入模式, 例如覆盖原数据集, 或者向原数据集合中尾部添加等 SaveMode.ErrorIfExists &quot;error&quot; 将 DataFrame 保存到 source 时, 如果目标已经存在, 则报错 SaveMode.Append &quot;append&quot; 将 DataFrame 保存到 source 时, 如果目标已经存在, 则添加到文件或者 Table中 SaveMode.Overwrite &quot;overwrite&quot; 将 DataFrame 保存到 source 时, 如果目标已经存在, 则使用 DataFrame 中的数据完全覆盖目标 SaveMode.Ignore &quot;ignore&quot; 将 DataFrame 保存到 source 时, 如果目标已经存在, 则不会保存 DataFrame 数据, 并且也不修改目标数据集, 类似于 CREATE TABLE IF NOT EXISTS 总结 1234567类似 DataFrameReader, Writer 中也有 format, options, 另外 schema 是包含在 DataFrame 中的DataFrameWriter 中还有一个很重要的概念叫做 mode, 指定写入模式, 如果目标集合已经存在时的行为DataFrameWriter 可以将数据保存到 Hive 表中, 所以也可以指定分区和分桶信息读写的默认格式都是parquet 3 读写parquet格式文件parquet简介 在ETL中 spark经常为T 的职务 就是清洗和数据转换 E 加载数据 L 落地数据 为了能够保存比较复杂的数据, 并且保证性能和压缩率, 通常使用 Parquet 是一个比较不错的选择. 所以外部系统收集过来的数据, 有可能会使用 Parquet, 而 Spark 进行读取和转换的时候, 就需要支持对 Parquet 格式的文件的支持. 123456789101112@Test def parquetDemo(): Unit =&#123; val sparkSession = new spark.sql.SparkSession.Builder() .master("local[3]") .appName("parquet") .getOrCreate() val frame = sparkSession.read.format("csv").option("header",true).load("G:\\develop\\data\\BeijingPM20100101_20151231.csv") //写文件parquet // frame.write.mode(SaveMode.Append).format("parquet").save("G:\\develop\\data\\beijin_json_wr_parquet") //读parquet文件 sparkSession.read.format("parquet").option("header",true).load("G:\\develop\\bigdatas\\BigData\\day28SparkSql\\data\\beijin_json_wr_parquet").show(10) &#125; 4 写入parquet的时候可以指定分区这个地方指的分区是类似 Hive 中表分区的概念, 而不是 RDD 分布式分区的含义 在读取常见文件格式的时候, Spark 会自动的进行分区发现, 分区自动发现的时候, 会将文件名中的分区信息当作一列. 例如 如果按照性别分区, 那么一般会生成两个文件夹 gender=male 和 gender=female, 那么在使用 Spark 读取的时候, 会自动发现这个分区信息, 并且当作列放入创建的 DataFrame 中 使用代码证明这件事可以有两个步骤, 第一步先读取某个分区的单独一个文件并打印其 Schema 信息, 第二步读取整个数据集所有分区并打印 Schema 信息, 和第一步做比较就可以确定 12345678910111213141516@Test def parquetPartition(): Unit = &#123; val sparkSession = new spark.sql.SparkSession.Builder() .master("local[3]") .appName("parquet") .getOrCreate() val frame = sparkSession.read.format("csv").option("header", true).load("G:\\develop\\bigdatas\\BigData\\day28SparkSql\\data\\BeijingPM20100101_20151231.csv") //写分区文件 // frame.write.partitionBy("year","month").option("header",true).save("G:\\\\develop\\\\bigdatas\\\\BigData\\\\day28SparkSql\\\\data\\\\beijin_json_wr_partition") //读分区文件 并打印信息 parquet可以直接自动发现分区 // 3. 读文件, 自动发现分区 // 写分区表的时候, 分区列不会包含在生成的文件中 // 直接通过文件来进行读取的话, 分区信息会丢失 // spark sql 会进行自动的分区发现 sparkSession.read.option("header",true).load("G:\\\\develop\\\\bigdatas\\\\BigData\\\\day28SparkSql\\\\data\\\\beijin_json_wr_partition").printSchema() &#125; SparkSession 中有关 Parquet 的配置 spark.sql.parquet.binaryAsString false 一些其他 Parquet 生产系统, 不区分字符串类型和二进制类型, 该配置告诉 SparkSQL 将二进制数据解释为字符串以提供与这些系统的兼容性 spark.sql.parquet.int96AsTimestamp true 一些其他 Parquet 生产系统, 将 Timestamp 存为 INT96, 该配置告诉 SparkSQL 将 INT96 解析为 Timestamp spark.sql.parquet.cacheMetadata true 打开 Parquet 元数据的缓存, 可以加快查询静态数据 spark.sql.parquet.compression.codec snappy 压缩方式, 可选 uncompressed, snappy, gzip, lzo spark.sql.parquet.mergeSchema false 当为 true 时, Parquet 数据源会合并从所有数据文件收集的 Schemas 和数据, 因为这个操作开销比较大, 所以默认关闭 spark.sql.optimizer.metadataOnly true 如果为 true, 会通过原信息来生成分区列, 如果为 false 则就是通过扫描整个数据集来确定 总结 12345Spark 不指定 format 的时候默认就是按照 Parquet 的格式解析文件Spark 在读取 Parquet 文件的时候会自动的发现 Parquet 的分区和分区字段Spark 在写入 Parquet 文件的时候如果设置了分区字段, 会自动的按照分区存储 5 读写json格式的文件在业务系统中, JSON 是一个非常常见的数据格式, 在前后端交互的时候也往往会使用 JSON, 所以从业务系统获取的数据很大可能性是使用 JSON 格式, 所以就需要 Spark 能够支持 JSON 格式文件的读取 就是etl 中的e dataframe与json的相互转换 123456789101112131415161718//读写json文件 @Test def json(): Unit =&#123; val sparkSession = new spark.sql.SparkSession.Builder() .master("local[3]") .appName("parquet") .getOrCreate() val frame = sparkSession.read .format("csv") .option("header", true) .load("G:\\develop\\bigdatas\\BigData\\day28SparkSql\\data\\BeijingPM20100101_20151231.csv") //dataframe 转为json // frame.toJSON.show(10) //将json转为dataframe //从消息队列中取出JSON格式的数据, 需要使用 SparkSQL 进行处理 val rdd = frame.toJSON.rdd sparkSession.read.json(rdd).show(10) &#125; 读写json 12345678910111213141516171819//读写json文件 @Test def json1(): Unit =&#123; val sparkSession = new spark.sql.SparkSession.Builder() .master("local[3]") .appName("parquet") .getOrCreate() val frame = sparkSession.read .format("csv") .option("header", true) .load("G:\\develop\\bigdatas\\BigData\\day28SparkSql\\data\\BeijingPM20100101_20151231.csv") //writer为json frame.repartition(1)//必须写为第一个 .write .format("json") .option("header",true) .save("G:\\develop\\bigdatas\\BigData\\day28SparkSql\\data\\beijing-json3") &#125; frame.repartition(1)//必须写为第一个 如果不重新分区, 则会为 DataFrame 底层的 RDD 的每个分区生成一个文件, 为了保持只有一个输出文件, 所以重新分区 保存为 JSON 格式的文件有一个细节需要注意, 这个 JSON 格式的文件中, 每一行是一个独立的 JSON, 但是整个文件并不只是一个 JSON 字符串, 所以这种文件格式很多时候被成为 JSON Line 文件, 有时候后缀名也会变为 jsonl Spark 读取 JSON Line 文件的时候, 会自动的推断类型信息 假设业务系统通过 Kafka 将数据流转进入大数据平台, 这个时候可能需要使用 RDD 或者 Dataset 来读取其中的内容, 这个时候一条数据就是一个 JSON 格式的字符串, 如何将其转为 DataFrame 或者 Dataset[Object] 这样具有 Schema 的数据集呢? 使用如下代码就可以 1234567val spark: SparkSession = ...import spark.implicits._val peopleDataset = spark.createDataset( """&#123;"name":"Yin","address":&#123;"city":"Columbus","state":"Ohio"&#125;&#125;""" :: Nil)spark.read.json(peopleDataset).show() 总结 JSON 通常用于系统间的交互, Spark 经常要读取 JSON 格式文件, 处理, 放在另外一处 使用 DataFrameReader 和 DataFrameWriter 可以轻易的读取和写入 JSON, 并且会自动处理数据类型信息 6 访问hive和一个文件格式不同, Hive 是一个外部的数据存储和查询引擎, 所以如果 Spark 要访问 Hive 的话, 就需要先整合 Hive 只需整合MetaStore, 元数据存储 因为SparkSQL 内置了 HiveSQL 的支持, 所以无需整合查询引擎 首先要开启Hive 的MetaStore Hive 的 MetaStore 是一个 Hive 的组件, 一个 Hive 提供的程序, 用以保存和访问表的元数据, 整个 Hive 的结构大致如下 其实 Hive 中主要的组件就三个, HiveServer2 负责接受外部系统的查询请求, 例如 JDBC, HiveServer2 接收到查询请求后, 交给 Driver 处理, Driver 会首先去询问 MetaStore 表在哪存, 后 Driver 程序通过 MR 程序来访问 HDFS从而获取结果返回给查询请求者 而 Hive 的 MetaStore 对 SparkSQL 的意义非常重大, 如果 SparkSQL 可以直接访问 Hive 的 MetaStore, 则理论上可以做到和 Hive 一样的事情, 例如通过 Hive 表查询数据 而 Hive 的 MetaStore 的运行模式有三种 内嵌 Derby 数据库模式 这种模式不必说了, 自然是在测试的时候使用, 生产环境不太可能使用嵌入式数据库, 一是不稳定, 二是这个 Derby 是单连接的, 不支持并发 Local 模式 Local 和 Remote 都是访问 MySQL 数据库作为存储元数据的地方, 但是 Local 模式的 MetaStore 没有独立进程, 依附于 HiveServer2 的进程 Remote 模式 和 Loca 模式一样, 访问 MySQL 数据库存放元数据, 但是 Remote 的 MetaStore 运行在独立的进程中 我们显然要选择 Remote 模式, 因为要让其独立运行, 这样才能让 SparkSQL 一直可以访问 hive 开启metastore 修改hive-site.xml 12345678910111213141516171819202122232425262728293031323334&lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/user/hive/warehouse&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://node01:3306/hive?createDatabaseIfNotExist=true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;username&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;password&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.metastore.local&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.metastore.uris&lt;/name&gt; &lt;value&gt;thrift://node01:9083&lt;/value&gt; //当前服务器&lt;/property&gt; 启动 hive的metastore后台 1nohup /export/servers/hive/bin/hive --service metastore 2&gt;&amp;1 &gt;&gt; /var/log.log &amp; 即使不去整合 MetaStore, Spark 也有一个内置的 MateStore, 使用 Derby 嵌入式数据库保存数据, 但是这种方式不适合生产环境, 因为这种模式同一时间只能有一个 SparkSession 使用, 所以生产环境更推荐使用 Hive 的 MetaStore SparkSQL 整合 Hive 的 MetaStore 主要思路就是要通过配置能够访问它, 并且能够使用 HDFS 保存 WareHouse, 这些配置信息一般存在于 Hadoop 和 HDFS 的配置文件中, 所以可以直接拷贝 Hadoop 和 Hive 的配置文件到 Spark 的配置目录 把一下三个配置文件copy进spark/conf目录下 123Spark 需要 hive-site.xml 的原因是, 要读取 Hive 的配置信息, 主要是元数据仓库的位置等信息Spark 需要 core-site.xml 的原因是, 要读取安全有关的配置 Hadoop中Spark 需要 hdfs-site.xml 的原因是, 有可能需要在 HDFS 中放置表文件, 所以需要 HDFS 的配置Hadoop中 注意如果不希望通过拷贝文件的方式整合 Hive, 也可以在 SparkSession 启动的时候, 通过指定 Hive 的 MetaStore 的位置来访问, 但是更推荐整合的方式 7 访问hive表1234567在 Hive 中创建表使用 SparkSQL 访问 Hive 中已经存在的表使用 SparkSQL 创建 Hive 表使用 SparkSQL 修改 Hive 表中的数据 在 Hive 中创建表 第一步, 需要先将文件上传到集群中, 使用如下命令上传到 HDFS 中 12hdfs dfs -mkdir -p /datasethdfs dfs -put studenttabl10k /dataset/ 第二步, 使用 Hive 或者 Beeline 执行如下 SQL 1234567891011121314151617CREATE DATABASE IF NOT EXISTS spark_integrition;USE spark_integrition;CREATE EXTERNAL TABLE student( name STRING, age INT, gpa string)ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' LINES TERMINATED BY '\n'STORED AS TEXTFILELOCATION '/dataset/hive';LOAD DATA INPATH '/dataset/studenttab10k' OVERWRITE INTO TABLE student; 通过 SparkSQL 查询 Hive 的表 查询 Hive 中的表可以直接通过 spark.sql(…) 来进行, 可以直接在其中访问 Hive 的 MetaStore, 前提是一定要将 Hive 的配置文件拷贝到 Spark 的 conf 目录 123scala&gt; spark.sql("use spark_integrition")scala&gt; val resultDF = spark.sql("select * from student limit 10")scala&gt; resultDF.show() 通过 SparkSQL 创建 Hive 表 通过 SparkSQL 可以直接创建 Hive 表, 并且使用 LOAD DATA 加载数据 1234567891011121314151617181920val createTableStr = """ |CREATE EXTERNAL TABLE student |( | name STRING, | age INT, | gpa string |) |ROW FORMAT DELIMITED | FIELDS TERMINATED BY '\t' | LINES TERMINATED BY '\n' |STORED AS TEXTFILE |LOCATION '/dataset/hive' """.stripMarginspark.sql("CREATE DATABASE IF NOT EXISTS spark_integrition1")spark.sql("USE spark_integrition1")spark.sql(createTableStr)spark.sql("LOAD DATA INPATH '/dataset/studenttab10k' OVERWRITE INTO TABLE student")spark.sql("select * from student limit").show() 目前 SparkSQL 支持的文件格式有 sequencefile, rcfile, orc, parquet, textfile, avro, 并且也可以指定 serde 的名称 使用 SparkSQL 处理数据并保存进 Hive 表 前面都在使用 SparkShell 的方式来访问 Hive, 编写 SQL, 通过 Spark 独立应用的形式也可以做到同样的事, 但是需要一些前置的步骤, 如下 Step 1: 导入 Maven 依赖 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-hive_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt;&lt;/dependency&gt; Step 2: 配置 SparkSession 如果希望使用 SparkSQL 访问 Hive 的话, 需要做两件事 开启 SparkSession 的 Hive 支持 经过这一步配置, SparkSQL 才会把 SQL 语句当作 HiveSQL 来进行解析 设置 WareHouse 的位置 虽然 hive-stie.xml 中已经配置了 WareHouse 的位置, 但是在 Spark 2.0.0 后已经废弃了 hive-site.xml中设置的 hive.metastore.warehouse.dir, 需要在 SparkSession 中设置 WareHouse 的位置 设置 MetaStore 的位置 1234567val spark = SparkSession .builder() .appName("hive example") .config("spark.sql.warehouse.dir", "hdfs://node01:8020/dataset/hive") //设置 WareHouse 的位置 .config("hive.metastore.uris", "thrift://node01:9083") //设置 MetaStore 的位置 .enableHiveSupport() //开启hive支持 .getOrCreate() 配置好了以后, 就可以通过 DataFrame 处理数据, 后将数据结果推入 Hive 表中了, 在将结果保存到 Hive 表的时候, 可以指定保存模式 12345678910111213141516val schema = StructType( List( StructField("name", StringType), StructField("age", IntegerType), StructField("gpa", FloatType) ))val studentDF = spark.read .option("delimiter", "\t") .schema(schema) .csv("dataset/studenttab10k")val resultDF = studentDF.where("age &lt; 50")resultDF.write.mode(SaveMode.Overwrite).saveAsTable("spark_integrition1.student") //通过 mode 指定保存模式, 通过 saveAsTable 保存数据到 Hive 8 访问MySQL jdbc123通过 SQL 操作 MySQL 的表将数据写入 MySQL 的表中 准备MySQL环境 Step 1: 连接 MySQL 数据库 在 MySQL 所在的主机上执行如下命令 1mysql -u root -p Step 2: 创建 Spark 使用的用户 登进 MySQL 后, 需要先创建用户 1CREATE USER 'spark'@'%' IDENTIFIED BY 'Spark123!'; Step 3: 创建库和表 1234567891011CREATE DATABASE spark_test;USE spark_test;CREATE TABLE IF NOT EXISTS `student`(`id` INT AUTO_INCREMENT,`name` VARCHAR(100) NOT NULL,`age` INT NOT NULL,`gpa` FLOAT,PRIMARY KEY ( `id` ))ENGINE=InnoDB DEFAULT CHARSET=utf8; 赋予权限 1GRANT ALL ON spark_test.* TO 'spark'@'%'; 1 写数据其实在使用 SparkSQL 访问 MySQL 是通过 JDBC, 那么其实所有支持 JDBC 的数据库理论上都可以通过这种方式进行访问 在使用 JDBC 访问关系型数据的时候, 其实也是使用 DataFrameReader, 对 DataFrameReader 提供一些配置, 就可以使用 Spark 访问 JDBC, 有如下几个配置可用 属性 含义 url 要连接的 JDBC URL dbtable 要访问的表, 可以使用任何 SQL 语句中 from 子句支持的语法 fetchsize 数据抓取的大小(单位行), 适用于读的情况 batchsize 数据传输的大小(单位行), 适用于写的情况 isolationLevel 事务隔离级别, 是一个枚举, 取值 NONE, READ_COMMITTED, READ_UNCOMMITTED, REPEATABLE_READ, SERIALIZABLE, 默认为 READ_UNCOMMITTED 12345678910111213141516171819202122232425val spark = SparkSession .builder() .appName("hive example") .master("local[6]") .getOrCreate()val schema = StructType( List( StructField("name", StringType), StructField("age", IntegerType), StructField("gpa", FloatType) ))val studentDF = spark.read .option("delimiter", "\t") .schema(schema) .csv("dataset/studenttab10k")studentDF.write.format("jdbc").mode(SaveMode.Overwrite) .option("url", "jdbc:mysql://node01:3306/spark_test") .option("dbtable", "student") .option("user", "spark") .option("password", "Spark123!") .save() 运行 本地运行导入依赖 12345&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.47&lt;/version&gt;&lt;/dependency&gt; 如果使用 Spark submit 或者 Spark shell 来运行任务, 需要通过 --jars 参数提交 MySQL 的 Jar 包, 或者指定 --packages 从 Maven 库中读取 1bin/spark-shell --packages mysql:mysql-connector-java:5.1.47 --repositories http://maven.aliyun.com/nexus/content/groups/public/ 2 读数据读取 MySQL 的方式也非常的简单, 只是使用 SparkSQL 的 DataFrameReader 加上参数配置即可访问 1234567spark.read.format("jdbc") .option("url", "jdbc:mysql://node01:3306/spark_test") .option("dbtable", "student") .option("user", "spark") .option("password", "Spark123!") .load() .show() 默认情况下读取 MySQL 表时, 从 MySQL 表中读取的数据放入了一个分区, 拉取后可以使用 DataFrame 重分区来保证并行计算和内存占用不会太高, 但是如果感觉 MySQL 中数据过多的时候, 读取时可能就会产生 OOM, 所以在数据量比较大的场景, 就需要在读取的时候就将其分发到不同的 RDD 分区 属性 含义 partitionColumn 指定按照哪一列进行分区, 只能设置类型为数字的列, 一般指定为 ID lowerBound, upperBound 确定步长的参数, lowerBound - upperBound 之间的数据均分给每一个分区, 小于 lowerBound 的数据分给第一个分区, 大于 upperBound 的数据分给最后一个分区 numPartitions 分区数量 1234567891011spark.read.format("jdbc") .option("url", "jdbc:mysql://node01:3306/spark_test") .option("dbtable", "student") .option("user", "spark") .option("password", "Spark123!") .option("partitionColumn", "age") .option("lowerBound", 1) .option("upperBound", 60) .option("numPartitions", 10) .load() .show() 有时候可能要使用非数字列来作为分区依据, Spark 也提供了针对任意类型的列作为分区依据的方法 123456789101112131415161718val predicates = Array( "age &lt; 20", "age &gt;= 20, age &lt; 30", "age &gt;= 30")val connectionProperties = new Properties()connectionProperties.setProperty("user", "spark")connectionProperties.setProperty("password", "Spark123!")spark.read .jdbc( url = "jdbc:mysql://node01:3306/spark_test", table = "student", predicates = predicates, connectionProperties = connectionProperties ) .show() SparkSQL 中并没有直接提供按照 SQL 进行筛选读取数据的 API 和参数, 但是可以通过 dbtable 来曲线救国, dbtable 指定目标表的名称, 但是因为 dbtable 中可以编写 SQL, 所以使用子查询即可做到 1234567891011spark.read.format("jdbc") .option("url", "jdbc:mysql://node01:3306/spark_test") .option("dbtable", "(select name, age from student where age &gt; 10 and age &lt; 20) as stu") .option("user", "spark") .option("password", "Spark123!") .option("partitionColumn", "age") .option("lowerBound", 1) .option("upperBound", 60) .option("numPartitions", 10) .load() .show() 七 dataset的基础操作1 有类型操作转换map t=&gt; r flatMap t=&gt; list mapPartitions list =&gt; list 数据必须可以放在内存才可以使用 数据不可以大到每个分区都存不下 不然会内存溢出 00m 堆溢出 transfrom 针对数据集 直接针对dataset进行操作 返回和参数都是dataset as 最常见操作 dataframe转为dataset 如读取数据的时候是dataframereader 大部分都是dataframe的数据类型 可以使用as 完成操作 12345678910111213141516import spark.implicits._val structType = StructType( Seq( StructField("name", StringType), StructField("age", IntegerType), StructField("gpa", FloatType) ))val sourceDF = spark.read .schema(structType) .option("delimiter", "\t") .csv("dataset/studenttab10k")val dataset = sourceDF. 过滤:filter 用来按照条件过滤数据集 返回值为boolean 聚合groupByKey grouByKey 算子的返回结果是 KeyValueGroupedDataset, 而不是一个 Dataset, 所以必须要先经过 KeyValueGroupedDataset 中的方法进行聚合, 再转回 Dataset, 才能使用 Action 得出结果 其实这也印证了分组后必须聚合的道理 切分randomSplit 12345/*randomSplit 会按照传入的权重随机将一个 Dataset 分为多个 Dataset, 传入 randomSplit 的数组有多少个权重, 最终就会生成多少个 Dataset, 这些权重的加倍和应该为 1, 否则将被标准化*/val ds = spark.range(15)val datasets: Array[Dataset[lang.Long]] = ds.randomSplit(Array[Double](2, 3))datasets.foreach(dataset =&gt; dataset.show()) sample 随机抽样 12val ds = spark.range(15)ds.sample(withReplacement = false, fraction = 0.4).show() 排序orderBy orderBy 配合 Column 的 API, 可以实现正反序排列 1234import spark.implicits._val ds = Seq(Person("zhangsan", 12), Person("zhangsan", 8), Person("lisi", 15)).toDS()ds.orderBy("age").show()ds.orderBy('age.desc).show() sort 其实 orderBy 是 sort 的别名, 所以它们所实现的功能是一样的 12Person("lisi", 15)).toDS()ds.sort('age.desc).show() 分区coalesce 1234/*减少分区, 此算子和 RDD 中的 coalesce 不同, Dataset 中的 coalesce 只能减少分区数, coalesce 会直接创建一个逻辑操作, 并且设置 Shuffle 为 false*/val ds = spark.range(15)ds.coalesce(1).explain(true) repartitions 12345/*repartitions 有两个作用, 一个是重分区到特定的分区数, 另一个是按照某一列来分区, 类似于 SQL 中的 DISTRIBUTE BY*/val ds = Seq(Person("zhangsan", 12), Person("zhangsan", 8), Person("lisi", 15)).toDS()ds.repartition(4)ds.repartition('name) 去重dropDuplicates 使用 dropDuplicates 可以去掉某一些列中重复的行 123import spark.implicits._val ds = spark.createDataset(Seq(Person("zhangsan", 15), Person("zhangsan", 15), Person("lisi", 15)))ds.dropDuplicates("age").show() distinct 当 dropDuplicates 中没有传入列名的时候, 其含义是根据所有列去重, dropDuplicates() 方法还有一个别名, 叫做 distinct 所以, 使用 distinct 也可以去重, 并且只能根据所有的列来去重 123import spark.implicits._val ds = spark.createDataset(Seq(Person("zhangsan", 15), Person("zhangsan", 15), Person("lisi", 15)))ds.distinct().show() 集合操作:except 差集 intersect 交集 union 并集 limit 限制结果集数量 12val ds = spark.range(1, 10)ds.limit(3).show() 2无类型转换选择select select 用来选择某些列出现在结果集中 123import spark.implicits._val ds = Seq(Person("zhangsan", 12), Person("zhangsan", 8), Person("lisi", 15)).toDS()ds.select($"name").show() selectExpr 在 SQL 语句中, 经常可以在 select 子句中使用 count(age), rand() 等函数, 在 selectExpr 中就可以使用这样的 SQL 表达式, 同时使用 select 配合 expr函数也可以做到类似的效果 123456import spark.implicits._import org.apache.spark.sql.functions._val ds = Seq(Person("zhangsan", 12), Person("zhangsan", 8), Person("lisi", 15)).toDS()ds.selectExpr("count(age) as count").show()ds.selectExpr("rand() as random").show()ds.select(expr("count(age) as count")).show() //这种必须导入import org.apache.spark.sql.functions._ withColumn通过 Column 对象在 Dataset 中创建一个新的列或者修改原来的列 1234import spark.implicits._import org.apache.spark.sql.functions._val ds = Seq(Person("zhangsan", 12), Person("zhangsan", 8), Person("lisi", 15)).toDS()ds.withColumn("random", expr("rand()")).show() withColumnRenamed 修改列名 123import spark.implicits._val ds = Seq(Person("zhangsan", 12), Person("zhangsan", 8), Person("lisi", 15)).toDS()ds.withColumnRenamed("name", "new_name").show() 剪除drop 剪除某个列 123import spark.implicits._val ds = Seq(Person("zhangsan", 12), Person("zhangsan", 8), Person("lisi", 15)).toDS()ds.drop('age).show() 聚合groupBy按照给定的行进行分组 123import spark.implicits._val ds = Seq(Person("zhangsan", 12), Person("zhangsan", 8), Person("lisi", 15)).toDS()ds.groupBy('name).count().show() 3 Column 对象 Column 表示了 Dataset 中的一个列, 并且可以持有一个表达式, 这个表达式作用于每一条数据, 对每条数据都生成一个值, 列的操作属于细节, 但是又比较常见, 会在很多算子中配合出现 无绑定创建1 单引号 &#39; 在 Scala 中是一个特殊的符号, 通过 &#39; 会生成一个 Symbol 对象, Symbol 对象可以理解为是一个字符串的变种, 但是比字符串的效率高很多, 在 Spark 中, 对 Scala 中的 Symbol 对象做了隐式转换, 转换为一个 ColumnName 对象, ColumnName 是 Column 的子类, 所以在 Spark 中可以如下去选中一个列 12345val spark = SparkSession.builder().appName("column").master("local[6]").getOrCreate()import spark.implicits._val personDF = Seq(Person("zhangsan", 12), Person("zhangsan", 8), Person("lisi", 15)).toDS()val c1: Symbol = 'name 2 $ 符号也是一个隐式转换, 同样通过 spark.implicits 导入, 通过 $ 可以生成一个 Column 对象 12345val spark = SparkSession.builder().appName("column").master("local[6]").getOrCreate()import spark.implicits._val personDF = Seq(Person("zhangsan", 12), Person("zhangsan", 8), Person("lisi", 15)).toDS()val c2: ColumnName = $"name" 3 col SparkSQL 提供了一系列的函数, 可以通过函数实现很多功能, 在后面课程中会进行详细介绍, 这些函数中有两个可以帮助我们创建 Column 对象, 一个是 col, 另外一个是 column 12345val spark = SparkSession.builder().appName("column").master("local[6]").getOrCreate()import org.apache.spark.sql.functions._val personDF = Seq(Person("zhangsan", 12), Person("zhangsan", 8), Person("lisi", 15)).toDS()val c3: sql.Column = col("name") 4 column 12val c4: sql.Column = column("name") 有绑定创建1 Dataset.col 前面的 Column 对象创建方式所创建的 Column 对象都是 Free 的, 也就是没有绑定任何 Dataset, 所以可以作用于任何 Dataset, 同时, 也可以通过 Dataset 的 col 方法选择一个列, 但是这个 Column 是绑定了这个 Dataset 的, 所以只能用于创建其的 Dataset 上 1234val spark = SparkSession.builder().appName("column").master("local[6]").getOrCreate()val personDF = Seq(Person("zhangsan", 12), Person("zhangsan", 8), Person("lisi", 15)).toDS()val c5: sql.Column = personDF.col("name") 2 Dataset.apply 1234567val spark = SparkSession.builder().appName("column").master("local[6]").getOrCreate()val personDF = Seq(Person("zhangsan", 12), Person("zhangsan", 8), Person("lisi", 15)).toDS()val c6: sql.Column = personDF.apply("name")apply 的调用有一个简写形式val c7: sql.Column = personDF("name") 别名和转换as[type]通过 as[Type] 的形式可以将一个列中数据的类型转为 Type 类型 1personDF.select(col("age").as[Long]).show() as(name) 123通过 as(name) 的形式使用 as 方法可以为列创建别名personDF.select(col("age").as("age_new")).show() 添加列withColumn 12通过 Column 在添加一个新的列时候修改 Column 所代表的列的数据personDF.withColumn("double_age", 'age * 2).show() 操作like 123//通过 Column 的 API, 可以轻松实现 SQL 语句中 LIKE 的功能personDF.filter('name like "%zhang%").show() isin 123//通过 Column 的 API, 可以轻松实现 SQL 语句中 ISIN 的功能personDF.filter('name isin ("hello", "zhangsan")).show() sort 123//在排序的时候, 可以通过 Column 的 API 实现正反序personDF.sort('age.asc).show() //desc降序 4 缺失值处理缺失值 null NaN 空字符串 等 产生原因 Spark 大多时候处理的数据来自于业务系统中, 业务系统中可能会因为各种原因, 产生一些异常的数据 例如说因为前后端的判断失误, 提交了一些非法参数. 再例如说因为业务系统修改 MySQL 表结构产生的一些空值数据等. 总之在业务系统中出现缺失值其实是非常常见的一件事, 所以大数据系统就一定要考虑这件事. 常见缺失值有两种 null, NaN 等特殊类型的值, 某些语言中 null 可以理解是一个对象, 但是代表没有对象, NaN 是一个数字, 可以代表不是数字 针对这一类的缺失值, Spark 提供了一个名为 DataFrameNaFunctions 特殊类型来操作和处理 &quot;Null&quot;, &quot;NA&quot;, &quot; &quot; 等解析为字符串的类型, 但是其实并不是常规字符串数据 针对这类字符串, 需要对数据集进行采样, 观察异常数据, 总结经验, 各个击破 DataFrameNaFunctions1234567891011//DataFrameNaFunctions 使用 Dataset 的 na 函数来获取val df = ...val naFunc: DataFrameNaFunctions = df.na//当数据集中出现缺失值的时候, 大致有两种处理方式, 一个是丢弃, 一个是替换为某值, DataFrameNaFunctions 中包含一系列针对空值数据的方案DataFrameNaFunctions.drop //可以在当某行中包含 null 或 NaN 的时候丢弃此行DataFrameNaFunctions.fill //可以在将 null 和 NaN 充为其它值DataFrameNaFunctions.replace //可以把 null 或 NaN 替换为其它值, 但是和 fill 略有一些不同, 这个方法针对值来进行替换 处理null和NaN12345678910111213141516171819202122232425262728293031323334首先要将数据读取出来, 此次使用的数据集直接存在 NaN, 在指定 Schema 后, 可直接被转为 Double.NaNval schema = StructType( List( StructField("id", IntegerType), StructField("year", IntegerType), StructField("month", IntegerType), StructField("day", IntegerType), StructField("hour", IntegerType), StructField("season", IntegerType), StructField("pm", DoubleType) ))val df = spark.read .option("header", value = true) .schema(schema) .csv("dataset/beijingpm_with_nan.csv")//对于缺失值的处理一般就是丢弃和填充//丢弃包含 null 和 NaN 的行//当某行数据所有值都是 null 或者 NaN 的时候丢弃此行df.na.drop("all").show()当某行中特定列所有值都是 null 或者 NaN 的时候丢弃此行df.na.drop("all", List("pm", "id")).show()当某行数据任意一个字段为 null 或者 NaN 的时候丢弃此行df.na.drop().show()df.na.drop("any").show()当某行中特定列任意一个字段为 null 或者 NaN 的时候丢弃此行df.na.drop(List("pm", "id")).show()df.na.drop("any", List("pm", "id")).show() 填充包含 null 和 NaN 的列 1234567891011填充所有包含 null 和 NaN 的列df.na.fill(0).show()填充特定包含 null 和 NaN 的列df.na.fill(0, List("pm")).show()根据包含 null 和 NaN 的列的不同来填充import scala.collection.JavaConverters._df.na.fill(Map[String, Any]("pm" -&gt; 0).asJava).show 使用是parkSQl处理异常字符串 12345678910111213141516171819202122//读取数据集, 这次读取的是最原始的那个 PM 数据集val df = spark.read .option("header", value = true) .csv("dataset/BeijingPM20100101_20151231.csv")//使用函数直接转换非法的字符串df.select('No as "id", 'year, 'month, 'day, 'hour, 'season, when('PM_Dongsi === "NA", 0) .otherwise('PM_Dongsi cast DoubleType) .as("pm")) .show()//使用 where 直接过滤df.select('No as "id", 'year, 'month, 'day, 'hour, 'season, 'PM_Dongsi) .where('PM_Dongsi =!= "NA") .show()//使用 DataFrameNaFunctions 替换, 但是这种方式被替换的值和新值必须是同类型df.select('No as "id", 'year, 'month, 'day, 'hour, 'season, 'PM_Dongsi) .na.replace("PM_Dongsi", Map("NA" -&gt; "NaN")) .show()]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>BigData</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark原理浅析2]]></title>
    <url>%2F2017%2F08%2F22%2Fspark%E5%8E%9F%E7%90%86%E6%B5%85%E6%9E%902.html</url>
    <content type="text"><![CDATA[一 原理1 spark部署情况在 Spark 部分的底层执行逻辑开始之前, 还是要先认识一下 Spark 的部署情况, 根据部署情况, 从而理解如何调度. 针对于上图, 首先可以看到整体上在集群中运行的角色有如下几个: Master Daemon 负责管理 Master 节点, 协调资源的获取, 以及连接 Worker 节点来运行 Executor, 是 Spark 集群中的协调节点 Worker Daemon Workers 也称之为叫 Slaves, 是 Spark 集群中的计算节点, 用于和 Master 交互并管理 Executor. 当一个 Spark Job 提交后, 会创建 SparkContext, 后 Worker 会启动对应的 Executor. Executor Backend 上面有提到 Worker 用于控制 Executor 的启停, 其实 Worker 是通过 Executor Backend 来进行控制的, Executor Backend 是一个进程(是一个 JVM 实例), 持有一个 Executor 对象 另外在启动程序的时候, 有三种程序需要运行在集群上: Driver action 操作的最终获得结果,是把结果存放在Driver中 Driver 是一个 JVM 实例, 是一个进程, 是 Spark Application 运行时候的领导者, 其中运行了 SparkContext. Driver 控制 Job 和 Task, 并且提供 WebUI. Executor Executor 对象中通过线程池来运行 Task, 一个 Executor 中只会运行一个 Spark Application 的 Task, 不同的 Spark Application 的 Task 会由不同的 Executor 来运行 2 逻辑执行图 简述描述数据如何流动,如何计算 实际并不存在,只是保存了rdd之间的关系 其实 RDD 并没有什么严格的逻辑执行图和物理执行图的概念, 这里也只是借用这个概念, 从而让整个 RDD 的原理可以解释, 好理解. 对于 RDD 的逻辑执行图, 起始于第一个入口 RDD 的创建, 结束于 Action 算子执行之前, 主要的过程就是生成一组互相有依赖关系的 RDD, 其并不会真的执行, 只是表示 RDD 之间的关系, 数据的流转过程. 1rdd.toDebugString 预运行 结果就是一个逻辑执行图 3 物理执行图 简述当触发 Action 执行的时候, 这一组互相依赖的 RDD 要被处理, 所以要转化为可运行的物理执行图, 调度到集群中执行. 因为大部分 RDD 是不真正存放数据的, 只是数据从中流转, 所以, 不能直接在集群中运行 RDD, 要有一种 Pipeline 的思想, 需要将这组 RDD 转为 Stage 和 Task, 从而运行 Task, 优化整体执行速度. 以上的逻辑执行图会生成如下的物理执行图, 这一切发生在 Action 操作被执行时. 从上图可以总结如下几个点 1 —&gt; 2–&gt;3在第一个 Stage 中, 每一个这样的执行流程是一个 Task, 也就是在同一个 Stage 中的所有 RDD 的对应分区, 在同一个 Task 中执行 Stage 的划分是由 Shuffle 操作来确定的, 有 Shuffle 的地方, Stage 断开 4 逻辑执行图 详述1 明确逻辑计划的边界在 Action 调用之前, 会生成一系列的 RDD, 这些 RDD 之间的关系, 其实就是整个逻辑计划 如果生成逻辑计划的, 会生成如下一些 RDD, 这些 RDD 是相互关联的, 这些 RDD 之间, 其实本质上生成的就是一个 计算链 接下来, 采用迭代渐进式的方式, 一步一步的查看一下整体上的生成过程 开始: 第一个rdd创建开始 结束: 逻辑图到action算子执行之前及结束 逻辑图就是一组rdd和其之间的关系 rdd五大属性: 1 分区列表 2 依赖关系 3 计算函数 4 最佳位置5 分区函数 2textFile 算子的背后研究 RDD 的功能或者表现的时候, 其实本质上研究的就是 RDD 中的五大属性, 因为 RDD 透过五大属性来提供功能和表现, 所以如果要研究 textFile 这个算子, 应该从五大属性着手, 那么第一步就要看看生成的 RDD 是什么类型的 RDD 1textFile 生成的是 HadoopRDDHadoopRDD 重写了分区列表和计算函数 源码: 1234567891011121314151617181920212223242526272829303132def textFile( path: String, minPartitions: Int = defaultMinPartitions): RDD[String] = withScope &#123; assertNotStopped() hadoopFile(path, classOf[TextInputFormat], classOf[LongWritable], classOf[Text], minPartitions).map(pair =&gt; pair._2.toString).setName(path) &#125;def hadoopFile[K, V]( path: String, inputFormatClass: Class[_ &lt;: InputFormat[K, V]], keyClass: Class[K], valueClass: Class[V], minPartitions: Int = defaultMinPartitions): RDD[(K, V)] = withScope &#123; assertNotStopped() // This is a hack to enforce loading hdfs-site.xml. // See SPARK-11227 for details. FileSystem.getLocal(hadoopConfiguration) // A Hadoop configuration can be about 10 KB, which is pretty big, so broadcast it. val confBroadcast = broadcast(new SerializableConfiguration(hadoopConfiguration)) val setInputPathsFunc = (jobConf: JobConf) =&gt; FileInputFormat.setInputPaths(jobConf, path) new HadoopRDD( this, confBroadcast, Some(setInputPathsFunc), inputFormatClass, keyClass, valueClass, minPartitions).setName(path) &#125; 2HadoopRDD 的 Partitions 对应了 HDFS 的 Blocks 其实本质上每个 HadoopRDD 的 Partition 都是对应了一个 Hadoop 的 Block, 通过 InputFormat 来确定 Hadoop 中的 Block 的位置和边界, 从而可以供一些算子使用 3HadoopRDD 的 compute 函数就是在读取 HDFS 中的 Block本质上, compute 还是依然使用 InputFormat 来读取 HDFS 中对应分区的 Block 4textFile 这个算子生成的其实是一个 MapPartitionsRDDtextFile 这个算子的作用是读取 HDFS 上的文件, 但是 HadoopRDD 中存放是一个元组, 其 Key 是行号, 其 Value 是 Hadoop 中定义的 Text 对象, 这一点和 MapReduce 程序中的行为是一致的 但是并不适合 Spark 的场景, 所以最终会通过一个 map 算子, 将 (LineNum, Text) 转为 String 形式的一行一行的数据, 所以最终 textFile 这个算子生成的 RDD 并不是 HadoopRDD, 而是一个 MapPartitionsRDD 2 map 算子的底层123456def map[U: ClassTag](f: T =&gt; U): RDD[U] = withScope &#123; val cleanF = sc.clean(f) new MapPartitionsRDD[U, T](this, (context, pid, iter) =&gt; iter.map(cleanF)) &#125;//iter为每个分区的数组// MapPartitionsRDD的compute函数,处理整个rdd中的每个分区的每一条数据,通过穿入map的函数来进行处理 map 算子生成了 MapPartitionsRDD 由源码可知, 当 val rdd2 = rdd1.map() 的时候, 其实生成的新 RDD 是 rdd2, rdd2 的类型是 MapPartitionsRDD, 每个 RDD 中的五大属性都会有一些不同, 由 map 算子生成的 RDD 中的计算函数, 本质上就是遍历对应分区的数据, 将每一个数据转成另外的形式 MapPartitionsRDD 的计算函数是 collection.map( function ) 真正运行的集群中的处理单元是 Task, 每个 Task 对应一个 RDD 的分区, 所以 collection 对应一个 RDD 分区的所有数据, 而这个计算的含义就是将一个 RDD 的分区上所有数据当作一个集合, 通过这个 Scala 集合的 map 算子, 来执行一个转换操作, 其转换操作的函数就是传入 map 算子的 function 传入 map 算子的函数会被清理 1val cleanF = sc.clean(f) 这个清理主要是处理闭包中的依赖, 使得这个闭包可以被序列化发往不同的集群节点运行 3 flatMap 算子的底层123456def flatMap[U: ClassTag](f: T =&gt; TraversableOnce[U]): RDD[U] = withScope &#123; val cleanF = sc.clean(f) new MapPartitionsRDD[U, T](this, (context, pid, iter) =&gt; iter.flatMap(cleanF)) &#125;map与flatmap的区别从本质上来说就是作用于分区数据集合的算子不同 一个是iter.flatMap(cleanF))和iter.map(cleanF))// MapPartitionsRDD的compute函数,处理整个rdd中的每个分区的每一条数据,通过传入flatMap的函数来进行处理 1flatMap` 和 `map` 算子其实本质上是一样的, 其步骤和生成的 `RDD` 都是一样, 只是对于传入函数的处理不同, `map` 是 `collect.map( function )` 而 `flatMap` 是 `collect.flatMap( function ) 从侧面印证了, 其实 Spark 中的 flatMap 和 Scala 基础中的 flatMap 其实是一样的 总结 如何生成 RDD ? 生成 RDD 的常见方式有三种 从本地集合创建 从外部数据集创建 从其它 RDD 衍生 通过外部数据集创建 RDD, 是通过 Hadoop 或者其它外部数据源的 SDK 来进行数据读取, 同时如果外部数据源是有分片的话, RDD 会将分区与其分片进行对照 通过其它 RDD 衍生的话, 其实本质上就是通过不同的算子生成不同的 RDD 的子类对象, 从而控制 compute 函数的行为来实现算子功能 生成哪些 RDD ? 不同的算子生成不同的 RDD, 生成 RDD 的类型取决于算子, 例如 map 和 flatMap 都会生成 RDD 的子类 MapPartitions 的对象 如何计算 RDD 中的数据 ? 虽然前面我们提到过 RDD 是偏向计算的, 但是其实 RDD 还只是表示数据, 纵观 RDD 的五大属性中有三个是必须的, 分别如下 Partitions List 分区列表 Compute function 计算函数 Dependencies 依赖 虽然计算函数是和计算有关的, 但是只有调用了这个函数才会进行计算, RDD 显然不会自己调用自己的 Compute 函数, 一定是由外部调用的, 所以 RDD 更多的意义是用于表示数据集以及其来源, 和针对于数据的计算 所以如何计算 RDD 中的数据呢? 一定是通过其它的组件来计算的, 而计算的规则, 由 RDD 中的 Compute 函数来指定, 不同类型的 RDD 子类有不同的 Compute 函数 4 rdd之间的依赖关系这个关系不是指rdd之间的关系,而是分区之间的关系 map 和flatMap这两个算子的关系是1 对1 有shuffle的不是一对一 而是多对一 4.1 什么是依赖 什么是关系(依赖关系) ? 从算子视角上来看, splitRDD 通过 map 算子得到了 tupleRDD, 所以 splitRDD 和 tupleRDD 之间的关系是 map 但是仅仅这样说, 会不够全面, 从细节上来看, RDD 只是数据和关于数据的计算, 而具体执行这种计算得出结果的是一个神秘的其它组件, 所以, 这两个 RDD 的关系可以表示为 splitRDD 的数据通过 map 操作, 被传入 tupleRDD, 这是它们之间更细化的关系 但是 RDD 这个概念本身并不是数据容器, 数据真正应该存放的地方是 RDD 的分区, 所以如果把视角放在数据这一层面上的话, 直接讲这两个 RDD 之间有关系是不科学的, 应该从这两个 RDD 的分区之间的关系来讨论它们之间的关系 那这些分区之间是什么关系? 如果仅仅说 splitRDD 和 tupleRDD 之间的话, 那它们的分区之间就是一对一的关系 但是 tupleRDD 到 reduceRDD 呢? tupleRDD 通过算子 reduceByKey 生成 reduceRDD, 而这个算子是一个 Shuffle操作, Shuffle 操作的两个 RDD 的分区之间并不是一对一, reduceByKey 的一个分区对应 tupleRDD 的多个分区 reduceByKey 算子会生成 ShuffledRDD reduceByKey 是由算子 combineByKey 来实现的, combineByKey 内部会创建 ShuffledRDD 返回, 而整个 reduceByKey 操作大致如下过程 去掉两个 reducer 端的分区, 只留下一个的话, 如下 所以, 对于 reduceByKey 这个 Shuffle 操作来说, reducer 端的一个分区, 会从多个 mapper 端的分区拿取数据, 是一个多对一的关系 至此为止, 出现了两种分区见的关系了, 一种是一对一, 一种是多对一 整体流程: 4.2 rdd之间依赖详解窄依赖 无shuffle的为窄依赖 宽依赖 有shuffle的为宽依赖 判断一个是什么依赖 只需看返回值是NarrowDependency 还是ShuffleDependency 如果有shuffle是没办法吧两个rdd的分区放在同一个流水线工作的 4.2.1 窄依赖 NarrowDependency 假如 rddB = rddA.transform(…), 如果 rddB 中一个分区依赖 rddA 也就是其父 RDD 的少量分区, 这种 RDD 之间的依赖关系称之为窄依赖 换句话说, 子 RDD 的每个分区依赖父 RDD 的少量个数的分区, 这种依赖关系称之为窄依赖 例如: 1234567891011lass ZDemo &#123; @Test def zD(): Unit =&#123; val conf = new SparkConf().setMaster("local[3]").setAppName("cache") val context =new SparkContext(conf) val rdd1 = context.parallelize(Seq(1,2,3)) val rdd2 = context.parallelize(Seq("a","b")) //求笛卡尔积 是一个窄依赖 rdd1.cartesian(rdd2).collect().foreach(println(_)) &#125;&#125; 上述代码的 cartesian 是求得两个集合的笛卡尔积 上述代码的运行结果是 rddA 中每个元素和 rddB 中的所有元素结合, 最终的结果数量是两个 RDD 数量之和 rddC 有两个父 RDD, 分别为 rddA 和 rddB 对于 cartesian 来说, 依赖关系如下 上述图形中清晰展示如下现象 rddC 中的分区数量是两个父 RDD 的分区数量之乘积 rddA 中每个分区对应 rddC 中的两个分区 (因为 rddB 中有两个分区), rddB 中的每个分区对应 rddC 中的三个分区 (因为 rddA 有三个分区) 它们之间是窄依赖, 事实上在 cartesian 中也是 NarrowDependency 这个所有窄依赖的父类的唯一一次直接使用, 为什么呢? 因为所有的分区之间是拷贝关系, 并不是 Shuffle 关系 rddC 中的每个分区并不是依赖多个父 RDD 中的多个分区 rddC 中每个分区的数量来自一个父 RDD 分区中的所有数据, 是一个 FullDependence, 所以数据可以直接从父 RDD 流动到子 RDD 不存在一个父 RDD 中一部分数据分发过去, 另一部分分发给其它的 RDD 4.2.2 宽依赖并没有所谓的宽依赖, 宽依赖应该称作为 ShuffleDependency 在 ShuffleDependency 的类声明上如下写到 1Represents a dependency on the output of a shuffle stage. 上面非常清楚的说道, 宽依赖就是 Shuffle 中的依赖关系, 换句话说, 只有 Shuffle 产生的地方才是宽依赖 那么宽窄依赖的判断依据就非常简单明确了, 是否有 Shuffle ? 举个 reduceByKey 的例子, rddB = rddA.reduceByKey( (curr, agg) ⇒ curr + agg ) 会产生如下的依赖关系 分区函数: key.hashCode% rddb的分区数 = 1 或者2 Hadoop.分区 =1 spark.分区=2 rddB 的每个分区都几乎依赖 rddA 的所有分区 对于 rddA 中的一个分区来说, 其将一部分分发给 rddB 的 p1, 另外一部分分发给 rddB 的 p2, 这不是数据流动, 而是分发 4.2.3 宽窄依赖分辨123首先 看是不是一对一 如是 就是窄依赖若是多对一 则看是否有数据分发 就是是否有shuffle 若是看不出来 直接看源码 的getDependencies 的返回值是什么是NarrowDependency为窄依赖 shuffleDependency为宽依赖 若是没有getDependencies方法 就看他的父类 4.2.4 常见窄依赖 类型NarrowDependency和shuffleDependency的共同父类为Dependency 一对一窄依赖 其实 RDD 中默认的是 OneToOneDependency, 后被不同的 RDD 子类指定为其它的依赖类型, 常见的一对一依赖是 map 算子所产生的依赖, 例如 rddB = rddA.map(…) 每个分区之间一一对应, 所以叫做一对一窄依赖 Range 窄依赖 Range 窄依赖其实也是一对一窄依赖, 但是保留了中间的分隔信息, 可以通过某个分区获取其父分区, 目前只有一个算子生成这种窄依赖, 就是 union 算子, 例如 rddC = rddA.union(rddB) rddC 其实就是 rddA 拼接 rddB 生成的, 所以 rddC 的 p5 和 p6 就是 rddB 的 p1 和 p2 所以需要有方式获取到 rddC 的 p5 其父分区是谁, 于是就需要记录一下边界, 其它部分和一对一窄依赖一样 多对一窄依赖 多对一窄依赖其图形和 Shuffle 依赖非常相似, 所以在遇到的时候, 要注意其 RDD 之间是否有 Shuffle 过程, 比较容易让人困惑, 常见的多对一依赖就是重分区算子 coalesce, 例如 rddB = rddA.coalesce(2, shuffle = false), 但同时也要注意, 如果 shuffle = true 那就是完全不同的情况了 因为没有 Shuffle, 所以这是一个窄依赖 再谈宽窄依赖的区别 宽窄依赖的区别非常重要, 因为涉及了一件非常重要的事情: 如何计算 RDD ? 宽窄以来的核心区别是: 窄依赖的 RDD 可以放在一个 Task 中运行 5 物理图执行详解1物理图作用问题一: 物理图的意义是什么? 物理图解决的其实就是 RDD 流程生成以后, 如何计算和运行的问题, 也就是如何把 RDD 放在集群中执行的问题 问题二: 如果要确定如何运行的问题, 则需要先确定集群中有什么组件 首先集群中物理元件就是一台一台的机器 其次这些机器上跑的守护进程有两种: Master, Worker 每个守护进程其实就代表了一台机器, 代表这台机器的角色, 代表这台机器和外界通信 例如我们常说一台机器是 Master, 其含义是这台机器中运行了一个 Master 守护进程, 如果一台机器运行了 Master的同时又运行了 Worker, 则说这台机器是 Master 也可以, 说它是 Worker 也行 真正能运行 RDD 的组件是: Executor, 也就是说其实 RDD 最终是运行在 Executor 中的, 也就是说, 无论是 Master 还是 Worker 其实都是用于管理 Executor 和调度程序的 结论是 RDD 一定在 Executor 中计算, 而 Master 和 Worker 负责调度和管理 Executor 问题三: 物理图的生成需要考虑什么问题? 要计算 RDD, 不仅要计算, 还要很快的计算 → 优化性能 要考虑容错, 容错的常见手段是缓存 → RDD 要可以缓存 结论是在生成物理图的时候, 不仅要考虑效率问题, 还要考虑一种更合适的方式, 让 RDD 运行的更好 2谁来计算 RDD ? 问题一: RDD 是什么, 用来做什么 ? 回顾一下 RDD 的五个属性A list of partitions`A function for computing each splitA list of dependencies on other RDDsOptionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned)Optionally, a list of preferred locations to compute each split on (e.g. block locations for an HDFS file)简单的说就是: 分区列表, 计算函数, 依赖关系, 分区函数, 最佳位置分区列表, 分区函数, 最佳位置, 这三个属性其实说的就是数据集在哪, 在哪更合适, 如何分区计算函数和依赖关系, 这两个属性其实说的是数据集从哪来所以结论是RDD` 是一个数据集的表示, 不仅表示了数据集, 还表示了这个数据集从哪来, 如何计算但是问题是, 谁来计算 ? 如果为一台汽车设计了一个设计图, 那么设计图自己生产汽车吗 ? 问题二: 谁来计算 ? 前面我们明确了两件事, RDD 在哪被计算? 在 Executor 中. RDD 是什么? 是一个数据集以及其如何计算的图纸.直接使用 Executor 也是不合适的, 因为一个计算的执行总是需要一个容器, 例如 JVM 是一个进程, 只有进程中才能有线程, 所以这个计算 RDD 的线程应该运行在一个进程中, 这个进程就是 Exeutor, Executor 有如下两个职责 和 Driver 保持交互从而认领属于自己的任务 接受任务后, 运行任务 所以, 应该由一个线程来执行 RDD 的计算任务, 而 Executor 作为执行这个任务的容器, 也就是一个进程, 用于创建和执行线程, 这个执行具体计算任务的线程叫做 Task 3 Task 该如何设计第一个想法是每个 RDD 都由一个 Task 来计算 第二个想法是一整个逻辑执行图中所有的 RDD 都由一组 Task 来执行 第三个想法是分阶段执行 第一个想法: 为每个 RDD 的分区设置一组 Task 大概就是每个 RDD 都有三个 Task, 每个 Task 对应一个 RDD 的分区, 执行一个分区的数据的计算 但是这么做有一个非常难以解决的问题, 就是数据存储的问题, 例如 Task 1, 4, 7, 10, 13, 16 在同一个流程上, 但是这些 Task 之间需要交换数据, 因为这些 Task 可能被调度到不同的机器上上, 所以 Task1 执行完了数据以后需要暂存, 后交给 Task4 来获取 这只是一个简单的逻辑图, 如果是一个复杂的逻辑图, 会有什么表现? 要存储多少数据? 无论是放在磁盘还是放在内存中, 是不是都是一种极大的负担? 第二个想法: 让数据流动 很自然的, 第一个想法的问题是数据需要存储和交换, 那不存储不就好了吗? 对, 可以让数据流动起来 第一个要解决的问题就是, 要为数据创建管道(Pipeline), 有了管道, 就可以流动 简单来说, 就是为所有的 RDD 有关联的分区使用同一个 Task, 但是就没问题了吗? 请关注红框部分 这两个 RDD 之间是 Shuffle 关系, 也就是说, 右边的 RDD 的一个分区可能依赖左边 RDD 的所有分区, 这样的话, 数据在这个地方流不动了, 怎么办? 第三个想法: 划分阶段 既然在 Shuffle 处数据流不动了, 那就可以在这个地方中断一下, 后面 Stage 部分详解 4如何划分阶段 ?为了减少执行任务, 减少数据暂存和交换的机会, 所以需要创建管道, 让数据沿着管道流动, 其实也就是原先每个 RDD 都有一组 Task, 现在改为所有的 RDD 共用一组 Task, 但是也有问题, 问题如下 就是说, 在 Shuffle 处, 必须断开管道, 进行数据交换, 交换过后, 继续流动, 所以整个流程可以变为如下样子 把 Task 断开成两个部分, Task4 可以从 Task 1, 2, 3 中获取数据, 后 Task4 又作为管道, 继续让数据在其中流动 但是还有一个问题, 说断开就直接断开吗? 不用打个招呼的呀? 这个断开即没有道理, 也没有规则, 所以可以为这个断开增加一个概念叫做阶段, 按照阶段断开, 阶段的英文叫做 Stage, 如下 所以划分阶段的本身就是设置断开点的规则, 那么该如何划分阶段呢? 第一步, 从最后一个 RDD, 也就是逻辑图中最右边的 RDD 开始, 向前滑动 Stage 的范围, 为 Stage0 第二步, 遇到 ShuffleDependency 断开 Stage, 从下一个 RDD 开始创建新的 Stage, 为 Stage1 第三步, 新的 Stage 按照同样的规则继续滑动, 直到包裹所有的 RDD 总结来看, 就是针对于宽窄依赖来判断, 一个 Stage 中只有窄依赖, 因为只有窄依赖才能形成数据的 Pipeline. 如果要进行 Shuffle 的话, 数据是流不过去的, 必须要拷贝和拉取. 所以遇到 RDD 宽依赖的两个 RDD 时, 要切断这两个 RDD 的 Stage. 这样一个 RDD 依赖的链条, 我们称之为 RDD 的血统, 其中有宽依赖也有窄依赖 5 数据流动123456789val sc = ...val textRDD = sc.parallelize(Seq("Hadoop Spark", "Hadoop Flume", "Spark Sqoop"))val splitRDD = textRDD.flatMap(_.split(" "))val tupleRDD = splitRDD.map((_, 1))val reduceRDD = tupleRDD.reduceByKey(_ + _)val strRDD = reduceRDD.map(item =&gt; s"$&#123;item._1&#125;, $&#123;item._2&#125;")strRDD.collect.foreach(item =&gt; println(item)) 上述代码是这个章节我们一直使用的代码流程, 如下是其完整的逻辑执行图 如果放在集群中运行, 通过 WebUI 可以查看到如下 DAG 结构 Step 1: 从 ResultStage 开始执行 最接近 Result 部分的 Stage id 为 0, 这个 Stage 被称之为 ResultStage 由代码可以知道, 最终调用 Action 促使整个流程执行的是最后一个 RDD, strRDD.collect, 所以当执行 RDD 的计算时候, 先计算的也是这个 RDD Step 2: RDD 之间是有关联的 前面已经知道, 最后一个 RDD 先得到执行机会, 先从这个 RDD 开始执行, 但是这个 RDD 中有数据吗 ? 如果没有数据, 它的计算是什么? 它的计算是从父 RDD 中获取数据, 并执行传入的算子的函数 简单来说, 从产生 Result 的地方开始计算, 但是其 RDD 中是没数据的, 所以会找到父 RDD 来要数据, 父 RDD 也没有数据, 继续向上要, 所以, 计算从 Result 处调用, 但是从整个逻辑图中的最左边 RDD 开始, 类似一个递归的过程 6 调度过程6.1 逻辑图调度是什么 怎么生成 具体怎么生成 12345val textRDD = sc.parallelize(Seq("Hadoop Spark", "Hadoop Flume", "Spark Sqoop"))val splitRDD = textRDD.flatMap(_.split(" "))val tupleRDD = splitRDD.map((_, 1))val reduceRDD = tupleRDD.reduceByKey(_ + _)val strRDD = reduceRDD.map(item =&gt; s"$&#123;item._1&#125;, $&#123;item._2&#125;") 逻辑图如何生成 上述代码在 Spark Application 的 main 方法中执行, 而 Spark Application 在 Driver 中执行, 所以上述代码在 Driver 中被执行, 那么这段代码执行的结果是什么呢?一段 Scala 代码的执行结果就是最后一行的执行结果, 所以上述的代码, 从逻辑上执行结果就是最后一个 RDD, 最后一个 RDD也可以认为就是逻辑执行图, 为什么呢?例如 rdd2 = rdd1.map(…) 中, 其实本质上 rdd2 是一个类型为 MapPartitionsRDD 的对象, 而创建这个对象的时候, 会通过构造函数传入当前 RDD 对象, 也就是父 RDD, 也就是调用 map 算子的 rdd1, rdd1 是 rdd2 的父 RDD 一个 RDD 依赖另外一个 RDD, 这个 RDD 又依赖另外的 RDD, 一个 RDD 可以通过 getDependency 获得其父 RDD, 这种环环相扣的关系, 最终从最后一个 RDD 就可以推演出前面所有的 RDD 逻辑图是什么, 干啥用 逻辑图其实本质上描述的就是数据的计算过程, 数据从哪来, 经过什么样的计算, 得到什么样的结果, 再执行什么计算, 得到什么结果 可是数据的计算是描述好了, 这种计算该如何执行呢? 6.2 物理图DAGScheduler 1 Stage中有多个task 2 每个task计算一个分区 job就是一个求值过程 数据的计算表示好了, 该正式执行了, 但是如何执行? 如何执行更快更好更酷? 就需要为其执行做一个规划, 所以需要生成物理执行图 1strRDD.collect.foreach(item =&gt; println(item)) 上述代码其实就是最后的一个 RDD 调用了 Action 方法, 调用 Action 方法的时候, 会请求一个叫做 DAGScheduler 的组件, DAGScheduler 会创建用于执行 RDD 的 Stage 和 Task DAGScheduler 是一个由 SparkContext 创建, 运行在 Driver 上的组件, 其作用就是将由 RDD 构建出来的逻辑计划, 构建成为由真正在集群中运行的 Task 组成的物理执行计划, DAGScheduler 主要做如下三件事 帮助每个 Job 计算 DAG 并发给 TaskSheduler 调度 确定每个 Task 的最佳位置 跟踪 RDD 的缓存状态, 避免重新计算 从字面意思上来看, DAGScheduler 是调度 DAG 去运行的, DAG 被称作为有向无环图, 其实可以将 DAG 理解为就是 RDD 的逻辑图, 其呈现两个特点: RDD 的计算是有方向的, RDD 的计算是无环的, 所以 DAGScheduler 也可以称之为 RDD Scheduler, 但是真正运行在集群中的并不是 RDD, 而是 Task 和 Stage, DAGScheduler 负责这种转换 6.3 job简介job 什么时候生成 ? 当一个 RDD 调用了 Action 算子的时候, 在 Action 算子内部, 会使用 sc.runJob() 调用 SparkContext 中的 runJob方法, 这个方法又会调用 DAGScheduler 中的 runJob, 后在 DAGScheduler 中使用消息驱动的形式创建 Job 简而言之, Job 在 RDD 调用 Action 算子的时候生成, 而且调用一次 Action 算子, 就会生成一个 Job, 如果一个 SparkApplication 中调用了多次 Action 算子, 会生成多个 Job 串行执行, 每个 Job 独立运作, 被独立调度, 所以 RDD 的计算也会被执行多次 Job 是什么 ? 如果要将 Spark 的程序调度到集群中运行, Job 是粒度最大的单位, 调度以 Job 为最大单位, 将 Job 拆分为 Stage 和 Task 去调度分发和运行, 一个 Job 就是一个 Spark 程序从 读取 → 计算 → 运行 的过程 一个 Spark Application 可以包含多个 Job, 这些 Job 之间是串行的, 也就是第二个 Job 需要等待第一个 Job 的执行结束后才会开始执行 6.4 job 与Stage的关系一个job有多个Stage stage之间是串行的 左侧的接近起始Stage的最先执行,从左向右执行 Job 是一个最大的调度单位, 也就是说 DAGScheduler 会首先创建一个 Job 的相关信息, 后去调度 Job, 但是没办法直接调度 Job, 比如说现在要做一盘手撕包菜, 不可能直接去炒一整颗包菜, 要切好撕碎, 再去炒 为什么 Job 需要切分 ? 因为 Job 的含义是对整个 RDD 血统求值, 但是 RDD 之间可能会有一些宽依赖 如果遇到宽依赖的话, 两个 RDD 之间需要进行数据拉取和复制 如果要进行拉取和复制的话, 那么一个 RDD 就必须等待它所依赖的 RDD 所有分区先计算完成, 然后再进行拉取 由上得知, 一个 Job 是无法计算完整个 RDD 血统的 如何切分 ? 创建一个 Stage, 从后向前回溯 RDD, 遇到 Shuffle 依赖就结束 Stage, 后创建新的 Stage 继续回溯. 这个过程上面已经详细的讲解过, 但是问题是切分以后如何执行呢, 从后向前还是从前向后, 是串行执行多个 Stage, 还是并行执行多个 Stage 问题一: 执行顺序 在图中, Stage 0 的计算需要依赖 Stage 1 的数据, 因为 reduceRDD 中一个分区可能需要多个 tupleRDD 分区的数据, 所以 tupleRDD 必须先计算完, 所以, 应该在逻辑图中自左向右执行 Stage 问题二: 串行还是并行 还是同样的原因, Stage 0 如果想计算, Stage 1 必须先计算完, 因为 Stage 0 中每个分区都依赖 Stage 1 中的所有分区, 所以 Stage 1 不仅需要先执行, 而且 Stage 1 执行完之前 Stage 0 无法执行, 它们只能串行执行 总结 一个 Stage 就是物理执行计划中的一个步骤, 一个 Spark Job 就是划分到不同 Stage 的计算过程 Stage 之间的边界由 Shuffle 操作来确定 Stage 内的 RDD 之间都是窄依赖, 可以放在一个管道中执行 而 Shuffle 后的 Stage 需要等待前面 Stage 的执行 Stage 有两种 ShuffMapStage, 其中存放窄依赖的 RDD ResultStage, 每个 Job 只有一个, 负责计算结果, 一个 ResultStage 执行完成标志着整个 Job 执行完毕 6.5 Stage 和 Task 的关系rdd本身不存储数据,数据存储在分区中 一个task对应一个分区,一个Stage 中有几个task(分区) 由Stage中的最后一个rdd决定 一个stage就是一个TaskSet 一个TaskSet就是一组task 前面我们说到 Job 无法直接执行, 需要先划分为多个 Stage, 去执行 Stage, 那么 Stage 可以直接执行吗? 第一点: Stage 中的 RDD 之间是窄依赖 因为 Stage 中的所有 RDD 之间都是窄依赖, 窄依赖 RDD 理论上是可以放在同一个 Pipeline(管道, 流水线) 中执行的, 似乎可以直接调度 Stage 了? 其实不行, 看第二点 第二点: 别忘了 RDD 还有分区 一个 RDD 只是一个概念, 而真正存放和处理数据时, 都是以分区作为单位的 Stage 对应的是多个整体上的 RDD, 而真正的运行是需要针对 RDD 的分区来进行的 第三点: 一个 Task 对应一个 RDD 的分区 一个比 Stage 粒度更细的单元叫做 Task, Stage 是由 Task 组成的, 之所以有 Task 这个概念, 是因为 Stage 针对整个 RDD, 而计算的时候, 要针对 RDD 的分区 假设一个 Stage 中有 10 个 RDD, 这些 RDD 中的分区各不相同, 但是分区最多的 RDD 有 30 个分区, 而且很显然, 它们之间是窄依赖关系 那么, 这个 Stage 中应该有多少 Task 呢? 应该有 30 个 Task, 因为一个 Task 计算一个 RDD 的分区. 这个 Stage 至多有 30 个分区需要计算 总结 一个 Stage 就是一组并行的 Task 集合 Task 是 Spark 中最小的独立执行单元, 其作用是处理一个 RDD 分区 一个 Task 只可能存在于一个 Stage 中, 并且只能计算一个 RDD 的分区 6.6 TaskSet梳理一下这几个概念, Job &gt; Stage &gt; Task, Job 中包含 Stage 中包含 Task 而 Stage 中经常会有一组 Task 需要同时执行, 所以针对于每一个 Task 来进行调度太过繁琐, 而且没有意义, 所以每个 Stage中的 Task 们会被收集起来, 放入一个 TaskSet 集合中 一个 Stage 有一个 TaskSet TaskSet 中 Task 的个数由 Stage 中的最大分区数决定 6.7 整体流程 6.8 shuffle过程6.8.1Shuffle 过程的组件结构从整体视角上来看, Shuffle 发生在两个 Stage 之间, 一个 Stage 把数据计算好, 整理好, 等待另外一个 Stage 来拉取 放大视角, 会发现, 其实 Shuffle 发生在 Task 之间, 一个 Task 把数据整理好, 等待 Reducer 端的 Task 来拉取 如果更细化一下, Task 之间如何进行数据拷贝的呢? 其实就是一方 Task 把文件生成好, 然后另一方 Task 来拉取 现在是一个 Reducer 的情况, 如果有多个 Reducer 呢? 如果有多个 Reducer 的话, 就可以在每个 Mapper 为所有的 Reducer 生成各一个文件, 这种叫做 Hash base shuffle, 这种 Shuffle 的方式问题大家也知道, 就是生成中间文件过多, 而且生成文件的话需要缓冲区, 占用内存过大 那么可以把这些文件合并起来, 生成一个文件返回, 这种 Shuffle 方式叫做 Sort base shuffle, 每个 Reducer 去文件的不同位置拿取数据 如果再细化一下, 把参与这件事的组件也放置进去, 就会是如下这样 有哪些 ShuffleWriter ? 大致上有三个 ShufflWriter, Spark 会按照一定的规则去使用这三种不同的 Writer BypassMergeSortShuffleWriter 这种 Shuffle Writer 也依然有 Hash base shuffle 的问题, 它会在每一个 Mapper 端对所有的 Reducer 生成一个文件, 然后再合并这个文件生成一个统一的输出文件, 这个过程中依然是有很多文件产生的, 所以只适合在小量数据的场景下使用 Spark 有考虑去掉这种 Writer, 但是因为结构中有一些依赖, 所以一直没去掉 当 Reducer 个数小于 spark.shuffle.sort.bypassMergeThreshold, 并且没有 Mapper 端聚合的时候启用这种方式 SortShuffleWriter 这种 ShuffleWriter 写文件的方式非常像 MapReduce 了, 后面详说 当其它两种 Shuffle 不符合开启条件时, 这种 Shuffle 方式是默认的 UnsafeShuffleWriter 这种 ShuffWriter 会将数据序列化, 然后放入缓冲区进行排序, 排序结束后 Spill 到磁盘, 最终合并 Spill 文件为一个大文件, 同时在进行内存存储的时候使用了 Java 得 Unsafe API, 也就是使用堆外内存, 是钨丝计划的一部分 也不是很常用, 只有在满足如下三个条件时候才会启用 序列化器序列化后的数据, 必须支持排序 没有 Mapper 端的聚合 Reducer 的个数不能超过支持的上限 (2 ^ 24) SortShuffleWriter 的执行过程 整个 SortShuffleWriter 如上述所说, 大致有如下几步 首先 SortShuffleWriter 在 write 方法中回去写文件, 这个方法中创建了 ExternalSorter write 中将数据 insertAll 到 ExternalSorter 中 在 ExternalSorter 中排序 如果要聚合, 放入 AppendOnlyMap 中, 如果不聚合, 放入 PartitionedPairBuffer 中 在数据结构中进行排序, 排序过程中如果内存数据大于阈值则溢写到磁盘 使用 ExternalSorter 的 writePartitionedFile 写入输入文件 将所有的溢写文件通过类似 MergeSort 的算法合并 将数据写入最终的目标文件中 7 闭包 RDD 的分布式共享变量什么是闭包 闭包是一个必须要理解, 但是又不太好理解的知识点, 先看一个小例子 123456789101112@Testdef test(): Unit = &#123; val areaFunction = closure() val area = areaFunction(2) println(area)&#125;def closure(): Int =&gt; Double = &#123; val factor = 3.14 val areaFunction = (r: Int) =&gt; math.pow(r, 2) * factor areaFunction&#125; 上述例子中, closure方法返回的一个函数的引用, 其实就是一个闭包, 闭包本质上就是一个封闭的作用域, 要理解闭包, 是一定要和作用域联系起来的. 能否在 test 方法中访问 closure 定义的变量? 12345678@Testdef test(): Unit = &#123; println(factor)&#125;def closure(): Int =&gt; Double = &#123; val factor = 3.14&#125; 有没有什么间接的方式? 1234567891011@Testdef test(): Unit = &#123; val areaFunction = closure() areaFunction()&#125;def closure(): () =&gt; Unit = &#123; val factor = 3.14 val areaFunction = () =&gt; println(factor) areaFunction&#125; 什么是闭包? 12val areaFunction = closure()areaFunction() 通过 closure 返回的函数 areaFunction 就是一个闭包, 其函数内部的作用域并不是 test 函数的作用域, 这种连带作用域一起打包的方式, 我们称之为闭包, 在 Scala 中Scala 中的闭包本质上就是一个对象, 是 FunctionX 的实例 分发闭包 ​ 123456789class MyClass &#123; val field = "Hello" def doStuff(rdd: RDD[String]): RDD[String] = &#123; rdd.map(x =&gt; field + x) &#125;&#125; // rdd.map(x =&gt; field + x)引用了MyClass 对象中的一个成员变量 说明其可以访问MyClass这个类的作用域//也是一个闭包 封闭的是MyClass这个作用域 总结 闭包就是一个封闭的作用域, 也是一个对象 Spark 算子所接受的函数, 本质上是一个闭包, 因为其需要封闭作用域, 并且序列化自身和依赖, 分发到不同的节点中运行 7.1 累加器123456789101112class ZDemo &#123; var num= 0 @Test def addErr(): Unit =&#123; val conf = new SparkConf().setMaster("local[3]").setAppName("cache") val context =new SparkContext(conf) context.parallelize(Seq(1,2,3,4,5,6),3) //闭包会报错 .foreach(num += _) println(num) &#125;&#125; 上面这段代码是一个非常错误的使用, 请不要仿照, 这段代码只是为了证明一些事情 先明确两件事, var count = 0 是在 Driver 中定义的, foreach(count += _) 这个算子以及传递进去的闭包运行在 Executor 中 这段代码整体想做的事情是累加一个变量, 但是这段代码的写法却做不到这件事, 原因也很简单, 因为具体的算子是闭包, 被分发给不同的节点运行, 所以这个闭包中累加的并不是 Driver 中的这个变量 全局累加器 Accumulators(累加器) 是一个只支持 added(添加) 的分布式变量, 可以在分布式环境下保持一致性, 并且能够做到高效的并发. 原生 Spark 支持数值型的累加器, 可以用于实现计数或者求和, 开发者也可以使用自定义累加器以实现更高级的需求 12345678910val config = new SparkConf().setAppName("ip_ana").setMaster("local[6]")val sc = new SparkContext(config)val counter = sc.longAccumulator("counter")sc.parallelize(Seq(1, 2, 3, 4, 5)) .foreach(counter.add(_))// 运行结果: 15println(counter.value) 注意点: Accumulator 是支持并发并行的, 在任何地方都可以通过 add 来修改数值, 无论是 Driver 还是 Executor 只能在 Driver 中才能调用 value 来获取数值 在 WebUI 中关于 Job 部分也可以看到 Accumulator 的信息, 以及其运行的情况 累计器件还有两个小特性 第一, 累加器能保证在 Spark 任务出现问题被重启的时候不会出现重复计算. 第二, 累加器只有在 Action 执行的时候才会被触发. 12345678910val config = new SparkConf().setAppName("ip_ana").setMaster("local[6]")val sc = new SparkContext(config)val counter = sc.longAccumulator("counter")sc.parallelize(Seq(1, 2, 3, 4, 5)) .map(counter.add(_)) // 这个地方不是 Action, 而是一个 Transformation// 运行结果是 0println(counter.value) 自定义累加器 开发者可以通过自定义累加器来实现更多类型的累加器, 累加器的作用远远不只是累加, 比如可以实现一个累加器, 用于向里面添加一些运行信息 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556//自定义累加器 //参数 一个累加的值是什么类型String 一个返回值类型Set[String] class NumAccu extends AccumulatorV2[String, Set[String]] &#123; private val nums: mutable.Set[String] = mutable.Set() //告诉spark框架 这个spark累加器是否是空的 override def isZero: Boolean = &#123; nums.isEmpty &#125; //提供给spark框架一个拷贝的累加器 override def copy(): AccumulatorV2[String, Set[String]] = &#123; val accu = new NumAccu() nums.synchronized() &#123; accu.nums ++= this.nums &#125; accu &#125; //帮助spark清理,累加器里的内容 override def reset(): Unit = &#123; nums.clear() &#125; //外部传入要累加的内容 ,在这个方法中累加 override def add(v: String): Unit = &#123; nums += v &#125; //累加器在累加的时候,可能每个分布式节点都有一个实例,在最后Driver进行合并,把所有的实例内容合并 //起来,会调用这个方法 override def merge(other: AccumulatorV2[String, Set[String]]): Unit = &#123; nums ++= other.value &#125; //就是上面的value //提供给外部累加结果 //为何不可变 ,因为外部有可能再次进行修改,如是可变集合,外部修改会影响内部得值 override def value: Set[String] = &#123; nums.toSet //不可变得set &#125; &#125; //测试自定义累加器 @Test def accTest(): Unit =&#123; val conf = new SparkConf().setMaster("local[3]").setAppName("cache") val context = new SparkContext(conf) val accu = new NumAccu() //注册给Spark context.register(accu,"acc") context.parallelize(Seq("1","2")) .foreach(it =&gt; accu.add(it)) println(accu.value) context.stop() &#125;//外部这个类要继承Serializable 才可以否则可能报错 注意点: 可以通过继承 AccumulatorV2 来创建新的累加器 有几个方法需要重写 reset 方法用于把累加器重置为 0 add 方法用于把其它值添加到累加器中 merge 方法用于指定如何合并其他的累加器 value 需要返回一个不可变的集合, 因为不能因为外部的修改而影响自身的值 7.2 广播变量广播变量的作用 广播变量允许开发者将一个 Read-Only 的变量缓存到集群中每个节点中, 而不是传递给每一个 Task 一个副本. 集群中每个节点, 指的是一个机器 每一个 Task, 一个 Task 是一个 Stage 中的最小处理单元, 一个 Executor 中可以有多个 Stage, 每个 Stage 有多个 Task 所以在需要跨多个 Stage 的多个 Task 中使用相同数据的情况下, 广播特别的有用 广播变量的API 方法名 描述 id 唯一标识 value 广播变量的值 unpersist 在 Executor 中异步的删除缓存副本 destroy 销毁所有此广播变量所关联的数据和元数据 toString 字符串表示 使用广播变量的一般套路 可以通过如下方式创建广播变量 1val b = sc.broadcast(1) 如果 Log 级别为 DEBUG 的时候, 会打印如下信息 12345DEBUG BlockManager: Put block broadcast_0 locally took 430 msDEBUG BlockManager: Putting block broadcast_0 without replication took 431 msDEBUG BlockManager: Told master about block broadcast_0_piece0DEBUG BlockManager: Put block broadcast_0_piece0 locally took 4 msDEBUG BlockManager: Putting block broadcast_0_piece0 without replication took 4 ms 创建后可以使用 value 获取数据 1b.value 获取数据的时候会打印如下信息 12DEBUG BlockManager: Getting local block broadcast_0DEBUG BlockManager: Level for block broadcast_0 is StorageLevel(disk, memory, deserialized, 1 replicas) 广播变量使用完了以后, 可以使用 unpersist 删除数据 1b.unpersist 删除数据以后, 可以使用 destroy 销毁变量, 释放内存空间 1b.destroy 销毁以后, 会打印如下信息 1234DEBUG BlockManager: Removing broadcast 0DEBUG BlockManager: Removing block broadcast_0_piece0DEBUG BlockManager: Told master about block broadcast_0_piece0DEBUG BlockManager: Removing block broadcast_0 使用 value 方法的注意点 方法签名 value: T 在 value 方法内部会确保使用获取数据的时候, 变量必须是可用状态, 所以必须在变量被 destroy 之前使用 value 方法, 如果使用 value 时变量已经失效, 则会爆出以下错误 1234org.apache.spark.SparkException: Attempted to use Broadcast(0) after it was destroyed (destroy at &lt;console&gt;:27) at org.apache.spark.broadcast.Broadcast.assertValid(Broadcast.scala:144) at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:69) ... 48 elided 使用 destroy 方法的注意点 方法签名 destroy(): Unit`destroy方法会移除广播变量, 彻底销毁掉, 但是如果你试图多次destroy` 广播变量, 则会爆出以下错误 12345org.apache.spark.SparkException: Attempted to use Broadcast(0) after it was destroyed (destroy at &lt;console&gt;:27) at org.apache.spark.broadcast.Broadcast.assertValid(Broadcast.scala:144) at org.apache.spark.broadcast.Broadcast.destroy(Broadcast.scala:107) at org.apache.spark.broadcast.Broadcast.destroy(Broadcast.scala:98) ... 48 elided 广播变量的使用场景 假设我们在某个算子中需要使用一个保存了项目和项目的网址关系的 Map[String, String] 静态集合, 如下 123val pws = Map("Apache Spark" -&gt; "http://spark.apache.org/", "Scala" -&gt; "http://www.scala-lang.org/")val websites = sc.parallelize(Seq("Apache Spark", "Scala")).map(pws).collect 上面这段代码是没有问题的, 可以正常运行的, 但是非常的低效, 因为虽然可能 pws 已经存在于某个 Executor 中了, 但是在需要的时候还是会继续发往这个 Executor, 如果想要优化这段代码, 则需要尽可能的降低网络开销 可以使用广播变量进行优化, 因为广播变量会缓存在集群中的机器中, 比 Executor 在逻辑上更 “大” 12val pwsB = sc.broadcast(pws)val websites = sc.parallelize(Seq("Apache Spark", "Scala")).map(pwsB.value).collect 上面两段代码所做的事情其实是一样的, 但是当需要运行多个 Executor (以及多个 Task) 的时候, 后者的效率更高 扩展 正常情况下使用 Task 拉取数据的时候, 会将数据拷贝到 Executor 中多次, 但是使用广播变量的时候只会复制一份数据到 Executor 中, 所以在两种情况下特别适合使用广播变量 一个 Executor 中有多个 Task 的时候 一个变量比较大的时候 而且在 Spark 中还有一个约定俗称的做法, 当一个 RDD 很大并且还需要和另外一个 RDD 执行 join 的时候, 可以将较小的 RDD 广播出去, 然后使用大的 RDD 在算子 map 中直接 join, 从而实现在 Map 端 join 123456val acMap = sc.broadcast(myRDD.map &#123; case (a,b,c,b) =&gt; (a, c) &#125;.collectAsMap)val otherMap = sc.broadcast(myOtherRDD.collectAsMap)myBigRDD.map &#123; case (a, b, c, d) =&gt; (acMap.value.get(a).get, otherMap.value.get(c).get)&#125;.collect 一般情况下在这种场景下, 会广播 Map 类型的数据, 而不是数组, 因为这样容易使用 Key 找到对应的 Value 简化使用 总结 广播变量用于将变量缓存在集群中的机器中, 避免机器内的 Executors 多次使用网络拉取数据 广播变量的使用步骤: (1) 创建 (2) 在 Task 中获取值 (3) 销毁]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>BigData</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark原理浅析]]></title>
    <url>%2F2017%2F08%2F20%2Fspark%E5%8E%9F%E7%90%86%E6%B5%85%E6%9E%90.html</url>
    <content type="text"><![CDATA[一spark特性1 rdd的分区和shuffle分区的作用 RDD 使用分区来分布式并行处理数据, 并且要做到尽量少的在不同的 Executor 之间使用网络交换数据, 所以当使用 RDD 读取数据的时候, 会尽量的在物理上靠近数据源, 比如说在读取 Cassandra 或者 HDFS 中数据的时候, 会尽量的保持 RDD 的分区和数据源的分区数, 分区模式等一一对应 分区和 Shuffle 的关系 分区的主要作用是用来实现并行计算, 本质上和 Shuffle 没什么关系, 但是往往在进行数据处理的时候, 例如 reduceByKey, groupByKey 等聚合操作, 需要把 Key 相同的 Value 拉取到一起进行计算, 这个时候因为这些 Key 相同的 Value 可能会坐落于不同的分区, 于是理解分区才能理解 Shuffle 的根本原理 Spark 中的 Shuffle 操作的特点 只有 Key-Value 型的 RDD 才会有 Shuffle 操作, 例如 RDD[(K, V)], 但是有一个特例, 就是 repartition 算子可以对任何数据类型 Shuffle 早期版本 Spark 的 Shuffle 算法是 Hash base shuffle, 后来改为 Sort base shuffle, 更适合大吞吐量的场景 2分区操作很多算子(支持shuffle的 )都可以指定分区数 这些算子,可以在最后一个参数的位置传入新的分区数 如没有重新指定分区数,默认从父rdd中继承分区数 1 查看分区数12scala&gt; sc.parallelize(1 to 100).countres0: Long = 100 spark-shell --master local[8], 这样会生成 1 个 Executors, 这个 Executors 有 8 个 Cores, 所以默认会有 8 个 Tasks, 每个 Cores 对应一个分区, 每个分区对应一个 Tasks, 可以通过 rdd.partitions.size 来查看分区数量 默认的分区数量是和 Cores 的数量有关的, 也可以通过如下三种方式修改或者重新指定分区数量 创建 RDD 时指定分区数 1234567891011scala&gt; val rdd1 = sc.parallelize(1 to 100, 6)rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at parallelize at &lt;console&gt;:24scala&gt; rdd1.partitions.sizeres1: Int = 6scala&gt; val rdd2 = sc.textFile("hdfs:///dataset/wordcount.txt", 6)rdd2: org.apache.spark.rdd.RDD[String] = hdfs:///dataset/wordcount.txt MapPartitionsRDD[3] at textFile at &lt;console&gt;:24scala&gt; rdd2.partitions.sizeres2: Int = 7 rdd1 是通过本地集合创建的, 创建的时候通过第二个参数指定了分区数量. rdd2 是通过读取 HDFS 中文件创建的, 同样通过第二个参数指定了分区数, 因为是从 HDFS 中读取文件, 所以最终的分区数是由 Hadoop 的 InputFormat 来指定的, 所以比指定的分区数大了一个. 通过 coalesce 算子指定 1coalesce(numPartitions: Int, shuffle: Boolean = false)(implicit ord: Ordering[T] = null): RDD[T] numPartitions 新生成的 RDD 的分区数 shuffle 是否 Shuffle 12345678910111213141516171819202122232425262728293031323334scala&gt; val source = sc.parallelize(1 to 100, 6)source: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:24scala&gt; source.partitions.sizeres0: Int = 6scala&gt; val noShuffleRdd = source.coalesce(numPartitions=8, shuffle=false)noShuffleRdd: org.apache.spark.rdd.RDD[Int] = CoalescedRDD[1] at coalesce at &lt;console&gt;:26scala&gt; noShuffleRdd.toDebugString res1: String =(6) CoalescedRDD[1] at coalesce at &lt;console&gt;:26 [] | ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:24 [] scala&gt; val noShuffleRdd = source.coalesce(numPartitions=8, shuffle=false) noShuffleRdd: org.apache.spark.rdd.RDD[Int] = CoalescedRDD[1] at coalesce at &lt;console&gt;:26scala&gt; shuffleRdd.toDebugString res3: String =(8) MapPartitionsRDD[5] at coalesce at &lt;console&gt;:26 [] | CoalescedRDD[4] at coalesce at &lt;console&gt;:26 [] | ShuffledRDD[3] at coalesce at &lt;console&gt;:26 [] +-(6) MapPartitionsRDD[2] at coalesce at &lt;console&gt;:26 [] | ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:24 []scala&gt; noShuffleRdd.partitions.size res4: Int = 6scala&gt; shuffleRdd.partitions.sizeres5: Int = 8如果 shuffle 参数指定为 false, 运行计划中确实没有 ShuffledRDD, 没有 shuffled 这个过程如果 shuffle 参数指定为 true, 运行计划中有一个 ShuffledRDD, 有一个明确的显式的 shuffled 过程如果 shuffle 参数指定为 false 却增加了分区数, 分区数并不会发生改变, 这是因为增加分区是一个宽依赖, 没有 shuffled 过程无法做到, 后续会详细解释宽依赖的概念 通过 repartition 算子指定 1repartition(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] repartition 算子本质上就是 coalesce(numPartitions, shuffle = true) 12345678910111213scala&gt; val source = sc.parallelize(1 to 100, 6)source: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[7] at parallelize at &lt;console&gt;:24scala&gt; source.partitions.sizeres7: Int = 6scala&gt; source.repartition(100).partitions.size res8: Int = 100scala&gt; source.repartition(1).partitions.size res9: Int = 1增加分区有效减少分区有效 repartition 算子无论是增加还是减少分区都是有效的, 因为本质上 repartition 会通过 shuffle 操作把数据分发给新的 RDD 的不同的分区, 只有 shuffle 操作才可能做到增大分区数, 默认情况下, 分区函数是 RoundRobin, 如果希望改变分区函数, 也就是数据分布的方式, 可以通过自定义分区函数来实现 2 RDD 的 Shuffle 是什么partitioner用于计算一个数据应该发往哪个机器上 (就是哪个分区) HashPartitioner 函数 对key求hashcode和对应的reduce的数量取模 摸出来是几就发往哪个分区 key .hashCode % reduce count 的值 取值范围在0 到 reduce count之间 hash base 和sort base 用于描述中间过程如何存放文件* rdd2 =rdd1.reduceByKey(_+_) 这个reduceByKey() 是属于rdd2的 1234val sourceRdd = sc.textFile("hdfs://node01:9020/dataset/wordcount.txt")val flattenCountRdd = sourceRdd.flatMap(_.split(" ")).map((_, 1))val aggCountRdd = flattenCountRdd.reduceByKey(_ + _) //这个reduceByKey() 是属于aggCountRddval result = aggCountRdd.collect reduceByKey 这个算子本质上就是先按照 Key 分组, 后对每一组数据进行 reduce, 所面临的挑战就是 Key 相同的所有数据可能分布在不同的 Partition 分区中, 甚至可能在不同的节点中, 但是它们必须被共同计算. 为了让来自相同 Key 的所有数据都在 reduceByKey 的同一个 reduce 中处理, 需要执行一个 all-to-all 的操作, 需要在不同的节点(不同的分区)之间拷贝数据, 必须跨分区聚集相同 Key 的所有数据, 这个过程叫做 Shuffle. 3 RDD 的 Shuffle 原理Spark 的 Shuffle 发展大致有两个阶段: Hash base shuffle 和 Sort base shuffle 3.1 Hash base shuffle 大致的原理是分桶, 假设 Reducer 的个数为 R, 那么每个 Mapper 有 R 个桶, 按照 Key 的 Hash 将数据映射到不同的桶中, Reduce 找到每一个 Mapper 中对应自己的桶拉取数据. 假设 Mapper 的个数为 M, 整个集群的文件数量是 M * R, 如果有 1,000 个 Mapper 和 Reducer, 则会生成 1,000,000 个文件, 这个量非常大了. 过多的文件会导致文件系统打开过多的文件描述符, 占用系统资源. 所以这种方式并不适合大规模数据的处理, 只适合中等规模和小规模的数据处理, 在 Spark 1.2 版本中废弃了这种方式. 3.2 Sort base shuffle 对于 Sort base shuffle 来说, 每个 Map 侧的分区只有一个输出文件, Reduce 侧的 Task 来拉取, 大致流程如下 Map 侧将数据全部放入一个叫做 AppendOnlyMap 的组件中, 同时可以在这个特殊的数据结构中做聚合操作 然后通过一个类似于 MergeSort 的排序算法 TimSort 对 AppendOnlyMap 底层的 Array 排序 先按照 Partition ID 排序, 后按照 Key 的 HashCode 排序 最终每个 Map Task 生成一个 输出文件, Reduce Task 来拉取自己对应的数据 从上面可以得到结论, Sort base shuffle 确实可以大幅度减少所产生的中间文件, 从而能够更好的应对大吞吐量的场景, 在 Spark 1.2 以后, 已经默认采用这种方式. 但是需要大家知道的是, Spark 的 Shuffle 算法并不只是这一种, 即使是在最新版本, 也有三种 Shuffle 算法, 这三种算法对每个 Map 都只产生一个临时文件, 但是产生文件的方式不同, 一种是类似 Hash 的方式, 一种是刚才所说的 Sort, 一种是对 Sort 的一种优化(使用 Unsafe API 直接申请堆外内存) 二 缓存缓存是将数据缓存到blockMananger 中 导入隐式转换的意义 是可以获得ds和df的底层 row 和internelRow 1 缓存的意义使用缓存的原因 - 多次使用 RDD 减少shuffle,减少其他算子执行,缓存算子生成结果 需求: 在日志文件中找到访问次数最少的 IP 和访问次数最多的 IP 123456789101112131415161718192021222324252627282930313233343536373839404142434445package com.nicai.demoimport org.apache.commons.lang3.StringUtilsimport org.apache.spark.&#123;SparkConf, SparkContext&#125;import org.junit.Testclass CacheDemo &#123; /** * 1. 创建sc * 2. 读取文件 * 3. 取出IP, 赋予初始频率 * 4. 清洗 * 5. 统计IP出现的次数 * 6. 统计出现次数最少的IP * 7. 统计出现次数最多的IP */ @Test def prepare(): Unit = &#123; // 1. 创建 SC val conf = new SparkConf().setAppName("cache_prepare").setMaster("local[6]") val sc = new SparkContext(conf) // 2. 读取文件 val source = sc.textFile("G:\\develop\\data\\access_log_sample.txt") // 3. 取出IP, 赋予初始频率 val countRDD = source.map( item =&gt; (item.split(" ")(0), 1) ) // 4. 数据清洗 val cleanRDD = countRDD.filter( item =&gt; StringUtils.isNotEmpty(item._1) ) // 5. 统计IP出现的次数(聚合) val aggRDD = cleanRDD.reduceByKey( (curr, agg) =&gt; curr + agg ) // 6. 统计出现次数最少的IP(得出结论) val lessIp = aggRDD.sortBy(item =&gt; item._2, ascending = true).first() // 7. 统计出现次数最多的IP(得出结论) 降序 val moreIp = aggRDD.sortBy(item =&gt; item._2, ascending = false).first() println((lessIp, moreIp)) &#125;&#125;5是一个 Shuffle 操作, Shuffle 操作会在集群内进行数据拷贝在上述代码中, 多次使用到了 aggRDD并执行了action操作, 导致文件读取两次,6之前的代码(执行)计算两次, 性能有损耗 使用缓存的原因 - 容错 当在计算 RDD3 的时候如果出错了, 会怎么进行容错? 会再次计算 RDD1 和 RDD2 的整个链条, 假设 RDD1 和 RDD2 是通过比较昂贵的操作得来的, 有没有什么办法减少这种开销? 上述两个问题的解决方案其实都是 缓存, 除此之外, 使用缓存的理由还有很多, 但是总结一句, 就是缓存能够帮助开发者在进行一些昂贵操作后, 将其结果保存下来, 以便下次使用无需再次执行, 缓存能够显著的提升性能. 所以, 缓存适合在一个 RDD 需要重复多次利用, 并且还不是特别大的情况下使用, 例如迭代计算等场景. 2 缓存相关的 API2.1 可以使用 cache 方法进行缓存1234567891011121314151617//使用cache缓存@Testdef cacheDemo(): Unit = &#123; val conf = new SparkConf().setMaster("local[3]").setAppName("cache") val context =new SparkContext(conf) //读取文件 val file = context.textFile("G:\\develop\\bigdatas\\BigData\\day26sparkActions\\data\\access_log_sample.txt") val rdd1 = file.map(it =&gt; ((it.split(" ")) (0), 1)) .reduceByKey((curr, agg) =&gt; curr + agg) .cache() //缓存结果 val rddMax = rdd1 .sortBy(item =&gt; item._2, true) .first() val rddMin = rdd1.sortBy(item2 =&gt; item2._2,false).first() println(rddMax,rddMin)&#125;//其实 cache 方法底层就是persist()方法 这个persist() 底层有调用了persist(StorageLevel.MEMORY_ONLY)方法 StorageLevel.MEMORY_ONLY为其默认 2.2 也可以使用 persist 方法进行缓存123456789101112131415161718//使用persist缓存 @Test def persistDemo(): Unit = &#123; val conf = new SparkConf().setMaster("local[3]").setAppName("cache") val context =new SparkContext(conf) //读取文件 val file = context.textFile("G:\\develop\\bigdatas\\BigData\\day26sparkActions\\data\\access_log_sample.txt") val rdd1 = file.map(it =&gt; ((it.split(" ")) (0), 1)) .reduceByKey((curr, agg) =&gt; curr + agg) .persist(StorageLevel.MEMORY_ONLY) //缓存结果 与cache, persist()一样 //.persist(参数) //缓存级别 val rddMax = rdd1 .sortBy(item =&gt; item._2, true) .first() val rddMin = rdd1.sortBy(item2 =&gt; item2._2,false).first() println(rddMax,rddMin) &#125; //其实 cache 方法底层就是persist()方法 这个persist() 底层有调用了persist(StorageLevel.MEMORY_ONLY)方法 StorageLevel.MEMORY_ONLY为其默认 2.3 清理缓存1234567891011121314151617181920@Test def cacheDemo(): Unit = &#123; val conf = new SparkConf().setMaster("local[3]").setAppName("cache") val context =new SparkContext(conf) //读取文件 val file = context.textFile("G:\\develop\\bigdatas\\BigData\\day26sparkActions\\data\\access_log_sample.txt") val rdd1 = file.map(it =&gt; ((it.split(" ")) (0), 1)) .reduceByKey((curr, agg) =&gt; curr + agg) .cache() //缓存结果 //清除缓存 rdd.unpersist() val rddMax = rdd1 .sortBy(item =&gt; item._2, true) .first() val rddMin = rdd1.sortBy(item2 =&gt; item2._2,false).first() println(rddMax,rddMin) context.close() &#125; 根据缓存级别的不同, 缓存存储的位置也不同, 但是使用 unpersist 可以指定删除 RDD 对应的缓存信息, 并指定缓存级别为 NONE 3 缓存级别缓存是一个技术活, 要考虑很多 是否使用磁盘缓存? 是否使用内存缓存? 是否使用堆外内存? 缓存前是否先序列化? 是否需要有副本? 3.1 查看缓存级别对象12345678910111213141516171819@Test def persistDemo(): Unit = &#123; val conf = new SparkConf().setMaster("local[3]").setAppName("cache") val context =new SparkContext(conf) //读取文件 val file = context.textFile("G:\\develop\\bigdatas\\BigData\\day26sparkActions\\data\\access_log_sample.txt") val rdd1 = file.map(it =&gt; ((it.split(" ")) (0), 1)) .reduceByKey((curr, agg) =&gt; curr + agg) .persist(StorageLevel.MEMORY_ONLY) //缓存结果 与cache, persist()一样 //.persist(参数) //缓存级别 //查看缓存级别 println(rdd1.getStorageLevel) val rddMax = rdd1 .sortBy(item =&gt; item._2, true) .first() val rddMin = rdd1.sortBy(item2 =&gt; item2._2,false).first() println(rddMax,rddMin) context.close() &#125; deserialized是否以反序列化形式存储 若是true 则不序列化 ,若是false 则存储序列化后的值 其值为true 存储的是对象 若是false 则存储的是二进制文件 带2的为副本数 缓存级别 userDisk 是否使用磁盘 useMemory 是否使用内存 useOffHeap 是否使用堆外内存 deserialized是否以反序列化形式存储 replication 副本数 NONE 就是不存 false false false false 1 DISK_ONLY 存到磁盘里 true false false false 1 DISK_ONLY_2 true false false false 2 MEMORY_ONLY 存到内存 false true false true 1 MEMORY_ONLY_2 false true false true 2 MEMORY_ONLY_SER存到内存以二进制文件的形式 false true false false 1 MEMORY_ONLY_SER_2 false true false false 2 MEMORY_AND_DISK内存和磁盘都有 true true false true 1 MEMORY_AND_DISK true true false true 2 MEMORY_AND_DISK_SER内存和磁盘都有以二进制形式存储 true true false false 1 MEMORY_AND_DISK_SER_2 true true false false 2 OFF_HEAP true true true false 1 3.2如何选择分区级别Spark 的存储级别的选择，核心问题是在 memory 内存使用率和 CPU 效率之间进行权衡。建议按下面的过程进行存储级别的选择: 如果您的 RDD 适合于默认存储级别（MEMORY_ONLY），leave them that way。这是 CPU 效率最高的选项，允许 RDD 上的操作尽可能快地运行. 如果不是，试着使用 MEMORY_ONLY_SER 和 selecting a fast serialization library 以使对象更加节省空间，但仍然能够快速访问。(Java和Scala) 不要溢出到磁盘，除非计算您的数据集的函数是昂贵的，或者它们过滤大量的数据。否则，重新计算分区可能与从磁盘读取分区一样快. 如果需要快速故障恢复，请使用复制的存储级别（例如，如果使用 Spark 来服务 来自网络应用程序的请求）。All 存储级别通过重新计算丢失的数据来提供完整的容错能力，但复制的数据可让您继续在 RDD 上运行任务，而无需等待重新计算一个丢失的分区. 三 Checkpoint1 Checkpoint 的作用​ Checkpoint 的主要作用是斩断 RDD 的依赖链, 并且将数据存储在可靠的存储引擎中, 例如支持分布式存储和副本机制的 HDFS. Checkpoint 的方式 可靠的 将数据存储在可靠的存储引擎中, 例如 HDFS 本地的 将数据存储在本地 什么是斩断依赖链 斩断依赖链是一个非常重要的操作, 接下来以 HDFS 的 NameNode 的原理来举例说明 HDFS 的 NameNode 中主要职责就是维护两个文件, 一个叫做 edits, 另外一个叫做 fsimage. edits 中主要存放 EditLog, FsImage 保存了当前系统中所有目录和文件的信息. 这个 FsImage 其实就是一个 Checkpoint. HDFS 的 NameNode 维护这两个文件的主要过程是, 首先, 会由 fsimage 文件记录当前系统某个时间点的完整数据, 自此之后的数据并不是时刻写入 fsimage, 而是将操作记录存储在 edits 文件中. 其次, 在一定的触发条件下, edits 会将自身合并进入 fsimage. 最后生成新的 fsimage 文件, edits 重置, 从新记录这次 fsimage 以后的操作日志. 如果不合并 edits 进入 fsimage 会怎样? 会导致 edits 中记录的日志过长, 容易出错. 所以当 Spark 的一个 Job 执行流程过长的时候, 也需要这样的一个斩断依赖链的过程, 使得接下来的计算轻装上阵. Checkpoint 和 Cache 的区别 Cache 可以把 RDD 计算出来然后放在内存中, 但是 RDD 的依赖链(相当于 NameNode 中的 Edits 日志)是不能丢掉的, 因为这种缓存是不可靠的, 如果出现了一些错误(例如 Executor 宕机), 这个 RDD 的容错就只能通过回溯依赖链, 重放计算出来. 但是 Checkpoint 把结果保存在 HDFS 这类存储中, 就是可靠的了, 所以可以斩断依赖, 如果出错了, 则通过复制 HDFS 中的文件来实现容错. 所以他们的区别主要在以下两点 Checkpoint 可以保存数据到 HDFS 这类可靠的存储上, Persist 和 Cache 只能保存在本地的磁盘和内存中 Checkpoint 可以斩断 RDD 的依赖链, 而 Persist 和 Cache 不行 因为 CheckpointRDD 没有向上的依赖链, 所以程序结束后依然存在, 不会被删除. 而 Cache 和 Persist 会在程序结束后立刻被清除. 2 使用 Checkpoint12345678910111213141516171819202122232425//checkpoint @Test def checkPointDemo(): Unit = &#123; val conf = new SparkConf().setMaster("local[3]").setAppName("cache") val context =new SparkContext(conf) //设置保存checkpoint的目录 也可以在hdfs上 context.setCheckpointDir("checkpoint") //读取文件 val file = context.textFile("G:\\develop\\bigdatas\\BigData\\day26sparkActions\\data\\access_log_sample.txt") val rdd1 = file.map(it =&gt; ((it.split(" ")) (0), 1)) .reduceByKey((curr, agg) =&gt; curr + agg) /* checkpoint的返回值为unit空 1 不准确的说checkpoint是一个action操作, 2 如果调用 checkpoint则会重新计算rdd,然后把结果存在hdfs或本地目录 所以在checkpoint之前应该先缓存一下 */ .persist(StorageLevel.MEMORY_ONLY) rdd1.checkpoint() val rddMax = rdd1 .sortBy(item =&gt; item._2, true) .first() val rddMin = rdd1.sortBy(item2 =&gt;item2._2,false).first() println(rddMax,rddMin) &#125;]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>BigData</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SparkRDD]]></title>
    <url>%2F2017%2F08%2F16%2FSparkRDD.html</url>
    <content type="text"><![CDATA[一 深入RDDS先来个小demo 1 需求在访问日志中,统计独立IP数量,TOP10 2 明确数据结构IP,时间戳,http method,URL…… 121/Nov/2017:00:00:15 +0000] "GET /axis2/services/WebFilteringService/getCategoryByUrl?app=chrome_antiporn&amp;ver=0.19.7.1&amp;url=https%3A//securepubads.g.doubleclick.net/static/3p_cookie.html&amp;cat=business-and-economy HTTP/1.1" 200 133 "-" "Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36"//这是一条数据 3 明确编码步骤1 取出IP,生成只有IP的数据集 2 简单清洗 3 统计IP出现次数 4 排序 按照IP出现次数 5 取出前十 1234567891011121314151617181920212223242526272829import org.apache.commons.lang3.StringUtilsimport org.apache.spark.&#123;SparkConf, SparkContext&#125;import org.junit.Testclass AccessLogsCount &#123; @Test def logs: Unit = &#123; //1 创建SparkContext val log_count = new SparkConf().setMaster("local[3]").setAppName("log_count") val context:SparkContext = new SparkContext(log_count) //2 获取文件 一次获取一行 val unit = context.textFile("G:\\develop\\bigdatas\\BigData\\day25SparkRdds\\data\\access_log_sample.txt") //3 读出IP 并赋予值为1 val rdd1 = unit.map(item =&gt; (item.split(" ")(0),1)) //4 简单清洗 //4.1 去除空的数据 val rdd2 = rdd1.filter(item =&gt; StringUtils.isNotEmpty(item._1)) //4.2 去除非法数据 //4.3 根据业务规整数据 //5 根据IP出现次数进行聚合 val rdd3 = rdd2.reduceByKey((curr,ag) =&gt; curr+ag) //6 根据IP出现次数进行排序 val result = rdd3.sortBy(item =&gt; item._2,ascending = false) //7 取出结果 result.take(10).foreach(println(_)) &#125;&#125; 4 解析1假设要针对整个网站的历史数据进行处理, 量有 1T, 如何处理? 因为单机运行瓶颈太多内存,磁盘,CPU等 放在集群中, 利用集群多台计算机来并行处理 2如何放在集群中运行? 简单来讲, 并行计算就是同时使用多个计算资源解决一个问题, 有如下四个要点 要解决的问题必须可以分解为多个可以并发计算的部分 每个部分要可以在不同处理器上被同时执行 需要一个共享内存的机制 需要一个总体上的协作机制来进行调度 3 如果放在集群中的话, 可能要对整个计算任务进行分解, 如何分解? 概述 对于 HDFS 中的文件, 是分为不同的 Block 的 在进行计算的时候, 就可以按照 Block 来划分, 每一个 Block 对应一个不同的计算单元 扩展 RDD 并没有真实的存放数据, 数据是从 HDFS 中读取的, 在计算的过程中读取即可 RDD 至少是需要可以 分片 的, 因为HDFS中的文件就是分片的, RDD 分片的意义在于表示对源数据集每个分片的计算, RDD 可以分片也意味着 可以并行计算 4 移动数据不如移动计算是一个基础的优化, 如何做到? 每一个计算单元需要记录其存储单元的位置, 尽量调度过去 5 在集群中运行, 需要很多节点之间配合, 出错的概率也更高, 出错了怎么办? RDD1 → RDD2 → RDD3 这个过程中, RDD2 出错了, 有两种办法可以解决 缓存 RDD2 的数据, 直接恢复 RDD2, 类似 HDFS 的备份机制 记录 RDD2 的依赖关系, 通过其父级的 RDD 来恢复 RDD2, 这种方式会少很多数据的交互和保存 如何通过父级 RDD 来恢复? 记录 RDD2 的父亲是 RDD1 记录 RDD2 的计算函数, 例如记录 RDD2 = RDD1.map(…), map(…) 就是计算函数 当 RDD2 计算出错的时候, 可以通过父级 RDD 和计算函数来恢复 RDD2 6 假如任务特别复杂, 流程特别长, 有很多 RDD 之间有依赖关系, 如何优化? 上面提到了可以使用依赖关系来进行容错, 但是如果依赖关系特别长的时候, 这种方式其实也比较低效, 这个时候就应该使用另外一种方式, 也就是记录数据集的状态 在 Spark 中有两个手段可以做到 缓存Checkpoint 5 rdd出现解决了什么问题在 RDD 出现之前, 当时 MapReduce 是比较主流的, 而 MapReduce 如何执行迭代计算的任务呢? 多个 MapReduce 任务之间没有基于内存的数据共享方式, 只能通过磁盘来进行共享 这种方式明显比较低效 rdd 执行迭代计算任务 在 Spark 中, 其实最终 Job3 从逻辑上的计算过程是: Job3 = (Job1.map).filter, 整个过程是共享内存的, 而不需要将中间结果存放在可靠的分布式文件系统中 这种方式可以在保证容错的前提下, 提供更多的灵活, 更快的执行速度, RDD 在执行迭代型任务时候的表现可以通过下面代码体现 12345678910// 线性回归val points = sc.textFile(...) .map(...) .persist(...)val w = randomValuefor (i &lt;- 1 to 10000) &#123; val gradient = points.map(p =&gt; p.x * (1 / (1 + exp(-p.y * (w dot p.x))) - 1) * p.y) .reduce(_ + _) w -= gradient&#125; 6 rdd的特点RDD 不仅是数据集, 也是编程模型 RDD 即是一种数据结构, 同时也提供了上层 API, 同时 RDD 的 API 和 Scala 中对集合运算的 API 非常类似, 同样也都是各种算子 RDD 的算子大致分为两类: Transformation 转换操作, 例如 map flatMap filter 等 Action 动作操作, 例如 reduce collect show 等 执行 RDD 的时候, 在执行到转换操作的时候, 并不会立刻执行, 直到遇见了 Action 操作, 才会触发真正的执行, 这个特点叫做 惰性求值 rdd可以分区 RDD 是一个分布式计算框架, 所以, 一定是要能够进行分区计算的, 只有分区了, 才能利用集群的并行计算能力 同时, RDD 不需要始终被具体化, 也就是说: RDD 中可以没有数据, 只要有足够的信息知道自己是从谁计算得来的就可以, 这是一种非常高效的容错方式 rdd是只读的RDD 是只读的, 不允许任何形式的修改. 虽说不能因为 RDD 和 HDFS 是只读的, 就认为分布式存储系统必须设计为只读的. 但是设计为只读的, 会显著降低问题的复杂度, 因为 RDD 需要可以容错, 可以惰性求值, 可以移动计算, 所以很难支持修改. RDD2 中可能没有数据, 只是保留了依赖关系和计算函数, 那修改啥? 如果因为支持修改, 而必须保存数据的话, 怎么容错? 如果允许修改, 如何定位要修改的那一行? RDD 的转换是粗粒度的, 也就是说, RDD 并不感知具体每一行在哪. rdd是可以容错的RDD 的容错有两种方式 保存 RDD 之间的依赖关系, 以及计算函数, 出现错误重新计算 直接将 RDD 的数据存放在外部存储系统, 出现错误直接读取, Checkpoint 弹性分布式数据集分布式 RDD 支持分区, 可以运行在集群中 弹性 RDD 支持高效的容错 RDD 中的数据即可以缓存在内存中, 也可以缓存在磁盘中, 也可以缓存在外部存储中 数据集 RDD 可以不保存具体数据, 只保留创建自己的必备信息, 例如依赖和计算函数 RDD 也可以缓存起来, 相当于存储具体数据 总结 rdd的五大属性 首先整理一下上面所提到的 RDD 所要实现的功能: RDD 有分区 RDD 要可以通过依赖关系和计算函数进行容错 RDD 要针对数据本地性进行优化 RDD 支持 MapReduce 形式的计算, 所以要能够对数据进行 Shuffled 对于 RDD 来说, 其中应该有什么内容呢? 如果站在 RDD 设计者的角度上, 这个类中, 至少需要什么属性? Partition List 分片列表, 记录 RDD 的分片, 可以在创建 RDD 的时候指定分区数目, 也可以通过算子来生成新的 RDD 从而改变分区数目 Compute Function 为了实现容错, 需要记录 RDD 之间转换所执行的计算函数 RDD Dependencies RDD 之间的依赖关系, 要在 RDD 中记录其上级 RDD 是谁, 从而实现容错和计算 Partitioner 为了执行 Shuffled 操作, 必须要有一个函数用来计算数据应该发往哪个分区 Preferred Location 优先位置, 为了实现数据本地性操作, 从而移动计算而不是移动存储, 需要记录每个 RDD 分区最好应该放置在什么位置 二 rdd算子1 分类RDD 中的算子从功能上分为两大类 Transformation(转换) 它会在一个已经存在的 RDD 上创建一个新的 RDD, 将旧的 RDD 的数据转换为另外一种形式后放入新的 RDD(map,flatMap等) 转换算子的本质就是生成各种rdd,让rdd之间具有联系,只是生成rdd链条,执行到转换的时候,并不会真的执行整个程序,而是在Action被调用后,程序才可以执行 Action(动作) 执行各个分区的计算任务, 将的到的结果返回到 Driver 中 执行操作 RDD 中可以存放各种类型的数据, 那么对于不同类型的数据, RDD 又可以分为三类 针对基础类型(例如 String)处理的普通算子 针对 Key-Value 数据处理的 byKey 算子 (reduceByKey等) 针对数字类型数据处理的计算算子 2 特点 Spark 中所有的 Transformations 是 Lazy(惰性) 的, 它们不会立即执行获得结果. 相反, 它们只会记录在数据集上要应用的操作. 只有当需要返回结果给 Driver 时, 才会执行这些操作, 通过 DAGScheduler 和 TaskScheduler 分发到集群中运行, 这个特性叫做 惰性求值 默认情况下, 每一个 Action 运行的时候, 其所关联的所有 Transformation RDD 都会重新计算, 但是也可以使用 presist 方法将 RDD 持久化到磁盘或者内存中. 这个时候为了下次可以更快的访问, 会把数据保存到集群上. 3 TransFormation算子1 map123sc.parallelize(Seq(1, 2, 3)) .map( num =&gt; num * 10 ) .collect() map(T ⇒ U) 作用 把 RDD 中的数据 一对一 的转为另一种形式 签名 1def map[U: ClassTag](f: T ⇒ U): RDD[U] 参数 f → Map 算子是 原RDD → 新RDD 的过程, 传入函数的参数是原 RDD 数据, 返回值是经过函数转换的新 RDD 的数据 注意点 Map 是一对一, 如果函数是 String → Array[String] 则新的 RDD 中每条数据就是一个数组 2 flatMap(T ⇒ List[U])123sc.parallelize(Seq("Hello lily", "Hello lucy", "Hello tim")) .flatMap( line =&gt; line.split(" ") ) .collect() 作用 FlatMap 算子和 Map 算子类似, 但是 FlatMap 是一对多 调用 1def flatMap[U: ClassTag](f: T ⇒ List[U]): RDD[U] 参数 f → 参数是原 RDD 数据, 返回值是经过函数转换的新 RDD 的数据, 需要注意的是返回值是一个集合, 集合中的数据会被展平后再放入新的 RDD 注意点 flatMap 其实是两个操作, 是 map + flatten, 也就是先转换, 后把转换而来的 List 展开 Spark 中并没有直接展平 RDD 中数组的算子, 可以使用 flatMap 做这件事 3mapPartitions(List[T] ⇒ List[U])DD[T] ⇒ RDD[U] 和 map 类似, 但是针对整个分区的数据转换 map是针对一条数据,mapPartitions是针对一个分区的数据转换 123456context.parallelize(Seq(1,2,3,4),2) //2 为分区数 .mapPartitions(item =&gt; &#123; item.foreach(iter =&gt; print(iter)) item &#125;) .collect() 注意 mapPartitions()的返回值和传入参数都是集合类型 因为是一个分区的数据 12345678context.parallelize(Seq(1,2,3,4),2) //2 为分区数 .mapPartitions(iter =&gt;&#123; //iter 是scala中的集合类型 iter.map(item =&gt; item*10) //返回一个集合 &#125;) .collect() .foreach(print(_)) &#125; 4 mapPartitionsWithIndex和 mapPartitions 类似, 只是在函数中增加了分区的 Index 1234567891011121314 context.parallelize(Seq(1,0,55,555,999),2) .mapPartitionsWithIndex((in,iter) =&gt;&#123; println("index"+in) iter.map(item =&gt; println(item)) &#125;) .collect()//两个结果一样 顺序可能不同context.parallelize(Seq(1,0,55,555,999),2) .mapPartitionsWithIndex((in,iter) =&gt;&#123; println("index"+in) iter.foreach(item =&gt; println(item)) iter &#125;) .collect() 结果: 123456789101112131415index010index155555999有可能出现index0index11550555999 1234567891011121314context.parallelize(Seq(1,2,3,4),2) .mapPartitionsWithIndex((in,iter) =&gt;&#123; println("index"+in) iter.foreach(item =&gt; println(item)) iter &#125;) .collect() context.parallelize(Seq(1,2,3,4),2) //Seq(1,0,55,555,999) .mapPartitionsWithIndex((in,iter) =&gt;&#123; println("index"+in) iter.map(item =&gt; println(item)) &#125;) .collect() 123456index0index13124 map,mapPartitions和mapPartitionsWithIndex区别 参数 返回值 参数个数 map 单条数据 单条数据 一个 mapPartitions 集合(一个分区所有数据) 集合 一个 mapPartitionsWithIndex 集合(一个分区所有数据)和分区数 集合 两个 5 filter作用 Filter 算子的主要作用是过滤掉不需要的内容 接收函数,参数为每一个元素,如果函数返回为true当前元素会被加入新的数据集,如果为false,会过滤掉该元素 1234context.parallelize(Seq(1,0,55,555,999,88),2) .filter(item =&gt; item % 2 ==0) .collect() .foreach(println(_)) 6 sample(withReplacement, fraction, seed)作用 Sample 算子可以从一个数据集中抽样出来一部分, 常用作于减小数据集以保证运行速度, 并且尽可能少规律的损失 参数 Sample 接受第一个参数为withReplacement, 意为是否取样以后是否还放回原数据集供下次使用, 简单的说, 如果这个参数的值为 true, 则抽样出来的数据集中可能会有重复 若为false 则不会有重复的值 Sample 接受第二个参数为fraction, 意为抽样的比例 double类型 Sample 接受第三个参数为seed, 随机数种子, 用于 Sample 内部随机生成下标, 一般不指定, 使用默认值 123val value = context.parallelize(Seq(1,2,3,4,5,6),2) //1,2,3,4,5,6 1,0,55,555,999,88 val unit = value.sample(false,0.5) unit.collect().foreach(println(_)) 1234567891023456 或者1234 结果不一定 7 mapValues作用 MapValues 只能作用于 Key-Value 型数据, 和 Map 类似, 也是使用函数按照转换数据, 不同点是 MapValues 只转换 Key-Value 中的 Value 1234context.parallelize(Seq(("a",1),("b",2),("c",3),("d",4),("e",5)),2) .mapValues(item =&gt; item*10) //item 指代("a",k) 中的k 若数据不为k-values格式 则这个方法调不到 .collect() .foreach(println(_)) 8 差集,交集和并集union(other) 并集 所有元素都会集合,包括重复的元素 intersection(other) 交集 Intersection 算子是一个集合操作, 用于求得 左侧集合 和 右侧集合 的交集, 换句话说, 就是左侧集合和右侧集合都有的元素, 并生成一个新的 RDD subtract(other, numPartitions)差集, 可以设置分区数 a中有 b中没有的 123456789101112131415161718//交集 并集 差集 @Test def union(): Unit =&#123; val a1 = context.parallelize(Seq(1,2,3,4,5,6)) val a2 = context.parallelize(Seq(4,5,6,7,8,9)) //并集 /* a1.union(a2) .collect() .foreach(println(_))*/ //交集 /*a1.intersection(a2) .collect() .foreach(println(_))*/ //差集 a1.subtract(a2) .collect() .foreach(println(_))//213 &#125; 9 distinct(numPartitions)作用 Distinct 算子用于去重 注意点 Distinct 是一个需要 Shuffled 的操作 本质上 Distinct 就是一个 reductByKey, 把重复的合并为一个 123sc.parallelize(Seq(1, 1, 2, 2, 3)) .distinct() .collect() 10groupByKey与reduceByKeyreduceByKey: reduceByKey((V, V) ⇒ V, numPartition) 作用 首先按照 Key 分组生成一个 Tuple, 然后针对每个组执行 reduce 算子 调用 1def reduceByKey(func: (V, V) ⇒ V): RDD[(K, V)] 参数 func → 执行数据处理的函数, 传入两个参数, 一个是当前值, 一个是局部汇总, 这个函数需要有一个输出, 输出就是这个 Key 的汇总结果 注意点 ReduceByKey 只能作用于 Key-Value 型数据, Key-Value 型数据在当前语境中特指 Tuple2 ReduceByKey 是一个需要 Shuffled 的操作 和其它的 Shuffled 相比, ReduceByKey是高效的, 因为类似 MapReduce 的, 在 Map 端有一个 Cominer, 这样 I/O 的数据便会减少 12 groupByKey 作用 GroupByKey 算子的主要作用是按照 Key 分组, 和 ReduceByKey 有点类似, 但是 GroupByKey 并不求聚合, 只是列举 Key 对应的所有 Value 注意点 GroupByKey 是一个 Shuffled GroupByKey 和 ReduceByKey 不同, 因为需要列举 Key 对应的所有数据, 所以无法在 Map 端做 Combine, 所以 GroupByKey 的性能并没有 ReduceByKey 好 12 总结 reduceByKey:在map端做Combiner 了可对结果做 聚合 groupByKey: 在map端不做聚合,也不做Combiner 结果格式: (k,(values,value,….)) ###11combineByKey() 作用 对数据集按照 Key 进行聚合 调用 combineByKey(createCombiner, mergeValue, mergeCombiners, [partitioner], [mapSideCombiner], [serializer]) 参数 createCombiner 将 Value 进行初步转换 mergeValue 在每个分区把上一步转换的结果聚合 mergeCombiners 在所有分区上把每个分区的聚合结果聚合 partitioner 可选, 分区函数 mapSideCombiner 可选, 是否在 Map 端 Combine serializer 序列化器 注意点 combineByKey 的要点就是三个函数的意义要理解 groupByKey, reduceByKey 的底层都是 combineByKey 例子: 12345678910111213141516171819202122@Test def combineByKeyDemo(): Unit =&#123; //创建数据 val rdd = context.parallelize(Seq( ("zhangsan", 99.0), ("zhangsan", 96.0), ("lisi", 97.0), ("lisi", 98.0), ("zhangsan", 97.0)) ) //处理数据 val rdd3 = rdd.combineByKey( //结果 ("zhangsan", (99.0,1)) createCombiner = (item: Double) =&gt; (item, 1), //结果 ("zhangsan", (99.0+96.0,1+1)) 分区上的聚合 mergeValue = (c: (Double, Int), newi: Double) =&gt; ((c._1 + newi), c._2 + 1), //结果 把分区上的结果再次聚合 形成最终结果 mergeCombiners = (cuur: (Double, Int), agg: (Double, Int)) =&gt; ((cuur._1 + agg._1), (cuur._2 + agg._2)) ) rdd3.collect().foreach(println(_)) &#125; 12 foldByKey(zeroValue)((V, V) ⇒ V)作用 和 ReduceByKey 是一样的, 都是按照 Key 做分组去求聚合, 但是 FoldByKey 的不同点在于可以指定初始值 调用 1foldByKey(zeroValue)(func) 参数 zeroValue 初始值 func seqOp 和 combOp 相同, 都是这个参数 注意点 FoldByKey 是 AggregateByKey 的简化版本, seqOp 和 combOp 是同一个函数 FoldByKey 指定的初始值作用于每一个 Value scala 中的foldleft或者foldRight 区别是这个值不会作用于每一个value 1234567@Test def foldByKeyDemo(): Unit =&#123; context.parallelize(Seq(("a",1),("a",2),("c",3),("d",4),("e",5))) .foldByKey(10)((curr,agg) =&gt; curr+agg) //每一个参数都会+10 .collect() .foreach(println(_)) &#125; 结果: 1234(d,14)(e,15)(a,13)(c,13) 13 aggregateByKey() 作用 聚合所有 Key 相同的 Value, 换句话说, 按照 Key 聚合 Value 调用 rdd.aggregateByKey(zeroValue)(seqOp, combOp) 参数 zeroValue 初始值seqOp 转换每一个值的函数comboOp 将转换过的值聚合的函数 注意点 * 为什么需要两个函数? aggregateByKey 运行将一个RDD[(K, V)]聚合为RDD[(K, U)], 如果要做到这件事的话, 就需要先对数据做一次转换, 将每条数据从V转为U, seqOp就是干这件事的 ** 当seqOp的事情结束以后, comboOp把其结果聚合 和 reduceByKey 的区别:: aggregateByKey 最终聚合结果的类型和传入的初始值类型保持一致 reduceByKey 在集合中选取第一个值作为初始值, 并且聚合过的数据类型不能改变 123456789101112131415/** 需求 求所有商品打八折 后 手机和电脑的总价** aggregateByKey(zeroValue)(seqOp,comOp)*zeroValue 指定初始值* seqOp: 作用于每一个元素,根据初始值进行计算*comOp: 将sepOp的结果进行聚合* */ @Test def agg(): Unit =&#123; context.parallelize(Seq(("手机", 10.0), ("手机", 15.0), ("电脑", 20.0))) .aggregateByKey(0.8)((zeroValue,item)=&gt; zeroValue*item,(curr,aggr)=&gt; curr+aggr) .collect() .foreach(println(_)) &#125; 适合针对每个数据先处理,后聚合 14 JOIN作用 将两个 RDD 按照相同的 Key 进行连接 调用 1join(other, [partitioner or numPartitions]) 参数 other 其它 RDD partitioner or numPartitions 可选, 可以通过传递分区函数或者分区数量来改变分区 注意点 Join 有点类似于 SQL 中的内连接, 只会再结果中包含能够连接到的 Key Join 的结果是一个笛卡尔积形式, 例如&quot;a&quot;, 1), (&quot;a&quot;, 2和&quot;a&quot;, 10), (&quot;a&quot;, 11的 Join 结果集是 &quot;a&quot;, 1, 10), (&quot;a&quot;, 1, 11), (&quot;a&quot;, 2, 10), (&quot;a&quot;, 2, 11 12345678//join //同key 时才参与join @Test def joinDemo(): Unit =&#123; val rdd1 = context.parallelize(Seq(("a", 1), ("a", 2), ("b", 1))) val rdd2 = context.parallelize(Seq(("a", 10), ("a", 11), ("a", 12))) rdd1.join(rdd2).collect().foreach(println(_)) &#125; 15 sortBy和sortByKey作用 排序相关相关的算子有两个, 一个是sortBy, 另外一个是sortByKey 调用 1sortBy(func, ascending, numPartitions) 参数 func通过这个函数返回要排序的字段 ascending是否升序 numPartitions分区数 注意点 普通的 RDD 没有sortByKey, 只有 Key-Value 的 RDD 才有 sortBy可以指定按照哪个字段来排序, sortByKey直接按照 Key 来排序 12345678910111213//sortBy 和sortByKey /*sortBy可以作用所有的类型的数据,sortByKey只能识别kv类型数据 sortBy可以按照任意部分来排序 sortByKey 只能按照k进行排序 * * */ @Test def sort(): Unit =&#123; val rdd1 = context.parallelize(Seq(1,5,2,8,3)) val rdd2 = context.parallelize(Seq(("a", 3), ("b", 2), ("c", 8))) rdd1.sortBy(it =&gt; it) rdd2.sortBy(it =&gt; it._2).collect().foreach(println(_)) rdd2.sortByKey() //按照key 进行排序 &#125; 16 repartitioin 和 coalesce作用 重分区 一般涉及到分区操作的算子常见的有两个, repartitioin 和 coalesce, 两个算子都可以调大或者调小分区数量 调用 repartitioin(numPartitions) coalesce(numPartitions, shuffle) 若coalesce想调大分区 则必须设置shuffle为true 参数 numPartitions 新的分区数 shuffle 是否 shuffle, 如果新的分区数量比原分区数大, 必须 Shuffled, 否则重分区无效 注意点 repartition 和 coalesce 的不同就在于 coalesce 可以控制是否 Shuffle repartition 是一个 Shuffled 操作 12345678@Test def re(): Unit =&#123; val rdd1 = context.parallelize(Seq(1,5,2,8,3),2) println(rdd1.repartition(5).partitions.size) //默认shuffle为true println(rdd1.coalesce(1))//调小可以 相对于初始分区数 println(rdd1.coalesce(5, shuffle = true))//调大指定shuffle为true &#125; repartitionAndSortWithinPartitions 重新分区的同时升序排序, 在partitioner中排序, 比先重分区再排序要效率高, 建议使用在需要分区后再排序的场景使用 cartesian(other) (RDD[T], RDD[U]) ⇒ RDD[(T, U)] 生成两个 RDD 的笛卡尔积 cogroup(other, numPartitions) 作用 多个 RDD 协同分组, 将多个 RDD 中 Key 相同的 Value 分组 调用 cogroup(rdd1, rdd2, rdd3, [partitioner or numPartitions]) 参数 rdd… 最多可以传三个 RDD 进去, 加上调用者, 可以为四个 RDD 协同分组 partitioner or numPartitions 可选, 可以通过传递分区函数或者分区数来改变分区 注意点 对 RDD1, RDD2, RDD3 进行 cogroup, 结果中就一定会有三个 List, 如果没有 Value 则是空 List, 这一点类似于 SQL 的全连接, 返回所有结果, 即使没有关联上 CoGroup 是一个需要 Shuffled 的操作 1234567891011121314151617181920212223242526val rdd1 = sc.parallelize(Seq(("a", 1), ("a", 2), ("a", 5), ("b", 2), ("b", 6), ("c", 3), ("d", 2)))val rdd2 = sc.parallelize(Seq(("a", 10), ("b", 1), ("d", 3)))val rdd3 = sc.parallelize(Seq(("b", 10), ("a", 1)))val result1 = rdd1.cogroup(rdd2).collect()val result2 = rdd1.cogroup(rdd2, rdd3).collect()/*执行结果:Array( (d,(CompactBuffer(2),CompactBuffer(3))), (a,(CompactBuffer(1, 2, 5),CompactBuffer(10))), (b,(CompactBuffer(2, 6),CompactBuffer(1))), (c,(CompactBuffer(3),CompactBuffer()))) */println(result1)/*执行结果:Array( (d,(CompactBuffer(2),CompactBuffer(3),CompactBuffer())), (a,(CompactBuffer(1, 2, 5),CompactBuffer(10),CompactBuffer(1))), (b,(CompactBuffer(2, 6),CompactBuffer(1),Co... */println(result2) 总结 所有的转换操作的算子都是惰性的,在执行的时候并不会真的去调度运行,求得结果,而是只是生成了对应的RDD,只有在Action时,才会真的运行求得结果 2 Action 算子1 reduce( (T, T) ⇒ U ) 不是一个shuffle操作作用 对整个结果集规约, 最终生成一条数据, 是整个数据集的汇总 调用 reduce( (currValue[T], agg[T]) ⇒ T ) 注意点 reduce 和 reduceByKey 是完全不同的, reduce 是一个 action, 并不是 Shuffled 操作 本质上 reduce 就是现在每个 partition 上求值, 最终把每个 partition 的结果再汇总 例如 一个RDD里面有一万条数据,大部分key相同,有十个key 不同,则reduceByKey会生成10条数据,但reduce只会生成1个结果 reduceByKey 是根据k分组,再把每组聚合 是针对kv数据进行计算 reduce 是针对一整个数据集来进行聚合 是针对任意类型进行计算 shuffle操作: 分为mapper和reduce,mapper将数据放入partition的函数计算求得分往哪个reduce,后分到对应的reduce中 reduce操作: 并没有mapper和reduce,因为reduce算子会作用于rdd中的每一个分区,然后在分区上求得局部结果,最终汇总求得最终结果 rdd中的五大属性: Partitioner 在shuffle过程中使用,partition只有KV型数据的RDD才有 12345678910111213class ActionDemo &#123; //定义变量 private val transformations: SparkConf = new SparkConf().setMaster("local[2]").setAppName("transformations") private val context = new SparkContext(transformations) @Test def reduceDemo(): Unit =&#123; val unit = context.parallelize(Seq(("a",1),("b",2),("c",3))) //curr为一条数据("a",1) reduce 整体上的结果,只有一个 val tuple:(String, Int) = unit.reduce((curr, agg) =&gt; ("总价",curr._2+agg._2)) println(tuple) &#125;&#125; 2 collect()以数组的形式返回数据集中所有元素 3 foreach( T ⇒ … )遍历每一个元素 4 count() 和countByKey()count()返回元素个数 countByKey() 作用 求得整个数据集中 Key 以及对应 Key 出现的次数 注意点 返回结果为 Map(key → count) 常在解决数据倾斜问题时使用, 查看倾斜的 Key 12345678910111213/* * 每次调用Action 都会生成一个job,job会运行获取结果(形成大量log日志) * * countByKey的结果是Map(k -&gt; k的count) * Map(b -&gt; 2, a -&gt; 2, c -&gt; 1) * */ @Test def count(): Unit =&#123; val unit = context.parallelize(Seq(("a",1),("b",2),("c",3),("a",2),("b",4))) println(unit.count()) println(unit.countByKey()) &#125; 5 take first 和takeSampletake 返回前 N 个元素 first 返回第一个元素 takeSample(withReplacement, fract) 类似于 sample, 区别在这是一个Action, 直接返回结果 1234take 和takeSample 一个直接获取 一个采样获取first : 会在所有分区获取数据,相对来说速度比较慢,但first只是获取第一个元素,所以first只会处理第一分区的数据所以速度快,无序处理所有数据 1234567@Test def takeSam(): Unit =&#123; val unit = context.parallelize(Seq(1,2,3,4,5,6)) unit.takeSample(false,3).foreach(println(_)) println(unit.first()) unit.take(3).foreach(println(_)) &#125; 总结 RDD 的算子大部分都会生成一些专用的 RDD map, flatMap, filter 等算子会生成 MapPartitionsRDD coalesce, repartition 等算子会生成 CoalescedRDD 常见的 RDD 有两种类型 转换型的 RDD, Transformation 动作型的 RDD, Action 常见的 Transformation 类型的 RDD map flatMap filter groupBy reduceByKey 常见的 Action 类型的 RDD collect countByKey reduce 3 rdd对不同类型数据的支持RDD 对 Key-Value 类型的数据是有专门支持的,,对数字类型也有专门支持 一般要处理的类型有三种 字符串 键值对 数字型 RDD 的算子设计对这三类不同的数据分别都有支持 对于以字符串为代表的基本数据类型是比较基础的一些的操作, 诸如 map, flatMap, filter 等基础的算子 对于键值对类型的数据, 有额外的支持, 诸如 reduceByKey, groupByKey 等 byKey 的算子 同样对于数字型的数据也有额外的支持, 诸如 max, min 等 RDD 对键值对数据的额外支持 键值型数据本质上就是一个二元元组, 键值对类型的 RDD 表示为 RDD[(K, V)] RDD 对键值对的额外支持是通过隐式支持来完成的, 一个 RDD[(K, V)], 可以被隐式转换为一个 PairRDDFunctions 对象, 从而调用其中的方法. 源码: 既然对键值对的支持是通过 PairRDDFunctions 提供的, 那么从 PairRDDFunctions 中就可以看到这些支持有什么 类别 算子 聚合操作 reduceByKey foldByKey combineByKey 分组操作 cogroup groupByKey 连接操作 join leftOuterJoin rightOuterJoin 排序操作 sortBy sortByKey Action countByKey take collect RDD 对数字型数据的额外支持 对于数字型数据的额外支持基本上都是 Action 操作, 而不是转换操作 算子 含义 count 个数 mean 均值 sum 求和 max 最大值 min 最小值 variance 方差 sampleVariance 从采样中计算方差 stdev 标准差 sampleStdev 采样的标准差 ###4 使用算子 来个小demo 需求; 以年月为基础,统计北京东四地区的PM值 1234567891011121314151617181920212223242526package com.nicai.demoimport org.apache.commons.lang3.StringUtilsimport org.apache.spark.rdd.RDDimport org.apache.spark.&#123;SparkConf, SparkContext&#125;import org.junit.Test//需求 以年月为基础,统计北京东四地区的PM值class PmCount &#123; private val conf: SparkConf = new SparkConf().setMaster("local[5]").setAppName("Bj-pm-count") private val context = new SparkContext(conf) @Test def pmCount(): Unit =&#123; //读取数据 一次一行 var data: RDD[String] = context.textFile("G:\\develop\\data\\BeijingPM20100101_20151231_noheader.csv") //清洗数据 //以年月为key ,pm 值为value data.map(item =&gt; (item.split(","))).map(items =&gt; ((items(1), items(2)), items(6))) .filter(it =&gt; StringUtils.isNoneEmpty(it._2) &amp;&amp; !"NA".equalsIgnoreCase(it._2)) .map(it =&gt; (it._1, it._2.toInt)) .reduceByKey((curr, agg) =&gt; curr + agg) .take(10) .foreach(println(_)) &#125;&#125;]]></content>
      <tags>
        <tag>BigData</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark入门]]></title>
    <url>%2F2017%2F08%2F15%2Fspark%E5%85%A5%E9%97%A8.html</url>
    <content type="text"><![CDATA[#一 Spark 概述 ##1 什么是Spark Apache Spark 是一个快速的, 多用途的集群计算系统, 相对于 Hadoop MapReduce 将中间结果保存在磁盘中, Spark 使用了内存保存中间结果, 能在数据尚未写入硬盘时在内存中进行运算. Spark 只是一个计算框架, 不像 Hadoop 一样包含了分布式文件系统和完备的调度系统, 如果要使用 Spark, 需要搭载其它的文件系统和更成熟的调度系统 Spark 产生之前, 已经有非常成熟的计算系统存在了, 例如 MapReduce, 这些计算系统提供了高层次的API, 把计算运行在集群中并提供容错能力, 从而实现分布式计算. 虽然这些框架提供了大量的对访问利用计算资源的抽象, 但是它们缺少了对利用分布式内存的抽象, 这些框架多个计算之间的数据复用就是将中间数据写到一个稳定的文件系统中(例如HDFS), 所以会产生数据的复制备份, 磁盘的I/O以及数据的序列化, 所以这些框架在遇到需要在多个计算之间复用中间结果的操作时会非常的不高效. 而这类操作是非常常见的, 例如迭代式计算, 交互式数据挖掘, 图计算等. 认识到这个问题后, 学术界的 AMPLab 提出了一个新的模型, 叫做 RDDs. RDDs 是一个可以容错且并行的数据结构, 它可以让用户显式的将中间结果数据集保存在内中, 并且通过控制数据集的分区来达到数据存放处理最优化. 同时 RDDs 也提供了丰富的 API 来操作数据集. 后来 RDDs 被 AMPLab 在一个叫做 Spark 的框架中提供并开源. mr 的问题 1 计算过程比较缓慢,不适应与交互式计算,和迭代计算 2 不是所有的计算都由Map和Reduce两个阶段组成 spark解决问题 1 第一个问题 解决 中间结果存在内存中 2 提供了更好的API 函数式 ##2 spark特点 速度快 Spark 的在内存时的运行速度是 Hadoop MapReduce 的100倍基于硬盘的运算速度大概是 Hadoop MapReduce 的10倍Spark 实现了一种叫做 RDDs 的 DAG 执行引擎, 其数据缓存在内存中可以进行迭代处理 易用 1234df = spark.read.json("logs.json")df.where("age &gt; 21") \ .select("name.first") \ .show() Spark 支持 Java, Scala, Python, R, SQL 等多种语言的API. Spark 支持超过80个高级运算符使得用户非常轻易的构建并行计算程序 Spark 可以使用基于 Scala, Python, R, SQL的 Shell 交互式查询. 通用 Spark 提供一个完整的技术栈, 包括 SQL执行, Dataset命令式API, 机器学习库MLlib, 图计算框架GraphX, 流计算SparkStreaming用户可以在同一个应用中同时使用这些工具, 这一点是划时代的 兼容 Spark 可以运行在 Hadoop Yarn, Apache Mesos, Kubernets, Spark Standalone等集群中Spark 可以访问 HBase, HDFS, Hive, Cassandra 在内的多种数据库 总结 支持 Java, Scala, Python 和 R 的 API 可扩展至超过 8K 个节点 能够在内存中缓存数据集, 以实现交互式数据分析 提供命令行窗口, 减少探索式的数据分析的反应时间 3 spark组件Spark 最核心的功能是 RDDs, RDDs 存在于 spark-core 这个包内, 这个包也是 Spark 最核心的包. 同时 Spark 在 spark-core 的上层提供了很多工具, 以便于适应不用类型的计算. Spark-Core 和 弹性分布式数据集(RDDs) Spark-Core 是整个 Spark 的基础, 提供了分布式任务调度和基本的 I/O 功能Spark 的基础的程序抽象是弹性分布式数据集(RDDs), 是一个可以并行操作, 有容错的数据集合RDDs 可以通过引用外部存储系统的数据集创建(如HDFS, HBase), 或者通过现有的 RDDs 转换得到RDDs 抽象提供了 Java, Scala, Python 等语言的APIRDDs 简化了编程复杂性, 操作 RDDs 类似通过 Scala 或者 Java8 的 Streaming 操作本地数据集合 Spark SQL Spark SQL 在 spark-core 基础之上带出了一个名为 DataSet 和 DataFrame 的数据抽象化的概念Spark SQL 提供了在 Dataset 和 DataFrame 之上执行 SQL 的能力Spark SQL 提供了 DSL, 可以通过 Scala, Java, Python 等语言操作 DataSet 和 DataFrame它还支持使用 JDBC/ODBC 服务器操作 SQL 语言 Spark Streaming Spark Streaming 充分利用 spark-core 的快速调度能力来运行流分析它截取小批量的数据并可以对之运行 RDD Transformation它提供了在同一个程序中同时使用流分析和批量分析的能力 MLlib MLlib 是 Spark 上分布式机器学习的框架. Spark分布式内存的架构 比 Hadoop磁盘式 的 Apache Mahout 快上 10 倍, 扩展性也非常优良MLlib 可以使用许多常见的机器学习和统计算法, 简化大规模机器学习汇总统计, 相关性, 分层抽样, 假设检定, 随即数据生成支持向量机, 回归, 线性回归, 逻辑回归, 决策树, 朴素贝叶斯协同过滤, ALSK-meansSVD奇异值分解, PCA主成分分析TF-IDF, Word2Vec, StandardScalerSGD随机梯度下降, L-BFGS GraphX GraphX 是分布式图计算框架, 提供了一组可以表达图计算的 API, GraphX 还对这种抽象化提供了优化运行 总结 Spark 提供了 批处理(RDDs), 结构化查询(DataFrame), 流计算(SparkStreaming), 机器学习(MLlib), 图计算(GraphX) 等组件 这些组件均是依托于通用的计算引擎 RDDs 而构建出的, 所以 spark-core 的 RDDs 是整个 Spark 的基础 Spark和Hadoop的异同 Hadoop Spark 类型 基础平台, 包含计算, 存储, 调度 分布式计算工具 场景 大规模数据集上的批处理 迭代计算, 交互式计算, 流计算 延迟 大 小 易用性 API 较为底层, 算法适应性差 API 较为顶层, 方便使用 价格 对机器要求低, 便宜 对内存有要求, 相对较贵 二 集群搭建1 spark集群结构Spark 自身是没有集群管理工具的, 但是如果想要管理数以千计台机器的集群, 没有一个集群管理工具还不太现实, 所以 Spark 可以借助外部的集群工具来进行管理 整个流程就是使用 Spark 的 Client 提交任务, 找到集群管理工具申请资源, 后将计算任务分发到集群中运行 名词解释 Driver 该进程调用 Spark 程序的 main 方法, 并且启动 SparkContext Cluster Manager 该进程负责和外部集群工具打交道, 申请或释放集群资源 Worker 该进程是一个守护进程, 负责启动和管理 Executor Executor 该进程是一个JVM虚拟机, 负责运行 Spark Task 运行一个 Spark 程序大致经历如下几个步骤 启动 Drive, 创建 SparkContext Client 提交程序给 Drive, Drive 向 Cluster Manager 申请集群资源 资源申请完毕, 在 Worker 中启动 Executor Driver 将程序转化为 Tasks, 分发给 Executor 执行 Spark 程序可以运行在什么地方? 集群: 一组协同工作的计算机, 通常表现的好像是一台计算机一样, 所运行的任务由软件来控制和调度 集群管理工具: 调度任务到集群的软件 常见的集群管理工具: Hadoop Yarn, Apache Mesos, Kubernetes Spark 可以将任务运行在两种模式下: 单机, 使用线程模拟并行来运行程序 集群, 使用集群管理器来和不同类型的集群交互, 将任务运行在集群中 Spark 可以使用的集群管理工具有: Spark Standalone Hadoop Yarn Apache Mesos Kubernetes Driver 和 Worker 什么时候被启动? Standalone 集群中, 分为两个角色: Master 和 Slave, 而 Slave 就是 Worker, 所以在 Standalone 集群中, 启动之初就会创建固定数量的 Worker Driver 的启动分为两种模式: Client 和 Cluster. 在 Client 模式下, Driver 运行在 Client 端, 在 Client 启动的时候被启动. 在 Cluster 模式下, Driver 运行在某个 Worker 中, 随着应用的提交而启动 在 Yarn 集群模式下, 也依然分为 Client 模式和 Cluster 模式, 较新的版本中已经逐渐在废弃 Client 模式了, 所以上图所示为 Cluster 模式 如果要在 Yarn 中运行 Spark 程序, 首先会和 RM 交互, 开启 ApplicationMaster, 其中运行了 Driver, Driver创建基础环境后, 会由 RM 提供对应的容器, 运行 Executor, Executor会反向向 Driver 反向注册自己, 并申请 Tasks 执行 总结 Master 负责总控, 调度, 管理和协调 Worker, 保留资源状况等 Slave 对应 Worker 节点, 用于启动 Executor 执行 Tasks, 定期向 Master汇报 Driver 运行在 Client 或者 Slave(Worker) 中, 默认运行在 Slave(Worker) 中 2 集群搭建集群规划 Node01 Node02 Node03 Master Slave Slave History Server 2.1 下载解压12网址https://archive.apache.org/dist/spark/spark-2.2.0/spark-2.2.0-bin-hadoop2.7.tgz 12上传解压tar -xzvf spark-2.2.0-bin-hadoop2.7.tgz -C /path 解压后改名为spark 修改配置文件spark-env.sh, 以指定运行参数 进入配置目录, 并复制一份新的配置文件, 以供在此基础之上进行修改 123cd /export/servers/spark/confcp spark-env.sh.template spark-env.shvi spark-env.sh 将以下内容复制进配置文件末尾 123456# 指定 Java Homeexport JAVA_HOME=/export/servers/jdk1.8.0_141# 指定 Spark Master 地址export SPARK_MASTER_HOST=node01export SPARK_MASTER_PORT=7077 2.2 配置 修改配置文件 slaves, 以指定从节点为止, 从在使用 sbin/start-all.sh 启动集群的时候, 可以一键启动整个集群所有的 Worker 进入配置目录, 并复制一份新的配置文件, 以供在此基础之上进行修改 123cd /export/servers/spark/confcp slaves.template slavesvi slaves 配置所有从节点的地址 12node02node03 配置 HistoryServer 默认情况下, Spark 程序运行完毕后, 就无法再查看运行记录的 Web UI 了, 通过 HistoryServer 可以提供一个服务, 通过读取日志文件, 使得我们可以在程序运行结束后, 依然能够查看运行过程 复制 spark-defaults.conf, 以供修改 123cd /export/servers/spark/confcp spark-defaults.conf.template spark-defaults.confvi spark-defaults.conf 将以下内容复制到spark-defaults.conf末尾处, 通过这段配置, 可以指定 Spark 将日志输入到 HDFS 中 123spark.eventLog.enabled truespark.eventLog.dir hdfs://node01:8020/spark_logspark.eventLog.compress true 将以下内容复制到spark-env.sh的末尾, 配置 HistoryServer 启动参数, 使得 HistoryServer 在启动的时候读取 HDFS 中写入的 Spark 日志 12# 指定 Spark History 运行参数export SPARK_HISTORY_OPTS="-Dspark.history.ui.port=4000 -Dspark.history.retainedApplications=3 -Dspark.history.fs.logDirectory=hdfs://node01:8020/spark_log" 为 Spark 创建 HDFS 中的日志目录 1hdfs dfs -mkdir -p /spark_log 2.3 分发运行 将 Spark 安装包分发给集群中其它机器 123cd /export/serversscp -r spark/ node03:$PWDscp -r spark/ node02:$PWD 启动 Spark Master 和 Slaves, 以及 HistoryServer 123cd /export/servers/sparksbin/start-all.shsbin/start-history-server.sh 3 spark集群高可用搭建了解如何使用 Zookeeper 帮助 Spark Standalone 高可用 对于 Spark Standalone 集群来说, 当 Worker 调度出现问题的时候, 会自动的弹性容错, 将出错的 Task 调度到其它 Worker 执行 但是对于 Master 来说, 是会出现单点失败的, 为了避免可能出现的单点失败问题, Spark 提供了两种方式满足高可用 使用 Zookeeper 实现 Masters 的主备切换 使用文件系统做主备切换 使用文件系统做主备切换的场景实在太小, 所以此处不做探讨 3.1 停止spark集群12cd /export/servers/sparksbin/stop-all.sh 3.2 修改配置文件, 增加 Spark 运行时参数, 从而指定 Zookeeper 的位置 进入 spark-env.sh 所在目录, 打开 vi 编辑 12cd /export/servers/spark/confvi spark-env.sh 编辑 spark-env.sh, 添加 Spark 启动参数, 并去掉 SPARK_MASTER_HOST 地址 123456789101112# 指定 Java Homeexport JAVA_HOME=/export/servers/jdk1.8.0_141# 指定 Spark Master 地址# export SPARK_MASTER_HOST=node01 //这个注释掉 修改1export SPARK_MASTER_PORT=7077# 指定 Spark History 运行参数export SPARK_HISTORY_OPTS=&quot;-Dspark.history.ui.port=4000 -Dspark.history.retainedApplications=3 -Dspark.history.fs.logDirectory=hdfs://node01:8020/spark_log&quot;# 指定 Spark 运行时参数 //添加 修改2export SPARK_DAEMON_JAVA_OPTS=&quot;-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=node01:2181,node02:2181,node03:2181 -Dspark.deploy.zookeeper.dir=/spark&quot; 3.3 分发配置文件到整个集群123cd /export/servers/spark/confscp spark-env.sh node02:$PWDscp spark-env.sh node03:$PWD 3.4 启动 在 node01 上启动整个集群 123cd /export/servers/sparksbin/start-all.shsbin/start-history-server.sh 在 node02 上单独再启动一个 Master 12cd /export/servers/sparksbin/start-master.sh 3.5 查看 node01 master 和 node02 master 的 WebUI你会发现一个是 ALIVE(主), 另外一个是 STANDBY(备) 如果关闭一个, 则另外一个成为ALIVE, 但是这个过程可能要持续两分钟左右, 需要耐心等待 123# 在 Node01 中执行如下指令cd /export/servers/spark/sbin/stop-master.sh Spark HA 选举 Spark HA 的 Leader 选举使用了一个叫做 Curator 的 Zookeeper 客户端来进行 Zookeeper 是一个分布式强一致性的协调服务, Zookeeper 最基本的一个保证是: 如果多个节点同时创建一个 ZNode, 只有一个能够成功创建. 这个做法的本质使用的是 Zookeeper 的 ZAB 协议, 能够在分布式环境下达成一致. spark各服务端口 Service port Master WebUI node01:8080 Worker WebUI node01:8081 History Server node01:4000 4 第一个应用程序流程: Step 1 进入 Spark 安装目录中 1cd /export/servers/spark/ Step 2 运行 Spark 示例任务 1234567bin/spark-submit \--class org.apache.spark.examples.SparkPi \ //提交spark程序--master spark://node01:7077,node02:7077,node03:7077 \ //master地址--executor-memory 1G \ //这两行为参数--total-executor-cores 2 \/export/servers/spark/examples/jars/spark-examples_2.11-2.2.0.jar \ //指定jar包位置100 //会提交到spark程序中 Step 3 运行结果 1Pi is roughly 3.1424627142462715 刚才所运行的程序是 Spark 的一个示例程序, 使用 Spark 编写了一个以蒙特卡洛算法来计算圆周率的任务 蒙特卡洛算法概述 通过迭代循环投点的方式实现蒙特卡洛算法求圆周率 计算过程 1不断的生成随机的点, 根据点距离圆心是否超过半径来判断是否落入园内 2通过 来计算圆周率 3不断的迭代 三 spark入门Spark 官方提供了两种方式编写代码, 都比较重要, 分别如下 spark-shellSpark shell 是 Spark 提供的一个基于 Scala 语言的交互式解释器, 类似于 Scala 提供的交互式解释器, Spark shell 也可以直接在 Shell 中编写代码执行这种方式也比较重要, 因为一般的数据分析任务可能需要探索着进行, 不是一蹴而就的, 使用 Spark shell 先进行探索, 当代码稳定以后, 使用独立应用的方式来提交任务, 这样是一个比较常见的流程 spark-submitSpark submit 是一个命令, 用于提交 Scala 编写的基于 Spark 框架, 这种提交方式常用作于在集群中运行任务 上面的计算pi就是一个spark-submit 1 Spark shell 的方式编写 WordCount (本地文件)Spark shell 简介 启动 Spark shell进入 Spark 安装目录后执行 spark-shell --master master 就可以提交Spark 任务 Spark shell 的原理是把每一行 Scala 代码编译成类, 最终交由 Spark 执行 Master地址的设置 地址 解释 local[N] 使用 N 条 Worker 线程在本地运行 spark://host:port 在 Spark standalone 中运行, 指定 Spark 集群的 Master 地址, 端口默认为 7077 mesos://host:port 在 Apache Mesos 中运行, 指定 Mesos 的地址 yarn 在 Yarn 中运行, Yarn 的地址由环境变量 HADOOP_CONF_DIR 来指定 接下来使用 Spark shell 的方式编写一个 WordCount 1.1 准备文件在 Node01 中创建文件 /export/data/wordcount.txt 1234word,hello,world,hadoophive,sqoop,flume,hellokitty,tom,jerry,worldhadoop 2.2 启动spark-shell 本地线程模式12cd /export/servers/sparkbin/spark-shell --master local[2] 2.3 执行代码12345678910//获取文件val rdd1=sc.textFile("file:///export/test/wordcount.txt")//获取单个单词val rdd2=rdd1.flatMap(_.split(",")) 相当于 (item =&gt; item.split(","))//给每个单词 符频次 组成元组val rdd3=rdd2.map((_,1)) 相当于 (item =&gt;(item,1))//聚合得到最终结果val rdd4=rdd3.reduceByKey(_ + _) 相当于 ((curr,age) =&gt; curr + age) 这一步并未求值//打印解结果 collect为收集数据val res=rdd4.collect 在这一步才求值 上述代码中 sc 变量指的是 SparkContext, 是 Spark 程序的上下文和入口 正常情况下我们需要自己创建, 但是如果使用 Spark shell 的话, Spark shell 会帮助我们创建, 并且以变量 sc 的形式提供给我们调用 运行流程 flatMap(_.split(&quot; &quot;)) 将数据转为数组的形式, 并展平为多个数据 map_, 1 将数据转换为元组的形式 reduceByKey(_ + _) 计算每个 Key 出现的次数 总结 使用 Spark shell 可以快速验证想法 Spark 框架下的代码非常类似 Scala 的函数式调用 2 读取hdfs上的文件2.1 上传文件呢到hdfs1hdfs dfs -put wordcount.txt /data 2.2 在spark-shell中访问hdfs12345678910//获取文件val rdd1=sc.textFile("hdfs://node01:8020/data") //或者("/data") spark默认的文件地址就是hdfs//获取单个单词val rdd2=rdd1.flatMap(_.split(",")) 相当于 (item =&gt; item.split(","))//给每个单词 符频次 组成元组val rdd3=rdd2.map((_,1)) 相当于 (item =&gt;(item,1))//聚合得到最终结果val rdd4=rdd3.reduceByKey(_ + _) 相当于 ((curr,age) =&gt; curr + age) 这一步并未求值//打印解结果 collect为收集数据val res=rdd4.collect 在这一步才求值 也可以通过向 Spark 配置 Hadoop 的路径, 来通过路径直接访问 1.在 spark-env.sh 中添加 Hadoop 的配置路径 export HADOOP_CONF_DIR=&quot;/etc/hadoop/conf&quot; 2.在配置过后, 可以直接使用 hdfs:///路径 的形式直接访问 在配置过后, 也可以直接使用路径访问 (“/data”) 3 编写独立应用提交spark任务创建maven工程 3.1 添加依赖123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121&lt;properties&gt; &lt;scala.version&gt;2.11.8&lt;/scala.version&gt; &lt;spark.version&gt;2.2.0&lt;/spark.version&gt; &lt;slf4j.version&gt;1.7.16&lt;/slf4j.version&gt; &lt;log4j.version&gt;1.2.17&lt;/log4j.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-library&lt;/artifactId&gt; &lt;version&gt;$&#123;scala.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;2.6.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;jcl-over-slf4j&lt;/artifactId&gt; &lt;version&gt;$&#123;slf4j.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;version&gt;$&#123;slf4j.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;version&gt;$&#123;slf4j.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;$&#123;log4j.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.10&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;!--scala文件夹需要自己创建--&gt; &lt;sourceDirectory&gt;src/main/scala&lt;/sourceDirectory&gt; &lt;testSourceDirectory&gt;src/test/scala&lt;/testSourceDirectory&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.0&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;net.alchim31.maven&lt;/groupId&gt; &lt;artifactId&gt;scala-maven-plugin&lt;/artifactId&gt; &lt;version&gt;3.2.0&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;compile&lt;/goal&gt; &lt;goal&gt;testCompile&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;args&gt; &lt;arg&gt;-dependencyfile&lt;/arg&gt; &lt;arg&gt;$&#123;project.build.directory&#125;/.scala_dependencies&lt;/arg&gt; &lt;/args&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;!--打包插件 因为默认的打包不包含maven的依赖--&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt; &lt;version&gt;3.1.1&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;shade&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;filters&gt; &lt;filter&gt; &lt;artifact&gt;*:*&lt;/artifact&gt; &lt;excludes&gt; &lt;exclude&gt;META-INF/*.SF&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.DSA&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.RSA&lt;/exclude&gt; &lt;/excludes&gt; &lt;/filter&gt; &lt;/filters&gt; &lt;transformers&gt; &lt;transformer implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer"&gt; &lt;mainClass&gt;&lt;/mainClass&gt; &lt;/transformer&gt; &lt;/transformers&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; 编写代码 1234567891011121314151617181920package com.nicai.sparkwordcountimport org.apache.spark.&#123;SparkConf, SparkContext&#125;object WordCount &#123; def main(args: Array[String]): Unit = &#123; //获取sparkContext上下文对象 val conf: SparkConf =new SparkConf().setMaster("local[2]").setAppName("word_count") val sc: SparkContext = new SparkContext(conf) //读取文件并计算频次 //加载文件 val rdd1 = sc.textFile("day24Spark/data/wc.txt") val rdd2 = rdd1.flatMap(item =&gt; item.split(",")) val rdd3 = rdd2.map(item =&gt; (item,1)) val rdd4 = rdd3.reduceByKey((curr,age) =&gt; curr + age) val result = rdd4.collect() result.foreach(println(_)) &#125;&#125; 3.2 运行####1以上代码可以直接在本地运行 若是本地运行有错: 12345Error:scalac: error while loading &lt;root&gt;, Error accessing G:\apache-maven-3.3.9\apache-maven-3.3.9\repository_pinyougou\javax\ws\rs\javax.ws.rs-api\2.0.1\javax.ws.rs-api-2.0.1.jar或者Error:scalac: error while loading &lt;root&gt;, Error accessing G:\apache-maven-3.3.9\apache-maven-3.3.9\repository_pinyougou\org\spache\commons\commons-crypto\1.0.0\commons-crypto-1.0.0.jar则这两个jar包又问题 请自己下载 2 打包在集群运行spark-submit修改代码 12val conf: SparkConf =new SparkConf().setAppName("word_count")val rdd1 = sc.textFile("hdfs://node01:8020/data/wc.txt") //改为hdfs路径 使用maven打包 spark-submit 命令 12345spark-submit [options] &lt;app jar&gt; &lt;app options&gt;app jar 程序 Jar 包app options 程序 Main 方法传入的参数options 提交应用的参数, 可以有如下选项 options 参数 参数 解释 --master &lt;url&gt; 同 Spark shell 的 Master, 可以是spark, yarn, mesos, kubernetes等 URL --deploy-mode &lt;client or cluster&gt; Driver 运行位置, 可选 Client 和 Cluster, 分别对应运行在本地和集群(Worker)中 --class &lt;class full name&gt; Jar 中的 Class, 程序入口 --jars &lt;dependencies path&gt; 依赖 Jar 包的位置 --driver-memory &lt;memory size&gt; Driver 程序运行所需要的内存, 默认 512M --executor-memory &lt;memory size&gt; Executor 的内存大小, 默认 1G 打包后有两个jar包 一个带有依赖 (大) 一个(小) 因为集群中的spark自带有Hadoop和spark的jar包 所以不需要上传大的,只需上传小的即可 在上传的jar包所在目录执行: 123spark-submit --master spark://node01:7077 \--class cn.itcast.spark.WordCounts \original-spark-0.1.0.jar 若想在任意目录执行 shell-submit 配置一下: 123456vim /etc/profileexport SPARK_BIN=/export/servers/spark/binexport PATH=$PATH:$SPARK_BIN//生效source /etc/profile 4 三种方式总结Spark shell 作用 一般用作于探索阶段, 通过 Spark shell 快速的探索数据规律 当探索阶段结束后, 代码确定以后, 通过独立应用的形式上线运行 功能 Spark shell 可以选择在集群模式下运行, 还是在线程模式下运行 Spark shell 是一个交互式的运行环境, 已经内置好了 SparkContext 和 SparkSession 对象, 可以直接使用 Spark shell 一般运行在集群中安装有 Spark client 的服务器中, 所以可以自有的访问 HDFS 本地运行 作用 在编写独立应用的时候, 每次都要提交到集群中还是不方便, 另外很多时候需要调试程序, 所以在 IDEA 中直接运行会比较方便, 无需打包上传了 功能 因为本地运行一般是在开发者的机器中运行, 而不是集群中, 所以很难直接使用 HDFS 等集群服务, 需要做一些本地配置, 用的比较少 需要手动创建 SparkContext 集群运行 作用 正式环境下比较多见, 独立应用编写好以后, 打包上传到集群中, 使用spark-submit来运行, 可以完整的使用集群资源 功能 同时在集群中通过spark-submit来运行程序也可以选择是用线程模式还是集群模式 集群中运行是全功能的, HDFS 的访问, Hive 的访问都比较方便 需要手动创建 SparkContext 四 RDD简介在idea中的wc代码中 使用 sc.textFile() 方法读取 HDFS 中的文件, 并生成一个 RDD 使用 flatMap 算子将读取到的每一行字符串打散成单词, 并把每个单词变成新的行 使用 map 算子将每个单词转换成 (word, 1) 这种元组形式 使用 reduceByKey 统计单词对应的频率 其中所使用到的算子有如下几个: flatMap 是一对多 map 是一对一 reduceByKey 是按照 Key 聚合, 类似 MapReduce 中的 Shuffled 1 rdd定义 RDD, 全称为 Resilient Distributed Datasets, 是一个容错的, 并行的数据结构, 可以让用户显式地将数据存储到磁盘和内存中, 并能控制数据的分区. 同时, RDD 还提供了一组丰富的操作来操作这些数据. 在这些操作中, 诸如 map, flatMap, filter 等转换操作实现了 Monad 模式, 很好地契合了 Scala 的集合操作. 除此之外, RDD 还提供了诸如 join, groupBy, reduceByKey 等更为方便的操作, 以支持常见的数据运算. 通常来讲, 针对数据处理有几种常见模型, 包括: Iterative Algorithms, Relational Queries, MapReduce, Stream Processing. 例如 Hadoop MapReduce 采用了 MapReduce 模型, Storm 则采用了 Stream Processing 模型. RDD 混合了这四种模型, 使得 Spark 可以应用于各种大数据处理场景. RDD 作为数据结构, 本质上是一个只读的分区记录集合. 一个 RDD 可以包含多个分区, 每个分区就是一个 DataSet 片段. RDD 之间可以相互依赖, 如果 RDD 的每个分区最多只能被一个子 RDD 的一个分区使用，则称之为窄依赖, 若被多个子 RDD 的分区依赖，则称之为宽依赖. 不同的操作依据其特性, 可能会产生不同的依赖. 例如 map 操作会产生窄依赖, 而 join 操作则产生宽依赖. 特点 RDD 是一个编程模型 RDD 允许用户显式的指定数据存放在内存或者磁盘 RDD 是分布式的, 用户可以控制 RDD 的分区 RDD 是一个编程模型 RDD 提供了丰富的操作 RDD 提供了 map, flatMap, filter 等操作符, 用以实现 Monad 模式 RDD 提供了 reduceByKey, groupByKey 等操作符, 用以操作 Key-Value 型数据 RDD 提供了 max, min, mean 等操作符, 用以操作数字型的数据 RDD 是混合型的编程模型, 可以支持迭代计算, 关系查询, MapReduce, 流计算 RDD 是只读的 RDD 之间有依赖关系, 根据执行操作的操作符的不同, 依赖关系可以分为宽依赖和窄依赖 2 rdd 分区 整个 WordCount 案例的程序从结构上可以用上图表示, 分为两个大部分 存储 文件如果存放在 HDFS 上, 是分块的, 类似上图所示, 这个 wordcount.txt 分了三块 计算 Spark 不止可以读取 HDFS, Spark 还可以读取很多其它的数据集, Spark 可以从数据集中创建出 RDD 例如上图中, 使用了一个 RDD 表示 HDFS 上的某一个文件, 这个文件在 HDFS 中是分三块, 那么 RDD 在读取的时候就也有三个分区, 每个 RDD 的分区对应了一个 HDFS 的分块 后续 RDD 在计算的时候, 可以更改分区, 也可以保持三个分区, 每个分区之间有依赖关系, 例如说 RDD2 的分区一依赖了 RDD1 的分区一 RDD 之所以要设计为有分区的, 是因为要进行分布式计算, 每个不同的分区可以在不同的线程, 或者进程, 甚至节点中, 从而做到并行计算 总结 RDD 是弹性分布式数据集 RDD 一个非常重要的前提和基础是 RDD 运行在分布式环境下, 其可以分区 3 创建rdd程序入口 SparkContext 12val conf = new SparkConf().setMaster("local[2]")val sc: SparkContext = new SparkContext(conf) SparkContext 是 spark-core 的入口组件, 是一个 Spark 程序的入口, 在 Spark 0.x 版本就已经存在 SparkContext 了, 是一个元老级的 API 如果把一个 Spark 程序分为前后端, 那么服务端就是可以运行 Spark 程序的集群, 而 Driver 就是 Spark 的前端, 在 Driver中 SparkContext 是最主要的组件, 也是 Driver 在运行时首先会创建的组件, 是 Driver 的核心 SparkContext 从提供的 API 来看, 主要作用是连接集群, 创建 RDD, 累加器, 广播变量等 简略的说, RDD 有三种创建方式 RDD 可以通过本地集合直接创建 RDD 也可以通过读取外部数据集来创建 RDD 也可以通过其它的 RDD 衍生而来 ###1 通过本地集合直接创建 123456val conf = new SparkConf().setMaster("local[2]")val sc = new SparkContext(conf)val list = List(1, 2, 3, 4, 5, 6)val rddParallelize = sc.parallelize(list, 2)val rddMake = sc.makeRDD(list, 2) 通过 parallelize 和 makeRDD 这两个 API 可以通过本地集合创建 RDD 这两个 API 本质上是一样的, 在 makeRDD 这个方法的内部, 最终也是调用了 parallelize 因为不是从外部直接读取数据集的, 所以没有外部的分区可以借鉴, 于是在这两个方法都都有两个参数, 第一个参数是本地集合, 第二个参数是分区数 2 通过读取外部文件创建 RDD1234val conf = new SparkConf().setMaster("local[2]")val sc = new SparkContext(conf)val source: RDD[String] = sc.textFile("hdfs://node01:8020/dataset/wordcount.txt") 访问方式 支持访问文件夹, 例如 sc.textFile(&quot;hdfs:///dataset&quot;) 支持访问压缩文件, 例如 sc.textFile(&quot;hdfs:///dataset/words.gz&quot;) 支持通过通配符访问, 例如 sc.textFile(&quot;hdfs:///dataset/*.txt&quot;) 如果把 Spark 应用跑在集群上, 则 Worker 有可能在任何一个节点运行所以如果使用 file:///…; 形式访问本地文件的话, 要确保所有的 Worker 中对应路径上有这个文件, 否则可能会报错无法找到文件 分区 默认情况下读取 HDFS 中文件的时候, 每个 HDFS 的 block 对应一个 RDD 的 partition, block 的默认是128M 通过第二个参数, 可以指定分区数量, 例如 sc.textFile(&quot;hdfs://node01:8020/dataset/wordcount.txt&quot;, 20) 如果通过第二个参数指定了分区, 这个分区数量一定不能小于block数 通常每个 CPU core 对应 2 - 4 个分区是合理的值 支持的平台 支持 Hadoop 的几乎所有数据格式, 支持 HDFS 的访问 通过第三方的支持, 可以访问AWS和阿里云中的文件, 详情查看对应平台的 API 3 通过其它的 RDD 衍生新的 RDD12345val conf = new SparkConf().setMaster("local[2]")val sc = new SparkContext(conf)val source: RDD[String] = sc.textFile("hdfs://node01:8020/dataset/wordcount.txt", 20)val words = source.flatMap &#123; line =&gt; line.split(" ") &#125; source 是通过读取 HDFS 中的文件所创建的 words 是通过 source 调用算子 map 生成的新 RDD 总结 RDD 的可以通过三种方式创建, 通过本地集合创建, 通过外部数据集创建, 通过其它的 RDD 衍生 4 rdd算子1 Map 算子123sc.parallelize(Seq(1, 2, 3)) .map( num =&gt; num * 10 ) .collect() 作用 把 RDD 中的数据 一对一 的转为另一种形式 调用 1def map[U: ClassTag](f: T ⇒ U): RDD[U] 参数 f → Map 算子是 原RDD → 新RDD 的过程, 这个函数的参数是原 RDD 数据, 返回值是经过函数转换的新 RDD 的数据 注意点 Map 是一对一, 如果函数是 String → Array[String] 则新的 RDD 中每条数据就是一个数组 2 FlatMap123sc.parallelize(Seq("Hello lily", "Hello lucy", "Hello tim")) .flatMap( line =&gt; line.split(" ") ) .collect() 作用 FlatMap 算子和 Map 算子类似, 但是 FlatMap 是一对多 调用 1def flatMap[U: ClassTag](f: T ⇒ List[U]): RDD[U] 参数 f → 参数是原 RDD 数据, 返回值是经过函数转换的新 RDD 的数据, 需要注意的是返回值是一个集合, 集合中的数据会被展平后再放入新的 RDD 注意点 flatMap 其实是两个操作, 是 map + flatten, 也就是先转换, 后把转换而来的 List 展开 3ReduceByKey123sc.parallelize(Seq(("a", 1), ("a", 1), ("b", 1))) .reduceByKey( (curr, agg) =&gt; curr + agg ) .collect() 作用 首先按照 Key 分组, 接下来把整组的 Value 计算出一个聚合值, 这个操作非常类似于 MapReduce 中的 Reduce 调用 1def reduceByKey(func: (V, V) ⇒ V): RDD[(K, V)] 参数 func → 执行数据处理的函数, 传入两个参数, 一个是当前值, 一个是局部汇总, 这个函数需要有一个输出, 输出就是这个 Key 的汇总结果 注意点 ReduceByKey 只能作用于 Key-Value 型数据, Key-Value 型数据在当前语境中特指 Tuple2 ReduceByKey 是一个需要 Shuffled 的操作 和其它的 Shuffled 相比, ReduceByKey是高效的, 因为类似 MapReduce 的, 在 Map 端有一个 Cominer, 这样 I/O 的数据便会减少 总结 map 和 flatMap 算子都是转换, 只是 flatMap 在转换过后会再执行展开, 所以 map 是一对一, flatMap 是一对多 reduceByKey 类似 MapReduce 中的 Reduce]]></content>
      <tags>
        <tag>BigData</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ElSearch]]></title>
    <url>%2F2017%2F08%2F14%2FElSearch.html</url>
    <content type="text"><![CDATA[#一 Lucene 全文检索: 索引&amp;检索 倒排索引 : 1我是中国人 2 中国是全球人口最多的国家 ,中国人也最多 例如这两句 用 倒排索引 1，我 （1:1）｛0｝ 第一个1 为第一行 第二个1 为个数 0 为行偏移量 2，中国 (1:1) {2},(2:2){0，15} #二 es 概述 全称 Elastic Search 基于Lucene ES只是封装了Lucene es 制定了一套规则 ,发送给我的数据只要满足一定的格式即可,而不管你的数据来源,只要满足这个规则es就可以对其进行操作. Elastic有一条完整的产品线及解决方案：Elasticsearch、Kibana、Logstash等，前面说的三个就是大家常说的ELK技术栈。 ##1特点 分布式，无需人工搭建集群（solr就需要人为配置，使用Zookeeper作为注册中心） Restful风格，一切API都遵循Rest原则，容易上手 近实时搜索，数据更新在Elasticsearch中几乎是完全同步的。 2原理与应用1.2.1索引结构​ 下图是ElasticSearch的索引结构，下边黑色部分是物理结构，上边黄色部分是逻辑结构，逻辑结构也是为了更好的去描述ElasticSearch的工作原理及去使用物理结构中的索引文件。 逻辑结构部分是一个倒排索引表： 1、将要搜索的文档内容分词，所有不重复的词组成分词列表。 2、将搜索的文档最终以Document方式存储起来。 3、每个词和docment都有关联。 如下： 现在，如果我们想搜索 quick brown ，我们只需要查找包含每个词条的文档： 两个文档都匹配，但是第一个文档比第二个匹配度更高。如果我们使用仅计算匹配词条数量的简单 相似性算法 ，那么，我们可以说，对于我们查询的相关性来讲，第一个文档比第二个文档更佳。 3 RESTful应用方法Elasticsearch提供 RESTful Api接口进行索引、搜索，并且支持多种客户端。 es在项目中的应用: 1) 用户在前端搜索关键字 2）项目前端通过http方式请求项目服务端 3）项目服务端通过Http RESTful方式请求ES集群进行搜索 4）ES集群从索引库检索数据。 三 安装与部署 集群1 新建用户出于安全考虑，elasticsearch默认不允许以root账号运行。 123useradd espasswd 123456切换用户 su - es 2 上传压塑包并解压1tar -zxvf elasticsearch-6.2.4.tar.gz 3 修改配置文件要修改两个配置文件在 config目录下 1cd config elasticsearch.yml和jvm.options 3.1 修改jvm.optionsElasticsearch基于Lucene的，而Lucene底层是java实现，因此我们需要配置jvm参数。 编辑jvm.options： 123456789vim jvm.options默认配置如下： -Xms1g -Xmx1g内存占用太多了，我们调小一些：-Xms512m-Xmx512m 3.2 修改elasticsearch.yml1vim elasticsearch.yml 修改数据和日志目录： 12path.data: /export/server/elasticsearch/data # 数据目录位置path.logs: /export/server/elasticsearch/logs # 日志目录位置 我们把data和logs目录修改指向了elasticsearch的安装目录。但是这两个目录并不存在，因此我们需要创建出来。 进入elasticsearch的根目录，然后创建： 12mkdir datamkdir logs 修改绑定的ip： 1network.host: 0.0.0.0 # 绑定到0.0.0.0，允许任何ip来访问 默认只允许本机访问，修改为0.0.0.0后则可以远程访问 这个是做的单机安装，如果要做集群，只需要在这个配置文件中添加其它节点信息即可。 elasticsearch.ym文件中的其他配置 属性名 说明 cluster.name 配置elasticsearch的集群名称，默认是elasticsearch。建议修改成一个有意义的名称。 node.name 节点名，es会默认随机指定一个名字，建议指定一个有意义的名称，方便管理 path.conf 设置配置文件的存储路径，tar或zip包安装默认在es根目录下的config文件夹，rpm安装默认在/etc/ elasticsearch path.data 设置索引数据的存储路径，默认是es根目录下的data文件夹，可以设置多个存储路径，用逗号隔开 path.logs 设置日志文件的存储路径，默认是es根目录下的logs文件夹 path.plugins 设置插件的存放路径，默认是es根目录下的plugins文件夹 bootstrap.memory_lock 设置为true可以锁住ES使用的内存，避免内存进行swap network.host 设置bind_host和publish_host，设置为0.0.0.0允许外网访问 http.port 设置对外服务的http端口，默认为9200。 transport.tcp.port 集群结点之间通信端口 discovery.zen.ping.timeout 设置ES自动发现节点连接超时的时间，默认为3秒，如果网络延迟高可设置大些 discovery.zen.minimum_master_nodes 主结点数量的最少值 ,此值的公式为：(master_eligible_nodes / 2) + 1 ，比如：有3个符合要求的主结点，那么这里要设置为2 cluster.name: ​ 配置elasticsearch的集群名称，默认是elasticsearch。建议修改成一个有意义的名称。 node.name: ​ 节点名，通常一台物理服务器就是一个节点，es会默认随机指定一个名字，建议指定一个有意义的名称，方便管理 ​ 一个或多个节点组成一个cluster集群，集群是一个逻辑的概念，节点是物理概念，后边章节会详细介绍。 path.conf: 设置配置文件的存储路径，tar或zip包安装默认在es根目录下的config文件夹，rpm安装默认在/etc/ elasticsearch path.data: 设置索引数据的存储路径，默认是es根目录下的data文件夹，可以设置多个存储路径，用逗号隔开。 path.logs: 设置日志文件的存储路径，默认是es根目录下的logs文件夹 path.plugins: 设置插件的存放路径，默认是es根目录下的plugins文件夹 bootstrap.memory_lock: true 设置为true可以锁住ES使用的内存，避免内存与swap分区交换数据。 network.host: 设置绑定主机的ip地址，设置为0.0.0.0表示绑定任何ip，允许外网访问，生产环境建议设置为具体的ip。 http.port: 9200 设置对外服务的http端口，默认为9200。 transport.tcp.port: 9300 集群结点之间通信端口 node.master: 指定该节点是否有资格被选举成为master结点，默认是true，如果原来的master宕机会重新选举新的master。 node.data: 指定该节点是否存储索引数据，默认为true。 discovery.zen.ping.unicast.hosts: [“host1:port”, “host2:port”, “…”] 设置集群中master节点的初始列表。 discovery.zen.ping.timeout: 3s 设置ES自动发现节点连接超时的时间，默认为3秒，如果网络延迟高可设置大些。 discovery.zen.minimum_master_nodes: ​ 主结点数量的最少值 ,此值的公式为：(master_eligible_nodes / 2) + 1 ，比如：有3个符合要求的主结点，那么这里要设置为2。 node.max_local_storage_nodes: ​ 单机允许的最大存储结点数，通常单机启动一个结点建议设置为1，开发环境如果单机启动多个节点可设置大于1.]]></content>
      <tags>
        <tag>BigData</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala高级]]></title>
    <url>%2F2017%2F08%2F13%2FScala%E9%AB%98%E7%BA%A7.html</url>
    <content type="text"><![CDATA[Scala高级一 高阶函数scala 混合了面向对象和函数式的特性，在函数式编程语言中，函数是“头等公民”，它和Int、String、Class等其他类型处于同等的地位，可以像其他类型的变量一样被传递和操作。 高阶函数包含 作为值的函数 匿名函数 闭包 柯里化等等 1作为值得函数在scala中，函数就像和数字、字符串一样，可以将函数传递给一个方法。我们可以对算法进行封装，然后将具体的动作传递给方法，这种特性很有用。 我们之前学习过List的map方法，它就可以接收一个函数，完成List的转换。 例子 示例说明 将一个整数列表中的每个元素转换为对应个数的小星星 1List(1, 2, 3...) =&gt; *, **, *** 步骤 创建一个函数，用于将数字转换为指定个数的小星星 创建一个列表，调用map方法 打印转换为的列表 参考代码 12345678910package com.nicai.highlevelobject Demo01 &#123; def main(args: Array[String]): Unit = &#123; val fun: Int =&gt; String = (num:Int) =&gt; "*" * num val strings = (1 to 10).map(fun) println(strings) &#125;&#125; 2 匿名函数定义 上面的代码，给(num:Int) =&gt; ““ num函数赋值给了一个变量，但是这种写法有一些啰嗦。在scala中，可以不需要给函数赋值给变量，没有赋值给变量的函数就是*匿名函数 123456val list = List(1, 2, 3, 4)// 字符串*方法，表示生成指定数量的字符串val func_num2star = (num:Int) =&gt; "*" * numprint(list.map(func_num2star)) 使用匿名函数优化上述代码 参考代码 1234567891011object Demo02 &#123; def main(args: Array[String]): Unit = &#123; val strings = (1 to 10).map(x =&gt; "*" * x) println(strings)// 因为此处num变量只使用了一次，而且只是进行简单的计算，所以可以省略参数列表，使用_替代参数 val strings2 = (1 to 10 ).map("*" * _) println(strings2) &#125;&#125; 3 柯里化在scala和spark的源代码中，大量使用到了柯里化。为了后续方便阅读源代码，我们需要来了解下柯里化。 定义: 柯里化（Currying）是指将原先接受多个参数的方法转换为多个只有一个参数的参数列表的过程。 柯里化过程解析 例子 示例说明 编写一个方法，用来完成两个Int类型数字的计算 具体如何计算封装到函数中 使用柯里化来实现上述操作 参考代码 1234567891011121314// 柯里化：实现对两个数进行计算package com.nicai.highlevelobject Demo3 &#123; def add(a:Int,b:Int)(cala:(Int,Int) =&gt; Int)=&#123; cala(a,b) &#125; def main(args: Array[String]): Unit = &#123; println(add(1, 2)(_ + _)) println(add(1, 2)(_ * _)) &#125;&#125; 闭包闭包其实就是一个函数，只不过这个函数的返回值依赖于声明在函数外部的变量。 可以简单认为，就是可以访问不在当前作用域范围的一个函数。 例子一 定义一个闭包 12345678910package com.nicai.highlevel//闭包object Demo4 &#123; var x=4 val add=(y:Int) =&gt; x+y def main(args: Array[String]): Unit = &#123; println(add(5)) &#125;&#125; add函数就是一个闭包 例子二 柯里化就是一个闭包 123def add(x:Int)(y:Int) = &#123; x + y&#125; 上述代码相当于 1234567891011121314151617181920 def add(x:Int) = &#123; (y:Int) =&gt; x + y &#125;总的:package com.nicai.highlevel//闭包二 柯里化就是一个闭包object Demo5 &#123; def add(a:Int)(b:Int): Int =&#123; a+b &#125; def add2(x:Int)=&#123; (y:Int) =&gt; x+y &#125; def main(args: Array[String]): Unit = &#123; println(add(5)(6)) //11 println(add2(5)(6))//11 &#125;&#125; 二 隐式转换与隐式参数隐式转换和隐式参数是scala非常有特色的功能，也是Java等其他编程语言没有的功能。我们可以很方便地利用隐式转换来丰富现有类的功能。后面在编写Akka并发编程、Spark SQL、Flink都会看到隐式转换和隐式参数的身影。 1 使用隐式转换定义: 所谓隐式转换，是指以implicit关键字声明的带有单个参数的方法。它是自动被调用的，自动将某种类型转换为另外一种类型。 使用步骤 在object中定义隐式转换方法（使用implicit） 在需要用到隐式转换的地方，引入隐式转换（使用import） 自动调用隐式转化后的方法 例子 示例说明 使用隐式转换，让File具备有read功能——实现将文本中的内容以字符串形式读取出来 步骤 1创建RichFile类，提供一个read方法，用于将文件内容读取为字符串 定义一个隐式转换方法，将File隐式转换为RichFile对象 创建一个File，导入隐式转换，调用File的read方法 参考代码 123456789101112131415161718192021222324252627package com.nicai.yinshizhuanhuanimport java.io.Fileimport scala.io.Source//隐式转换/*1. 创建RichFile类，提供一个read方法，用于将文件内容读取为字符串2. 定义一个隐式转换方法，将File隐式转换为RichFile对象3. 创建一个File，导入隐式转换，调用File的read方法*/object Demo6 &#123; class RichFile(f:File)&#123; def read()=&#123; Source.fromFile(f).mkString &#125; &#125; object Im&#123; implicit def fileToRichFile(file:File) =new RichFile(file) &#125; def main(args: Array[String]): Unit = &#123; val file = new File("day23Scala4/data/a.txt") import Im.fileToRichFile println(file.read()) &#125;&#125; 隐式转换的时机 当对象调用类中不存在的方法或者成员时，编译器会自动将对象进行隐式转换 当方法中的参数的类型与目标类型不一致时 2 自动导入隐式转化方法前面，我们手动使用了import来导入隐式转换。是否可以不手动import呢？ 在scala中，如果在当前作用域中有隐式转换方法，会自动导入隐式转换。 示例：将隐式转换方法定义在main所在的object中 1234567891011121314151617181920package com.nicai.yinshizhuanhuanimport java.io.Fileimport scala.io.Source//自动导入隐式转换object Demo7 &#123; class RichFile(f:File)&#123; def read()=&#123; Source.fromFile(f).mkString &#125; &#125; def main(args: Array[String]): Unit = &#123; val file = new File("day23Scala4/data/a.txt") implicit def fileToRichFile(file:File) =new RichFile(file) println(file.read()) &#125;&#125; 3 隐式参数方法可以带有一个标记为implicit的参数列表。这种情况，编译器会查找缺省值，提供给该方法。 定义 在方法后面添加一个参数列表，参数使用implicit修饰 在object中定义implicit修饰的隐式值 调用方法，可以不传入implicit修饰的参数列表，编译器会自动查找缺省值 注意: 和隐式转换一样，可以使用import手动导入隐式参数 如果在当前作用域定义了隐式值，会自动进行导入 例子 示例说明 定义一个方法，可将传入的值，使用一个分隔符前缀、后缀包括起来 使用隐式参数定义分隔符 调用该方法，并打印测试 参考代码 123456789101112131415161718192021package com.nicai.yinshizhuanhuan//隐式参数/*- 定义一个方法，可将传入的值，使用一个分隔符前缀、后缀包括起来- 使用隐式参数定义分隔符- 调用该方法，并打印测试*///与java中的动态代理 作用有点类似object Demo8 &#123; def qu(str:String)(implicit im:(String,String)) =&#123; im._1+str+im._2 &#125; //定义隐式参数 object Im&#123; implicit val delim= ("&lt;&lt;","&gt;&gt;") &#125; def main(args: Array[String]): Unit = &#123; import Im.delim println(qu("aa")) &#125;&#125; 三 Akka 并发编程1 介绍Akka是一个用于构建高并发、分布式和可扩展的基于事件驱动的应用的工具包。Akka是使用scala开发的库，同时可以使用scala和Java语言来开发基于Akka的应用程序。 2 特性 提供基于异步非阻塞、高性能的事件驱动编程模型 内置容错机制，允许Actor在出错时进行恢复或者重置操作 超级轻量级的事件处理（每GB堆内存几百万Actor） 使用Akka可以在单机上构建高并发程序，也可以在网络中构建分布式程序。 3 Akka通信过程以下图片说明了Akka Actor的并发编程模型的基本流程： 学生创建一个ActorSystem 通过ActorSystem来创建一个ActorRef（老师的引用），并将消息发送给ActorRef ActorRef将消息发送给Message Dispatcher（消息分发器） Message Dispatcher将消息按照顺序保存到目标Actor的MailBox中 Message Dispatcher将MailBox放到一个线程中 MailBox按照顺序取出消息，最终将它递给TeacherActor接受的方法中 4 入门案例基于Akka创建两个Actor，Actor之间可以互相发送消息。 实现步骤 创建Maven模块 创建并加载Actor 发送/接收消息 1创建Maven模块 使用Akka需要导入Akka库，我们这里使用Maven来管理项目 创建Maven模块 打开pom.xml文件，导入akka Maven依赖和插件 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788&lt;properties&gt; &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;scala.version&gt;2.11.8&lt;/scala.version&gt; &lt;scala.compat.version&gt;2.11&lt;/scala.compat.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-library&lt;/artifactId&gt; &lt;version&gt;$&#123;scala.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.typesafe.akka&lt;/groupId&gt; &lt;artifactId&gt;akka-actor_2.11&lt;/artifactId&gt; &lt;version&gt;2.3.14&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.typesafe.akka&lt;/groupId&gt; &lt;artifactId&gt;akka-remote_2.11&lt;/artifactId&gt; &lt;version&gt;2.3.14&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;sourceDirectory&gt;src/main/scala&lt;/sourceDirectory&gt; &lt;testSourceDirectory&gt;src/test/scala&lt;/testSourceDirectory&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;net.alchim31.maven&lt;/groupId&gt; &lt;artifactId&gt;scala-maven-plugin&lt;/artifactId&gt; &lt;version&gt;3.2.2&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;compile&lt;/goal&gt; &lt;goal&gt;testCompile&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;args&gt; &lt;arg&gt;-dependencyfile&lt;/arg&gt; &lt;arg&gt;$&#123;project.build.directory&#125;/.scala_dependencies&lt;/arg&gt; &lt;/args&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt; &lt;version&gt;2.4.3&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;shade&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;filters&gt; &lt;filter&gt; &lt;artifact&gt;*:*&lt;/artifact&gt; &lt;excludes&gt; &lt;exclude&gt;META-INF/*.SF&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.DSA&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.RSA&lt;/exclude&gt; &lt;/excludes&gt; &lt;/filter&gt; &lt;/filters&gt; &lt;transformers&gt; &lt;transformer implementation="org.apache.maven.plugins.shade.resource.AppendingTransformer"&gt; &lt;resource&gt;reference.conf&lt;/resource&gt; &lt;/transformer&gt; &lt;transformer implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer"&gt; &lt;mainClass&gt;&lt;/mainClass&gt; &lt;/transformer&gt; &lt;/transformers&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; 2创建并加载Actor 创建两个Actor SenderActor：用来发送消息 12345678910package com.nicai.akkademoimport akka.actor.Actor//发送消息object SenderActor extends Actor&#123; //不在使用loop+react了 在akka中直接在receive中编写偏函数直接处理消息就可以持续接受消息 override def receive: Receive = &#123; case x =&gt; println(x) &#125;&#125; ReceiveActor：用来接收，回复消息 123456789package com.nicai.akkademoimport akka.actor.Actor//接收 回复消息object ReceiveActor extends Actor&#123; override def receive: Receive = &#123; case x =&gt; println(x) &#125;&#125; 创建Actor 创建ActorSystem 创建自定义Actor ActorSystem加载Actor 1234567891011121314package com.nicai.akkademoimport akka.actor.&#123;ActorSystem, Props&#125;import com.typesafe.config.ConfigFactoryobject Entrance &#123; def main(args: Array[String]): Unit = &#123; //创建ActorSystem val actorSystem = ActorSystem("actorSystem", ConfigFactory.load()) //加载Actor val senderActor = actorSystem.actorOf(Props(SenderActor),"senderActor") val receiveActor = actorSystem.actorOf(Props(ReceiveActor),"receiveActor") &#125;&#125; 3发送/接收消息 使用样例类封装消息 SubmitTaskMessage——提交任务消息 SuccessSubmitTaskMessage——任务提交成功消息 使用类似于之前学习的Actor方式，使用!发送异步消息 参考代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354完整版//sender发送消息case class MessageSub(msg:String)//receive回复消息case class MsgSuccess(msg:String)............................................................................import akka.actor.Actor//发送消息object SenderActor extends Actor&#123; //不在使用loop+react了 在akka中直接在receive中编写偏函数直接处理消息就可以持续接受消息 override def receive: Receive = &#123; //匹配 entrance 的消息 case MessageSub("start") =&gt; &#123; println("收到消息") //格式akka://actorSystem的名字/user(固定)/receiveActor的名字 若是远程连接则加端口号等 val receiveActor = context.actorSelection("akka://actorSystem/user/receiveActor") //向receive发送消息 receiveActor ! MessageSub("nicai") &#125; //接收receive的消息 case MsgSuccess(name) =&gt;&#123; println(name) &#125; &#125;&#125;...............................................................................import akka.actor.Actor//接收 回复消息object ReceiveActor extends Actor&#123; override def receive: Receive = &#123; //匹配消息 case MessageSub(name) =&gt; &#123; println(name) //回复消息 sender ! MsgSuccess("我不猜") &#125; case _ =&gt; println("未匹配的消息类型") &#125;&#125;............................................................................import akka.actor.&#123;ActorSystem, Props&#125;import com.typesafe.config.ConfigFactoryobject Entrance &#123; def main(args: Array[String]): Unit = &#123; //创建ActorSystem val actorSystem = ActorSystem("actorSystem", ConfigFactory.load()) //加载Actor val senderActor = actorSystem.actorOf(Props(SenderActor),"senderActor") val receiveActor = actorSystem.actorOf(Props(ReceiveActor),"receiveActor") //向senderActor发送消息 senderActor ! MessageSub("start") &#125;&#125; 程序输出： 123收到消息nicai我不猜 5 Akka定时任务如果我们想要使用Akka框架定时的执行一些任务，该如何处理呢？ 使用方式: Akka中，提供一个scheduler对象来实现定时调度功能。使用ActorSystem.scheduler.schedule方法，可以启动一个定时任务。 schedule方法针对scala提供两种使用形式： 第一种：发送消息 123456def schedule( initialDelay: FiniteDuration, // 延迟多久后启动定时任务 interval: FiniteDuration, // 每隔多久执行一次 receiver: ActorRef, // 给哪个Actor发送消息 message: Any) // 要发送的消息(implicit executor: ExecutionContext) // 隐式参数：需要手动导入 第二种：自定义实现 12345def schedule( initialDelay: FiniteDuration, // 延迟多久后启动定时任务 interval: FiniteDuration // 每隔多久执行一次)(f: ⇒ Unit) // 定期要执行的函数，可以将逻辑写在这里(implicit executor: ExecutionContext) // 隐式参数：需要手动导入 示例一示例说明 定义一个Actor，每1秒发送一个消息给Actor，Actor收到后打印消息 使用发送消息方式实现 参考代码 123456789101112131415161718192021222324252627import akka.actor.&#123;Actor, ActorSystem, Props&#125;import com.typesafe.config.ConfigFactoryobject Demo1 &#123;//创建actor object ReceiveActor extends Actor &#123; override def receive: Receive = &#123; case x =&gt; println(x) &#125; &#125; def main(args: Array[String]): Unit = &#123; val actorSystem = ActorSystem("actorSystem",ConfigFactory.load()) val receiveActor = actorSystem.actorOf(Props(ReceiveActor)) //导入 隐式转换 import scala.concurrent.duration._ //导入隐式参数 import actorSystem.dispatcher actorSystem.scheduler.schedule(0 seconds, //延迟后多久启动定时任务 1 seconds, //每隔多久执行一次 receiveActor, //给那个actor发送消息 "hello" //消息正文 ) &#125;&#125; 示例二示例说明 定义一个Actor，每1秒发送一个消息给Actor，Actor收到后打印消息 使用自定义方式实现 参考代码 12345678910111213141516171819202122232425import akka.actor.&#123;Actor, ActorSystem, Props&#125;import com.typesafe.config.ConfigFactoryobject Demo2 &#123; object ReceiceActor extends Actor&#123; override def receive: Receive = &#123; case x =&gt; println(x) &#125; &#125; def main(args: Array[String]): Unit = &#123; val actorSystem = ActorSystem("actorSystem",ConfigFactory.load()) val receiveActor = actorSystem.actorOf(Props(ReceiceActor)) //导入隐式装换 不到无法使用 0 seconds import scala.concurrent.duration._ //导入隐式参数 import actorSystem.dispatcher actorSystem.scheduler.schedule(0 seconds,1 seconds)( receiveActor ! " iac" ) &#125;&#125; 注意: 需要导入隐式转换import scala.concurrent.duration._才能调用0 seconds方法 需要导入隐式参数import actorSystem.dispatcher才能启动定时任务 6 实现两个进程间的通信 master实现基于Akka实现在两个进程间发送、接收消息。Worker启动后去连接Master，并发送消息，Master接收到消息后，再回复Worker消息。 1. Worker实现步骤 创建一个Maven模块，导入依赖和配置文件 创建配置文件 12345application.confakka.actor.provider = &quot;akka.remote.RemoteActorRefProvider&quot;akka.remote.netty.tcp.hostname = &quot;127.0.0.1&quot;akka.remote.netty.tcp.port = &quot;8888&quot; 创建启动WorkerActor 发送”setup”消息给WorkerActor，WorkerActor接收打印消息 启动测试 参考代码 Worker.scala 1234567891011import akka.actor.&#123;Actor, ActorSystem, Props&#125;import com.typesafe.config.ConfigFactoryobject Worker &#123; def main(args: Array[String]): Unit = &#123; val actorSystem = ActorSystem("actorSystem",ConfigFactory.load()) val worker = actorSystem.actorOf(Props(WorkerActor)) //发送消息 worker ! "nicai" &#125;&#125; WorkerActor.scala 1234567import akka.actor.Actorobject WorkerActor extends Actor&#123; override def receive: Receive = &#123; case x =&gt; println(x) &#125;&#125; 2. Master实现步骤 创建Maven模块，导入依赖和配置文件 创建启动MasterActor WorkerActor发送”connect”消息给MasterActor MasterActor回复”success”消息给WorkerActor WorkerActor接收并打印接收到的消息 启动Master、Worker测试 参考代码 Master.scala 123456789import akka.actor.&#123;ActorSystem, Props&#125;import com.typesafe.config.ConfigFactoryobject Master &#123; def main(args: Array[String]): Unit = &#123; val actorSystem = ActorSystem("actorSystem",ConfigFactory.load()) val masterActor = actorSystem.actorOf(Props(MasterActor),"masterActor") &#125;&#125; MasterActor.scala 12345678910import akka.actor.Actorobject MasterActor extends Actor&#123; override def receive: Receive = &#123; case "connect" =&gt; &#123; println("worker连接成功") sender ! "success" &#125; &#125;&#125; WorkerActor.scala 12345678910111213141516import akka.actor.Actorobject WorkerActor extends Actor&#123; override def receive: Receive = &#123; case "start" =&gt; &#123; println("start") //设置连接 val masterActor = context.actorSelection("akka.tcp://actorSystem@127.0.0.1:9999/user/masterActor") masterActor ! "connect" &#125; case "success" =&gt; &#123; println("连接master成功") &#125; &#125;&#125; 四 简易Spark通信框架案例案例介绍 模拟Spark的Master与Worker通信 一个Master 管理Worker 若干个Worker（Worker可以按需添加） 注册 发送心跳 实现思路 构建Master、Worker阶段 构建Master ActorSystem、Actor 构建Worker ActorSystem、Actor Worker注册阶段 Worker进程向Master注册（将自己的ID、CPU核数、内存大小(M)发送给Master） Worker定时发送心跳阶段 Worker定期向Master发送心跳消息 Master定时心跳检测阶段 Master定期检查Worker心跳，将一些超时的Worker移除，并对Worker按照内存进行倒序排序 多个Worker测试阶段 启动多个Worker，查看是否能够注册成功，并停止某个Worker查看是否能够正确移除 工程搭建 项目使用Maven搭建工程 分别搭建几下几个项目 工程名 说明 spark-demo-common 存放公共的消息、实体类 spark-demo-master Akka Master节点 spark-demo-worker Akka Worker节点 导入依赖 master/worker添加common依赖,其余同上 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576common依赖 &lt;properties&gt; &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;scala.version&gt;2.11.8&lt;/scala.version&gt; &lt;scala.compat.version&gt;2.11&lt;/scala.compat.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-library&lt;/artifactId&gt; &lt;version&gt;$&#123;scala.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;build&gt; &lt;sourceDirectory&gt;src/main/scala&lt;/sourceDirectory&gt; &lt;testSourceDirectory&gt;src/test/scala&lt;/testSourceDirectory&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;net.alchim31.maven&lt;/groupId&gt; &lt;artifactId&gt;scala-maven-plugin&lt;/artifactId&gt; &lt;version&gt;3.2.2&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;compile&lt;/goal&gt; &lt;goal&gt;testCompile&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;args&gt; &lt;arg&gt;-dependencyfile&lt;/arg&gt; &lt;arg&gt;$&#123;project.build.directory&#125;/.scala_dependencies&lt;/arg&gt; &lt;/args&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt; &lt;version&gt;2.4.3&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;shade&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;filters&gt; &lt;filter&gt; &lt;artifact&gt;*:*&lt;/artifact&gt; &lt;excludes&gt; &lt;exclude&gt;META-INF/*.SF&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.DSA&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.RSA&lt;/exclude&gt; &lt;/excludes&gt; &lt;/filter&gt; &lt;/filters&gt; &lt;transformers&gt; &lt;transformer implementation="org.apache.maven.plugins.shade.resource.AppendingTransformer"&gt; &lt;resource&gt;reference.conf&lt;/resource&gt; &lt;/transformer&gt; &lt;transformer implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer"&gt; &lt;mainClass&gt;&lt;/mainClass&gt; &lt;/transformer&gt; &lt;/transformers&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugins&gt; &lt;/build&gt; 导入配置文件(同上) 修改Master的端口为7000(或者自定义) 修改Worker的端口为7100(或者自定义 最好 8000以上) 构建master和worker master和masterActor worker和workerActor 同上: 1234567891011121314151617181920212223242526272829303132333435363738//MasterActorimport akka.actor.Actorobject MasterActor extends Actor&#123; override def receive: Receive = &#123; case x =&gt; println(x) &#125;&#125;........................................//MasterMainimport akka.actor.&#123;ActorSystem, Props&#125;import com.typesafe.config.ConfigFactoryobject MasterMain &#123; def main(args: Array[String]): Unit = &#123; val actorSystem = ActorSystem("actorSystem",ConfigFactory.load()) val masterActor = actorSystem.actorOf(Props(MasterActor),"masterActor") &#125;&#125;.............................................................//WorkerActor import akka.actor.Actorobject WorkerActor extends Actor&#123; override def receive: Receive = &#123; case x =&gt; println(x) &#125;&#125;..............................//WorkerMain import akka.actor.&#123;ActorSystem, Props&#125;import com.typesafe.config.ConfigFactoryobject WorkerMain &#123; def main(args: Array[String]): Unit = &#123; val actorSystem = ActorSystem("actorSystem",ConfigFactory.load()) val workerActor = actorSystem.actorOf(Props(WorkerActor),"workerActor") &#125;&#125; worker注册实现 在Worker启动时，发送注册消息给Master 步骤 Worker向Master发送注册消息（workerid、cpu核数、内存大小） 随机生成CPU核（1、2、3、4、6、8） 随机生成内存大小（512、1024、2048、4096）（单位M） Master保存Worker信息，并给Worker回复注册成功消息 启动测试 参考代码 MasterActor.scala 1234567891011121314151617181920import akka.actor.Actorimport com.nicai.common.&#123;MsgRegin, MsgSuccess, Pojo&#125;import scala.collection.mutableobject MasterActor extends Actor&#123; //保存消息 private var stringToPojo: mutable.Map[String, Pojo] = collection.mutable.Map[String,Pojo]() override def receive: Receive = &#123; case MsgRegin(workid,cpu,mem) =&gt; &#123; println("收到注册消息"+workid+"-"+cpu+"-"+mem) //保存消息到实体类 stringToPojo += workid -&gt; Pojo(workid,cpu,mem) //回复消息 sender ! MsgSuccess("success") &#125; &#125;&#125; Pojo.scala 12//实体类保存 worker的注册信息case class Pojo(wokid:String,cpu:Int,mem:Int) MsgPackage.scala 1234//注册消息case class MsgRegin(workid:String,cpu:Int,mem:Int)//注册成功回复消息case class MsgSuccess(success:String) WorkerActor.scala 12345678910111213141516171819202122232425262728293031323334import java.util.UUIDimport akka.actor.&#123;Actor, ActorSelection, ActorSystem&#125;import com.nicai.common.&#123;MsgRegin, MsgSuccess&#125;import scala.util.Randomobject WorkerActor extends Actor&#123; private var actorSelection:ActorSelection=_ private var CPU_lIST:Int=_ private var MEM_LIST:Int=_ private var cpuList=List(1,2,4,8) private var memList=List(128,256,512,1024,2048) //在actor启动前要做的事 override def preStart()=&#123; //1 获取发送的对象 actorSelection = context.system.actorSelection("akka.tcp://actorSystem@127.0.0.1:7000/user/masterActor") //2 封装消息 val workerid:String=UUID.randomUUID().toString var a=new Random() CPU_lIST=cpuList(a.nextInt(cpuList.length)) MEM_LIST=memList(a.nextInt(memList.length)) val regin = MsgRegin(workerid,CPU_lIST,MEM_LIST) //3 发送消息 actorSelection ! regin &#125; override def receive: Receive = &#123; case MsgSuccess(name) =&gt; &#123; println("注册后的回复"+name) &#125; &#125;&#125; worker定时发送心跳 Worker接收到Master返回注册成功后，发送心跳消息。而Master收到Worker发送的心跳消息后，需要更新对应Worker的最后心跳时间。 步骤 编写工具类读取心跳发送时间间隔 创建心跳消息 Worker接收到注册成功后，定时发送心跳消息 Master收到心跳消息，更新Worker最后心跳时间 启动测试 参考代码 修改配置文件: 123添加 workerActor中的//定时发送消息worker.heartbeat.interval=5 在workerActor中ConfUtil.scala 1234567import com.typesafe.config.&#123;Config, ConfigFactory&#125;object ConfUtil&#123; private val config: Config = ConfigFactory.load() val `worker.heartbeat.interval` = config.getInt("worker.heartbeat.interval")&#125; MsgPackage.scala 12//心跳信息case class MsgHeartBeat(workid:String,cpu:Int,mem:Int) WorkerActor.scala 12345678910111213141516171819202122232425262728293031323334353637383940414243import java.util.UUIDimport akka.actor.&#123;Actor, ActorSelection, ActorSystem&#125;import com.nicai.common.&#123;MsgHeartBeat, MsgRegin, MsgSuccess&#125;import scala.util.Randomobject WorkerActor extends Actor &#123; private var actorSelection: ActorSelection = _ private var workerid: String = _ private var CPU_lIST: Int = _ private var MEM_LIST: Int = _ private var cpuList = List(1, 2, 4, 8) private var memList = List(128, 256, 512, 1024, 2048) //在actor启动前要做的事 override def preStart() = &#123; //1 获取发送的对象 actorSelection = context.system.actorSelection("akka.tcp://actorSystem@127.0.0.1:7000/user/masterActor") //2 封装消息 workerid = UUID.randomUUID().toString var a = new Random() CPU_lIST = cpuList(a.nextInt(cpuList.length)) MEM_LIST = memList(a.nextInt(memList.length)) val regin = MsgRegin(workerid, CPU_lIST, MEM_LIST) //3 发送消息 actorSelection ! regin &#125; override def receive: Receive = &#123; case MsgSuccess(name) =&gt; &#123; println("注册后的回复" + name) //导入隐式转换 import scala.concurrent.duration._ //导入隐式参数 import context.dispatcher //心跳发送 context.system.scheduler.schedule(0 seconds, ConfUtil.`worker.heartbeat.interval` seconds ) &#123; actorSelection ! MsgHeartBeat(workerid,CPU_lIST,MEM_LIST) &#125; &#125; &#125;&#125; MasterActor.scala 12345678910111213141516171819202122232425262728import java.util.Dateimport akka.actor.Actorimport com.nicai.common.&#123;MsgHeartBeat, MsgRegin, MsgSuccess, Pojo&#125;import scala.collection.mutableobject MasterActor extends Actor&#123; //保存消息 private var stringToPojo: mutable.Map[String, Pojo] = collection.mutable.Map[String,Pojo]() override def receive: Receive = &#123; case MsgRegin(workid,cpu,mem) =&gt; &#123; println("收到注册消息"+workid+"-"+cpu+"-"+mem) //保存消息到实体类 stringToPojo += workid -&gt; Pojo(workid,cpu,mem,new Date().getTime) //回复消息 sender ! MsgSuccess("success") &#125; //心跳 case MsgHeartBeat(workid,cpu,mem)=&gt;&#123; println("接收到心跳") //更新消息 stringToPojo += workid -&gt; Pojo(workid,cpu,mem,new Date().getTime) println(stringToPojo) &#125; &#125;&#125; master定时心跳检测 如果某个worker超过一段时间没有发送心跳，Master需要将该worker从当前的Worker集合中移除。可以通过Akka的定时任务，来实现心跳超时检查。 步骤 编写工具类，读取检查心跳间隔时间间隔、超时时间 定时检查心跳，过滤出来大于超时时间的Worker 移除超时的Worker 对现有Worker按照内存进行降序排序，打印可用Worker 参考代码 123456修改 master的配置文件//检查worker心跳的时间周期master.heartbeat.check.interval=6//配置worker的心跳超时时间master.heartbeat.check.timeout=15 ConfigUtil.scala 123456789import com.typesafe.config.&#123;Config, ConfigFactory&#125;object ConfigUtil &#123; private val config: Config = ConfigFactory.load() // 心跳检查时间间隔 val `master.heartbeat.check.interval` = config.getInt("master.heartbeat.check.interval") // 心跳超时时间 val `master.heartbeat.check.timeout` = config.getInt("master.heartbeat.check.timeout")&#125; MasterActor.scala 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162import java.util.Dateimport akka.actor.Actorimport com.nicai.common.&#123;MsgHeartBeat, MsgRegin, MsgSuccess, Pojo&#125;import scala.collection.mutableobject MasterActor extends Actor&#123; //保存消息 private var stringToPojo: mutable.Map[String, Pojo] = collection.mutable.Map[String,Pojo]() override def preStart(): Unit = &#123; // 导入时间单位隐式转换 import scala.concurrent.duration._ // 导入隐式参数 import context.dispatcher // 1. 启动定时任务 context.system.scheduler.schedule(0 seconds, ConfigUtil.`master.heartbeat.check.interval` seconds)&#123; // 2. 过滤大于超时时间的Worker val timeoutWorkerMap = stringToPojo.filter &#123; keyval =&gt; // 获取最后一次心跳更新时间 val lastHeartBeatTime = keyval._2.heartBeat // 当前系统时间 - 最后一次心跳更新时间 &gt; 超时时间（配置文件） * 1000，返回true，否则返回false if (new Date().getTime - lastHeartBeatTime &gt; ConfigUtil.`master.heartbeat.check.timeout` * 1000) &#123; true &#125; else &#123; false &#125; &#125; // 3. 移除超时Worker if(!timeoutWorkerMap.isEmpty) &#123; stringToPojo --= timeoutWorkerMap.map(_._1) // 4. 对Worker按照内存进行降序排序，打印Worker val workerList = stringToPojo.map(_._2).toList val sortedWorkerList = workerList.sortBy(_.mem).reverse println("按照内存降序排序后的Worker列表：") println(sortedWorkerList) &#125; &#125; &#125; override def receive: Receive = &#123; case MsgRegin(workid,cpu,mem) =&gt; &#123; println("收到注册消息"+workid+"-"+cpu+"-"+mem) //保存消息到实体类 stringToPojo += workid -&gt; Pojo(workid,cpu,mem,new Date().getTime) //回复消息 sender ! MsgSuccess("success") &#125; //心跳 case MsgHeartBeat(workid,cpu,mem)=&gt;&#123; println("接收到心跳") //更新消息 stringToPojo += workid -&gt; Pojo(workid,cpu,mem,new Date().getTime) println(stringToPojo) &#125; &#125;&#125; 多个worker测试 修改配置文件，启动多个worker进行测试。 步骤 测试启动新的Worker是否能够注册成功 (修改worker的端口号即可) 停止Worker，测试是否能够从现有列表删除]]></content>
      <tags>
        <tag>BigData</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala进阶2]]></title>
    <url>%2F2017%2F08%2F12%2FScala%E8%BF%9B%E9%98%B62.html</url>
    <content type="text"><![CDATA[#一 样例类 样例类是一种特殊类，它可以用来快速定义一个用于保存数据的类（类似于Java POJO类），而且它会自动生成apply方法，允许我们快速地创建样例类实例对象。后面，在并发编程和spark、flink这些框架也都会经常使用它。 样例类可以使用类名(参数1, 参数2…)快速创建实例对象 定义样例类成员变量时，可以指定var类型，表示可变。默认是不可变的 val 可省略 样例类自动生成了toString、equals、hashCode、copy方法 样例对象没有主构造器，可以使用样例对象来创建枚举、或者标识一类没有任何数据的消息 语法: 1case class 样例类名(成员变量名1:类型1, 成员变量名2:类型2, 成员变量名3:类型3) 定义样例类 12345678910object Demo1 &#123; case class Per(name:String, var age:Int) def main(args: Array[String]): Unit = &#123; val per = Per("你猜",25) //per.name="njj" 会报错 per.age=25 //正常可以修改 println(per) &#125;&#125; 样例类的方法定义样例类编译器自动帮我们实现了一下几个方法 12345apply 快速的用类名创建对象toString 与java同equals 比较两个样例类的成员变量是否相等 与==类似hashCode 如两个样例类的所有的成员变量的值相等 则hash值相等 否则 只要一个不同 则hash值就不等copy 样例类的克隆 12345678910object Demo2 &#123; case class Per(name:String,age:Int) def main(args: Array[String]): Unit = &#123; val nicai = Per("nicai",55) val nn = nicai.copy("nn") //可以修改成员变量的值 println(nn) &#125;&#125; 样例对象使用case object可以创建样例对象。样例对象是单例的，而且它没有主构造器。样例对象是可序列化的。格式： 1case object 样例对象名 它主要用在两个地方： 定义枚举 作为没有任何参数的消息传递（后面Akka编程会讲到） 12345678910111213141516object Demo3 &#123; //定义一个枚举 trait Sex case object Man extends Sex case object Wonmn extends Sex case class Per(name:String,sex: Sex) def main(args: Array[String]): Unit = &#123; val per = Per("小明",Man) val p = Per("小红",Wonmn) println(per) println(p) &#125;&#125; 定义消息 12345case class StartSpeakingMessage(textToSpeak: String)// 消息如果没有任何参数，就可以定义为样例对象case object StopSpeakingMessagecase object PauseSpeakingMessagecase object ResumeSpeakingMessage 二模板匹配scala中有一个非常强大的模式匹配机制，可以应用在很多场景： switch语句 类型查询 以及快速获取数据 简单模式匹配相当于java中的switch语句 语法 123456变量 match &#123; case "常量1" =&gt; 表达式1 case "常量2" =&gt; 表达式2 case "常量3" =&gt; 表达式3 case _ =&gt; 表达式4 // 默认匹配&#125; 12345678910111213object Demo1 &#123; def main(args: Array[String]): Unit = &#123; val str = StdIn.readLine() //从键盘录入 val unit = str match &#123; case "hadoop" =&gt; "nicai" case "spaker" =&gt; "分布式计算框架" case _ =&gt; "未匹配" &#125; println(unit) &#125;&#125; 匹配类型根据不同的数据类型进行匹配 1234567变量 match &#123; case 类型1变量名: 类型1 =&gt; 表达式1 case 类型2变量名: 类型2 =&gt; 表达式2 case 类型3变量名: 类型3 =&gt; 表达式3 ... case _ =&gt; 表达式4&#125; 12345678910111213object Demo2 &#123; var a:Any="hadoop" def main(args: Array[String]): Unit = &#123; val unit = a match &#123; case x: String =&gt; s"$&#123;x&#125;字符串" //若后面没有用到这个变量 可以写为 _:String case x: Int =&gt; "整形" case x: Double =&gt; "浮点型" case _ =&gt; "没匹配" &#125; println(unit) &#125;&#125; 守卫在Java中，只能简单地添加多个case标签，例如：要匹配0-7，就需要写出来8个case语句。例如： 123456789101112int a = 0;switch(a) &#123; case 0: a += 1; case 1: a += 1; case 2: a += 1; case 3: a += 1; case 4: a += 2; case 5: a += 2; case 6: a += 2; case 7: a += 2; default: a = 0;&#125; 在scala中，可以使用守卫来简化上述代码——也就是在case语句中添加if条件判断。 12345678910111213object Demo3 &#123; private val i: Int = StdIn.readInt() def main(args: Array[String]): Unit = &#123; val unit = i match &#123; case x if x &gt; 0 &amp;&amp; x &lt; 3 =&gt; "0-3" case x if x &gt; 3 &amp;&amp; x &lt; 10 =&gt; println("3-10") //若这样鞋 则0-3 之后会打印() case x if x &gt; 10 &amp;&amp; x &lt; 13 =&gt; println("10-13") case _ =&gt; println("weipi") &#125; println(unit) &#125;&#125; 匹配样例类scala可以使用模式匹配来匹配样例类，从而可以快速获取样例类中的成员数据。后续，我们在开发Akka案例时，还会用到。 1234567891011121314object Demo4 &#123; case class Per(name:String,age:Int) case class Stu(name:String,age:Int) def main(args: Array[String]): Unit = &#123; val ni:Any = Per("ni",55) //若不为any则会报错 val unit = ni match &#123; case Per(name, age) =&gt; s"$&#123;name&#125;:$&#123;age&#125;per" case Stu(name, age) =&gt; s"$&#123;name&#125;:$&#123;age&#125;stu" case _ =&gt; "未匹配" &#125; println(unit) &#125;&#125; 匹配集合1 匹配数组 依次修改代码定义以下三个数组 123Array(1,x,y) // 以1开头，后续的两个元素不固定Array(0) // 只匹配一个0元素的元素Array(0, ...) // 可以任意数量，但是以0开头 使用模式匹配上述数组 参考代码 1234567val arr = Array(1, 3, 5)arr match &#123; case Array(1, x, y) =&gt; println(x + " " + y) case Array(0) =&gt; println("only 0") case Array(0, _*) =&gt; println("0 ...") case _ =&gt; println("something else")&#125; 2匹配列表 依次修改代码定义以下三个列表 123List(0) // 只保存0一个元素的列表List(0,...) // 以0开头的列表，数量不固定List(x,y) // 只包含两个元素的列表 使用模式匹配上述列表 参考代码 12345678val list = List(0, 1, 2)list match &#123; case 0 :: Nil =&gt; println("只有0的列表") case 0 :: tail =&gt; println("0开头的列表") case x :: y :: Nil =&gt; println(s"只有另两个元素$&#123;x&#125;, $&#123;y&#125;的列表") case _ =&gt; println("未匹配")&#125; 3 匹配元组 依次修改代码定义以下两个元组 12(1, x, y) // 以1开头的、一共三个元素的元组(x, y, 5) // 一共有三个元素，最后一个元素为5的元组 使用模式匹配上述元素 参考代码 1234567val tuple = (2, 2, 5)tuple match &#123; case (1, x, y) =&gt; println(s"三个元素，1开头的元组：1, $&#123;x&#125;, $&#123;y&#125;") case (x, y, 5) =&gt; println(s"三个元素，5结尾的元组：$&#123;x&#125;, $&#123;y&#125;, 5") case _ =&gt; println("未匹配")&#125; 变量声明中的模式匹配在定义变量的时候，可以使用模式匹配快速获取数据。 1 获取数组中的元素 生成包含0-10数字的数组，使用模式匹配分别获取第二个、第三个、第四个元素 参考代码 123456789101112package com.nicai.demo.matchdemo//变量声明中的模式匹配object Demo8 &#123; def main(args: Array[String]): Unit = &#123; var array=(0 to 10).toArray var Array(_,x,y,z,_*)=array println(x) //1 println(y) //2 println(z) //3 &#125;&#125; 2 获取列表中的数据 生成包含0-10数字的列表，使用模式匹配分别获取第一个、第二个元素 参考代码 123456789101112package com.nicai.demo.matchdemo//变量声明中的模式匹配object Demo9 &#123; def main(args: Array[String]): Unit = &#123; var a= (0 to 10).toList var x :: y :: tail =a println(x) //0 println(y) //1 &#125;&#125; option类型scala中，Option类型来表示可选值。这种类型的数据有两种形式： Some(x)：表示实际的值 None：表示没有值 使用Option类型，可以用来有效避免空引用(null)异常。也就是说，将来我们返回某些数据时，可以返回一个Option类型来替代。 getOrElse方法 使用getOrElse方法，当Option对应的实例是None时，可以指定一个默认值，从而避免空指针异常 scala鼓励使用Option类型来封装数据，可以有效减少，在代码中判断某个值是否为null 可以使用getOrElse方法来针对None返回一个默认值 例子一 定义一个两个数相除的方法，使用Option类型来封装结果 然后使用模式匹配来打印结果 不是除零，打印结果 除零打印异常错误 参考代码 1234567891011121314151617181920 package com.nicai.demo.matchdemoobject Demo10 &#123; def div(a:Double,b:Int): Option[Double] =&#123; if (b != 0)&#123; Some(a/b) &#125;else &#123; None &#125; &#125; def main(args: Array[String]): Unit = &#123; val option = div(15.2,2) val unit = option match &#123; case Some(x) =&gt; x case None =&gt; "除数不可为0" &#125; println(unit) //7.6 &#125;&#125; 例子二 重写上述案例，使用getOrElse方法，当除零时，或者默认值为0 参考代码 12345678910111213141516package com.nicai.demo.matchdemoobject Demo11 &#123; def div(a: Double, b: Int):Option[Double]= &#123; if( b != 0)&#123; Some(a/b) &#125;else&#123; None &#125; &#125; def main(args: Array[String]): Unit = &#123; val d = div(15.6,0).getOrElse(0) println(d) &#125;&#125; 偏函数偏函数可以提供了简洁的语法，可以简化函数的定义。配合集合的函数式编程，可以让代码更加优雅。 定义 偏函数被包在花括号内没有match的一组case语句是一个偏函数 偏函数是PartialFunction[A, B]的一个实例 A代表输入参数类型 B代表返回结果类型 可以理解为：偏函数是一个参数和一个返回值的函数。 1234567891011121314package com.nicai.demo.PartialFunctionDemoobject Demo12 &#123; private val value: PartialFunction[Int, String] = &#123; case 1 =&gt; "一" case 2 =&gt; "二" case _ =&gt; "其他" &#125; def main(args: Array[String]): Unit = &#123; println(value(1)) &#125;&#125; 定义一个列表，包含1-10的数字 请将1-3的数字都转换为[1-3] 请将4-8的数字都转换为[4-8] 将其他的数字转换为(8-星] 参考代码 123456789val list = (1 to 10).toListval list2 = list.map&#123; case x if x &gt;= 1 &amp;&amp; x &lt;= 3 =&gt; "[1-3]" case x if x &gt;= 4 &amp;&amp; x &lt;= 8 =&gt; "[4-8]" case x if x &gt; 8 =&gt; "(8-*]"&#125;println(list2) 正则表达式在scala中，可以很方便地使用正则表达式来匹配数据。 scala中提供了Regex类来定义正则表达式，要构造一个RegEx对象，直接使用String类的r方法即可。 建议使用三个双引号来表示正则表达式，不然就得对正则中的反斜杠来进行转义。 1val regEx = """正则表达式""".r findAllMatchIn方法 使用findAllMatchIn方法可以获取所有正则匹配到的字符串 1234567891011121314151617181920212223示例说明定义一个正则表达式，来匹配邮箱是否合法合法邮箱测试：qq12344@163.com不合法邮箱测试：qq12344@.comval r = """.+@.+\..+""".rval eml1 = "qq12344@163.com"val eml2 = "qq12344@.com"if(r.findAllMatchIn(eml1).size &gt; 0) &#123; //z\size 为0 没有匹配上 大于0 为匹配上 println(eml1 + "邮箱合法")&#125;else &#123; println(eml1 + "邮箱不合法")&#125;if(r.findAllMatchIn(eml2).size &gt; 0) &#123; println(eml2 + "邮箱合法")&#125;else &#123; println(eml2 + "邮箱不合法")&#125; 找出以下列表中的所有不合法的邮箱 1"38123845@qq.com", "a1da88123f@gmail.com", "zhansan@163.com", "123afadff.com" 12345678910111213141516package com.nicai.demo.zhengzebiaodashi//匹配多个邮箱object Demo15 &#123; def main(args: Array[String]): Unit = &#123; var a= List("38123845@qq.com", "a1da88123f@gmail.com", "zhansan@163.com", "123afadff.com") val r=""".+@.+\.com""".r val strings = a.filter &#123; //过滤出不合法的 case x if r.findAllMatchIn(x).size == 0 =&gt; true case _ =&gt; false &#125; println(strings) &#125;&#125; 有以下邮箱列表 1"38123845@qq.com", "a1da88123f@gmail.com", "zhansan@163.com", "123afadff.com" 使用正则表达式进行模式匹配，匹配出来邮箱运营商的名字。例如：邮箱zhansan@163.com，需要将163匹配出来 使用括号来匹配分组 打印匹配到的邮箱以及运营商 123456789101112131415package com.nicai.demo.zhengzebiaodashiobject Demo16 &#123; def main(args: Array[String]): Unit = &#123; //括号为分组 val re =""".+@(.+)\.com""".r //此处必为 val var li=List("38123845@qq.com", "a1da88123f@gmail.com", "zhansan@163.com", "123afadff.com") val strings = li.map &#123; //company为分组的名字 就是分组的字段 case x@re(company) =&gt; s"$&#123;x&#125; -&gt; $&#123;company&#125;" case x =&gt; s"$&#123;x&#125; + 未知" &#125; println(strings) &#125;&#125; 异常处理来看看下面一段代码。 123456789 def main(args: Array[String]): Unit = &#123; val i = 10 / 0 println("你好！") &#125;Exception in thread "main" java.lang.ArithmeticException: / by zero at ForDemo$.main(ForDemo.scala:3) at ForDemo.main(ForDemo.scala) 执行程序，可以看到scala抛出了异常，而且没有打印出来”你好”。说明程序出现错误后就终止了。 那怎么解决该问题呢？ 在scala中，可以使用异常处理来解决这个问题 捕获异常语法格式 12345678910try &#123; // 代码&#125;catch &#123; case ex:异常类型1 =&gt; // 代码 case ex:异常类型2 =&gt; // 代码&#125;finally &#123; // 代码&#125; try中的代码是我们编写的业务处理代码 在catch中表示当出现某个异常时，需要执行的代码 在finally中，是不管是否出现异常都会执行的代码 示例示例说明 使用try..catch来捕获除零异常 参考代码 123456789101112package com.nicai.demo.exceptionDemoobject Demo17 &#123; def main(args: Array[String]): Unit = &#123; try&#123; var a= 4/0 &#125;catch &#123; case ex:Exception =&gt; println(ex.getMessage) &#125; &#125;&#125; ###抛出异常 我们也可以在一个方法中，抛出异常。语法格式和Java类似，使用throw new Exception... 例子: 在main方法中抛出一个异常 参考代码 12345678910111213 package com.nicai.demo.exceptionDemoobject Demo18 &#123; def main(args: Array[String]): Unit = &#123; throw new Exception("这是一个异常") &#125;&#125;Exception in thread "main" java.lang.Exception: 这是一个异常 at ForDemo$.main(ForDemo.scala:3) at ForDemo.main(ForDemo.scala) scala不需要在方法上声明要抛出的异常，它已经解决了再Java中被认为是设计失败的检查型异常。 下面是Java代码 123public static void main(String[] args) throws Exception &#123; throw new Exception("这是一个异常");&#125; 提取器我们之前已经使用过scala中非常强大的模式匹配功能了，通过模式匹配，我们可以快速匹配样例类中的成员变量. 那是不是所有的类都可以进行这样的模式匹配呢？答案是： 不可以的。要支持模式匹配，必须要实现一个提取器。 样例类自动实现了apply、unapply方法 定义提取器之前我们学习过了，实现一个类的伴生对象中的apply方法，可以用类名来快速构建一个对象。伴生对象中，还有一个unapply方法。与apply相反，unapply是将该类的对象，拆解为一个个的元素。 要实现一个类的提取器，只需要在该类的伴生对象中实现一个unapply方法即可 语法格式 12345678def unapply(stu:Student):Option[(类型1, 类型2, 类型3...)] = &#123; if(stu != null) &#123; Some((变量1, 变量2, 变量3...)) &#125; else &#123; None &#125;&#125; 示例说明 创建一个Student类，包含姓名年龄两个字段 实现一个类的解构器，并使用match表达式进行模式匹配，提取类中的字段。 参考代码 123456789101112131415161718192021package com.nicai.demo.tiquqiobject Demo19 &#123; class Stu(var name:String,var age:Int) object Stu&#123; def apply(name: String, age: Int): Stu = new Stu(name, age) def unapply(stu :Stu) = &#123; val tuple =(stu.name,stu.age) Some(tuple) &#125; &#125; def main(args: Array[String]): Unit = &#123; val nicai = Stu("nicai",55) val unit = nicai match &#123; case Stu(name, age) =&gt; s"$&#123;name&#125;:$&#123;age&#125;" &#125; println(unit) &#125;&#125; 泛型scala和Java一样，类和特质、方法都可以支持泛型。我们在学习集合的时候，一般都会涉及到泛型。 定义一个泛型方法在scala中，使用方括号来定义类型参数。 语法格式 123def 方法名[泛型名称](..) = &#123; //...&#125; 示例说明 用一个方法来获取任意类型数组的中间的元素 不考虑泛型直接实现（基于Array[Int]实现） 加入泛型支持 参考代码 不考虑泛型的实现 1234567def getMiddle(arr:Array[Int]) = arr(arr.length / 2)def main(args: Array[String]): Unit = &#123; val arr1 = Array(1,2,3,4,5) println(getMiddle(arr1))&#125; 加入泛型支持 12345678910package com.nicai.demo.fanxingobject Demo20 &#123; def getMid[T](array: Array[T])= array(array.length/2) def main(args: Array[String]): Unit = &#123; println(getMid(Array(1, 2, 3))) println(getMid(Array("dd", "uu", "sss"))) &#125;&#125; ##泛型类 scala的类也可以定义泛型。接下来，我们来学习如何定义scala的泛型类 定义语法格式 1class 类[T](val 变量名: T) 定义一个泛型类，直接在类名后面加上方括号，指定要使用的泛型参数 指定类对应的泛型参数后，就使用这些类型参数来定义变量了 示例参考代码 1234567891011121314package com.nicai.demo.fanxingobject Demo21 &#123; case class Per[y] (name:y,age:y) def main(args: Array[String]): Unit = &#123; val list = List( Per("NJJS", 45), Per("jsjj", 789), Per(56456, "SSS") ) println(list) &#125;&#125; 上下界需求： 我们在定义方法/类的泛型时，限定必须从哪个类继承、或者必须是哪个类的父类。此时，就需要使用到上下界。 上界定义: 使用&lt;: 类型名表示给类型添加一个上界，表示泛型参数必须要从该类（或本身）继承 语法格式 1[T &lt;: 类型] 示例说明 参考代码 123456789101112131415package com.nicai.demo.fanxingobject Demo22 &#123; //上界 class Per class Stu extends Per class Man extends Stu def m[t &lt;: Stu](a:Array[t]) = println(a) //Per 本身及其子类 def main(args: Array[String]): Unit = &#123; // 编译报错 // m(Array(new Per)) m(Array(new Stu)) &#125;&#125; 下界 上界是要求必须是某个类的子类，或者必须从某个类继承，而下界是必须是某个类的父类（或本身） 语法格式 1[T &gt;: 类型] 注意: 如果类既有上界、又有下界。下界写在前面，上界写在后面 (同时又上下界,可能会守不住,即范围之外的也可以) 示例说明 参考代码 123456789101112131415161718package com.nicai.demo.fanxingobject Demo23 &#123;//下界 class Per class Stu extends Per class Man extends Stu def m[T &gt;: Stu](a:Array[T])= println(a) //Stu 本身及其父类 def main(args: Array[String]): Unit = &#123; m(Array(new Stu)) m(Array(new Per)) //会报错 //m(Array(new Man)) &#125;&#125; 协变 逆变 非变spark的源代码中大量使用到了协变、逆变、非变，学习该知识点对我们将来阅读spark源代码很有帮助。 来看一个类型转换的问题： 1234567891011class Pair[T]object Pair &#123; def main(args: Array[String]): Unit = &#123; val p1 = Pair("hello") // 编译报错，无法将p1转换为p2 val p2:Pair[AnyRef] = p1 println(p2) &#125;&#125; 如何让带有泛型的类支持类型转换呢？ 非变 语法格式 1class Pair[T]&#123;&#125; 默认泛型类是非变的 类型B是A的子类型，Pair[A]和Pair[B]没有任何从属关系 Java是一样的 协变语法格式 1class Pair[+T] 类型B是A的子类型，Pair[B]可以认为是Pair[A]的子类型 参数化类型的方向和类型的方向是一致的。 逆变语法格式 1class Pair[-T] 类型B是A的子类型，Pair[A]反过来可以认为是Pair[B]的子类型 参数化类型的方向和类型的方向是相反的 参考代码 123456789101112131415161718192021class Superclass Sub extends Superclass Temp1[T]class Temp2[+T]class Temp3[-T]def main(args: Array[String]): Unit = &#123; val a:Temp1[Sub] = new Temp1[Sub] // 编译报错 // 非变 //val b:Temp1[Super] = a // 协变 val c: Temp2[Sub] = new Temp2[Sub] val d: Temp2[Super] = c // 逆变 val e: Temp3[Super] = new Temp3[Super] val f: Temp3[Sub] = e&#125; Actor并发编程scala的Actor并发编程模型可以用来开发比Java线程效率更高的并发程序。我们学习scala Actor的目的主要是为后续学习Akka做准备。 Java并发编程的问题在Java并发编程中，每个对象都有一个逻辑监视器（monitor），可以用来控制对象的多线程访问。我们添加sychronized关键字来标记，需要进行同步加锁访问。这样，通过加锁的机制来确保同一时间只有一个线程访问共享数据。但这种方式存在资源争夺、以及死锁问题，程序越大问题越麻烦。 思索问题 例子: 123456789101112131415161718192021222324252627282930313233343536373839package com.nicai.Demo;public class MyLock &#123; public static Object obja = new Object(); public static Object objb = new Object();&#125;class DieLock extends Thread &#123; private boolean flag; public DieLock(boolean flag) &#123; this.flag=flag; &#125; @Override public void run() &#123; if(flag)&#123; synchronized (MyLock.obja)&#123; System.out.println("a"); synchronized (MyLock.objb)&#123; System.out.println("b"); &#125; &#125; &#125;else &#123; synchronized (MyLock.objb)&#123; System.out.println("bb"); synchronized (MyLock.obja)&#123; System.out.println("aa"); &#125; &#125; &#125; &#125; public static void main(String[] args)&#123; DieLock lock1 = new DieLock(true); DieLock lock2 = new DieLock(false); lock1.start(); lock2.start(); &#125;&#125; Artor并发编程模型Actor并发编程模型，是scala提供给程序员的一种与Java并发编程完全不一样的并发编程模型，是一种基于事件模型的并发机制。Actor并发编程模型是一种不共享数据，依赖消息传递的一种并发编程模式，有效避免资源争夺、死锁等情况。 java 并发编程 与Actor并发编程对比 Java内置线程模型 scala Actor模型 “共享数据-锁”模型 (share data and lock) share nothing 每个object有一个monitor，监视线程对共享数据的访问 不共享数据，Actor之间通过Message通讯 加锁代码使用synchronized标识 死锁问题 每个线程内部是顺序执行的 每个Actor内部是顺序执行的 注意 scala在2.11.x版本中加入了Akka并发编程框架，老版本已经废弃。Actor的编程模型和Akka很像，我们这里学习Actor的目的是为学习Akka做准备。 创建Actor创建Actor的方式和Java中创建线程很类似，也是通过继承来创建。 使用方式 定义class或object继承Actor特质 重写act方法 调用Actor的start方法执行Actor 类似于Java线程，这里的每个Actor是并行执行的 示例说明 创建两个Actor，一个Actor打印1-10，另一个Actor打印11-20 使用class继承Actor创建（如果需要在程序中创建多个相同的Actor） 使用object继承Actor创建（如果在程序中只创建一个Actor） 参考代码 使用class继承Actor创建 1234567891011121314object _05ActorDemo &#123; class Actor1 extends Actor &#123; override def act(): Unit = (1 to 10).foreach(println(_)) &#125; class Actor2 extends Actor &#123; override def act(): Unit = (11 to 20).foreach(println(_)) &#125; def main(args: Array[String]): Unit = &#123; new Actor1().start() new Actor2().start() &#125;&#125; 使用object继承Actor创建 1234567891011121314151617package com.nicai.demo.actorDemoimport scala.actors.Actorobject Demo26 &#123; object A1 extends Actor&#123; override def act(): Unit = (1 to 10).foreach(println(_)+",") &#125; object A2 extends Actor &#123; override def act(): Unit = (11 to 20).foreach(print(_)+",") &#125; def main(args: Array[String]): Unit = &#123; A1.start() A2.start() &#125;&#125; Actor程序运行流程 调用start()方法启动Actor 自动执行act()方法 向Actor发送消息 act方法执行完成后，程序会调用exit()方法 发送消息 与接收消息我们之前介绍Actor的时候，说过Actor是基于事件（消息）的并发编程模型，那么Actor是如何发送消息和接收消息的呢？ 使用方式发送消息 我们可以使用三种方式来发送消息： ！ 发送异步消息，没有返回值 !? 发送同步消息，等待返回值 !! 发送异步消息，返回值是Future[Any] 例如： 要给actor1发送一个异步字符串消息，使用以下代码： 1actor1 ! "你好!" 接收消息 Actor中使用receive方法来接收消息，需要给receive方法传入一个偏函数 12345&#123; case 变量名1:消息类型1 =&gt; 业务处理1, case 变量名2:消息类型2 =&gt; 业务处理2, ...&#125; 注意: receive方法只接收一次消息，接收完后继续执行act方法 示例说明 创建两个Actor（ActorSender、ActorReceiver） ActorSender发送一个异步字符串消息给ActorReceiver ActorReceive接收到该消息后，打印出来 1234567891011121314151617181920212223242526272829package com.nicai.demo.actorDemoimport java.util.concurrent.TimeUnitimport scala.actors.Actorobject Demo27 &#123;//发送消息 与 接收消息//发送 object MsgSender extends Actor&#123; override def act(): Unit = &#123; MsgReceiver ! "nicai" //给谁发消息 TimeUnit.SECONDS.sleep(3) &#125; &#125; //接收 object MsgReceiver extends Actor&#123; override def act(): Unit = &#123; receive&#123; case msg: String =&gt; println(msg) &#125; &#125; &#125; def main(args: Array[String]): Unit = &#123; MsgSender.start() MsgReceiver.start() &#125;&#125; 持续接收消息通过上一个案例，ActorReceiver调用receive来接收消息，但接收一次后，Actor就退出了。 我们希望ActorReceiver能够一直接收消息，怎么实现呢？ ——我们只需要使用一个while(true)循环，不停地调用receive来接收消息就可以啦 1234567891011121314151617181920212223242526272829package com.nicai.demo.actorDemoimport java.util.concurrent.TimeUnitimport scala.actors.Actorobject Demo27 &#123;//发送消息 与 接收消息//发送 object MsgSender extends Actor&#123; override def act(): Unit = &#123; MsgReceiver ! "nicai" TimeUnit.SECONDS.sleep(3) &#125; &#125; //接收 object MsgReceiver extends Actor&#123; override def act(): Unit = &#123; receive&#123; case msg: String =&gt; println(msg) &#125; &#125; &#125; def main(args: Array[String]): Unit = &#123; MsgSender.start() MsgReceiver.start() &#125;&#125; 使用loop和react 优化接收消息上述代码，使用while循环来不断接收消息。 如果当前Actor没有接收到消息，线程就会处于阻塞状态 如果有很多的Actor，就有可能会导致很多线程都是处于阻塞状态 每次有新的消息来时，重新创建线程来处理 频繁的线程创建、销毁和切换，会影响运行效率 在scala中，可以使用loop + react来复用线程。比while + receive更高效 使用loop + react重写上述案例 参考代码 123456789101112131415161718192021222324252627282930313233343536373839// 持续接收消息loop &#123; react &#123; case msg:String =&gt; println("接收到消息：" + msg) &#125;&#125;改写:package com.nicai.demo.actorDemoimport java.util.concurrent.TimeUnitimport scala.actors.Actorobject Demo29 &#123; object MsgSender extends Actor&#123; override def act(): Unit = &#123; while(true)&#123; MsgReceice ! "NICAII" TimeUnit.SECONDS.sleep(3) &#125; &#125; &#125; object MsgReceice extends Actor&#123; override def act(): Unit = &#123; loop&#123; react&#123; case msg :String =&gt; println(msg) &#125; &#125; &#125; &#125; def main(args: Array[String]): Unit = &#123; MsgReceice.start() MsgSender.start() &#125;&#125; 发送和接收自定义消息我们前面发送的消息是字符串类型，Actor中也支持发送自定义消息，常见的如：使用样例类封装消息，然后进行发送处理。 例子1 示例说明 创建一个MsgActor，并向它发送一个同步消息，该消息包含两个字段（id、message） MsgActor回复一个消息，该消息包含两个字段（message、name） 打印回复消息 注意: 使用!?来发送同步消息 在Actor的act方法中，可以使用sender获取发送者的Actor引用 12345678910111213141516171819202122232425262728293031323334//同步的方式import scala.actors.Actor//发送和接收自定义消息object Demo30 &#123; //封装发送消息 case class Msg(name:String,Age:Int) //封装回复消息 case class ReplyMsg(name:String,addres:String) //接收消息 object MsgActor extends Actor&#123; override def act(): Unit = &#123; loop&#123; react&#123; case Msg(name,age) =&gt;&#123; println("收到消息"+s"$&#123;name&#125;:$&#123;age&#125;") //获取发送者队象 并回复消息 sender ! ReplyMsg("wobucai","bbb") &#125; &#125; &#125; &#125; &#125; def main(args: Array[String]): Unit = &#123; MsgActor.start()//发送消息 并获取返回的消息 val unit:Any = MsgActor !? Msg("nicai",22) //转换 消息类型 if(unit.isInstanceOf[ReplyMsg])&#123; println("回复消息"+unit.asInstanceOf[ReplyMsg]) &#125; &#125;&#125; 实例2 创建一个MsgActor，并向它发送一个异步无返回消息，该消息包含两个字段（message, company） 使用!发送异步无返回消息 12345678910111213141516171819202122232425//异步无返回值import com.nicai.demo.actorDemo.Demo30.Msgimport scala.actors.Actorobject Demo31 &#123; //封装 消息 case class Mag(name:String,age:Int) object MsgActor extends Actor&#123; override def act(): Unit = &#123; loop&#123; react&#123; case Msg(name,age) =&gt; &#123; println(s"$&#123;name&#125;:$&#123;age&#125;") &#125; &#125; &#125; &#125; &#125; def main(args: Array[String]): Unit = &#123; MsgActor.start() //发送消息 MsgActor ! Msg("你猜",55) &#125;&#125; 例子3 创建一个MsgActor，并向它发送一个异步有返回消息，该消息包含两个字段（id、message） MsgActor回复一个消息，该消息包含两个字段（message、name） 打印回复消息 注意: 使用!!发送异步有返回消息 发送后，返回类型为Future[Any]的对象 Future表示异步返回数据的封装，虽获取到Future的返回值，但不一定有值，可能在将来某一时刻才会返回消息 Future的isSet()可检查是否已经收到返回消息，apply()方法可获取返回数据 12345678910111213141516171819202122232425262728293031323334353637383940//异步有返回值package com.nicai.demo.actorDemoimport scala.actors.Actorobject Demo32 &#123; //封装发送消息 case class Msg(name:String ,age: Int) //封装返回消息 case class ReMsg(name:String ,age: Int) //设置接收消息 object MsgActor extends Actor&#123; override def act(): Unit = &#123; loop&#123; react&#123; case Msg(name,age) =&gt;&#123; println(s"$&#123;name&#125;:$&#123;age&#125;") //返回消息 sender ! ReMsg("NICAI",4564) &#125; &#125; &#125; &#125; &#125; def main(args: Array[String]): Unit = &#123; MsgActor.start() val unit = MsgActor !! Msg("温暖你的空间",777) //if(unit.isInstanceOf[ReMsg])&#123; //检查是否已经收到返回消息 apply()方法可获取返回数据 // 等待所有结果都已返回 while(!unit.isSet)&#123; &#125;println(unit.apply().asInstanceOf[ReMsg]) //&#125; &#125;&#125; ##WordCount案例 我们要使用Actor并发编程模型实现多文件的单词统计 需求: 给定几个文本文件（文本文件都是以空格分隔的），使用Actor并发编程来统计单词的数量 实现思路 MainActor获取要进行单词统计的文件 根据文件数量创建对应的WordCountActor 将文件名封装为消息发送给WordCountActor WordCountActor接收消息，并统计单个文件的单词计数 将单词计数结果发送给MainActor MainActor等待所有的WordCountActor都已经成功返回消息，然后进行结果合并 步骤1 | 获取文件列表实现思路 在main方法中读取指定目录(${project_root_dir}/data/)下的所有文件，并打印所有的文件名 实现步骤 创建用于测试的数据文件 加载工程根目录，获取到所有文件 将每一个文件名，添加目录路径 打印所有文件名 参考代码 12345678//获取文件目录 // val DIR="G:\\develop\\bigdatas\\BigData\\day22Scala3\\data/" val DIR="day22Scala3/data/" //当为maven的子工程时 不可使用 "./data/" //获取文件流 val list = new File(DIR).list().toList //把每个文件加上前缀 形成完整路径 val pathAll = list.map(DIR + _) println(pathAll) 步骤2 | 创建WordCountActor实现思路 根据文件数量创建WordCountActor，为了方便后续发送消息给Actor，将每个Actor与文件名关联在一起 实现步骤 创建WordCountActor 将文件列表转换为WordCountActor 为了后续方便发送消息给Actor，将Actor列表和文件列表拉链到一起 打印测试 参考代码 MainActor.scala 1234567//获取wordcountList val wordCountList = list.map &#123; fileNmae =&gt; new WordCountActor() &#125; //每个 文件路径与 wordcount建立连接 val tuplesList = wordCountList.zip(pathAll) println(tuplesList) WordCountActor.scala 12345class WordCountActor extends Actor&#123; override def act(): Unit = &#123; &#125; &#125; 步骤3 | 启动Actor/发送/接收任务消息实现思路 启动所有WordCountActor，并发送单词统计任务消息给每个WordCountActor 注意 此处应发送异步有返回消息 实现步骤 创建一个WordCountTask样例类消息，封装要进行单词计数的文件名 启动所有WordCountTask，并发送异步有返回消息 获取到所有的WordCount中获取到的消息（封装到一个Future列表中） 在WordCountActor中接收并打印消息 参考代码 MainActor.scala 123456789//启动 actor /发送和接收消息 tuplesList.map&#123; actorFileName =&gt;&#123; val actor = actorFileName._1 actor.start() val future = actor !! Msg(actorFileName._2) future &#125; &#125; MessagePackage.scala 12345/** * 单词统计任务消息 * @param fileName 文件名 */case class Msg(name:String) WordCountActor.scala 12345678910class WordCountActor extends Actor&#123; override def act(): Unit = &#123; loop&#123; react&#123; //获取消息 case Msg(fileName) =&gt; println("对"+fileName+"进行单词统计") &#125; &#125; &#125;&#125; 步骤4 | 消息统计文件单词计数实现思路 读取文件文本，并统计出来单词的数量。例如： 1(hadoop, 3), (spark, 1)... 实现步骤 读取文件内容，并转换为列表 按照空格切割文本，并转换为一个一个的单词 为了方便进行计数，将单词转换为元组 按照单词进行分组，然后再进行聚合统计 打印聚合统计结果 参考代码 WordCountActor.scala 12345678910111213141516171819202122232425class WordCountActor extends Actor&#123; override def act(): Unit = &#123; loop&#123; react&#123; //获取消息 case Msg(fileName) =&gt; println("对"+fileName+"进行单词统计") //一 读取文件 获取列表 hadoop sqoop hadoop val wordLineList = Source.fromFile(fileName).getLines().toList //二 切割字符串,转换为一个一个的单词[hadoop, sqoop, hadoop] val wordList = wordLineList.flatMap(_.split(" ")) //三将单词转换为元组 [&lt;hadoop,1&gt;, &lt;sqoop,1&gt;, &lt;hadoop,1&gt;] val wordAndCountList = wordList.map(_ -&gt; 1) // 四 对其进行分组 聚合计算 //4.1 分组 &#123;hadoop-&gt;List(&lt;hadoop,1&gt;,&lt;hadoop,1&gt;), sqoop-&gt;List(&lt;sqoop,1&gt;)&#125; val wordGroubList = wordAndCountList.groupBy(_._1) //4.2 聚合 &#123;hadoop-&gt;2, sqoop-&gt;1&#125; var wordSum=wordGroubList.map&#123; keyValue =&gt; keyValue._1 -&gt; keyValue._2.map(_._2).sum &#125; println(wordSum) &#125; &#125; &#125;&#125; 步骤5 | 封装单词计数结果回复给MainActor实现思路 将单词计数的结果封装为一个样例类消息，并发送给MainActor MainActor等待所有WordCount均已返回后获取到每个WordCountActor单词计算后的结果 实现步骤 定义一个样例类封装单词计数结果 将单词计数结果发送给MainActor MainActor中检测所有WordActor是否均已返回，如果均已返回，则获取并转换结果 打印结果 参考代码 MessagePackage.scala 123456/** * 单词统计结果 * @param wordCount 单词计数 *///封装单词统计结果case class WordCountResult(wordSum:Map[String,Int]) WordCountActor.scala 12//6将结果封装到样例类中,发送给WcMainsender ! WordCountResult(wordSum) MainActor.scala 1234567// 编写一个while循环来等待所有的Actor都已经返回数据 while (futureList.filter(!_.isSet).size!=0)&#123;&#125; // 获取Future中封装的数据 val wordCountResultList = futureList.map(_.apply().asInstanceOf[WordCountResult]) // 获取样例类中封装的单词统计结果 val stringToInts = wordCountResultList.map(_.wordSum) println(stringToInts) 步骤6 | 结果合并实现思路 对接收到的所有单词计数进行合并。因为该部分已经在WordCountActor已经编写过，所以抽取这部分一样的代码到一个工具类中，再调用合并得到最终结果 实现步骤 创建一个用于单词合并的工具类 抽取重复代码为一个方法 在MainActor调用该合并方法，计算得到最终结果，并打印 参考代码 WordCountUtil.scala 12345678910111213141516171819202122/** * 单词分组统计 * @param wordCountList 单词计数列表 * @return 分组聚合结果 */def reduce(wordCountList:List[(String, Int)]) = &#123; // 按照单词进行分组 // [单词分组] = &#123;hadoop-&gt;List(hadoop-&gt;1, hadoop-&gt;1, hadoop-&gt;1), spark-&gt;List(spark -&gt;1)&#125; val grouped: Map[String, List[(String, Int)]] = wordCountList.groupBy(_._1) // 将分组内的数据进行聚合 // [单词计数] = (hadoop, 3), (spark, 1) val wordCount: Map[String, Int] = grouped.map &#123; tuple =&gt; // 单词 val word = tuple._1 // 进行计数 // 获取到所有的单词数量，然后进行累加 val total = tuple._2.map(_._2).sum word -&gt; total &#125; wordCount&#125; MainActor.scala 1234// 扁平化后再聚合计算val result: Map[String, Int] = WordCountUtil.reduce(resultList.flatten)println("最终结果:" + result)]]></content>
      <tags>
        <tag>BigData</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala进阶]]></title>
    <url>%2F2017%2F08%2F11%2FScala%E8%BF%9B%E9%98%B6.html</url>
    <content type="text"><![CDATA[Scala 进阶一 类与对象scala是支持面向对象的，也有类和对象的概念。 创建类和对象 使用class关键字来定义类 使用var/val来定义成员变量 使用def来定义成员方法 使用new来创建一个实例对象 var name:String = _，_表示使用默认值进行初始化 例如：String类型默认值是null，Int类型默认值是0，Boolean类型默认值是false… val变量不能使用_来进行初始化，因为val是不可变的，所以必须手动指定一个默认值 main方法必须要放在一个scala的object（单例对象）中才能执行 12345678910111213object Demo4 &#123; class Pe&#123; var name:String=_ var age:Int =_ def add(m:String) = print(m) private def a(a:String )= print(a) //private不可被访问 &#125; def main(args: Array[String]): Unit = &#123; val pe = new Pe pe.add("mm") &#125;&#125; geter和setter方法: scala会自动为成员变量生成scala语言的getter/setter scala的getter为字段名()，setter为字段名_=() 要生成Java的getter/setter，可以在成员变量上加一个@BeanProperty注解，这样将来去调用一些Java库的时候很有 12345@BeanPropertyvar name:String = _ // 姓名@BeanPropertyval registerDate = new Date() // 注册时间 二构造器1 主构造器123456789101112131415161718object Demo5 &#123; class Per(var name:String ="",var sex:String= "")&#123; print("构造") &#125; def main(args: Array[String]): Unit = &#123; val ni = new Per("ni","男") println(ni.name) println(ni.sex) val per = new Per() println(ni.sex) println(ni.name) println(new Per(sex = "女").sex) &#125;&#125; 2 辅助构造器与定义方法一样,且方法名一定为this 注意: 辅助构造器第一行必须调用主构造器或者其他辅助构造器 123456789101112131415161718192021222324object Demo6 &#123; class Cus(var name: String = "", var add: String = "") &#123; //辅助构造器 def this(fu: Array[String]) &#123; this(fu(0), fu(1)) &#125; def this(name:String)&#123; this(name,"郑州") &#125; /* def this (add:String)&#123; this("你猜",add) &#125;*/ &#125; def main(args: Array[String]): Unit = &#123; val cus = new Cus(Array[String]("aa","北京")) println(cus.add) val cus2 = new Cus("niu") println(cus2.add)//郑州 print(cus2.name)//niu &#125;&#125; 主构造器直接在类名后面定义 主构造器中的参数列表会自动定义为私有的成员变量 一般在主构造器中直接使用val/var定义成员变量，这样看起来会更简洁 在辅助构造器中必须调用其他构造器（主构造器、其他辅助构造器） 三 单例对象 (类似于java中的static)scala要比Java更加面向对象，所以，scala中是没有Java中的静态成员的。如果将来我们需要用到static变量、static方法，就要用到scala中的单例对象——object。可以把object理解为全是包含静态字段、静态方法的class，object本质上也是一个class。 定义object 定义单例对象和定义类很像，就是把class换成object 123456789101112131415161718定义一个工具类，用来格式化日期时间object DateUtils &#123; // 在object中定义的成员变量，相当于Java中定义一个静态变量 // 定义一个SimpleDateFormat日期时间格式化对象 val simpleDateFormat = new SimpleDateFormat("yyyy-MM-dd HH:mm") // 构造代码 println("构造代码") // 相当于Java中定义一个静态方法 def format(date:Date) = simpleDateFormat.format(date) // main是一个静态方法，所以必须要写在object中 def main(args: Array[String]): Unit = &#123; println &#123; DateUtils.format(new Date()) &#125;; &#125;&#125; 使用object 单例对象名定义一个单例对象，可以用object作为工具类或者存放常量 在单例对象中定义的变量，类似于Java中的static成员变量 在单例对象中定义的方法，类似于Java中的static方法 object单例对象的构造代码可以直接写在花括号中 调用单例对象的方法，直接使用单例对象名.方法名，访问单例对象的成员变量也是使用单例对象名.变量名 单例对象只能有一个无参的主构造器，不能添加其他参数 123456789object Demo7 &#123; object Dog&#123; val num=4; &#125; def main(args: Array[String]): Unit = &#123; print(Dog.num) &#125;&#125; 12345678910111213//调用方法object Demo8 &#123; object Uf&#123; def add(): Unit =&#123; print("-" * 15) //生成15 个- print("nicai") &#125; &#125; def main(args: Array[String]): Unit = &#123; Uf.add() &#125;&#125; 伴生对象 在Java中，经常会有一些类，同时有实例成员又有静态成员。如下： 12345678910111213public class CustomerService &#123; private static Logger logger = Logger.getLogger("customerService"); public void save(String customer) &#123; logger.info("添加一个用户"); // 保存客户 &#125; public static void main(String[] args) &#123; new CustomerService().save("客户"); &#125;&#125; 在scala中，可以使用伴生对象来实现。 一个class和object具有同样的名字。这个object称为伴生对象，这个class称为伴生类 注意: 半生类和伴生对象一样的名字 这两个要在同一个scala源文件中 这两个可以互相访问private属性 1234567891011121314151617object Demo11 &#123; //半生类 class Ssm()&#123; def fin(): Unit =&#123; print(Ssm.name) &#125; &#125; //半生对象 object Ssm&#123; var name="nicai" &#125; def main(args: Array[String]): Unit = &#123; val ssm = new Ssm ssm.fin() &#125;&#125; private[this] 访问权限 表示只能在当前类中访问,伴生对象也不可访问1234567891011object Demo12 &#123; class De(private[this] var name:String,var age:Int) object De&#123; def p(d:De): Unit =print(d.name) //直接报错 &#125; def main(args: Array[String]): Unit = &#123; De.p(new De("aa",788)) &#125;&#125; 工具类案例: 需求: 编写一个工具类专门格式化日期时间 定义一个方法用于将日期转换为年月日字符串 2012-10-15 12345678910111213object Demo9 &#123; object Forma&#123; //java 中的类 private val format = new SimpleDateFormat("yyyy-MM-dd") //定义方法 def fm(data:Date)=format.format(data) &#125; def main(args: Array[String]): Unit = &#123; println(Forma.fm(new Date())) &#125;&#125; apply方法 必须用在伴生对象中 我们之前使用过这种方式来创建一个Array对象。 1var a=Array(1,2) 这种写法非常简便，不需要再写一个new，然后敲一个空格，再写类名。如何直接使用类名来创建对象呢？ 答案就是：实现伴生对象的apply方法 伴生对象的apply方法用来快速地创建一个伴生类的对象。 123456789101112131415161718192021222324object Demo13 &#123; class Person(var name:String, var age:Int) &#123; override def toString = s"Person($name, $age)" &#125; object Person &#123; // 实现apply方法 // 返回的是伴生类的对象 def apply(name:String, age:Int): Person = new Person(name, age) // apply方法支持重载 def apply(name:String):Person = new Person(name, 20) def apply(age:Int):Person = new Person("某某某", age) def apply():Person = new Person("某某某", 20) &#125; def main(args: Array[String]): Unit = &#123; println(Person("jjj").name) &#125;&#125; 当遇到类名(参数1, 参数2...)会自动调用apply方法，在apply方法中来创建对象 定义apply时，如果参数列表是空，也不能省略括号()，否则引用的是伴生对象 main方法 scala和Java一样，如果要运行一个程序，必须有一个main方法。而在Java中main方法是静态的，而在scala中没有静态方法。在scala中，这个main方法必须放在一个object中。 实例: 12345object Main5 &#123; def main(args:Array[String]) = &#123; println("hello, scala") &#125;&#125; 也可以继承自App Trait（特质），然后将需要编写在main方法中的代码，写在object的构造方法体内。 123object Main extends App&#123;println("heoo")&#125; 四继承scala和Java一样，使用extends关键字来实现继承。可以在子类中定义父类中没有的字段和方法，或者重写父类的方法。 类和单例对象都可以从某个父类继承 12345678910111213object Demo14 &#123; class Per()&#123; var name="" def getName()=this.name &#125; class Student extends Per def main(args: Array[String]): Unit = &#123; val student = new Student student.name="ni" println(student.name) &#125;&#125; 单例继承 123456789101112object Demo15&#123; class Per()&#123; var name="" def a()=print("nicai") &#125; object Stu extends Per def main(args: Array[String]): Unit = &#123; Stu.name="ni" println(Stu.name) &#125;&#125; override 和supper 如果子类要覆盖父类中的一个非抽象方法，必须要使用override关键字 可以使用override关键字来重写一个val字段 可以使用super关键字来访问父类的成员 123456789101112131415161718object Demo16 &#123; class Per()&#123; val name:String ="22" var age:Int=0 def getName()=this.name &#125; class Stu extends Per&#123; override val name: String = "nn" override def getName(): String = "ni"+super.getName() &#125; def main(args: Array[String]): Unit = &#123; val stu = new Stu println(stu.getName()) //ninn println(stu.name) //nn &#125;&#125; 类型的判断与转换我们经常要在代码中进行类型的判断和类型的转换。在Java中，我们可以使用instanceof关键字、以及(类型)object来实现，在scala中如何实现呢？ scala中对象提供isInstanceOf和asInstanceOf方法。 isInstanceOf判断对象是否为指定类的对象 asInstanceOf将对象转换为指定类型 1234567891011121314object Demo17 &#123; class per class Stu extends per def main(args: Array[String]): Unit = &#123; val stu = new Stu if(stu.isInstanceOf[Stu])&#123; stu.asInstanceOf[Stu] print(stu) &#125;else&#123; print("不是stu类型") &#125; &#125;&#125; getClass和classOfisInstanceOf 只能判断出对象是否为指定类以及其子类的对象，而不能精确的判断出，对象就是指定类的对象。如果要求精确地判断出对象就是指定类的对象，那么就只能使用 getClass 和 classOf 。 p.getClass可以精确获取对象的类型 classOf[x]可以精确获取类型 使用==操作符就可以直接比较 123456789101112131415161718192021222324252627object Demo18 &#123; class Per() class Stu extends Per def main(args: Array[String]): Unit = &#123; val stu:Per=new Stu if(stu.isInstanceOf[Per])&#123; println("stu是一个per类型") ///y &#125;else&#123; println("stu 不是一个per类型") &#125; if(stu.getClass == classOf[Per])&#123; println("stu是一个per类型") &#125;else&#123; println("stu 不是一个per类型") ////y &#125; if(stu.getClass == classOf[Stu])&#123; println("stu是一个STU类型") //y &#125;else&#123; println("stu 不是一个stu类型") &#125; &#125;&#125; 调用父类的constructor 实例化子类对象，必须要调用父类的构造器，在scala中，只能在子类的主构造器中调用父类的构造器 步骤： 创建一个Person类，编写带有一个可变的name字段的主构造器 创建一个Student类，继承自Person类 编写带有一个name参数、clazz班级字段的主构造器 调用父类的构造器 创建main方法，创建Student对象实例，并打印它的姓名、班级 12345678910 class Person5(var name:String)// 直接在父类的类名后面调用父类构造器class Student5(name:String, var clazz:String) extends Person5(name)object Main5 &#123; def main(args: Array[String]): Unit = &#123; val s1 = new Student5("张三", "三年二班") println(s"$&#123;s1.name&#125; - $&#123;s1.clazz&#125;") &#125;&#125; 抽象类如果类的某个成员在当前类中的定义是不包含完整的，它就是一个抽象类 不完整定义有两种情况： 方法没有方法体 变量没有初始化 没有方法体的方法称为抽象方法，没有初始化的变量称为抽象字段。定义抽象类和Java一样，在类前面加上abstract关键字就可以了 123456789101112131415161718192021222324//抽象方法object Demo19 &#123; abstract class Sop()&#123; def m():Double //或者 def m:Double &#125; class z(l:Double ) extends Sop&#123; override def m:Double = l*l &#125; class Cyc(r:Double) extends Sop&#123; override def m:Double = Math.PI*r*r &#125; class Ju(l:Double,k:Double) extends Sop&#123; override def m: Double = l*k &#125; def main(args: Array[String]): Unit = &#123; val z = new z(12.3) val cyc = new Cyc(2) val ju = new Ju(1,3.0) println(z.m) println(cyc.m) println(ju.m) &#125;&#125; 抽象字段: 1234567891011121314151617181920212223// 定义一个人的抽象类abstract class Person6 &#123; // 没有初始化的val字段就是抽象字段 val WHO_AM_I:String&#125;class Student6 extends Person6 &#123; override val WHO_AM_I: String = "学生"&#125;class Policeman6 extends Person6 &#123; override val WHO_AM_I: String = "警察"&#125;object Main6 &#123; def main(args: Array[String]): Unit = &#123; val p1 = new Student6 val p2 = new Policeman6 println(p1.WHO_AM_I) println(p2.WHO_AM_I) &#125;&#125; 匿名内部类匿名内部类是没有名称的子类，直接用来创建实例对象。Spark的源代码中有大量使用到匿名内部类。 123456789101112object Demo20 &#123; abstract class Per()&#123; def say() &#125; def main(args: Array[String]): Unit = &#123; val per = new Per &#123; override def say(): Unit = println("nicai") &#125; per.say() &#125;&#125; 特质(trait)scala中没有接口的概念 替代的就是特质 特质是scala中代码复用的基础单元 它可以将方法和字段定义封装起来，然后添加到类中 与类继承不一样的是，类继承要求每个类都只能继承一个超类，而一个类可以添加任意数量的特质。 特质的定义和抽象类的定义很像，但它是使用trait关键字 用法一 作为接口使用 使用extends来继承trait（scala不论是类还是特质，都是使用extends关键字） 如果要继承多个trait，则使用with关键字 继承单个特质 12345678910111213object Demo21 &#123; trait Loger&#123; def pr(msg:String) &#125; class LogerE extends Loger&#123; override def pr(msg: String): Unit = print(msg) &#125; def main(args: Array[String]): Unit = &#123; var e:Loger=new LogerE() e.pr("nicai") &#125;&#125; 继承多个特质 12345678910111213141516171819object Demo22 &#123; trait D1&#123; def ms(msg:String) &#125; trait D2&#123; def ms():String &#125; class D3 extends D1 with D2&#123; override def ms(msg: String): Unit = println(msg) override def ms(): String = "nicai" &#125; def main(args: Array[String]): Unit = &#123; val d = new D3 d.ms("ni") println(d.ms()) &#125;&#125; object 继承trait 123456789101112object Demo23 &#123; trait D1&#123; def m(msg:String) &#125; object D2 extends D1&#123; override def m(msg: String): Unit = print(msg) &#125; def main(args: Array[String]): Unit = &#123; D2.m("nicai") &#125;&#125; 在trait中可以定义抽象方法，不写方法体就是抽象方法 和继承类一样，使用extends来继承trait 继承多个trait，使用with关键字 单例对象也可以继承trait 特质 定义具体的方法和类一样，trait中还可以定义具体的方法。· 12345678910111213object Demo24 &#123; trait D&#123; def add(msg:String) = println(msg) &#125; class D2 extends D&#123; def add2()= add("nicai") &#125; def main(args: Array[String]): Unit = &#123; val d = new D2 d.add2() &#125;&#125; trait中定义具体的字段和抽象字段 在trait中，可以混合使用具体方法和抽象方法 使用具体方法依赖于抽象方法，而抽象方法可以放到继承trait的子类中实现，这种设计方式也称为模板模式 1234567891011121314151617object Demo25 &#123; trait D&#123; val s = new SimpleDateFormat("yyyy-MM-dd") var TYPE:String def pr(msg:String) &#125; class D2 extends D&#123; override var TYPE: String = "控制台消息" override def pr(msg: String): Unit = println(s"$&#123;TYPE&#125;:$&#123;s.format(new Date)&#125;:$&#123;msg&#125;") &#125; def main(args: Array[String]): Unit = &#123; val d = new D2 d.pr("nnicai") &#125;&#125; 使用trait实现模板模式在特质中,具体方法依赖于抽象方法,而抽象 方法可以放在继承trait中的子类中实现,这种方式为模板设计模式 123456789101112131415161718object Demo26 &#123; trait Logger&#123; def log(msg:String) def info(msg:String) = log(msg) def exce(msg:String) = log(msg) def erro(msg:String) = log(msg) &#125; class LogE extends Logger&#123; override def log(msg: String): Unit = println(msg) &#125; def main(args: Array[String]): Unit = &#123; val e = new LogE e.info("info") e.exce("exec") e.erro("erro") &#125;&#125; 对象混入trait trait还可以混入到实例对象中，给对象实例添加额外的行为 只有混入了trait的对象才具有trait中的方法，其他的类对象不具有trait中的行为 使用with将trait混入到实例对象中 语法: 1var /val da=new 类 with 特质 1234567891011object Demo27 &#123; trait Logger&#123; def log()=println("nicai") &#125; class Aa def main(args: Array[String]): Unit = &#123; val aa = new Aa with Logger aa.log() &#125;&#125; trait 实现调用链模式类继承了多个trait后，可以依次调用多个trait中的同一个方法，只要让多个trait中的同一个方法在最后都依次执行super关键字即可。类中调用多个tait中都有这个方法时，首先会从最右边的trait方法开始执行，然后依次往左执行，形成一个调用链条。 如支付连 等 说明 : 一个子类 继承多个父trait 且还有祖父trait 则 会先执行自己的 在执行从右到左的父trait(继承顺序相反),最后执行祖父trait 12345678910111213141516171819202122232425262728293031323334object Demo28 &#123; trait Zf&#123; def log(data:String ) = println("祖父") &#125; trait Login extends Zf&#123; override def log(data: String): Unit =&#123; println("父1") super.log(data) &#125; &#125; trait Handle extends Zf&#123; override def log(data: String): Unit = &#123; println("父2") super.log(data) &#125; &#125; class Ser() extends Handle with Login &#123; override def log(data: String): Unit = &#123; println("子类") super.log(data) &#125; &#125; def main(args: Array[String]): Unit = &#123; val ser = new Ser() ser.log("nn") &#125;&#125;//子类//父1//父2//祖父 trait的构造机制 trait也有构造代码，但和类不一样，特质不能有构造器参数 每个特质只有一个无参数的构造器。 一个类继承另一个类、以及多个trait，当创建该类的实例时，它的构造顺序如下： 执行父类的构造器 从左到右依次执行trait的构造器 如果trait有父trait，先构造父trait，如果多个trait有同样的父trait，则只初始化一次 执行子类构造器 123456789101112131415161718192021222324252627class Person_One &#123; println("执行Person构造器!")&#125;trait Logger_One &#123; println("执行Logger构造器!")&#125;trait MyLogger_One extends Logger_One &#123; println("执行MyLogger构造器!")&#125;trait TimeLogger_One extends Logger_One &#123; println("执行TimeLogger构造器!")&#125;class Student_One extends Person_One with MyLogger_One with TimeLogger_One &#123; println("执行Student构造器!") &#125;object exe_one &#123; def main(args: Array[String]): Unit = &#123; val student = new Student_One &#125;&#125;// 程序运行输出如下：// 执行Person构造器!// 执行Logger构造器!// 执行MyLogger构造器!// 执行TimeLogger构造器!// 执行Student构造器! trait继承class trait也可以继承class 会把class中的成员都继承下来 这个class就会成为所有该trait子类的超级父类。 123456789101112131415161718192021class MyUtil &#123; def printMsg(msg: String) = println(msg)&#125;trait Logger_Two extends MyUtil &#123; def log(msg: String) = this.printMsg("log: " + msg)&#125;class Person_Three(val name: String) extends Logger_Two &#123; def sayHello &#123; this.log("Hi, I'm " + this.name) this.printMsg("Hello, I'm " + this.name) &#125;&#125;object Person_Three&#123; def main(args: Array[String]) &#123; val p=new Person_Three("Tom") p.sayHello //执行结果：// log: Hi, I'm Tom// Hello, I'm Tom &#125;&#125;]]></content>
      <tags>
        <tag>BigData</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scala入门]]></title>
    <url>%2F2017%2F08%2F10%2Fscala%E5%85%A5%E9%97%A8.html</url>
    <content type="text"><![CDATA[Scala入门一 什么是Scala​ scala是运行在 JVM 上的多范式编程语言，同时支持面向对象和面向函数编程 (如java是面向对象的也是面向接口的,懂得自然懂) 早期，scala刚出现的时候，并没有怎么引起重视，随着Spark和Kafka这样基于scala的大数据框架的兴起，scala逐步进入大数据开发者的眼帘。scala的主要优势是它的表达性。 1 使用场景:开发大数据应用程序（Spark程序、Flink程序）表达能力强，一行代码抵得上Java多行，开发速度快兼容Java，可以访问庞大的Java类库，例如：操作mysql、redis、freemarker、activemq等等 2 scala与java的简单对比scala定义三个实体类: 123case class User(var name:String,var orders:List[Order]) //用户实体类case class Order(var id:Int,var products:List[Product]) //订单实体类case class Product(var id:Int,var category:String) //商品实体类 讲一个列表中的字符串(数字类型) 转为数字列表: 1val ints = list.map(s =&gt; s.toInt) 二 开发环境搭建1 java与scala的编译流程java java源代码&gt; (javac编译)&gt;java字节码和java类库&gt;(加载)&gt;jvm&gt;(解释执行)&gt;操作系统 scala: scala源代码&gt;(scalac编译)&gt;java字节码和java类库和scala类库&gt;(加载)&gt;jvm&gt;(解释执行)&gt;操作系统 scala程序运行需要依赖于Java类库，必须要有Java运行环境，scala才能正确执行.scala源文件也是编译为class文件 根据上述流程图，要编译运行scala程序，需要jdk（jvm）scala编译器（scala SDK） 2jdk安装略 3安装SDK下载安装即可. idea安装scala插件 三scala的解释器后续我们会使用scala解释器来学习scala基本语法，scala解释器像Linux命令一样，执行一条代码，马上就可以让我们看到执行结果，用来测试比较方便。 启动: win+r 后输入scala 退出: 1:quit 四 语法1 定义变量格式: 1var/val 变量标识:变量类型 = 初始值 val 定义的是不可重新赋值的变量var 定义的是可重新赋值的变量 注意: 句尾不用写分号. 12重心赋值:name = "ni" 若为val 则会报错 var 不会报错 使用类型推断来定义变量 1var name = "nicai" scala可以自动根据变量的值来自动推断变量的类型，这样编写代码更加简洁。 惰性赋值 在企业的大数据开发中，有时候会编写非常复杂的SQL语句，这些SQL语句可能有几百行甚至上千行。这些SQL语句，如果直接加载到JVM中，会有很大的内存开销。如何解决？ 当有一些变量保存的数据较大时，但是不需要马上加载到JVM内存。可以使用惰性赋值来提高效率。语法格式： 123lazy var/val 变量标书:变量类型 = 值或者lazy var/val 变量标识 = 值 示例在程序中需要执行一条以下复杂的SQL语句，我们希望只有用到这个SQL语句才加载它。 123456789101112131415161718192021222324252627282930313233343536"""insert overwrite table adm.itcast_adm_personasselecta.user_id,a.user_name,a.user_sex,a.user_birthday,a.user_age,a.constellation,a.province,a.city,a.city_level,a.hex_mail,a.op_mail,a.hex_phone,a.fore_phone,a.figure_model,a.stature_model,b.first_order_time,b.last_order_time,...d.month1_hour025_cnt,d.month1_hour627_cnt,d.month1_hour829_cnt,d.month1_hour10212_cnt,d.month1_hour13214_cnt,d.month1_hour15217_cnt,d.month1_hour18219_cnt,d.month1_hour20221_cnt,d.month1_hour22223_cntfrom gdm.itcast_gdm_user_basic aleft join gdm.itcast_gdm_user_consume_order b on a.user_id=b.user_idleft join gdm.itcast_gdm_user_buy_category c on a.user_id=c.user_idleft join gdm.itcast_gdm_user_visit d on a.user_id=d.user_id;"""参考代码scala&gt; lazy val sql = """insert overwrite table adm.itcast_adm_personas略 2 字符串多种定义字符串; 使用双引号使用插值表达式使用三引号 1使用双引号 1var/val name= "值" name.length 长度 2 使用插值表达式 scala中，可以使用插值表达式来定义字符串，有效避免大量字符串的拼接。 12345var name="n"var age=123var sex="man"插值拼接:var info = s"name=$&#123;name&#125;,age=$&#123;age&#125;,sex=$&#123;sex&#125;" 3 使用三引号 如果有大段的文本需要保存，就可以使用三引号来定义字符串。例如：保存一大段的SQL语句。三个引号中间的所有字符串都将作为字符串的值。(包括空行,空格等) 12val/var 变量名 = """字符串1字符串2""" 12345678val sql = """select| *| from| t_user| where| name = "zhangsan""""println(sql) 五 数据类型与操作符scala中的类型以及操作符绝大多数和Java一样，数据类型: 12345678910基础类型 类型说明Byte 8位带符号整数Short 16位带符号整数Int 32位带符号整数Long 64位带符号整数Char 16位无符号Unicode字符String Char类型的序列（字符串）Float 32位单精度浮点数Double 64位双精度浮点数Boolean true或false 与java的区别: 1scala中所有的类型都使用大写字母开头 2 整形使用 Int 而不是Integer 3 scala中定义变量可以不写类型，让scala编译器自动推断 运算符 123456789类别 操作符算术运算符 +、-、*、/关系运算符 &gt;、&lt;、==、!=、&gt;=、&lt;=逻辑运算符 &amp;&amp;、||、!位运算符 &amp;、||、^、&lt;&lt;、&gt;&gt; 12345与java的异同:scala中没有，++、--运算符== equals 都表示比较值eq 表示比较地址值是否相等 例子: 123456var a="aa"var b=a + ""a == b truea.equals(b) truea.eq(b) false scala的类型层次结构 123456789101112131415161718192021类型 说明Any 所有类型的父类，,它有两个子类AnyRef与AnyValAnyVal 所有数值类型的父类AnyRef 所有对象类型（引用类型）的父类Unit 表示空，Unit是AnyVal的子类，它只有一个的实例()它类似于Java中的void，但scala要比Java更加面向对象Null Null是AnyRef的子类，也就是说它是所有引用类型的子类。它的实例是null可以将null赋值给任何对象类型Nothing所有类型的子类不能直接创建该类型实例，某个方法抛出异常时，返回的就是Nothing类型，因为Nothing是所有类的子类，那么它可以赋值为任何类型 noting 例子: 1234567def main(args: Array[String]): Unit = &#123; val c = m3(1,0)&#125;def m3(x:Int, y:Int):Int = &#123; if(y == 0) throw new Exception("这是一个异常") x / y&#125; 1val b:Int = null 会报错 null不属于Int的子类 六 条件,循环表达式1 if语句语法与java一样 不一样的是: 在scala中，条件表达式也是有返回值的在scala中，没有三元表达式，可以使用if表达式替代三元表达式 12val sex = "male"val result = if(sex == "male") 1 else 0 2 块表达式scala中，使用{}表示一个块表达式和if表达式一样，块表达式也是有值的值就是最后一个表达式的值 12345var a=&#123;println("55")1+1&#125; 输出结果微博2 3 循环语法: 123for(i &lt;- 表达式/数组/集合) &#123;// 表达式&#125; 循环打印1到10 123456var nums=1.to(10)for(i &lt;- nums) &#123;println(i)&#125;或者:for(i &lt;- 1 to 10)println(i) 嵌套循环: 123456for (i &lt;- 1 to 10;j &lt;- 1 to 3)&#123;print("*");if(j==3)&#123;println()&#125;&#125; 守卫: for表达式中，可以添加if判断语句，这个if判断就称之为守卫。我们可以使用守卫让for表达式更简洁。 语法: 123for(i &lt;- 表达式/数组/集合 if 表达式) &#123;// 表达式&#125; 12// 添加守卫，打印能够整除3的数字for(i &lt;- 1 to 10; if i % 3 == 0) println(i) for推导式: 将来可以使用for推导式生成一个新的集合（一组数据）在for循环体中，可以使用yield表达式构建出一个集合，我们把使用yield的for表达式称之为推导式 生成一个10,20…..100的集合 123// for推导式：for表达式中以yield开始，该for表达式会构建出一个集合val v = for(i &lt;- 1 to 10) yield i * 10print(v) while循环 语法与java中一致 打印1到10 123456scala&gt; var i = 1i: Int = 1scala&gt; while(i &lt;= 10) &#123;| println(i)| i = i+1| &#125; 七 方法一个类可以有自己的方法，scala中的方法和Java方法类似。但scala与Java定义方法的语法是不一样的，而且scala支持多种调用方式。 1 定义方法123def add(参数名:参数类型,参数名:参数类型): [返回值类型 return type] =&#123;方法体&#125; 参数列表的参数类型不能省略返回值类型可以省略返回值可以不写return，默认就是{}块表达式的值 定方法:实现两数相加: 12345def add(a:Int,b:Int):Int =&#123;a+b&#125;调用 add(1,2) 2 方法参数:scala中的方法参数，使用比较灵活。它支持以下几种类型的参数：默认参数带名参数变长参数 默认参数 在定义方法时可以给参数定义一个默认值。 1def add(a:Int =1,b:Int = 2): Int = a+b 带名参数 在调用方法时，可以指定参数的名称来进行调用。 1234def add(a:Int =1,b:Int = 2): Int = a+b调用时:add(a=2) 变长参数 如果方法的参数是不固定的，可以定义一个方法的参数是变长参数。 12def add (num:Int*)=num.sumadd(1,2,3) 在参数类型后面加一个 * 号，表示参数可以是0个或者多个 3 方法返回值类型推断scala定义方法可以省略返回值，由scala自动推断返回值类型。这样方法定义后更加简洁。 1def add(x:Int, y:Int) = x + y 定义递归返回，不能省略返回值 12 4 方法调用方式在scala中，有以下几种方法调用方式， 后缀调用法 中缀调用法 花括号调用法 无括号调用法在spark、flink程序时，使用到这些方法。后缀调用: 12对象名.方法名(参数)Math.abs(-1) 中缀调用 12对象名 方法名 参数 若有多个参数用括号Math abs -1 操作符就是方法 11 + 1 与中缀 在scala中，+ - * / %等这些操作符和Java一样，但在scala中，所有的操作符都是方法操作符是一个方法名字是符号的方法 花括号调用发 语法 1234Math.abs&#123;// 表达式1// 表达式2 最后一个表达式 的是传入abs 的参数&#125; 123Math.abs&#123;print("nn")-10&#125; 无括号调用发 如果方法没有参数，可以省略方法名后面的括号 示例定义一个无参数的方法，打印”hello”使用无括号调用法调用该方法参考代码 1def m3()=println("hello") 八 函数scala支持函数式编程，将来编写Spark/Flink程序中，会大量经常使用到函数 语法: 1234567val 函数变量名 = (参数名:参数类型, 参数名:参数类型....) =&gt; 函数体注意:函数是一个对象（变量）类似于方法，函数也有输入参数和返回值函数定义不需要使用 def 定义无需指定返回值类型 1234scala&gt; val add = (x:Int, y:Int) =&gt; x + yadd: (Int, Int) =&gt; Int = &lt;function2&gt;scala&gt; add(1,2)res3: Int = 3 123456方法和函数的区别方法是隶属于类或者对象的，在运行时，它是加载到JVM的方法区中可以将函数对象赋值给一个变量，在运行时，它是加载到JVM的堆内存中函数是一个对象，继承自FunctionN，函数对象有apply，curried，toString，tupled这些方法。方法则没有 方法转换为函数有时候需要将方法转换为函数，作为变量传递，就需要将方法转换为函数使用 _ 即可将方法转换为函数 12345scala&gt; def add(x:Int,y:Int)=x+yadd: (x: Int, y: Int)Intscala&gt; val a = add _a: (Int, Int) =&gt; Int = &lt;function2&gt;43 九数组scala中数组的概念是和Java类似，可以用数组来存放一组数据。scala中，有两种数组，一种是定长数组，另一种是变长数组 定长数组定长数组指的是数组的长度是不允许改变的 1234567// 通过指定长度定义数组val/var 变量名 = new Array[元素类型](数组长度)// 用元素直接初始化数组val/var 变量名 = Array(元素1, 元素2, 元素3...)在scala中，数组的泛型使用 [] 来指定使用 () 来获取元素 12345678scala&gt; val a = new Array[Int](100)a: Array[Int] = Array(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)scala&gt; a(0) = 110scala&gt; println(a(0))110 12345// 定义包含jave、scala、python三个元素的数组scala&gt; val a = Array("java", "scala", "python")a: Array[String] = Array(java, scala, python)scala&gt; a.lengthres17: Int = 3 变长数组变长数组指的是数组的长度是可变的，可以往数组中添加、删除元素 定义变长数组语法创建空的ArrayBuffer变长数组，语法结构： 1val/var a = ArrayBuffer[元素类型]() 创建带有初始元素的ArrayBuffer 1val/var a = ArrayBuffer(元素1，元素2，元素3....) 注意: 1创建变长数组，需要提前导入ArrayBuffer类 import scala.collection.mutable.ArrayBuffer 1val a = ArrayBuffer[Int]() 长度为0 12scala&gt; val a = ArrayBuffer("hadoop", "storm", "spark")a: scala.collection.mutable.ArrayBuffer[String] = ArrayBuffer(hadoop, storm, spark) 添加/修改/删除元素使用 += 添加元素使用 -= 删除元素使用 ++= 追加一个数组到变长数组示例 定义一个变长数组，包含以下元素: “hadoop”, “spark”, “flink” 往该变长数组添加一个”flume”元素 从该变长数组删除”hadoop”元素 再将一个数组，该数组包含”hive”, “sqoop”追加到变长数组中 123456789101112// 定义变长数组scala&gt; val a = ArrayBuffer("hadoop", "spark", "flink")a: scala.collection.mutable.ArrayBuffer[String] = ArrayBuffer(hadoop, spark, flink)// 追加一个元素scala&gt; a += "flume"res10: a.type = ArrayBuffer(hadoop, spark, flink, flume)// 删除一个元素scala&gt; a -= "hadoop"res11: a.type = ArrayBuffer(spark, flink, flume)// 追加一个数组scala&gt; a ++= Array("hive", "sqoop")res12: a.type = ArrayBuffer(spark, flink, flume, hive, sqoop) 遍历数组可以使用以下两种方式来遍历数组：使用 for表达式 直接遍历数组中的元素使用 索引 遍历数组中的元素 12345678scala&gt; val a = Array(1,2,3,4,5)a: Array[Int] = Array(1, 2, 3, 4, 5)scala&gt; for(i&lt;-a) println(i)12345 1234567891011121314scala&gt; val a = Array(1,2,3,4,5)a: Array[Int] = Array(1, 2, 3, 4, 5)scala&gt; for(i &lt;- 0 to a.length - 1) println(a(i))12345scala&gt; for(i &lt;- 0 until a.length) println(a(i))12345 注意: 120 until n——生成一系列的数字，包含0，不包含n0 to n ——包含0，也包含n 数组常用算法:scala中的数组封装了丰富的计算操作，将来在对数据处理的时候，不需要我们自己再重新实现。以下为常用的几个算法：求和——sum方法求最大值——max方法求最小值——min方法排序——sorted方法 1234567scala&gt; val a = Array(1,2,3,4)a: Array[Int] = Array(1, 2, 3, 4)求和 a.sum最大值 a.max最小值 a.min排序 a.sorted降序 a.sorted.reverse 十 元组元组元组可以用来包含一组不同类型的值。例如：姓名，年龄，性别，出生年月。元组的元素是不可变的。 语法: 1234使用括号来定义元组val/var 元组 = (元素1, 元素2, 元素3....)使用尽头来定义元素（元组只有两个元素）val/var 元组 = 元素1-&gt;元素2 1234567例子// 可以直接使用括号来定义一个元组scala&gt; val a = (1, "张三", 20, "北京市")a: (Int, String, Int, String) = (1,张三,20,北京市)例子二scala&gt; val a = 1-&gt;2a: (Int, Int) = (1,2) 访问元组使用_1,2,3…来访问元祖中的元素，1表示访问第一个元素，一次类推 123456789101112131415// 可以直接使用括号来定义一个元组scala&gt; val a = (1, "张三", 20, "北京市")a: (Int, String, Int, String) = (1,张三,20,北京市)// 获取第一个元素scala&gt; a._1res57: Int = 1// 获取第二个元素scala&gt; a._2res58: String = 张三// 不能修改元组中的值scala&gt; a._1 = 2&lt;console&gt;:13: error: reassignment to vala._1 = 2^54 十一列表列表List是scala中最重要的、也是最常用的数据结构。List具备以下性质：可以保存重复的值有先后顺序在scala中，也有两种列表，一种是不可变列表、另一种是可变列表 1 不可变列表123456789不可变列表就是列表的元素、长度都是不可变的。语法使用 List(元素1, 元素2, 元素3, ...) 来创建一个不可变列表，语法格式：val/var 变量名 = List(元素1, 元素2, 元素3...)使用 Nil 创建一个不可变的空列表val/var 变量名 = Nil使用::拼接方式来创建列表，必须在最后添加一个Nil使用 :: 方法创建一个不可变列表val/var 变量名 = 元素1 :: 元素2 :: Nil 例子: 123456789101112131415示例一创建一个不可变列表，存放以下几个元素（1,2,3,4）参考代码scala&gt; val a = List(1,2,3,4)a: List[Int] = List(1, 2, 3, 4)示例二使用Nil创建一个不可变的空列表参考代码scala&gt; val a = Nila: scala.collection.immutable.Nil.type = List()示例三使用 :: 方法创建列表，包含-2、-1两个元素参考代码scala&gt; val a = -2 :: -1 :: Nila: List[Int] = List(-2, -1) 2 可变列表可变列表就是列表的元素、长度都是可变的。要使用可变列表，先要导入 import scala.collection.mutable.ListBuffer 注意: 可变集合都在 mutable 包中不可变集合都在 immutable 包中（默认导入） 123456初始化列表语法使用ListBuffer[元素类型]()创建空的可变列表，语法结构：val/var 变量名 = ListBuffer[Int]()使用ListBuffer(元素1, 元素2, 元素3...)创建可变列表，语法结构：val/var 变量名 = ListBuffer(元素1，元素2，元素3...) 123456例子:scala&gt; val a = ListBuffer[Int]()a: scala.collection.mutable.ListBuffer[Int] = ListBuffer()例子2:scala&gt; val a = ListBuffer(1,2,3,4)a: scala.collection.mutable.ListBuffer[Int] = ListBuffer(1, 2, 3, 4) 列表操作获取元素（使用括号访问 (索引值) ）添加元素（ += ）追加一个列表（ ++= ）更改元素（ 使用括号获取元素，然后进行赋值 ）删除元素（ -= ）转换为List（ toList ）转换为Array（ toArray ） 123456789101112131415161718192021222324// 导入不可变列表scala&gt; import scala.collection.mutable.ListBufferimport scala.collection.mutable.ListBuffer// 创建不可变列表scala&gt; val a = ListBuffer(1,2,3)a: scala.collection.mutable.ListBuffer[Int] = ListBuffer(1, 2, 3)// 获取第一个元素scala&gt; a(0)res19: Int = 1// 追加一个元素scala&gt; a += 4res20: a.type = ListBuffer(1, 2, 3, 4)// 追加一个列表scala&gt; a ++= List(5,6,7)res21: a.type = ListBuffer(1, 2, 3, 4, 5, 6, 7)// 删除元素scala&gt; a -= 7res22: a.type = ListBuffer(1, 2, 3, 4, 5, 6)// 转换为不可变列表scala&gt; a.toListres23: List[Int] = List(1, 2, 3, 4, 5, 6)// 转换为数组scala&gt; a.toArrayres24: Array[Int] = Array(1, 2, 3, 4, 5, 6 列表操作 以下是列表常用的操作判断列表是否为空（ isEmpty ）拼接两个列表（ ++ ）获取列表的首个元素（ head ）和剩余部分( tail )反转列表（ reverse ）获取前缀（ take ）、获取后缀（ drop ）扁平化（ flaten ） ​ 扁平化表示将列表中的列表中的所有元素放到一个列表中。 拉链（ zip ）和拉开（ unzip ）转换字符串（ toString ）生成字符串（ mkString ）并集（ union ）交集（ intersect ）差集（ diff ） 例子: 1234567a.isEmpty a ++ b a.head //获取列表的首个元素（ head ）和剩余部分( tail ) a.tail a.reverse a.take(3) //获取前缀（ take ）、获取后缀（ drop ） a.drop(3) 123456扁平化表示将列表中的列表中的所有元素放到一个列表中。scala&gt; val a = List(List(1,2), List(3), List(4,5))a: List[List[Int]] = List(List(1, 2), List(3), List(4, 5))scala&gt; a.flattenres0: List[Int] = List(1, 2, 3, 4, 5) 123456789101112131415拉链：使用zip将两个列表，组合成一个元素为元组的列表拉开：将一个包含元组的列表，解开成包含两个列表的元组scala&gt; val a = List("zhangsan", "lisi", "wangwu")a: List[String] = List(zhangsan, lisi, wangwu)scala&gt; val b = List(19, 20, 21)b: List[Int] = List(19, 20, 21)scala&gt; a.zip(b)res1: List[(String, Int)] = List((zhangsan,19), (lisi,20), (wangwu,21))拉开scala&gt; res1.unzipres2: (List[String], List[Int]) = (List(zhangsan, lisi, wangwu),List(19, 20, 21)) 12345转换字符串（ toString ）生成字符串（ mkString ）a.toStringa.mkString a.mkString(":") 1234并集union表示对两个列表取并集，不去重 a1.union(a2) a1.union(a2).distinct 去重 12345678910交集intersect表示对两个列表取交集scala&gt; val a1 = List(1,2,3,4)a1: List[Int] = List(1, 2, 3, 4)scala&gt; val a2 = List(3,4,5,6)a2: List[Int] = List(3, 4, 5, 6)scala&gt; a1.intersect(a2)res19: List[Int] = List(3, 4) 12345678910差集diff表示对两个列表取差集，例如： a1.diff(a2)，表示获取a1在a2中不存在的元素scala&gt; val a1 = List(1,2,3,4)a1: List[Int] = List(1, 2, 3, 4)scala&gt; val a2 = List(3,4,5,6)a2: List[Int] = List(3, 4, 5, 6)scala&gt; a1.diff(a2)res24: List[Int] = List(1, 2) 集 setSet(集)是代表没有重复元素的集合。Set具备以下性质： 元素不重复 不保证插入顺序 scala中的集也分为两种，一种是不可变集，另一种是可变集。 不可变集语法: 1val/var 变量名 = Set[类型]() ​ 给定元素来创建一个不可变集，语法格式： 1val/var 变量名 = Set(元素1, 元素2, 元素3...) 操作: 12345获取集的大小（size） a.size遍历集（和遍历数组一致） for(i &lt;- a) println(i)添加一个元素，生成一个Set（+） a - 1 不是下标 拼接两个集，生成一个Set（++） a ++ Set(6,7,8)拼接集和列表，生成一个Set（++） a ++ List(6,7,8,9) 可变集可变集合不可变集的创建方式一致，只不过需要提前导入一个可变集类。 1import scala.collection.mutable.Set 12345678910scala&gt; val a = Set(1,2,3,4)a: scala.collection.mutable.Set[Int] = Set(1, 2, 3, 4) // 添加元素scala&gt; a += 5res25: a.type = Set(1, 5, 2, 3, 4)// 删除元素scala&gt; a -= 1res26: a.type = Set(5, 2, 3, 4) 映射Map可以称之为映射。它是由键值对组成的集合。在scala中，Map也分为不可变Map和可变Map。 不可变Map语法: 1234567val/var map = Map(键-&gt;值, 键-&gt;值, 键-&gt;值...) // 推荐，可读性更好val/var map = Map((键, 值), (键, 值), (键, 值), (键, 值)...)例val map = Map("zhangsan"-&gt;30, "lisi"-&gt;40)val map = Map(("zhangsan", 30), ("lisi", 30))map("zhangsan")// 根据key获取value 可变Map定义语法与不可变Map一致。但定义可变Map需要手动导入import scala.collection.mutable.Map 1234val map = Map("zhangsan"-&gt;30, "lisi"-&gt;40)// 修改valuescala&gt; map("zhangsan") = 20 interator 迭代器scala针对每一类集合都提供了一个迭代器（iterator）用来迭代访问集合 使用迭代器遍历集合 使用iterator方法可以从集合获取一个迭代器 迭代器的两个基本操作 hasNext——查询容器中是否有下一个元素 next——返回迭代器的下一个元素，如果没有，抛出NoSuchElementException 每一个迭代器都是有状态的 迭代完后保留在最后一个元素的位置 再次使用则抛出NoSuchElementException 可以使用while或者for来逐个返回元素 例子: 1234val ite = a.iterator while(ite.hasNext) &#123; | println(ite.next) | &#125; 函数式编程我们将来使用Spark/Flink的大量业务代码都会使用到函数式编程。下面的这些操作是学习的重点。 遍历（foreach） 映射（map） 映射扁平化（flatmap） 过滤（filter） 是否存在（exists） 排序（sorted、sortBy、sortWith） 分组（groupBy） 聚合计算（reduce） 折叠（fold） 可以使用类型推断简化函数定义 scala可以自动来推断出来集合中每个元素参数的类型 创建函数时，可以省略其参数列表的类型 使用下划线简化函数定义 ​ 函数参数，只在函数体中出现一次，而且函数体没有嵌套调用时，可以使用下划线来简化函数定义 ​ 如果方法参数是函数，如果出现了下划线，scala编译器会自动将代码封装到一个函数中 ​ 参数列表也是由scala编译器自动处理 1 遍历1234567val a = List(1,2,3,4)a.foreach((x:Int)=&gt;println(x))使用类型推断简化a.foreach(x=&gt;println(x))使用下划线简化a.foreach(println(_)) 2 映射集合的映射操作是将来在编写Spark/Flink用得最多的操作，是我们必须要掌握的。因为进行数据计算的时候，就是一个将一种数据类型转换为另外一种数据类型的过程。 map方法接收一个函数，将这个函数应用到每一个元素，返回一个新的列表 用法: 1def map[B](f: (A) ⇒ B): TraversableOnce[B] map方法 API 说明 泛型 [B] 指定map方法最终返回的集合泛型 参数 f: (A) ⇒ B 传入一个函数对象 该函数接收一个类型A（要转换的列表元素），返回值为类型B 返回值 TraversableOnce[B] B类型的集合 例: 123var a=List(1,2,3,4)scala&gt; a.map(x=&gt;x+1)res4: List[Int] = List(2, 3, 4, 5) 12val a = List(1,2,3,4)a.map(_ + 1) 3 扁平化映射扁平化映射也是将来用得非常多的操作，也是必须要掌握的。 定义 123可以把flatMap，理解为先map，然后再flattenmap是将列表中的元素转换为一个Listflatten再将整个列表进行扁平化 方法签名: 1def flatMap[B](f: (A) ⇒ GenTraversableOnce[B]): TraversableOnce[B] flatmap方法 API 说明 泛型 [B] 最终要转换的集合元素类型 参数 f: (A) ⇒ GenTraversableOnce[B] 传入一个函数对象 函数的参数是集合的元素 函数的返回值是一个集合 返回值 TraversableOnce[B] B类型的集合 案例说明 有一个包含了若干个文本行的列表：”hadoop hive spark flink flume”, “kudu hbase sqoop storm” 获取到文本行中的每一个单词，并将每一个单词都放到列表中 1234567891011// 定义文本行列表scala&gt; val a = List("hadoop hive spark flink flume", "kudu hbase sqoop storm")a: List[String] = List(hadoop hive spark flink flume, kudu hbase sqoop storm)// 使用map将文本行转换为单词数组scala&gt; a.map(x=&gt;x.split(" "))res5: List[Array[String]] = List(Array(hadoop, hive, spark, flink, flume), Array(kudu, hbase, sqoop, storm))// 扁平化，将数组中的scala&gt; a.map(x=&gt;x.split(" ")).flattenres6: List[String] = List(hadoop, hive, spark, flink, flume, kudu, hbase, sqoop, storm) 使用flatMap简化操作 12345scala&gt; val a = List("hadoop hive spark flink flume", "kudu hbase sqoop storm")a: List[String] = List(hadoop hive spark flink flume, kudu hbase sqoop storm)scala&gt; a.flatMap(_.split(" "))res7: List[String] = List(hadoop, hive, spark, flink, flume, kudu, hbase, sqoop, storm) 4 过滤过滤符合一定条件的元素 1def filter(p: (A) ⇒ Boolean): TraversableOnce[A] filter方法 API 说明 参数 p: (A) ⇒ Boolean 传入一个函数对象 接收一个集合类型的参数 返回布尔类型，满足条件返回true, 不满足返回false 返回值 TraversableOnce[A] 列表 12//过滤偶数 结果得到的权威偶数List(1,2,3,4,5,6,7,8,9).filter(_ % 2 == 0) 5 排序在scala集合中，可以使用以下几种方式来进行排序 sorted默认排序 sortBy指定字段排序 sortWith自定义排序 12//默认排序List(3,1,2,9,7).sorted 指定字段排序 1def sortBy[B](f: (A) ⇒ B): List[A] sortBy方法 API 说明 泛型 [B] 按照什么类型来进行排序 参数 f: (A) ⇒ B 传入函数对象 接收一个集合类型的元素参数 返回B类型的元素进行排序 返回值 List[A] 返回排序后的列表 123val a = List("01 hadoop", "02 flume", "03 hive", "04 spark") a.sortBy(_.split(" ")(1))res8: List[String] = List(02 flume, 01 hadoop, 03 hive, 04 spark) //按照单词字母进行排序 自定义排序自定义排序，根据一个函数来进行自定义排序 方法签名 1def sortWith(lt: (A, A) ⇒ Boolean): List[A] sortWith方法 API 说明 参数 lt: (A, A) ⇒ Boolean 传入一个比较大小的函数对象 接收两个集合类型的元素参数 返回两个元素大小，小于返回true，大于返回false 返回值 List[A] 返回排序后的列表 12345678scala&gt; val a = List(2,3,1,6,4,5)a: List[Int] = List(2, 3, 1, 6, 4, 5)scala&gt; a.sortWith((x,y) =&gt; if(x&lt;y)true else false) res15: List[Int] = List(1, 2, 3, 4, 5, 6)scala&gt; res15.reverseres18: List[Int] = List(6, 5, 4, 3, 2, 1) 使用下划线简写上述案例 123456scala&gt; val a = List(2,3,1,6,4,5)a: List[Int] = List(2, 3, 1, 6, 4, 5)// 函数参数只在函数中出现一次，可以使用下划线代替scala&gt; a.sortWith(_ &lt; _).reverseres19: List[Int] = List(6, 5, 4, 3, 2, 1) 分组我们如果要将数据按照分组来进行统计分析，就需要使用到分组方法 groupBy表示按照函数将列表分成不同的组 12//方法签名def groupBy[K](f: (A) ⇒ K): Map[K, List[A]] groupBy方法 API 说明 泛型 [K] 分组字段的类型 参数 f: (A) ⇒ K 传入一个函数对象 接收集合元素类型的参数 返回一个K类型的key，这个key会用来进行分组，相同的key放在一组中 返回值 Map[K, List[A]] 返回一个映射，K为分组字段，List为这个分组字段对应的一组数据 有一个列表，包含了学生的姓名和性别: 123"张三", "男""李四", "女""王五", "男" 请按照性别进行分组，统计不同性别的学生人数 1234567891011scala&gt; val a = List("张三"-&gt;"男", "李四"-&gt;"女", "王五"-&gt;"男")a: List[(String, String)] = List((张三,男), (李四,女), (王五,男))// 按照性别分组scala&gt; a.groupBy(_._2)res0: scala.collection.immutable.Map[String,List[(String, String)]] = Map(男 -&gt; List((张三,男), (王五,男)),女 -&gt; List((李四,女)))// 将分组后的映射转换为性别/人数元组列表scala&gt; res0.map(x =&gt; x._1 -&gt; x._2.size)res3: scala.collection.immutable.Map[String,Int] = Map(男 -&gt; 2, 女 -&gt; 1) 聚合聚合操作，可以将一个列表中的数据合并为一个。这种操作经常用来统计分析中 reduce表示将列表，传入一个函数进行聚合计算 方法签名 1def reduce[A1 &gt;: A](op: (A1, A1) ⇒ A1): A1 reduce方法 API 说明 泛型 [A1 &gt;: A] （下界）A1必须是集合元素类型的父类 参数 op: (A1, A1) ⇒ A1 传入函数对象，用来不断进行聚合操作 第一个A1类型参数为：当前聚合后的变量 第二个A1类型参数为：当前要进行聚合的元素 返回值 A1 列表最终聚合为一个元素 注意: reduce和reduceLeft效果一致，表示从左到右计算 reduceRight表示从右到左计算 定义一个列表，包含以下元素：1,2,3,4,5,6,7,8,9,10 使用reduce计算所有元素的和 123456789101112131415161718scala&gt; val a = List(1,2,3,4,5,6,7,8,9,10)a: List[Int] = List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)scala&gt; a.reduce((x,y) =&gt; x + y)res5: Int = 55// 第一个下划线表示第一个参数，就是历史的聚合数据结果// 第二个下划线表示第二个参数，就是当前要聚合的数据元素scala&gt; a.reduce(_ + _)res53: Int = 55// 与reduce一样，从左往右计算scala&gt; a.reduceLeft(_ + _)res0: Int = 55// 从右往左聚合计算scala&gt; a.reduceRight(_ + _)res1: Int = 55 折叠fold与reduce很像，但是多了一个指定初始值参数 方法签名 1def fold[A1 &gt;: A](z: A1)(op: (A1, A1) ⇒ A1): A1 reduce方法 API 说明 泛型 [A1 &gt;: A] （下界）A1必须是集合元素类型的父类 参数1 z: A1 初始值 参数2 op: (A1, A1) ⇒ A1 传入函数对象，用来不断进行折叠操作 第一个A1类型参数为：当前折叠后的变量 第二个A1类型参数为：当前要进行折叠的元素 返回值 A1 列表最终折叠为一个元素 注意: fold和foldLet效果一致，表示从左往右计算 foldRight表示从右往左计算 定义一个列表，包含以下元素：1,2,3,4,5,6,7,8,9,10 使用fold方法计算所有元素的和 12345scala&gt; val a = List(1,2,3,4,5,6,7,8,9,10)a: List[Int] = List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)scala&gt; a.fold(0)(_ + _)res4: Int = 155]]></content>
      <tags>
        <tag>BigData</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Storm]]></title>
    <url>%2F2017%2F08%2F09%2FStorm.html</url>
    <content type="text"><![CDATA[离线处理与实时处理离线处理方面Hadoop提供了很好的解决方案，但是针对海量数据的实时处理却一直没有比较好的解决方案 1.1 实现实时计算系统需要解决那些问题如果让我们自己设计一个实时计算系统，我们要解决哪些问题。 （1）低延迟：都说了是实时计算系统了，延迟是一定要低的。 （2）高性能：性能不高就是浪费机器，浪费机器是要受批评的哦。 （3）分布式：系统都是为应用场景而生的，如果你的应用场景、你的数据和计算单机就能搞定，那么不用考虑这些复杂的问题了。我们所说的是单机搞不定的情况。 （4）可扩展：伴随着业务的发展，我们的数据量、计算量可能会越来越大，所以希望这个系统是可扩展的。 （5）容错：这是分布式系统中通用问题。一个节点挂了不能影响我的应用。 （6）通信：设计的系统需要应用程序开发人员考虑各个处理组件的分布、消息的传递吗？如果是，发人员可能会用不好，也不会想去用。 （7）消息不丢失：用户发布的一个宝贝消息不能在实时处理的时候给丢了，对吧？ 1.1 离线计算是什么离线计算：批量获取数据、批量传输数据、周期性批量计算数据、数据展示 ​ 代表技术：Sqoop批量导入数据、HDFS批量存储数据、MapReduce批量计算数据、Hive批量计算数据、任务调度 日常业务： 1，hivesql 2、调度平台 3、Hadoop集群运维 4、数据清洗（脚本语言） 5、元数据管理 6、数据稽查 7、数据仓库模型架构 流式计算是什么​ 流式计算：数据实时产生、数据实时传输、数据实时计算、实时展示 ​ 代表技术：Flume实时获取数据、Kafka/metaq实时数据存储、Storm/JStorm实时数据计算、Redis实时结果缓存、持久化存储(mysql)。 ​ 一句话总结：将源源不断产生的数据实时收集并实时计算，尽可能快的得到计算结果，用来支持决策。 离线计算与实时计算的区别最大的区别：实时收集、实时计算、实时展示 离线计算，一次计算很多条数据 实时计算，数据被一条一条的计算 一概述Apache Strom 流式计算框架 Hadoop处理数据时效性不够,Strom能够尽快得到处理后的数据 Strom只负责数据计算,不负责数据存储 一般是kafka的消费者 然后把数据存入Redis 用处 1日志分析，从海量日志中分析出特定的数据，并将分析的结果存入外部存储器用来辅佐决策。 2管道系统， 将一个数据从一个系统传输到另外一个系统， 比如将数据库同步到Hadoop 3消息转化器， 将接受到的消息按照某种格式进行转化，存储到另外一个系统如消息中间件 4统计分析器， 从日志或消息中，提炼出某个字段，然后做count或sum计算，最后将统计值存入外部存储器。中间处理过程可能更复杂。 二 架构1主从架构 Nimbus：负责资源分配和任务调度。 Supervisor：负责接受nimbus分配的任务，启动和停止属于自己管理的worker进程。 Worker：运行具体处理组件逻辑的进程。 Task：worker中每一个spout/bolt的线程称为一个task. 在storm0.8之后，task不再与物理线程对应，同一个spout/bolt的task可能会共享一个物理线程，该线程称为executor。 2编程模型 Topology：Storm中运行的一个实时应用程序，因为各个组件间的消息流动形成逻辑上的一个拓扑结构。 Spout：在一个topology中产生源数据流的组件。通常情况下spout会从外部数据源(kafaka)中读取数据，然后转换为topology内部的源数据。Spout是一个主动的角色，其接口中有个nextTuple()函数，storm框架会不停地调用此函数，用户只要在其中生成源数据即可。 Bolt：在一个topology中接受数据然后执行处理的组件。Bolt可以执行过滤、函数操作、合并、写数据库等任何操作。Bolt是一个被动的角色，其接口中有个execute(Tuple input)函数,在接受到消息后会调用此函数，用户可以在其中执行自己想要的操作。 Tuple：一次消息传递的基本单元。本来应该是一个key-value的map，但是由于各个组件间传递的tuple的字段名称已经事先定义好，所以tuple中只要按序填入各个value就行了，所以就是一个value list. Stream：源源不断传递的tuple就组成了stream。 3分组策略 Stream grouping：即消息的partition方法。 Stream Grouping定义了一个流在Bolt任务间该如何被切分。这里有Storm提供的6个Stream Grouping类型： \1. 随机分组(Shuffle grouping)：随机分发tuple到Bolt的任务，保证每个任务获得相等数量的tuple。 跨服务器通信，浪费网络资源，尽量不适用 \2. 字段分组(Fields grouping)：根据指定字段分割数据流，并分组。例如，根据“user-id”字段，相同“user-id”的元组总是分发到同一个任务，不同“user-id”的元组可能分发到不同的任务。 跨服务器，除非有必要，才使用这种方式。 \3. 全部分组(All grouping)：tuple被复制到bolt的所有任务。这种类型需要谨慎使用。 人手一份，完全不必要 \4. 全局分组(Global grouping)：全部流都分配到bolt的同一个任务。明确地说，是分配给ID最小的那个task。 欺负新人 \5. 无分组(None grouping)：你不需要关心流是如何分组。目前，无分组等效于随机分组。但最终，Storm将把无分组的Bolts放到Bolts或Spouts订阅它们的同一线程去执行(如果可能)。 \6. 直接分组(Direct grouping)：这是一个特别的分组类型。元组生产者决定tuple由哪个元组处理者任务接收。 点名分配 AckerBolt 消息容错 7.LocalOrShuffle 分组。 优先将数据发送到本地的Task，节约网络通信的资源。 使用storm 进行计算需求: ​ 单词统计 依赖 12345678910111213141516171819202122232425262728&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.storm&lt;/groupId&gt; &lt;artifactId&gt;storm-core&lt;/artifactId&gt; &lt;!-- apache storm 1.x | 2.x jstorm和storm合并版本--&gt; &lt;version&gt;1.1.1&lt;/version&gt; &lt;!-- 目前&lt;scope&gt;可以使用5个值： * compile，缺省值，适用于所有阶段，会随着项目一起发布。 * provided，类似compile，期望JDK、容器或使用者会提供这个依赖。如servlet.jar。 * runtime，只在运行时使用，如JDBC驱动，适用运行和测试阶段。 * test，只在测试时使用，用于编译和运行测试代码。不会随项目发布。 * system，类似provided，需要显式提供包含依赖的jar，Maven不会在Repository中查找它。 --&gt; &lt;!--&lt;scope&gt;provided&lt;/scope&gt;--&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.7.0&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; 驱动类 123456789101112131415161718192021222324252627282930313233import org.apache.storm.Config;import org.apache.storm.LocalCluster;import org.apache.storm.StormSubmitter;import org.apache.storm.generated.AlreadyAliveException;import org.apache.storm.generated.AuthorizationException;import org.apache.storm.generated.InvalidTopologyException;import org.apache.storm.topology.TopologyBuilder;/** * wordcount的驱动类，用来提交任务的。 */public class WordCountTopology &#123; public static void main(String[] args) throws InvalidTopologyException, AuthorizationException, AlreadyAliveException &#123; // 通过TopologyBuilder来封装任务信息 TopologyBuilder topologyBuilder = new TopologyBuilder();// 设置spout，获取数据 topologyBuilder.setSpout("readfilespout",new ReadFileSpout(),2);// 设置splitbolt，对句子进行切割 topologyBuilder.setBolt("splitbolt",new SplitBolt(),4).shuffleGrouping("readfilespout");// 设置wordcountbolt，对单词进行统计 topologyBuilder.setBolt("wordcountBolt",new WordCountBolt(),2).shuffleGrouping("splitbolt");// 准备一个配置文件 Config config = new Config();// storm中任务提交有两种方式，一种方式是本地模式，另一种是集群模式。// LocalCluster localCluster = new LocalCluster();// localCluster.submitTopology("wordcount",config,topologyBuilder.createTopology()); //在storm集群中，worker是用来分配的资源。如果一个程序没有指定worker数，那么就会使用默认值。 config.setNumWorkers(2); //提交到集群 StormSubmitter.submitTopology("wordcount1",config,topologyBuilder.createTopology()); &#125;&#125; 读取文件Spout 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859import org.apache.storm.spout.SpoutOutputCollector;import org.apache.storm.task.TopologyContext;import org.apache.storm.topology.OutputFieldsDeclarer;import org.apache.storm.topology.base.BaseRichSpout;import org.apache.storm.tuple.Fields;import java.io.*;import java.util.Arrays;import java.util.Map;/** * 读取外部的文件，将一行一行的数据发送给下游的bolt * 类似于hadoop MapReduce中的inputformat */public class ReadFileSpout extends BaseRichSpout &#123; private SpoutOutputCollector collector; private BufferedReader bufferedReader; /** * 初始化方法，类似于这个类的构造器，只被运行一次 * 一般用来打开数据连接，打开网络连接。 * * @param conf 传入的是storm集群的配置文件和用户自定义配置文件，一般不用。 * @param context 上下文对象，一般不用 * @param collector 数据输出的收集器，spout类将数据发送给collector，由collector发送给storm框架。 */ public void open(Map conf, TopologyContext context, SpoutOutputCollector collector) &#123; try &#123; bufferedReader = new BufferedReader(new FileReader(new File("//data//wordcount.txt"))); &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; this.collector = collector; &#125; /** * 下一个tuple，tuple是数据传送的基本单位。 * 后台有个while循环一直调用该方法，每调用一次，就发送一个tuple出去 */ public void nextTuple() &#123; String line = null; try &#123; line = bufferedReader.readLine(); if (line!=null)&#123; collector.emit(Arrays.asList(line)); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; /** * 声明发出的数据是什么 * * @param declarer */ public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; declarer.declare(new Fields("juzi")); &#125;&#125; SplitBolt 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051import org.apache.storm.task.OutputCollector;import org.apache.storm.task.TopologyContext;import org.apache.storm.topology.OutputFieldsDeclarer;import org.apache.storm.topology.base.BaseRichBolt;import org.apache.storm.tuple.Fields;import org.apache.storm.tuple.Tuple;import java.util.Arrays;import java.util.HashSet;import java.util.Map;/** * 输入：一行数据 * 计算：对一行数据进行切割 * 输出：单词及单词出现的次数 */public class SplitBolt extends BaseRichBolt&#123; private OutputCollector collector; /** * 初始化方法，只被运行一次。 * @param stormConf 配置文件 * @param context 上下文对象，一般不用 * @param collector 数据收集器 */ @Override public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) &#123; this.collector = collector; &#125; /** * 执行业务逻辑的方法 * @param input 获取上游的数据 */ @Override public void execute(Tuple input) &#123; // 获取上游的句子 String juzi = input.getStringByField("juzi"); // 对句子进行切割 String[] words = juzi.split(" "); // 发送数据 for (String word : words) &#123; // 需要发送单词及单词出现的次数，共两个字段 collector.emit(Arrays.asList(word,"1")); &#125; &#125; @Override public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; declarer.declare(new Fields("word","num")); &#125;&#125; wordcountBolt 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051import org.apache.storm.task.OutputCollector;import org.apache.storm.task.TopologyContext;import org.apache.storm.topology.OutputFieldsDeclarer;import org.apache.storm.topology.base.BaseRichBolt;import org.apache.storm.tuple.Tuple;import java.util.HashMap;import java.util.Map;/** * 输入：单词及单词出现的次数 * 输出：打印在控制台 * 负责统计每个单词出现的次数 * 类似于hadoop MapReduce中的reduce函数 */public class WordCountBolt extends BaseRichBolt &#123; private Map&lt;String, Integer&gt; wordCountMap = new HashMap&lt;String, Integer&gt;(); /** * 初始化方法 * * @param stormConf 集群及用户自定义的配置文件 * @param context 上下文对象 * @param collector 数据收集器 */ @Override public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) &#123; // 由于wordcount是最后一个bolt，所有不需要自定义OutputCollector collector，并赋值。 &#125; @Override public void execute(Tuple input) &#123; //获取单词出现的信息（单词、次数） String word = input.getStringByField("word"); String num = input.getStringByField("num"); // 定义map记录单词出现的次数 // 开始计数 if (wordCountMap.containsKey(word)) &#123; Integer integer = wordCountMap.get(word); wordCountMap.put(word, integer + Integer.parseInt(num)); &#125; else &#123; wordCountMap.put(word, Integer.parseInt(num)); &#125; // 打印整个map System.out.println(wordCountMap); &#125; @Override public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; // 不发送数据，所以不用实现。 &#125;&#125; 结果分析 三 集群安装1上传解压 修改配置文件 12345vim /etc/hosts192.168.140.128 node01 zk01 kafka01 storm01192.168.140.129 node02 zk02 kafka02 storm02192.168.140.130 node03 zk03 kafka03 storm03 mysql 修改 storm文件 1234567891011121314151617cd storm/confrm storm.yamlvim storm.yamlstorm.zookeeper.servers: - "zk01" - "zk02" - "zk03"nimbus.seeds: ["storm01", "storm02", "storm03"]storm.local.dir: "/export/data/storm" supervisor.slots.ports: - 6700 - 6701 - 6702 - 6703 然后把这个storm 发给其他节点 启动: 1234后台启动 cd bin主节点 nohup ./storm nimbus &amp;其他节点 nohup ./storm supervisor &amp;然后主节点 启动UI nohup ./storm ui &amp; 把以上代码提交到集群运行 修改代码 12在驱动类中 修改提交方式是提交的集群 在读取文件的勒种 修改文件位置为集群上的 运行 1storm jar jar包名 驱动类名(包韩路径信息)]]></content>
      <tags>
        <tag>BigData</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hbase增强]]></title>
    <url>%2F2017%2F07%2F18%2FHbase%E5%A2%9E%E5%BC%BA.html</url>
    <content type="text"><![CDATA[Hbase增强一 Hbase与MapReduce的集成HBase当中的数据最终都是存储在HDFS上面的，HBase天生的支持MR的操作，我们可以通过MR直接处理HBase当中的数据，并且MR可以将处理后的结果直接存储到HBase当中去 需求一 读取myuser这张表当中的数据写入到HBase的另外一张表当中去1 创建myuser2 表其中列簇名与myuser中列簇名一致 依赖: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485&lt;dependencies&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.hbase/hbase-client --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-client&lt;/artifactId&gt; &lt;version&gt;2.0.0&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.hbase/hbase-server --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-server&lt;/artifactId&gt; &lt;version&gt;2.0.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.testng&lt;/groupId&gt; &lt;artifactId&gt;testng&lt;/artifactId&gt; &lt;version&gt;6.14.3&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.hbase/hbase-mapreduce --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-mapreduce&lt;/artifactId&gt; &lt;version&gt;2.0.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;2.7.5&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt; &lt;version&gt; 2.7.5&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;2.7.5&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.0&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;!-- &lt;verbal&gt;true&lt;/verbal&gt;--&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;!--将我们其他用到的一些jar包全部都打包进来 --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt; &lt;version&gt;2.4.3&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;shade&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;minimizeJar&gt;false&lt;/minimizeJar&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; 定义mapper类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748/** * 负责读取myuser表当中的数据 * 如果mapper类需要读取hbase表数据，那么我们mapper类需要继承TableMapper这样的一个类 * 将key2 value2定义成 text 和put类型 * text里面装rowkey * put装我们需要插入的数据 */public class HBaseSourceMapper extends TableMapper&lt;Text,Put&gt; &#123; /** * * @param key rowkey * @param value result对象，封装了我们一条条的数据 * @param context 上下文对象 * @throws IOException * @throws InterruptedException * * 需求：读取myuser表当中f1列族下面的name和age列 * ImmutableBytesWritable 封装了rowkey */ @Override protected void map(ImmutableBytesWritable key, Result value, Context context) throws IOException, InterruptedException &#123; //获取到rowkey的字节数组 byte[] bytes = key.get(); String rowkey = Bytes.toString(bytes); Put put = new Put(bytes); //获取到所有的cell List&lt;Cell&gt; cells = value.listCells(); for (Cell cell : cells) &#123; //获取cell对应的列族 byte[] familyBytes = CellUtil.cloneFamily(cell); //获取对应的列 byte[] qualifierBytes = CellUtil.cloneQualifier(cell); //这里判断我们只需要f1列族，下面的name和age列 if(Bytes.toString(familyBytes).equals("f1") &amp;&amp; Bytes.toString(qualifierBytes).equals("name") || Bytes.toString(qualifierBytes).equals("age"))&#123; put.add(cell); &#125; &#125; //将数据写出去 if(!put.isEmpty())&#123; context.write(new Text(rowkey),put); &#125; &#125;&#125; 定义 reduce123456789101112131415161718192021import org.apache.hadoop.hbase.client.Put;import org.apache.hadoop.hbase.io.ImmutableBytesWritable;import org.apache.hadoop.hbase.mapreduce.TableReducer;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;import java.io.IOException;/** * 负责将数据写入到myuser2 * */public class HBaseSinkReducer extends TableReducer&lt;Text,Put,ImmutableBytesWritable&gt; &#123; @Override protected void reduce(Text key, Iterable&lt;Put&gt; values, Context context) throws IOException, InterruptedException &#123; for (Put put : values) &#123; context.write(new ImmutableBytesWritable(key.toString().getBytes()),put); &#125; &#125;&#125; 定义主类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.conf.Configured;import org.apache.hadoop.hbase.HBaseConfiguration;import org.apache.hadoop.hbase.client.Put;import org.apache.hadoop.hbase.client.Scan;import org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.util.Tool;import org.apache.hadoop.util.ToolRunner;import javax.swing.plaf.nimbus.AbstractRegionPainter;public class HBaseMain extends Configured implements Tool &#123; @Override public int run(String[] args) throws Exception &#123; Job job = Job.getInstance(super.getConf(), "hbaseMR"); //打包运行，必须设置main方法所在的主类 job.setJarByClass(HBaseMain.class); Scan scan = new Scan(); //定义我们的mapper类和reducer类 /** * String table, Scan scan, Class&lt;? extends TableMapper&gt; mapper, Class&lt;?&gt; outputKeyClass, Class&lt;?&gt; outputValueClass, Job job, boolean addDependencyJars */ TableMapReduceUtil.initTableMapperJob("myuser",scan,HBaseSourceMapper.class, Text.class, Put.class,job,false); //使用工具类初始化reducer类 TableMapReduceUtil.initTableReducerJob("myuser2",HBaseSinkReducer.class,job); boolean b = job.waitForCompletion(true); return b?0:1; &#125; //程序入口类 public static void main(String[] args) throws Exception &#123; //Configuration conf, Tool tool, String[] args Configuration configuration = HBaseConfiguration.create(); configuration.set("hbase.zookeeper.quorum","node01:2181,node02:2181,node03:2181"); int run = ToolRunner.run(configuration, new HBaseMain(), args); System.exit(run); &#125;&#125; 运行1 本地运行 直接选中main方法所在的类，运行即可 2 打包集群运行 注意，我们需要使用打包插件，将HBase的依赖jar包都打入到工程jar包里面去 pom.xml当中添加打包插件 12345678910111213141516&lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt; &lt;version&gt;2.4.3&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;shade&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;minimizeJar&gt;true&lt;/minimizeJar&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; 代码中添加: 1job.setJarByClass(HBaseMain.class); 使用maven打包 将jar包上传服务器:运行 1yarn jar hbaseStudy-1.0-SNAPSHOT.jar cn.baidu.hbasemr.HBaseMR 或者我们也可以自己设置我们的环境变量，然后运行original那个比较小的jar包 1234export HADOOP_HOME=/export/servers/hadoop-2.7.5/export HBASE_HOME=/export/servers/hbase-2.0.0/export HADOOP_CLASSPATH=$&#123;HBASE_HOME&#125;/bin/hbase mapredcpyarn jar original-hbaseStudy-1.0-SNAPSHOT.jar cn.baidu.hbasemr.HbaseMR 需求2 读取HDFS文件，写入到HBase表当中去读取hdfs路径/hbase/input/user.txt，然后将数据写入到myuser2这张表当中去 准备数据: 1234567hdfs dfs -mkdir -p /hbase/inputcd /export/servers/vim user.txt 0007 zhangsan 180008 lisi 250009 wangwu 20 上传hdfs: 1hdfs dfs -put user.txt /hbase/input 定义mapper1234567891011121314151617181920import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;import java.io.IOException;/** * 通过这个mapper读取hdfs上面的文件，然后进行处理 */public class HDFSMapper extends Mapper&lt;LongWritable,Text,Text,NullWritable&gt;&#123; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; //读取到数据之后不做任何处理，直接将数据写入到reduce里面去进行处理 context.write(value,NullWritable.get()); &#125;&#125; 定义reduce123456789101112131415161718192021222324252627282930313233343536import org.apache.hadoop.hbase.client.Put;import org.apache.hadoop.hbase.io.ImmutableBytesWritable;import org.apache.hadoop.hbase.mapreduce.TableReducer;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;import java.io.IOException;public class HBaseWriteReducer extends TableReducer&lt;Text,NullWritable,ImmutableBytesWritable&gt; &#123; /** * 0007 zhangsan 18 0008 lisi 25 0009 wangwu 20 * @param key * @param values * @param context * @throws IOException * @throws InterruptedException */ @Override protected void reduce(Text key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException, InterruptedException &#123; String[] split = key.toString().split("\t"); Put put = new Put(split[0].getBytes()); put.addColumn("f1".getBytes(),"name".getBytes(),split[1].getBytes()); put.addColumn("f1".getBytes(),"age".getBytes(),split[2].getBytes()); //将我们的数据写出去，key3是ImmutableBytesWritable，这个里面装的是rowkey //然后将写出去的数据封装到put对象里面去了 context.write(new ImmutableBytesWritable(split[0].getBytes()),put); &#125;&#125; 定义主类12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.conf.Configured;import org.apache.hadoop.fs.Path;import org.apache.hadoop.hbase.HBaseConfiguration;import org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;import org.apache.hadoop.util.Tool;import org.apache.hadoop.util.ToolRunner;import javax.swing.plaf.nimbus.AbstractRegionPainter;public class HdfsHBaseMain extends Configured implements Tool &#123; @Override public int run(String[] args) throws Exception &#123; //获取job对象 Job job = Job.getInstance(super.getConf(), "hdfs2Hbase"); //第一步：读取文件，解析成key，value对 job.setInputFormatClass(TextInputFormat.class); TextInputFormat.addInputPath(job,new Path("hdfs://node01:8020/hbase/input")); //第二步：自定义map逻辑，接受k1,v1，转换成为k2 v2进行输出 job.setMapperClass(HDFSMapper.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(NullWritable.class); //分区，排序，规约，分组 //第七步：设置reduce类 TableMapReduceUtil.initTableReducerJob("myuser2",HBaseWriteReducer.class,job); boolean b = job.waitForCompletion(true); return b?0:1; &#125; public static void main(String[] args) throws Exception &#123; Configuration configuration = HBaseConfiguration.create(); configuration.set("hbase.zookeeper.quorum","node01:2181,node02:2181,node03:2181"); int run = ToolRunner.run(configuration, new HdfsHBaseMain(), args); System.exit(run); &#125;&#125; 需求三 通过bulkload的方式批量加载数据到HBase当中去加载数据到HBase当中去的方式多种多样，我们可以使用HBase的javaAPI或者使用sqoop将我们的数据写入或者导入到HBase当中去，但是这些方式不是慢就是在导入的过程的占用Region资源导致效率低下，我们也可以通过MR的程序，将我们的数据直接转换成HBase的最终存储格式HFile，然后直接load数据到HBase当中去即可 HBase中每张Table在根目录（/HBase）下用一个文件夹存储，Table名为文件夹名，在Table文件夹下每个Region同样用一个文件夹存储，每个Region文件夹下的每个列族也用文件夹存储，而每个列族下存储的就是一些HFile文件，HFile就是HBase数据在HFDS下存储格式，所以HBase存储文件最终在hdfs上面的表现形式就是HFile，如果我们可以直接将数据转换为HFile的格式，那么我们的HBase就可以直接读取加载HFile格式的文件，就可以直接读取了 优点： 1231.导入过程不占用Region资源 2.能快速导入海量的数据3.节省内存 使用bulkload的方式将我们的数据直接生成HFile格式，然后直接加载到HBase的表当中去,不走hlog和hRegionServer. 例如: 将我们hdfs上面的这个路径/hbase/input/user.txt的数据文件，转换成HFile格式，然后load到myuser2这张表里面去 定义mapper123456789101112131415161718192021222324252627282930313233import org.apache.hadoop.hbase.client.Put;import org.apache.hadoop.hbase.io.ImmutableBytesWritable;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;import java.io.IOException;public class HDFSReadMapper extends Mapper&lt;LongWritable,Text,ImmutableBytesWritable,Put&gt;&#123; /** * 0007 zhangsan 18 0008 lisi 25 0009 wangwu 20 * @param key * @param value * @param context * @throws IOException * @throws InterruptedException */ @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String[] split = value.toString().split("\t"); Put put = new Put(split[0].getBytes()); put.addColumn("f1".getBytes(),"name".getBytes(),split[1].getBytes()); put.addColumn("f1".getBytes(),"age".getBytes(),split[2].getBytes()); context.write(new ImmutableBytesWritable(split[0].getBytes()),put); &#125;&#125; 主类 程序入口123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.conf.Configured;import org.apache.hadoop.fs.Path;import org.apache.hadoop.hbase.HBaseConfiguration;import org.apache.hadoop.hbase.TableName;import org.apache.hadoop.hbase.client.Connection;import org.apache.hadoop.hbase.client.ConnectionFactory;import org.apache.hadoop.hbase.client.Put;import org.apache.hadoop.hbase.client.Table;import org.apache.hadoop.hbase.io.ImmutableBytesWritable;import org.apache.hadoop.hbase.mapreduce.HFileOutputFormat2;import org.apache.hadoop.hdfs.DFSUtil;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;import org.apache.hadoop.util.Tool;import org.apache.hadoop.util.ToolRunner;public class BulkLoadMain extends Configured implements Tool &#123; @Override public int run(String[] args) throws Exception &#123; Configuration conf = super.getConf(); //获取job对象 Job job = Job.getInstance(conf, "bulkLoad"); Connection connection = ConnectionFactory.createConnection(conf); Table table = connection.getTable(TableName.valueOf("myuser2")); //读取文件 job.setInputFormatClass(TextInputFormat.class); TextInputFormat.addInputPath(job,new Path("hdfs://node01:8020/hbase/input")); job.setMapperClass(HDFSReadMapper.class); job.setMapOutputKeyClass(ImmutableBytesWritable.class); job.setMapOutputValueClass(Put.class); //将数据输出成为HFile格式 //Job job, Table table, RegionLocator regionLocator //配置增量的添加数据 HFileOutputFormat2.configureIncrementalLoad(job,table,connection.getRegionLocator(TableName.valueOf("myuser2"))); //设置输出classs类，决定了我们输出数据格式 job.setOutputFormatClass(HFileOutputFormat2.class); //设置输出路径 HFileOutputFormat2.setOutputPath(job,new Path("hdfs://node01:8020/hbase/hfile_out")); boolean b = job.waitForCompletion(true); return b?0:1; &#125; public static void main(String[] args) throws Exception &#123; Configuration configuration = HBaseConfiguration.create(); configuration.set("hbase.zookeeper.quorum","node01:2181,node02:2181,node03:2181"); int run = ToolRunner.run(configuration, new BulkLoadMain(), args); System.exit(run); &#125;&#125; 打jar包上传运行: 1yarn jar original-hbaseStudy-1.0-SNAPSHOT.jar cn.baidu.hbasemr.HBaseLoad 开发代码 加载数据将我们的输出路径下面的HFile文件，加载到我们的hbase表当中去 12345678910111213141516171819202122232425import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.hbase.HBaseConfiguration;import org.apache.hadoop.hbase.TableName;import org.apache.hadoop.hbase.client.Admin;import org.apache.hadoop.hbase.client.Connection;import org.apache.hadoop.hbase.client.ConnectionFactory;import org.apache.hadoop.hbase.client.Table;import org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles;public class LoadData &#123; public static void main(String[] args) throws Exception &#123; Configuration configuration = HBaseConfiguration.create(); configuration.set("hbase.zookeeper.property.clientPort", "2181"); configuration.set("hbase.zookeeper.quorum", "node01,node02,node03"); Connection connection = ConnectionFactory.createConnection(configuration); Admin admin = connection.getAdmin(); Table table = connection.getTable(TableName.valueOf("myuser2")); LoadIncrementalHFiles load = new LoadIncrementalHFiles(configuration); load.doBulkLoad(new Path("hdfs://node01:8020/hbase/hfile_out"), admin,table,connection.getRegionLocator(TableName.valueOf("myuser2"))); &#125;&#125; 或者我们也可以通过命令行来进行加载数据 先将hbase的jar包添加到hadoop的classpath路径下 123export HBASE_HOME=/export/servers/hbase-2.0.0/export HADOOP_HOME=/export/servers/hadoop-2.7.5/export HADOOP_CLASSPATH=$&#123;HBASE_HOME&#125;/bin/hbase mapredcp 然后执行以下命令，将hbase的HFile直接导入到表myuser2当中来 1yarn jar /export/servers/hbase-2.0.0/lib/hbase-server-1.2.0-cdh5.14.0.jar completebulkload /hbase/hfile_out myuser2 ##二 hive 与Hbase的对比 Hive数据仓库工具Hive的本质其实就相当于将HDFS中已经存储的文件在Mysql中做了一个双射关系，以方便使用HQL去管理查询。 用于数据分析、清洗Hive适用于离线的数据分析和清洗，延迟较高 基于HDFS、MapReduceHive存储的数据依旧在DataNode上，编写的HQL语句终将是转换为MapReduce代码执行。 HBasenosql数据库是一种面向列存储的非关系型数据库。 用于存储结构化和非结构话的数据适用于单表非关系型数据的存储，不适合做关联查询，类似JOIN等操作。 基于HDFS数据持久化存储的体现形式是Hfile，存放于DataNode中，被ResionServer以region的形式进行管理。 延迟较低，接入在线业务使用面对大量的企业数据，HBase可以直线单表大量数据的存储，同时提供了高效的数据访问速度。 总结：Hive与HBaseHive和Hbase是两种基于Hadoop的不同技术，Hive是一种类SQL的引擎，并且运行MapReduce任务，Hbase是一种在Hadoop之上的NoSQL 的Key/vale数据库。这两种工具是可以同时使用的。就像用Google来搜索，用FaceBook进行社交一样，Hive可以用来进行统计查询，HBase可以用来进行实时查询，数据也可以从Hive写到HBase，或者从HBase写回Hive。 ##三 hive 与hbase的整合 hive与我们的HBase各有千秋，各自有着不同的功能，但是归根接地，hive与hbase的数据最终都是存储在hdfs上面的，一般的我们为了存储磁盘的空间，不会将一份数据存储到多个地方，导致磁盘空间的浪费，我们可以直接将数据存入hbase，然后通过hive整合hbase直接使用sql语句分析hbase里面的数据即可，非常方便 需求一将hive分析结果的数据，保存到HBase当中去1 拷贝hbase的五个依赖jar包到hive的lib目录下将我们HBase的五个jar包拷贝到hive的lib目录下 hbase的jar包都在/export/servers/hbase-2.0.0/lib 我们需要拷贝五个jar包名字如下 hbase-client-2.0.0.jar hbase-hadoop2-compat-2.0.0.jar hbase-hadoop-compat-2.0.0.jar hbase-it-2.0.0.jar hbase-server-2.0.0.jar 我们直接在node03执行以下命令，通过创建软连接的方式来进行jar包的依赖 12345ln -s /export/servers/hbase-2.0.0/lib/hbase-client-2.0.0.jar /export/servers/apache-hive-2.1.0-bin/lib/hbase-client-2.0.0.jarln -s /export/servers/hbase-2.0.0/lib/hbase-hadoop2-compat-2.0.0.jar /export/servers/apache-hive-2.1.0-bin/lib/hbase-hadoop2-compat-2.0.0.jarln -s /export/servers/hbase-2.0.0/lib/hbase-hadoop-compat-2.0.0.jar /export/servers/apache-hive-2.1.0-bin/lib/hbase-hadoop-compat-2.0.0.jarln -s /export/servers/hbase-2.0.0/lib/hbase-it-2.0.0.jar /export/servers/apache-hive-2.1.0-bin/lib/hbase-it-2.0.0.jarln -s /export/servers/hbase-2.0.0/lib/hbase-server-2.0.0.jar /export/servers/apache-hive-2.1.0-bin/lib/hbase-server-2.0.0.jar 2 修改hive的配置文件编辑node03服务器上面的hive的配置文件hive-site.xml添加以下两行配置 123456789 &lt;property&gt; &lt;name&gt;hive.zookeeper.quorum&lt;/name&gt; &lt;value&gt;node01,node02,node03&lt;/value&gt;&lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;node01,node02,node03&lt;/value&gt;&lt;/property&gt; 3 修改hive-env.sh配置文件添加以下配置123export HADOOP_HOME=/export/servers/hadoop-2.7.5export HBASE_HOME=/export/servers/hbase-2.0.0export HIVE_CONF_DIR=/export/servers/apache-hive-2.1.0-bin/conf 4 hive当中建表并加载以下数据hive当中建表进入hive客户端 1bin/hive 创建hive数据库与hive对应的数据库表 123create database course;use course;create external table if not exists course.score(id int,cname string,score int) row format delimited fields terminated by '\t' stored as textfile; 准备数据内容如下node03执行以下命令，准备数据文件 123456vim hive-hbase.txt1 zhangsan 802 lisi 603 wangwu 304 zhaoliu 70 进行加载数据进入hive客户端进行加载数据 12hive (course)&gt; load data local inpath '/export/hive-hbase.txt' into table score;hive (course)&gt; select * from score; 5 创建hive管理表与HBase进行映射我们可以创建一个hive的管理表与hbase当中的表进行映射，hive管理表当中的数据，都会存储到hbase上面去 hive当中创建内部表 1234create table course.hbase_score(id int,cname string,score int) stored by 'org.apache.hadoop.hive.hbase.HBaseStorageHandler' with serdeproperties("hbase.columns.mapping" = "cf:name,cf:score") tblproperties("hbase.table.name" = "hbase_score"); 通过insert overwrite select 插入数据 1insert overwrite table course.hbase_score select id,cname,score from course.score; 6 hbase当中查看表hbase_score进入hbase的客户端查看表hbase_score，并查看当中的数据 123456789101112131415161718192021hbase(main):023:0&gt; listTABLE hbase_score myuser myuser2 student user 5 row(s) in 0.0210 seconds=&gt; ["hbase_score", "myuser", "myuser2", "student", "user"]hbase(main):024:0&gt; scan 'hbase_score'ROW COLUMN+CELL 1 column=cf:name, timestamp=1550628395266, value=zhangsan 1 column=cf:score, timestamp=1550628395266, value=80 2 column=cf:name, timestamp=1550628395266, value=lisi 2 column=cf:score, timestamp=1550628395266, value=60 3 column=cf:name, timestamp=1550628395266, value=wangwu 3 column=cf:score, timestamp=1550628395266, value=30 4 column=cf:name, timestamp=1550628395266, value=zhaoliu 4 column=cf:score, timestamp=1550628395266, value=70 4 row(s) in 0.0360 seconds 需求二创建hive外部表，映射HBase当中已有的表模型，第一步：HBase当中创建表并手动插入加载一些数据进入HBase的shell客户端，手动创建一张表，并插入加载一些数据进去 1234567create 'hbase_hive_score',&#123; NAME =&gt;'cf'&#125;put 'hbase_hive_score','1','cf:name','zhangsan'put 'hbase_hive_score','1','cf:score', '95'put 'hbase_hive_score','2','cf:name','lisi'put 'hbase_hive_score','2','cf:score', '96'put 'hbase_hive_score','3','cf:name','wangwu'put 'hbase_hive_score','3','cf:score', '97' 操作成功结果如下： 123456789101112131415161718192021hbase(main):049:0&gt; create 'hbase_hive_score',&#123; NAME =&gt;'cf'&#125;0 row(s) in 1.2970 seconds=&gt; Hbase::Table - hbase_hive_scorehbase(main):050:0&gt; put 'hbase_hive_score','1','cf:name','zhangsan'0 row(s) in 0.0600 secondshbase(main):051:0&gt; put 'hbase_hive_score','1','cf:score', '95'0 row(s) in 0.0310 secondshbase(main):052:0&gt; put 'hbase_hive_score','2','cf:name','lisi'0 row(s) in 0.0230 secondshbase(main):053:0&gt; put 'hbase_hive_score','2','cf:score', '96'0 row(s) in 0.0220 secondshbase(main):054:0&gt; put 'hbase_hive_score','3','cf:name','wangwu'0 row(s) in 0.0200 secondshbase(main):055:0&gt; put 'hbase_hive_score','3','cf:score', '97'0 row(s) in 0.0250 seconds 第二步：建立hive的外部表，映射HBase当中的表以及字段在hive当中建立外部表， 进入hive客户端，然后执行以下命令进行创建hive外部表，就可以实现映射HBase当中的表数据 1CREATE external TABLE course.hbase2hive(id int, name string, score int) STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler' WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,cf:name,cf:score") TBLPROPERTIES("hbase.table.name" ="hbase_hive_score"); 四 hbase预分区1、为何要预分区？* 增加数据读写效率 * 负载均衡，防止数据倾斜 * 方便集群容灾调度region * 优化Map数量 2、如何预分区？每一个region维护着startRow与endRowKey，如果加入的数据符合某个region维护的rowKey范围，则该数据交给这个region维护。 3、如何设定预分区？1、手动指定预分区1hbase(main):001:0&gt; create 'staff','info','partition1',SPLITS =&gt; ['1000','2000','3000','4000'] 2、使用16进制算法生成预分区1hbase(main):003:0&gt; create 'staff2','info','partition2',&#123;NUMREGIONS =&gt; 15, SPLITALGO =&gt; 'HexStringSplit'&#125; 3、使用JavaAPI创建预分区1同 hbase上篇 五 HBase的rowKey设计技巧HBase是三维有序存储的，通过rowkey（行键），column key（column family和qualifier）和TimeStamp（时间戳）这个三个维度可以对HBase中的数据进行快速定位。 HBase中rowkey可以唯一标识一行记录，在HBase查询的时候，有以下几种方式： 通过get方式，指定rowkey获取唯一一条记录 通过scan方式，设置startRow和stopRow参数进行范围匹配 全表扫描，即直接扫描整张表中所有行记录 1 rowkey长度原则rowkey是一个二进制码流，可以是任意字符串，最大长度64kb，实际应用中一般为10-100bytes，以byte[]形式保存，一般设计成定长。 建议越短越好，不要超过16个字节，原因如下： v 数据的持久化文件HFile中是按照KeyValue存储的，如果rowkey过长，比如超过100字节，1000w行数据，光rowkey就要占用100*1000w=10亿个字节，将近1G数据，这样会极大影响HFile的存储效率； v MemStore将缓存部分数据到内存，如果rowkey字段过长，内存的有效利用率就会降低，系统不能缓存更多的数据，这样会降低检索效率。 2 rowkey散列原则如果rowkey按照时间戳的方式递增，不要将时间放在二进制码的前面，建议将rowkey的高位作为散列字段，由程序随机生成，低位放时间字段，这样将提高数据均衡分布在每个RegionServer，以实现负载均衡的几率。如果没有散列字段，首字段直接是时间信息，所有的数据都会集中在一个RegionServer上，这样在数据检索的时候负载会集中在个别的RegionServer上，造成热点问题，会降低查询效率。 3 rowkey唯一原则必须在设计上保证其唯一性，rowkey是按照字典顺序排序存储的，因此，设计rowkey的时候，要充分利用这个排序的特点，将经常读取的数据存储到一块，将最近可能会被访问的数据放到一块。 4什么是热点HBase中的行是按照rowkey的字典顺序排序的，这种设计优化了scan操作，可以将相关的行以及会被一起读取的行存取在临近位置，便于scan。然而糟糕的rowkey设计是热点的源头。 热点发生在大量的client直接访问集群的一个或极少数个节点（访问可能是读，写或者其他操作）。大量访问会使热点region所在的单个机器超出自身承受能力，引起性能下降甚至region不可用，这也会影响同一个RegionServer上的其他region，由于主机无法服务其他region的请求。 设计良好的数据访问模式以使集群被充分，均衡的利用。为了避免写热点，设计rowkey使得不同行在同一个region，但是在更多数据情况下，数据应该被写入集群的多个region，而不是一个。下面是一些常见的避免热点的方法以及它们的优缺点： 1加盐这里所说的加盐不是密码学中的加盐，而是在rowkey的前面增加随机数，具体就是给rowkey分配一个随机前缀以使得它和之前的rowkey的开头不同。分配的前缀种类数量应该和你想使用数据分散到不同的region的数量一致。加盐之后的rowkey就会根据随机生成的前缀分散到各个region上，以避免热点。 2哈希哈希会使同一行永远用一个前缀加盐。哈希也可以使负载分散到整个集群，但是读却是可以预测的。使用确定的哈希可以让客户端重构完整的rowkey，可以使用get操作准确获取某一个行数据。 3反转第三种防止热点的方法时反转固定长度或者数字格式的rowkey。这样可以使得rowkey中经常改变的部分（最没有意义的部分）放在前面。这样可以有效的随机rowkey，但是牺牲了rowkey的有序性。 反转rowkey的例子以手机号为rowkey，可以将手机号反转后的字符串作为rowkey，这样的就避免了以手机号那样比较固定开头导致热点问题 3时间戳反转一个常见的数据处理问题是快速获取数据的最近版本，使用反转的时间戳作为rowkey的一部分对这个问题十分有用，可以用 Long.Max_Value - timestamp 追加到key的末尾，例如 [key][reverse_timestamp] , [key] 的最新值可以通过scan [key]获得[key]的第一条记录，因为HBase中rowkey是有序的，第一条记录是最后录入的数据。 其他一些建议： 尽量减少行键和列族的大小在HBase中，value永远和它的key一起传输的。当具体的值在系统间传输时，它的rowkey，列名，时间戳也会一起传输。如果你的rowkey和列名很大，这个时候它们将会占用大量的存储空间。 列族尽可能越短越好，最好是一个字符。 冗长的属性名虽然可读性好，但是更短的属性名存储在HBase中会更好。 六 Hbase的协处理器1http://hbase.apache.org/book.html#cp 1、 起源 Hbase 作为列族数据库最经常被人诟病的特性包括：无法轻易建立“二级索引”，难以执 行求和、计数、排序等操作。比如，在旧版本的(&lt;0.92)Hbase 中，统计数据表的总行数，需 要使用 Counter 方法，执行一次 MapReduce Job 才能得到。虽然 HBase 在数据存储层中集成 了 MapReduce，能够有效用于数据表的分布式计算。然而在很多情况下，做一些简单的相 加或者聚合计算的时候， 如果直接将计算过程放置在 server 端，能够减少通讯开销，从而获 得很好的性能提升。于是， HBase 在 0.92 之后引入了协处理器(coprocessors)，实现一些激动 人心的新特性：能够轻易建立二次索引、复杂过滤器(谓词下推)以及访问控制等。 2、协处理器有两种： observer 和 endpoint (1) Observer 类似于传统数据库中的触发器，当发生某些事件的时候这类协处理器会被 Server 端调用。Observer Coprocessor 就是一些散布在 HBase Server 端代码中的 hook 钩子， 在固定的事件发生时被调用。比如： put 操作之前有钩子函数 prePut，该函数在 put 操作 执行前会被 Region Server 调用；在 put 操作之后则有 postPut 钩子函数 以 Hbase2.0.0 版本为例，它提供了三种观察者接口： ● RegionObserver：提供客户端的数据操纵事件钩子： Get、 Put、 Delete、 Scan 等。 ● WALObserver：提供 WAL 相关操作钩子。 ● MasterObserver：提供 DDL-类型的操作钩子。如创建、删除、修改数据表等。 到 0.96 版本又新增一个 RegionServerObserver 下图是以 RegionObserver 为例子讲解 Observer 这种协处理器的原理： (2) Endpoint 协处理器类似传统数据库中的存储过程，客户端可以调用这些 Endpoint 协处 理器执行一段 Server 端代码，并将 Server 端代码的结果返回给客户端进一步处理，最常 见的用法就是进行聚集操作。如果没有协处理器，当用户需要找出一张表中的最大数据，即 max 聚合操作，就必须进行全表扫描，在客户端代码内遍历扫描结果，并执行求最大值的 操作。这样的方法无法利用底层集群的并发能力，而将所有计算都集中到 Client 端统一执 行，势必效率低下。利用 Coprocessor，用户可以将求最大值的代码部署到 HBase Server 端， HBase 将利用底层 cluster 的多个节点并发执行求最大值的操作。即在每个 Region 范围内 执行求最大值的代码，将每个 Region 的最大值在 Region Server 端计算出，仅仅将该 max 值返回给客户端。在客户端进一步将多个 Region 的最大值进一步处理而找到其中的最大值。 这样整体的执行效率就会提高很多 下图是 EndPoint 的工作原理： (3)总结 Observer 允许集群在正常的客户端操作过程中可以有不同的行为表现 Endpoint 允许扩展集群的能力，对客户端应用开放新的运算命令 observer 类似于 RDBMS 中的触发器，主要在服务端工作 endpoint 类似于 RDBMS 中的存储过程，主要在 client 端工作 observer 可以实现权限管理、优先级设置、监控、 ddl 控制、 二级索引等功能 endpoint 可以实现 min、 max、 avg、 sum、 distinct、 group by 等功能 3、协处理器加载方式​ 协处理器的加载方式有两种，我们称之为静态加载方式（ Static Load） 和动态加载方式 （ Dynamic Load）。 静态加载的协处理器称之为 System Coprocessor，动态加载的协处理器称 之为 Table Coprocessor​ 1、静态加载 通过修改 hbase-site.xml 这个文件来实现， 启动全局 aggregation，能过操纵所有的表上 的数据。只需要添加如下代码： 1234&lt;property&gt;&lt;name&gt;hbase.coprocessor.user.region.classes&lt;/name&gt;&lt;value&gt;org.apache.hadoop.hbase.coprocessor.AggregateImplementation&lt;/value&gt;&lt;/property&gt; 为所有 table 加载了一个 cp class，可以用” ,”分割加载多个 class 2、动态加载 启用表 aggregation，只对特定的表生效。通过 HBase Shell 来实现。 disable 指定表。 hbase&gt; disable ‘mytable’ 添加 aggregation hbase&gt; alter ‘mytable’, METHOD =&gt; ‘table_att’,’coprocessor’=&gt; ‘|org.apache.Hadoop.hbase.coprocessor.AggregateImplementation||’ 重启指定表 hbase&gt; enable ‘mytable’ 协处理器卸载 1234三步disable 'test'alter 'test',METHOD=&gt;'table_att_unset',NAME=&gt;'coprocessor$1'enable 'test' 4、协处理器Observer应用实战通过协处理器Observer实现hbase当中一张表插入数据，然后通过协处理器，将数据复制一份保存到另外一张表当中去，但是只取当第一张表当中的部分列数据保存到第二张表当中去 第一步：HBase当中创建第一张表proc1在HBase当中创建一张表，表名user2，并只有一个列族info 123cd /export/servers/hbase-2.0.0/bin/hbase shellhbase(main):053:0&gt; create 'proc1','info' 第二步：Hbase当中创建第二张表proc2创建第二张表’proc2，作为目标表，将第一张表当中插入数据的部分列，使用协处理器，复制到’proc2表当中来 1hbase(main):054:0&gt; create 'proc2','info' 第三步：开发HBase的协处理器开发HBase的协处理器Copo 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.hbase.*;import org.apache.hadoop.hbase.client.*;import org.apache.hadoop.hbase.coprocessor.ObserverContext;import org.apache.hadoop.hbase.coprocessor.RegionCoprocessor;import org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment;import org.apache.hadoop.hbase.coprocessor.RegionObserver;import org.apache.hadoop.hbase.util.Bytes;import org.apache.hadoop.hbase.wal.WALEdit;import java.io.IOException;import java.util.List;import java.util.Optional;public class MyProcessor implements RegionObserver,RegionCoprocessor &#123; static Connection connection = null; static Table table = null; //使用静态代码块来创建连接对象，避免频繁的创建连接对象 static&#123; Configuration conf = HBaseConfiguration.create(); conf.set("hbase.zookeeper.quorum","node01:2181"); try &#123; connection = ConnectionFactory.createConnection(conf); table = connection.getTable(TableName.valueOf("proc2")); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; private RegionCoprocessorEnvironment env = null; //定义列族名 private static final String FAMAILLY_NAME = "info"; //定义列名 private static final String QUALIFIER_NAME = "name"; //2.0加入该方法，否则无法生效 @Override public Optional&lt;RegionObserver&gt; getRegionObserver() &#123; // Extremely important to be sure that the coprocessor is invoked as a RegionObserver return Optional.of(this); &#125; /** * 初始化协处理器环境 * @param e * @throws IOException */ @Override public void start(CoprocessorEnvironment e) throws IOException &#123; env = (RegionCoprocessorEnvironment) e; &#125; @Override public void stop(CoprocessorEnvironment e) throws IOException &#123; // nothing to do here &#125; /** * 覆写prePut方法，在我们数据插入之前进行拦截， * @param e * @param put put对象里面封装了我们需要插入到目标表的数据 * @param edit * @param durability * @throws IOException */ @Override public void prePut(final ObserverContext&lt;RegionCoprocessorEnvironment&gt; e, final Put put, final WALEdit edit, final Durability durability) throws IOException &#123; try &#123; //通过put对象获取插入数据的rowkey byte[] rowBytes = put.getRow(); String rowkey = Bytes.toString(rowBytes); //获取我们插入数据的name字段的值 List&lt;Cell&gt; list = put.get(Bytes.toBytes(FAMAILLY_NAME), Bytes.toBytes(QUALIFIER_NAME)); //判断如果没有获取到info列族，和name列，直接返回即可 if (list == null || list.size() == 0) &#123; return; &#125; //获取到info列族，name列对应的cell Cell cell2 = list.get(0); //通过cell获取数据值 String nameValue = Bytes.toString(CellUtil.cloneValue(cell2)); //创建put对象，将数据插入到proc2表里面去 Put put2 = new Put(rowkey.getBytes()); put2.addColumn(Bytes.toBytes(FAMAILLY_NAME), Bytes.toBytes(QUALIFIER_NAME), nameValue.getBytes()); table.put(put2); table.close(); &#125; catch (Exception e1) &#123; return ; &#125; &#125;&#125; 第四步：将项目打成jar包，并上传到HDFS上面将我们的协处理器打成一个jar包，此处不需要用任何的打包插件即可，然后上传到hdfs 将打好的jar包上传到linux的/export/servers路径下 1234cd /export/serversmv original-hbase-1.0-SNAPSHOT.jar processor.jarhdfs dfs -mkdir -p /processorhdfs dfs -put processor.jar /processor 第五步：将打好的jar包挂载到proc1表当中去12hbase(main):056:0&gt; describe 'proc1'hbase(main):055:0&gt; alter 'proc1',METHOD =&gt; 'table_att','Coprocessor'=&gt;'hdfs://node01:8020/processor/processor.jar|cn.itcast.hbasemr.demo4.MyProcessor|1001|' 再次查看’proc1’表， 1hbase(main):043:0&gt; describe 'proc1' 可以查看到我们的卸载器已经加载了 第六步：proc1表当中添加数据进入hbase-shell客户端，然后直接执行以下命令向proc1表当中添加数据 1234put 'proc1','0001','info:name','zhangsan'put 'proc1','0001','info:age','28'put 'proc1','0002','info:name','lisi'put 'proc1','0002','info:age','25' 向proc1表当中添加数据，然后通过 1scan 'proc2' 我们会发现，proc2表当中也插入了数据，并且只有info列族，name列 ​ 注意：如果需要卸载我们的协处理器，那么进入hbase的shell命令行，执行以下命令即可 123disable 'proc1'alter 'proc1',METHOD=&gt;'table_att_unset',NAME=&gt;'coprocessor$1'enable 'proc1' 七 HBase当中的二级索引的基本介绍由于HBase的查询比较弱，如果需要实现类似于 select name,salary,count(1),max(salary) from user group by name,salary order by salary 等这样的复杂性的统计需求，基本上不可能，或者说比较困难，所以我们在使用HBase的时候，一般都会借助二级索引的方案来进行实现 HBase的一级索引就是rowkey，我们只能通过rowkey进行检索。如果我们相对hbase里面列族的列列进行一些组合查询，就需要采用HBase的二级索引方案来进行多条件的查询。 12345678\1. MapReduce方案 \2. ITHBASE（Indexed-Transanctional HBase）方案 \3. IHBASE（Index HBase）方案 \4. Hbase Coprocessor(协处理器)方案 \5. Solr+hbase方案\6. CCIndex（complementalclustering index）方案还有 MySQL 等数据库常见的二级索引我们一般可以借助各种其他的方式来实现，例如Phoenix或者solr或者ES等 八 HBase调优1、通用优化1、NameNode的元数据备份使用SSD2、定时备份NameNode上的元数据，每小时或者每天备份，如果数据极其重要，可以5~10分钟备份一次。备份可以通过定时任务复制元数据目录即可。 3、为NameNode指定多个元数据目录，使用dfs.name.dir或者dfs.namenode.name.dir指定。一个指定本地磁盘，一个指定网络磁盘。这样可以提供元数据的冗余和健壮性，以免发生故障。4、设置dfs.namenode.name.dir.restore为true，允许尝试恢复之前失败的dfs.namenode.name.dir目录，在创建checkpoint时做此尝试，如果设置了多个磁盘，建议允许。5、NameNode节点必须配置为RAID1（镜像盘）结构。6、补充：什么是Raid0、Raid0+1、Raid1、Raid5 Standalone 最普遍的单磁盘储存方式。 Cluster 集群储存是通过将数据分布到集群中各节点的存储方式,提供单一的使用接口与界面,使用户可以方便地对所有数据进行统一使用与管理。 Hot swap 用户可以再不关闭系统,不切断电源的情况下取出和更换硬盘,提高系统的恢复能力、拓展性和灵活性。 Raid0 Raid0是所有raid中存储性能最强的阵列形式。其工作原理就是在多个磁盘上分散存取连续的数据,这样,当需要存取数据是多个磁盘可以并排执行,每个磁盘执行属于它自己的那部分数据请求,显著提高磁盘整体存取性能。但是不具备容错能力,适用于低成本、低可靠性的台式系统。 Raid1 又称镜像盘,把一个磁盘的数据镜像到另一个磁盘上,采用镜像容错来提高可靠性,具有raid中最高的数据冗余能力。存数据时会将数据同时写入镜像盘内,读取数据则只从工作盘读出。发生故障时,系统将从镜像盘读取数据,然后再恢复工作盘正确数据。这种阵列方式可靠性极高,但是其容量会减去一半。广泛用于数据要求极严的应用场合,如商业金融、档案管理等领域。只允许一颗硬盘出故障。 Raid0+1 将Raid0和Raid1技术结合在一起,兼顾两者的优势。在数据得到保障的同时,还能提供较强的存储性能。不过至少要求4个或以上的硬盘，但也只允许一个磁盘出错。是一种三高技术。 Raid5 Raid5可以看成是Raid0+1的低成本方案。采用循环偶校验独立存取的阵列方式。将数据和相对应的奇偶校验信息分布存储到组成RAID5的各个磁盘上。当其中一个磁盘数据发生损坏后,利用剩下的磁盘和相应的奇偶校验信息 重新恢复/生成丢失的数据而不影响数据的可用性。至少需要3个或以上的硬盘。适用于大数据量的操作。成本稍高、储存性强、可靠性强的阵列方式。 RAID还有其他方式，请自行查阅。 7、保持NameNode日志目录有足够的空间，这些日志有助于帮助你发现问题。8、因为Hadoop是IO密集型框架，所以尽量提升存储的速度和吞吐量（类似位宽）。2 、Linux优化1、开启文件系统的预读缓存可以提高读取速度123$ sudo blockdev --setra 32768 /dev/sda（注意：ra是readahead的缩写） 2、关闭进程睡眠池1$ sudo sysctl -w vm.swappiness=0 3、调整ulimit上限，默认值为比较小的数字$ ulimit -n 查看允许最大进程数 $ ulimit -u 查看允许打开最大文件数 修改: 123456789101112$ sudo vi /etc/security/limits.conf 修改打开文件数限制末尾添加：* soft nofile 1024000* hard nofile 1024000Hive - nofile 1024000hive - nproc 1024000 $ sudo vi /etc/security/limits.d/20-nproc.conf 修改用户打开进程数限制修改为：#* soft nproc 4096#root soft nproc unlimited* soft nproc 40960root soft nproc unlimited 4、开启集群的时间同步NTP，请参看之前文档5、更新系统补丁（注意：更新补丁前，请先测试新版本补丁对集群节点的兼容性）3、HDFS优化（hdfs-site.xml）1、保证RPC调用会有较多的线程数属性：dfs.namenode.handler.count 解释：该属性是NameNode服务默认线程数，的默认值是10，根据机器的可用内存可以调整为50~100 属性：dfs.datanode.handler.count 解释：该属性默认值为10，是DataNode的处理线程数，如果HDFS客户端程序读写请求比较多，可以调高到15~20，设置的值越大，内存消耗越多，不要调整的过高，一般业务中，5~10即可。 2、副本数的调整属性：dfs.replication 解释：如果数据量巨大，且不是非常之重要，可以调整为2~3，如果数据非常之重要，可以调整为3~5。 3.、文件块大小的调整属性：dfs.blocksize 解释：块大小定义，该属性应该根据存储的大量的单个文件大小来设置，如果大量的单个文件都小于100M，建议设置成64M块大小，对于大于100M或者达到GB的这种情况，建议设置成256M，一般设置范围波动在64m~256m之间。置范围波动在64M~256M之间。 4、MapReduce优化（mapred-site.xml）1、Job任务服务线程数调整mapreduce.jobtracker.handler.count 该属性是job任务线程数，默认值10，根据机器的可用内存可调整为50-100 2、Http服务器工作线程数属性：mapreduce.tasktracker.http.threads解释：定义HTTP服务器工作线程数，默认值40，对于大集群可调整为80-100 3、文件排序合并优化属性：mapreduce.task.io.sort.factor 解释：文件排序时同时合并的数据流的数量，这也定义了同时打开文件的个数，默认值为10，如果调高该参数，可以明显减少磁盘IO，即减少文件读取的次数。 4、设置任务并发属性：mapreduce.map.speculative 解释：该属性可以设置任务是否可以并发执行，如果任务多而小，该属性设置为true可以明显加快任务执行效率，但是对于延迟非常高的任务，建议改为false，这就类似于迅雷下载。 5、MR输出数据的压缩属性：mapreduce.map.output.compress、mapreduce.output.fileoutputformat.compress 解释：对于大集群而言，建议设置Map-Reduce的输出为压缩的数据，而对于小集群，则不需要。 6、优化Mapper和Reducer的个数属性： mapreduce.tasktracker.map.tasks.maximum mapreduce.tasktracker.reduce.tasks.maximum 解释：以上两个属性分别为一个单独的Job任务可以同时运行的Map和Reduce的数量。 设置上面两个参数时，需要考虑CPU核数、磁盘和内存容量。假设一个8核的CPU，业务内容非常消耗CPU，那么可以设置map数量为4，如果该业务不是特别消耗CPU类型的，那么可以设置map数量为40，reduce数量为20。这些参数的值修改完成之后，一定要观察是否有较长等待的任务，如果有的话，可以减少数量以加快任务执行，如果设置一个很大的值，会引起大量的上下文切换，以及内存与磁盘之间的数据交换，这里没有标准的配置数值，需要根据业务和硬件配置以及经验来做出选择。 在同一时刻，不要同时运行太多的MapReduce，这样会消耗过多的内存，任务会执行的非常缓慢，我们需要根据CPU核数，内存容量设置一个MR任务并发的最大值，使固定数据量的任务完全加载到内存中，避免频繁的内存和磁盘数据交换，从而降低磁盘IO，提高性能。 大概配比： CPU CORE MEM（GB） Map Reduce 1 1 1 1 1 5 1 1 4 5 1~4 2 16 32 16 8 16 64 16 8 24 64 24 12 24 128 24 12 123大概估算公式：map = 2 + ⅔cpu_corereduce = 2 + ⅓cpu_core 5、HBase优化1、在HDFS的文件中追加内容不是不允许追加内容么？没错，请看背景故事： 属性：dfs.support.append 文件：hdfs-site.xml、hbase-site.xml 解释：开启HDFS追加同步，可以优秀的配合HBase的数据同步和持久化。默认值为true。 2、优化DataNode允许的最大文件打开数属性：dfs.datanode.max.transfer.threads 文件：hdfs-site.xml 解释：HBase一般都会同一时间操作大量的文件，根据集群的数量和规模以及数据动作，设置为4096或者更高。默认值：4096 3、优化延迟高的数据操作的等待时间属性：dfs.image.transfer.timeout 文件：hdfs-site.xml 解释：如果对于某一次数据操作来讲，延迟非常高，socket需要等待更长的时间，建议把该值设置为更大的值（默认60000毫秒），以确保socket不会被timeout掉。 4、优化数据的写入效率属性： mapreduce.map.output.compress mapreduce.map.output.compress.codec 文件：mapred-site.xml 解释：开启这两个数据可以大大提高文件的写入效率，减少写入时间。第一个属性值修改为true，第二个属性值修改为：org.apache.hadoop.io.compress.GzipCodec 5、优化DataNode存储属性：dfs.datanode.failed.volumes.tolerated 文件：hdfs-site.xml 解释：默认为0，意思是当DataNode中有一个磁盘出现故障，则会认为该DataNode shutdown了。如果修改为1，则一个磁盘出现故障时，数据会被复制到其他正常的DataNode上，当前的DataNode继续工作。 6、设置RPC监听数量属性：hbase.regionserver.handler.count 文件：hbase-site.xml 解释：默认值为30，用于指定RPC监听的数量，可以根据客户端的请求数进行调整，读写请求较多时，增加此值。 7、优化HStore文件大小属性：hbase.hregion.max.filesize 文件：hbase-site.xml 解释：默认值10737418240（10GB），如果需要运行HBase的MR任务，可以减小此值，因为一个region对应一个map任务，如果单个region过大，会导致map任务执行时间过长。该值的意思就是，如果HFile的大小达到这个数值，则这个region会被切分为两个Hfile。 8、优化hbase客户端缓存属性：hbase.client.write.buffer 文件：hbase-site.xml 解释：用于指定HBase客户端缓存，增大该值可以减少RPC调用次数，但是会消耗更多内存，反之则反之。一般我们需要设定一定的缓存大小，以达到减少RPC次数的目的。 9、指定scan.next扫描HBase所获取的行数属性：hbase.client.scanner.caching 文件：hbase-site.xml 解释：用于指定scan.next方法获取的默认行数，值越大，消耗内存越大。 6、内存优化HBase操作过程中需要大量的内存开销，毕竟Table是可以缓存在内存中的，一般会分配整个可用内存的70%给HBase的Java堆。但是不建议分配非常大的堆内存，因为GC过程持续太久会导致RegionServer处于长期不可用状态，一般16~48G内存就可以了，如果因为框架占用内存过高导致系统内存不足，框架一样会被系统服务拖死。 7、JVM优化涉及文件：hbase-env.sh 1、并行GC参数：-XX:+UseParallelGC 解释：开启并行GC 2、同时处理垃圾回收的线程数参数：-XX:ParallelGCThreads=cpu_core – 1 解释：该属性设置了同时处理垃圾回收的线程数。 3、禁用手动GC参数：-XX:DisableExplicitGC 解释：防止开发人员手动调用GC 8、Zookeeper优化1、优化Zookeeper会话超时时间参数：zookeeper.session.timeout 文件：hbase-site.xml 解释：In hbase-site.xml, set zookeeper.session.timeout to 30 seconds or less to bound failure detection (20-30 seconds is a good start).该值会直接关系到master发现服务器宕机的最大周期，默认值为30秒，如果该值过小，会在HBase在写入大量数据发生而GC时，导致RegionServer短暂的不可用，从而没有向ZK发送心跳包，最终导致认为从节点shutdown。一般20台左右的集群需要配置5台zookeeper。]]></content>
      <tags>
        <tag>BigData</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hbase]]></title>
    <url>%2F2017%2F07%2F17%2FHbase.html</url>
    <content type="text"><![CDATA[Hbase一概述hbase是bigtable的开源java版本。是建立在hdfs之上，提供高可靠性、高性能、列存储、可伸缩、实时读写nosql的数据库系统。 它介于nosql和RDBMS之间，仅能通过主键(row key)和主键的range来检索数据，仅支持单行事务(可通过hive支持来实现多表join等复杂操作)。 主要用来存储结构化和半结构化的松散数据。 Hbase查询数据功能很简单，不支持join等复杂操作，不支持复杂的事务（行级的事务） Hbase中只支持的数据类型为：byte[] 与hadoop一样，Hbase目标主要依靠横向扩展，通过不断增加廉价的商用服务器，来增加计算和存储能力。 HBase中的表一般有这样的特点： ² 大：一个表可以有上十亿行，上百万列 ² 面向列:面向列(族)的存储和权限控制，列(族)独立检索。 ² 稀疏:对于为空(null)的列，并不占用存储空间，因此，表可以设计的非常稀疏。 HBase的发展历程 HBase的原型是Google的BigTable论文，受到了该论文思想的启发，目前作为Hadoop的子项目来开发维护，用于支持结构化的数据存储。 官方网站：http://hbase.apache.org * 2006年Google发表BigTable白皮书 * 2006年开始开发HBase * 2008 HBase成为了 Hadoop的子项目 * 2010年HBase成为Apache顶级项目 二HBASE与Hadoop的关系1、HDFS* 为分布式存储提供文件系统 * 针对存储大尺寸的文件进行优化，不需要对HDFS上的文件进行随机读写 * 直接使用文件 * 数据模型不灵活 * 使用文件系统和处理框架 * 优化一次写入，多次读取的方式 2、HBase* 提供表状的面向列的数据存储 * 针对表状数据的随机读写进行优化 * 使用key-value操作数据 * 提供灵活的数据模型 * 使用表状存储，支持MapReduce，依赖HDFS * 优化了多次读，以及多次写 hbase是基于hdfs的，hbase的数据都是存储在hdfs上面的。hbase支持随机读写，hbase的数据存储在hdfs上面的，hbase是如何基于hdfs的数据做到随机读写的？？ 三rdbms与HBASE的关系1、关系型数据库结构： * 数据库以表的形式存在 * 支持FAT、NTFS、EXT、文件系统 * 使用Commit log存储日志 * 参考系统是坐标系统 * 使用主键（PK） * 支持分区 * 使用行、列、单元格 功能： * 支持向上扩展 * 使用SQL查询 * 面向行，即每一行都是一个连续单元 * 数据总量依赖于服务器配置 * 具有ACID支持 * 适合结构化数据 * 传统关系型数据库一般都是中心化的 * 支持事务 * 支持Join 2、HBase结构： * 数据库以region的形式存在 * 支持HDFS文件系统 * 使用WAL（Write-Ahead Logs）存储日志 * 参考系统是Zookeeper * 使用行键（row key） * 支持分片 * 使用行、列、列族和单元格 功能： * 支持向外扩展 * 使用API和MapReduce来访问HBase表数据 * 面向列，即每一列都是一个连续的单元 * 数据总量不依赖具体某台机器，而取决于机器数量 * HBase不支持ACID（Atomicity、Consistency、Isolation、Durability） * 适合结构化数据和非结构化数据 * 一般都是分布式的 * HBase不支持事务，支持的是单行数据的事务操作 * 不支持Join 四 hbase 特点1**）海量存储** Hbase适合存储PB级别的海量数据，在PB级别的数据以及采用廉价PC存储的情况下，能在几十到百毫秒内返回数据。这与Hbase的极易扩展性息息相关。正式因为Hbase良好的扩展性，才为海量数据的存储提供了便利。 2**）列式存储** 这里的列式存储其实说的是列族存储，Hbase是根据列族来存储数据的。列族下面可以有非常多的列，列族在创建表的时候就必须指定。 3**）极易扩展** Hbase的扩展性主要体现在两个方面，一个是基于上层处理能力（RegionServer）的扩展，一个是基于存储的扩展（HDFS）。 通过横向添加RegionSever的机器，进行水平扩展，提升Hbase上层的处理能力，提升Hbsae服务更多Region的能力。 备注：RegionServer的作用是管理region、承接业务的访问，这个后面会详细的介绍通过横向添加Datanode的机器，进行存储层扩容，提升Hbase的数据存储能力和提升后端存储的读写能力。 4**）高并发** 由于目前大部分使用Hbase的架构，都是采用的廉价PC，因此单个IO的延迟其实并不小，一般在几十到上百ms之间。这里说的高并发，主要是在并发的情况下，Hbase的单个IO延迟下降并不多。能获得高并发、低延迟的服务。 5**）稀疏** 稀疏主要是针对Hbase列的灵活性，在列族中，你可以指定任意多的列，在列数据为空的情况下，是不会占用存储空间的。 五架构 1、HMaster功能： 1) 监控RegionServer 2) 处理RegionServer故障转移 3) 处理元数据的变更 4) 处理region的分配或移除 5) 在空闲时间进行数据的负载均衡 6) 通过Zookeeper发布自己的位置给客户端 2、RegionServer功能： 1) 负责存储HBase的实际数据 2) 处理分配给它的Region 3) 刷新缓存到HDFS 4) 维护HLog 5) 执行压缩 6) 负责处理Region分片 组件： 1) Write-Ahead logs HBase的修改记录，当对HBase读写数据的时候，数据不是直接写进磁盘，它会在内存中保留一段时间（时间以及数据量阈值可以设定）。但把数据保存在内存中可能有更高的概率引起数据丢失，为了解决这个问题，数据会先写在一个叫做Write-Ahead logfile的文件中，然后再写入内存中。所以在系统出现故障的时候，数据可以通过这个日志文件重建。 2) HFile 这是在磁盘上保存原始数据的实际的物理文件，是实际的存储文件。 3) Store HFile存储在Store中，一个Store对应HBase表中的一个列族。 4) MemStore 顾名思义，就是内存存储，位于内存中，用来保存当前的数据操作，所以当数据保存在WAL中之后，RegsionServer会在内存中存储键值对。 5) Region Hbase表的分片，HBase表会根据RowKey值被切分成不同的region存储在RegionServer中，在一个RegionServer中可以有多个不同的region。 存储架构: 主节点：HMaster ​ 监控regionServer的健康状态 ​ 处理regionServer的故障转移 ​ 处理元数据变更 ​ 处理region的分配或者移除 ​ 空闲时间做数据的负载均衡 从节点：HRegionServer ​ 负责存储HBase的实际数据 ​ 处理分配给他的region ​ 刷新缓存的数据到HDFS上面去 ​ 维护HLog ​ 执行数据的压缩 ​ 负责处理region的分片 1234一个HRegionServer = 1个HLog + 很多个region1个region = 很多个store模块1个store模块 = 1个memoryStore + 很多个storeFile 当memoryStore达到128m或者一个小时 会落地到storeFile中HLog：hbase当中预写日志模块，write ahead log 123将storeFile 文件合并压缩存到hdfs上格式Hfile当memoryStore达到128m或者一个小时 会落地到storeFile中当hdfs的数据达到阀值,会分region,创建另一个region存储这个数据 用其他的HregionServer 管理 对比MySQL的分库分表 六集群搭建注意事项：Hbase强依赖于HDFS以及zookeeper，所以安装Hbase之前一定要保证Hadoop和zookeeper正常启动 上传压缩包并解压: 1tar -zxf hbase-2.0.0-bin.tar.gz -C /export/servers/ 修改配置文件 1cd /export/servers/hbase-2.0.0/conf 1 修改hbase-env.sh 1234vim hbase-env.shexport JAVA_HOME=/export/servers/jdk1.8.0_141export HBASE_MANAGES_ZK=false 2 修改hbase-site.xml 12345678910111213141516171819202122232425262728293031cd /export/servers/hbase-2.0.0/confvim hbase-site.xml&lt;configuration&gt; &lt;!-- hbase根路径 --&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://node01:8020/hbase&lt;/value&gt; &lt;/property&gt;&lt;!-- 指定hbase为分布式 --&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 0.98后的新变动，之前版本没有.port,默认端口为60000 --&gt; &lt;property&gt; &lt;name&gt;hbase.master.port&lt;/name&gt; &lt;value&gt;16000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;node01:2181,node02:2181,node03:2181&lt;/value&gt; &lt;/property&gt;&lt;!-- 指顶存在哪里 --&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt; &lt;value&gt;/export/servers/zookeeper-3.4.9/zkdatas&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 3 修改regionservers 123node01node02node03 4 创建back-masters文件 123cd /export/servers/hbase-2.0.0/confvim backup-mastersnode02 向其他节点发送安装包: 1scp -r hbase-2.0.0/ node02:$PWD 三台节点都要创建软连接 12ln -s /export/servers/hadoop-2.7.5/etc/hadoop/core-site.xml /export/servers/hbase-2.0.0/conf/core-site.xmlln -s /export/servers/hadoop-2.7.5/etc/hadoop/hdfs-site.xml /export/servers/hbase-2.0.0/conf/hdfs-site.xml 三台节点都要配值环境变量; 123vim /etc/profileexport HBASE_HOME=/export/servers/hbase-2.0.0export PATH=:$HBASE_HOME/bin:$PATH 集群启动: 1在第一台机器上: 12cd /export/servers/hbase-2.0.0bin/start-hbase.sh 警告提示：HBase启动的时候会产生一个警告，这是因为jdk7与jdk8的问题导致的，如果linux服务器安装jdk8就会产生这样的一个警告 我们可以只是掉所有机器的hbase-env.sh当中的 “HBASE_MASTER_OPTS”和“HBASE_REGIONSERVER_OPTS”配置 来解决这个问题。不过警告不影响我们正常运行，可以不用解决 2另外一种启动方式： 我们也可以执行以下命令单节点进行启动 启动HMaster命令 1bin/hbase-daemon.sh start master 启动HRegionServer命令 1bin/hbase-daemon.sh start regionserver 页面访问: 12http://node02:16010/master-statushttp://node01:16010/master-status HBase的表模型rowKey：行键，每一条数据都是使用行键来进行唯一标识的 columnFamily：列族。列族下面可以有很多列 column：列的概念。每一个列都必须归属于某一个列族 timestamp：时间戳，每条数据都会有时间戳的概念 versionNum：版本号，每条数据都会有版本号，每次数据变化，版本号都会进行更新 七 HBASE常用shell操作1、进入HBase客户端命令操作界面node01服务器执行以下命令进入hbase的shell客户端 12cd /export/servers/hbase-2.0.0bin/hbase shell 2、查看帮助命令1hbase(main):001:0&gt; help 3、查看当前数据库中有哪些表1hbase(main):002:0&gt; list 4、创建一张表创建user表，包含info、data两个列族 123hbase(main):010:0&gt; create 'user', 'info', 'data'或者hbase(main):010:0&gt; create 'user', &#123;NAME =&gt; 'info', VERSIONS =&gt; '3'&#125;，&#123;NAME =&gt; 'data'&#125; 若建表时指定了多个版本,则在更新操作时,会保存多个版本 ,会把以前的也会保存,但查询时,查到的是最新的,若想获取以前的可以指定版本去查询. 5、添加数据操作12345678向user表中插入信息，row key为rk0001，列族info中添加name列标示符，值为zhangsanhbase(main):011:0&gt; put 'user', 'rk0001', 'info:name', 'zhangsan'向user表中插入信息，row key为rk0001，列族info中添加gender列标示符，值为femalehbase(main):012:0&gt; put 'user', 'rk0001', 'info:gender', 'female'向user表中插入信息，row key为rk0001，列族info中添加age列标示符，值为20hbase(main):013:0&gt; put 'user', 'rk0001', 'info:age', 20向user表中插入信息，row key为rk0001，列族data中添加pic列标示符，值为picturehbase(main):014:0&gt; put 'user', 'rk0001', 'data:pic', 'picture' 6、查询数据操作第一种查询方式： get rowkey 直接获取某一条数据 第二种查询方式 ： scan startRow stopRow 范围值扫描 第三种查询方式：scan tableName 全表扫描 1、通过rowkey进行查询123获取user表中row key为rk0001的所有信息hbase(main):015:0&gt; get 'user', 'rk0001' 2、查看rowkey下面的某个列族的信息123获取user表中row key为rk0001，info列族的所有信息hbase(main):016:0&gt; get 'user', 'rk0001', 'info' 3、查看rowkey指定列族指定字段的值123获取user表中row key为rk0001，info列族的name、age列标示符的信息hbase(main):017:0&gt; get 'user', 'rk0001', 'info:name', 'info:age' 4、查看rowkey指定多个列族的信息123456获取user表中row key为rk0001，info、data列族的信息hbase(main):018:0&gt; get 'user', 'rk0001', 'info', 'data'或者你也可以这样写hbase(main):019:0&gt; get 'user', 'rk0001', &#123;COLUMN =&gt; ['info', 'data']&#125;或者你也可以这样写，也行hbase(main):020:0&gt; get 'user', 'rk0001', &#123;COLUMN =&gt; ['info:name', 'data:pic']&#125; 4、指定rowkey与列值查询123获取user表中row key为rk0001，cell的值为zhangsan的信息hbase(main):030:0&gt; get 'user', 'rk0001', &#123;FILTER =&gt; "ValueFilter(=, 'binary:zhangsan')"&#125; 5、指定rowkey与列值模糊查询1234567获取user表中row key为rk0001，列标示符中含有a的信息hbase(main):031:0&gt; get 'user', 'rk0001', &#123;FILTER =&gt; "(QualifierFilter(=,'substring:a'))"&#125;继续插入一批数据hbase(main):032:0&gt; put 'user', 'rk0002', 'info:name', 'fanbingbing'hbase(main):033:0&gt; put 'user', 'rk0002', 'info:gender', 'female'hbase(main):034:0&gt; put 'user', 'rk0002', 'info:nationality', '中国'hbase(main):035:0&gt; get 'user', 'rk0002', &#123;FILTER =&gt; "ValueFilter(=, 'binary:中国')"&#125; 6、查询所有数据12查询user表中的所有信息scan 'user' 7、列族查询12345查询user表中列族为info的信息scan 'user', &#123;COLUMNS =&gt; 'info'&#125;scan 'user', &#123;COLUMNS =&gt; 'info', RAW =&gt; true, VERSIONS =&gt; 5&#125;scan 'user', &#123;COLUMNS =&gt; 'info', RAW =&gt; true, VERSIONS =&gt; 3&#125; 8、多列族查询1234查询user表中列族为info和data的信息scan 'user', &#123;COLUMNS =&gt; ['info', 'data']&#125;scan 'user', &#123;COLUMNS =&gt; ['info:name', 'data:pic']&#125; 9、指定列族与某个列名查询123查询user表中列族为info、列标示符为name的信息scan 'user', &#123;COLUMNS =&gt; 'info:name'&#125; 10、指定列族与列名以及限定版本查询123查询user表中列族为info、列标示符为name的信息,并且版本最新的5个scan 'user', &#123;COLUMNS =&gt; 'info:name', VERSIONS =&gt; 5&#125; 11、指定多个列族与按照数据值模糊查询123查询user表中列族为info和data且列标示符中含有a字符的信息scan 'user', &#123;COLUMNS =&gt; ['info', 'data'], FILTER =&gt; "(QualifierFilter(=,'substring:a'))"&#125; 12、rowkey的范围值查询123查询user表中列族为info，rk范围是[rk0001, rk0003)的数据scan 'user', &#123;COLUMNS =&gt; 'info', STARTROW =&gt; 'rk0001', ENDROW =&gt; 'rk0003'&#125; 13、指定rowkey模糊查询123查询user表中row key以rk字符开头的scan 'user',&#123;FILTER=&gt;"PrefixFilter('rk')"&#125; 14、指定数据范围值查询123查询user表中指定范围的数据scan 'user', &#123;TIMERANGE =&gt; [1392368783980, 1392380169184]&#125; 7、更新数据操作1、更新数据值更新操作同插入操作一模一样，只不过有数据就更新，没数据就添加 2、更新版本号将user表的f1列族版本号改为5 1alter 'user',NAME =&gt;'info',VERSIONS =&gt;5 8、删除数据以及删除表操作1、指定rowkey以及列名进行删除12删除user表row key为rk0001，列标示符为info:name的数据delete 'user','rk001','info;name' 2、指定rowkey，列名以及字段值进行删除12删除user表row key为rk0001，列标示符为info:name，timestamp为1392383705316的数据delete 'user','rk001','info:name',1392383705316 3、删除一个列族123alter 'user',NAME =&gt; 'info',METHOD =&gt;'delete'或者alter 'user','delete' =&gt;'info' 4、清空表数据1truncate 'user' 5、删除表12345该表处于disable状态:disable 'user'然后删除表:drop 'user'如直接drop表 会报错 Drop the named table. Table must first be disabled 9、统计一张表有多少行数据1count 'user' 八 HBASE的高级shell管理命令1、status例如：显示服务器状态 1status 'node01' 2、whoami显示HBase当前用户，例如： 1whoami 3、list显示当前所有的表 4、count统计指定表的记录数，例如： 1hbase&gt; count 'user' 5、describe展示表结构信息 6、exists检查表是否存在，适用于表量特别多的情况 7、is_enabled、is_disabled检查表是否启用或禁用 8、alter该命令可以改变表和列族的模式，例如： 为当前表增加列族： 1alter 'user',NAME=&gt; 'CF2',versions =&gt;2 为当前表删除列族： 1alter 'user'.'delete'=&gt;'cf2' 9、disable/enable禁用一张表/启用一张表 10、drop删除一张表，记得在删除表之前必须先禁用 11、truncate禁用表-删除表-创建表 九 HBASE的java代码开发依赖: 12345678910111213141516171819202122232425262728293031323334353637383940414243 &lt;dependencies&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.hbase/hbase-client --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-client&lt;/artifactId&gt; &lt;version&gt;2.0.0&lt;/version&gt;&lt;/dependency&gt;&lt;!-- https://mvnrepository.com/artifact/org.apache.hbase/hbase-server --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-server&lt;/artifactId&gt; &lt;version&gt;2.0.0&lt;/version&gt;&lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.testng&lt;/groupId&gt; &lt;artifactId&gt;testng&lt;/artifactId&gt; &lt;version&gt;6.14.3&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.0&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;!-- &lt;verbal&gt;true&lt;/verbal&gt;--&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; 步骤 12345一 配置类 设置zookeeper的连接地址* 二根据 配置传参 得到连接对象* 三 根据连接对象 得到管理员对象* 四创建表的对象 创建列簇的对象 将列簇的对象添加到 表对象* 五 管理员对象创建表 并将表对象传入 1创建表 1234567891011121314151617181920212223242526/** * 创建hbase表 myuser，带有两个列族 f1 f2 */ @Test public void createTable() throws IOException &#123; //连接hbase集群 Configuration configuration = HBaseConfiguration.create(); //指定hbase的zk连接地址 configuration.set("hbase.zookeeper.quorum","node01:2181,node02:2181,node03:2181"); Connection connection = ConnectionFactory.createConnection(configuration); //获取管理员对象 Admin admin = connection.getAdmin(); //通过管理员对象创建表 HTableDescriptor hTableDescriptor = new HTableDescriptor(TableName.valueOf("myuser")); //给我们的表添加列族，指定两个列族 f1 f2 HColumnDescriptor f1 = new HColumnDescriptor("f1"); HColumnDescriptor f2 = new HColumnDescriptor("f2"); //将两个列族设置到 hTableDescriptor里面去 hTableDescriptor.addFamily(f1); hTableDescriptor.addFamily(f2); //创建表 admin.createTable(hTableDescriptor); //关闭资源 admin.close(); connection.close(); &#125; 2插入数据 步骤 1234567一 配置类 设置zookeeper连接地址二 根据配置类 获取连接对象三 根据连接对象 获得表对象 传参为表名 四 new一个put对象 传参为 rowkey的值得字节数组五 根据put对象 添加例六 表对象 调put方法 传参为put对象 七 关闭表 123456789101112131415161718192021/*** * 向表当中添加数据 */ @Test public void addData() throws IOException &#123; //获取连接 Configuration configuration = HBaseConfiguration.create(); configuration.set("hbase.zookeeper.quorum","node01:2181,node02:2181,node03:2181"); Connection connection = ConnectionFactory.createConnection(configuration); //获取表对象 Table myuser = connection.getTable(TableName.valueOf("myuser")); Put put = new Put("0001".getBytes()); put.addColumn("f1".getBytes(),"id".getBytes(), Bytes.toBytes(1)); put.addColumn("f1".getBytes(),"name".getBytes(),Bytes.toBytes("张三")); put.addColumn("f1".getBytes(),"age".getBytes(),Bytes.toBytes(18)); put.addColumn("f2".getBytes(),"address".getBytes(),Bytes.toBytes("地球人")); put.addColumn("f2".getBytes(),"phone".getBytes(),Bytes.toBytes("15845678952")); myuser.put(put); //关闭表 myuser.close(); &#125; 查询数据 初始化数据用于查询: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677@Test public void insertBatchData() throws IOException &#123; //获取连接 Configuration configuration = HBaseConfiguration.create(); configuration.set("hbase.zookeeper.quorum", "node01:2181,node02:2181"); Connection connection = ConnectionFactory.createConnection(configuration); //获取表 Table myuser = connection.getTable(TableName.valueOf("myuser")); //创建put对象，并指定rowkey Put put = new Put("0002".getBytes()); put.addColumn("f1".getBytes(),"id".getBytes(),Bytes.toBytes(1)); put.addColumn("f1".getBytes(),"name".getBytes(),Bytes.toBytes("曹操")); put.addColumn("f1".getBytes(),"age".getBytes(),Bytes.toBytes(30)); put.addColumn("f2".getBytes(),"sex".getBytes(),Bytes.toBytes("1")); put.addColumn("f2".getBytes(),"address".getBytes(),Bytes.toBytes("沛国谯县")); put.addColumn("f2".getBytes(),"phone".getBytes(),Bytes.toBytes("16888888888")); put.addColumn("f2".getBytes(),"say".getBytes(),Bytes.toBytes("helloworld")); Put put2 = new Put("0003".getBytes()); put2.addColumn("f1".getBytes(),"id".getBytes(),Bytes.toBytes(2)); put2.addColumn("f1".getBytes(),"name".getBytes(),Bytes.toBytes("刘备")); put2.addColumn("f1".getBytes(),"age".getBytes(),Bytes.toBytes(32)); put2.addColumn("f2".getBytes(),"sex".getBytes(),Bytes.toBytes("1")); put2.addColumn("f2".getBytes(),"address".getBytes(),Bytes.toBytes("幽州涿郡涿县")); put2.addColumn("f2".getBytes(),"phone".getBytes(),Bytes.toBytes("17888888888")); put2.addColumn("f2".getBytes(),"say".getBytes(),Bytes.toBytes("talk is cheap , show me the code")); Put put3 = new Put("0004".getBytes()); put3.addColumn("f1".getBytes(),"id".getBytes(),Bytes.toBytes(3)); put3.addColumn("f1".getBytes(),"name".getBytes(),Bytes.toBytes("孙权")); put3.addColumn("f1".getBytes(),"age".getBytes(),Bytes.toBytes(35)); put3.addColumn("f2".getBytes(),"sex".getBytes(),Bytes.toBytes("1")); put3.addColumn("f2".getBytes(),"address".getBytes(),Bytes.toBytes("下邳")); put3.addColumn("f2".getBytes(),"phone".getBytes(),Bytes.toBytes("12888888888")); put3.addColumn("f2".getBytes(),"say".getBytes(),Bytes.toBytes("what are you 弄啥嘞！")); Put put4 = new Put("0005".getBytes()); put4.addColumn("f1".getBytes(),"id".getBytes(),Bytes.toBytes(4)); put4.addColumn("f1".getBytes(),"name".getBytes(),Bytes.toBytes("诸葛亮")); put4.addColumn("f1".getBytes(),"age".getBytes(),Bytes.toBytes(28)); put4.addColumn("f2".getBytes(),"sex".getBytes(),Bytes.toBytes("1")); put4.addColumn("f2".getBytes(),"address".getBytes(),Bytes.toBytes("四川隆中")); put4.addColumn("f2".getBytes(),"phone".getBytes(),Bytes.toBytes("14888888888")); put4.addColumn("f2".getBytes(),"say".getBytes(),Bytes.toBytes("出师表你背了嘛")); Put put5 = new Put("0005".getBytes()); put5.addColumn("f1".getBytes(),"id".getBytes(),Bytes.toBytes(5)); put5.addColumn("f1".getBytes(),"name".getBytes(),Bytes.toBytes("司马懿")); put5.addColumn("f1".getBytes(),"age".getBytes(),Bytes.toBytes(27)); put5.addColumn("f2".getBytes(),"sex".getBytes(),Bytes.toBytes("1")); put5.addColumn("f2".getBytes(),"address".getBytes(),Bytes.toBytes("哪里人有待考究")); put5.addColumn("f2".getBytes(),"phone".getBytes(),Bytes.toBytes("15888888888")); put5.addColumn("f2".getBytes(),"say".getBytes(),Bytes.toBytes("跟诸葛亮死掐")); Put put6 = new Put("0006".getBytes()); put6.addColumn("f1".getBytes(),"id".getBytes(),Bytes.toBytes(5)); put6.addColumn("f1".getBytes(),"name".getBytes(),Bytes.toBytes("xiaobubu—吕布")); put6.addColumn("f1".getBytes(),"age".getBytes(),Bytes.toBytes(28)); put6.addColumn("f2".getBytes(),"sex".getBytes(),Bytes.toBytes("1")); put6.addColumn("f2".getBytes(),"address".getBytes(),Bytes.toBytes("内蒙人")); put6.addColumn("f2".getBytes(),"phone".getBytes(),Bytes.toBytes("15788888888")); put6.addColumn("f2".getBytes(),"say".getBytes(),Bytes.toBytes("貂蝉去哪了")); List&lt;Put&gt; listPut = new ArrayList&lt;Put&gt;(); listPut.add(put); listPut.add(put2); listPut.add(put3); listPut.add(put4); listPut.add(put5); listPut.add(put6); myuser.put(listPut); myuser.close(); &#125; 查询 初始化操作: 1234567891011121314151617以下所有共用这个: private Connection connection; private Configuration configuration; private Table table; /** * 初始化的操作 */ @BeforeTest public void initTable() throws IOException &#123; //获取连接 configuration = HBaseConfiguration.create(); configuration.set("hbase.zookeeper.quorum","node01:2181,node02:2181,node03:2181"); connection = ConnectionFactory.createConnection(configuration); table= connection.getTable(TableName.valueOf("myuser")); &#125; 1234567释放资源: @AfterTest public void closeTable() throws IOException &#123; connection.close(); table.close(); &#125; 按照rowkey进行查询获取所有列的所有值 步骤 12345前三同 与插入同四 new 一个Get 对象 get设置查询参数六 表对象 调get方法 得到结果对象七 根据结果对象调方法 得到集合 元素为Cell cell包含列簇 列名 列值等八 遍历 集合 根据需要得到数据 123456789101112131415161718192021222324252627/** * 查询rowkey为0003的人，所有的列 */ @Test public void getData() throws IOException &#123; Get get = new Get("0003".getBytes()); // get.addFamily("f1".getBytes()); //get.addColumn("f1".getBytes(),"id".getBytes());//查询指定列簇下,的指定列的值就加这一条****** //Result是一个对象，封装了我们所有的结果数据 Result result = table.get(get); //获取0003这条数据所有的cell值 List&lt;Cell&gt; cells = result.listCells(); for (Cell cell : cells) &#123; //获取列族的名称 String familyName = Bytes.toString(cell.getFamilyArray(), cell.getFamilyOffset(), cell.getFamilyLength()); //获取列的名称 String columnName = Bytes.toString(cell.getQualifierArray(),cell.getQualifierOffset(),cell.getQualifierLength()); if(familyName.equals("f1") &amp;&amp; columnName.equals("id") || columnName.equals("age"))&#123; //获取int类型列值 int value = Bytes.toInt(cell.getValueArray(),cell.getValueOffset(),cell.getValueLength()); System.out.println("列族名为"+familyName+"列名为" + columnName + "列的值为" + value); &#125;else&#123; String value = Bytes.toString(cell.getValueArray(),cell.getValueOffset(),cell.getValueLength()); System.out.println("列族名为"+familyName+"列名为" + columnName + "列的值为" + value); &#125; &#125; &#125; 通过startRowKey和endRowKey进行范围扫描123456789101112131415161718192021222324252627282930/** * 按照rowkey进行范围值的扫描 * 扫描rowkey范围是0004到0006的所有的值 */ @Test public void scanRange() throws IOException &#123; Scan scan = new Scan(); //设置我们起始和结束rowkey,范围值扫描是包括前面的，不包括后面的 scan.setStartRow("0004".getBytes()); scan.setStopRow("0006".getBytes()); //返回多条数据结果值都封装在resultScanner里面了 ResultScanner scanner = table.getScanner(scan); for (Result result : scanner) &#123; List&lt;Cell&gt; cells = result.listCells(); for (Cell cell : cells) &#123; String rowkey = Bytes.toString(cell.getRowArray(), cell.getRowOffset(), cell.getRowLength()); //获取列族名 String familyName = Bytes.toString(cell.getFamilyArray(), cell.getFamilyOffset(), cell.getFamilyLength()); //获取列名 String columnName = Bytes.toString(cell.getQualifierArray(), cell.getQualifierOffset(), cell.getQualifierLength()); if(familyName.equals("f1") &amp;&amp; columnName.equals("id") || columnName.equals("age"))&#123; int value = Bytes.toInt(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()); System.out.println("数据的rowkey为" + rowkey + " 数据的列族名为" + familyName + " 列名为" + columnName + " 列值为" + value); &#125;else&#123; String value = Bytes.toString(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()); System.out.println("数据的rowkey为" + rowkey + " 数据的列族名为" + familyName + " 列名为" + columnName + " 列值为" + value); &#125; &#125; &#125; &#125; 全表扫描 1234567891011121314151617181920212223//全表扫描@Testpublic void scanAll() throws Exception&#123; Scan scan=new Scan(); ResultScanner scanner=table.getScanner(scan); for (Result result: scanner)&#123; List&lt;Cell&gt; cells=result.listCells(); for(Cell cell:cells)&#123; String rowkey = Bytes.toString(cell.getRowArray(), cell.getRowOffset(), cell.getRowLength()); //获取列族名 String familyName = Bytes.toString(cell.getFamilyArray(), cell.getFamilyOffset(), cell.getFamilyLength()); //获取列名 String columnName = Bytes.toString(cell.getQualifierArray(), cell.getQualifierOffset(), cell.getQualifierLength()); if(familyName.equals("f1") &amp;&amp; columnName.equals("id") || columnName.equals("age"))&#123; int value = Bytes.toInt(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()); System.out.println("数据的rowkey为" + rowkey + " 数据的列族名为" + familyName + " 列名为" + columnName + " 列值为" + value); &#125;else&#123; String value = Bytes.toString(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()); System.out.println("数据的rowkey为" + rowkey + " 数据的列族名为" + familyName + " 列名为" + columnName + " 列值为" + value); &#125; &#125; &#125;&#125; 十 过滤查询过滤器的类型很多，但是可以分为两大类——比较过滤器，专用过滤器 过滤器的作用是在服务端判断数据是否满足条件，然后只将满足条件的数据返回给客户端； hbase过滤器的比较运算符： 1234567LESS &lt;LESS_OR_EQUAL &lt;=EQUAL =NOT_EQUAL &lt;&gt;GREATER_OR_EQUAL &gt;=GREATER &gt;NO_OP 排除所有 Hbase过滤器的比较器（指定比较机制）： 123456BinaryComparator 按字节索引顺序比较指定字节数组，采用Bytes.compareTo(byte[])BinaryPrefixComparator 跟前面相同，只是比较左端的数据是否相同NullComparator 判断给定的是否为空BitComparator 按位比较RegexStringComparator 提供一个正则的比较器，仅支持 EQUAL 和非EQUALSubstringComparator 判断提供的子串是否出现在value中。 1 比较过滤器四种: rowkey过滤器: RowFilter 列簇过滤器: FamilyFilter 列过滤器: QualifierFilter 列值过滤器: ValueFilter 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152@Testpublic void filterStudy() throws IOException &#123; Scan scan = new Scan(); //查询rowkey比0003小的所有的数据 rowkey过滤器: // RowFilter rowFilter = new RowFilter(CompareOperator.LESS, new BinaryComparator(Bytes.toBytes("0003"))); // scan.setFilter(rowFilter); //查询比f2列族小的所有的列族里面的数据 FamilyFilter // FamilyFilter f2 = new FamilyFilter(CompareOperator.LESS, new SubstringComparator("f2")); // scan.setFilter(f2); //只查询name列的值 列过滤器: // QualifierFilter name = new QualifierFilter(CompareOperator.EQUAL, new SubstringComparator("name")); // scan.setFilter(name); //查询value值当中包含8的所有的数据 列值过滤器: // ValueFilter valueFilter = new ValueFilter(CompareOperator.EQUAL, new SubstringComparator("8")); // scan.setFilter(valueFilter); //查询name值为刘备的数据 //SingleColumnValueFilter singleColumnValueFilter = new SingleColumnValueFilter("f1".getBytes(), "name".getBytes(), CompareOperator.EQUAL, "刘备".getBytes()); //scan.setFilter(singleColumnValueFilter); //查询rowkey以00开头所有的数据 PrefixFilter prefixFilter = new PrefixFilter("00".getBytes()); scan.setFilter(prefixFilter); //返回多条数据结果值都封装在resultScanner里面了 ResultScanner scanner = table.getScanner(scan); for (Result result : scanner) &#123; List&lt;Cell&gt; cells = result.listCells(); for (Cell cell : cells) &#123; String rowkey = Bytes.toString(cell.getRowArray(), cell.getRowOffset(), cell.getRowLength()); //获取列族名 String familyName = Bytes.toString(cell.getFamilyArray(), cell.getFamilyOffset(), cell.getFamilyLength()); String columnName = Bytes.toString(cell.getQualifierArray(), cell.getQualifierOffset(), cell.getQualifierLength()); if(familyName.equals("f1") &amp;&amp; columnName.equals("id") || columnName.equals("age"))&#123; int value = Bytes.toInt(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()); System.out.println("数据的rowkey为" + rowkey + " 数据的列族名为" + familyName + " 列名为" + columnName + " 列值为" + value); &#125;else&#123; String value = Bytes.toString(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()); System.out.println("数据的rowkey为" + rowkey + " 数据的列族名为" + familyName + " 列名为" + columnName + " 列值为" + value); &#125; &#125; &#125;&#125; 2 专用过滤器1、单列值过滤器 SingleColumnValueFilter2、列值排除过滤器SingleColumnValueExcludeFilter与SingleColumnValueFilter相反，会排除掉指定的列，其他的列全部返回 3、rowkey前缀过滤器PrefixFilter1234567891011121314151617181920212223242526272829303132333435363738@Test public void filterStudy() throws IOException &#123; Scan scan = new Scan(); //查询name值为刘备的数据 单列值过滤器 //SingleColumnValueFilter singleColumnValueFilter = new SingleColumnValueFilter("f1".getBytes(), "name".getBytes(), CompareOperator.EQUAL, "刘备".getBytes()); //scan.setFilter(singleColumnValueFilter); //查询name值不为刘备的数据 列值排除过滤器 //SingleColumnValueExcludeFilter singleColumnValueExcludeFilter = new SingleColumnValueExcludeFilter("f1".getBytes(), "name".getBytes(), CompareOperator.EQUAL, "刘备".getBytes()); //查询rowkey以00开头所有的数据 PrefixFilter prefixFilter = new PrefixFilter("00".getBytes()); scan.setFilter(prefixFilter); //返回多条数据结果值都封装在resultScanner里面了 ResultScanner scanner = table.getScanner(scan); for (Result result : scanner) &#123; List&lt;Cell&gt; cells = result.listCells(); for (Cell cell : cells) &#123; String rowkey = Bytes.toString(cell.getRowArray(), cell.getRowOffset(), cell.getRowLength()); //获取列族名 String familyName = Bytes.toString(cell.getFamilyArray(), cell.getFamilyOffset(), cell.getFamilyLength()); String columnName = Bytes.toString(cell.getQualifierArray(), cell.getQualifierOffset(), cell.getQualifierLength()); if(familyName.equals("f1") &amp;&amp; columnName.equals("id") || columnName.equals("age"))&#123; int value = Bytes.toInt(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()); System.out.println("数据的rowkey为" + rowkey + " 数据的列族名为" + familyName + " 列名为" + columnName + " 列值为" + value); &#125;else&#123; String value = Bytes.toString(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()); System.out.println("数据的rowkey为" + rowkey + " 数据的列族名为" + familyName + " 列名为" + columnName + " 列值为" + value); &#125; &#125; &#125; &#125; 4、分页过滤器PageFilter1234567891011121314151617181920212223242526272829303132333435363738394041424344454647/** * 实现hbase的分页的功能 */ @Test public void hbasePage() throws IOException &#123; int pageNum= 3; int pageSize = 2 ; if(pageNum == 1)&#123; Scan scan = new Scan(); //如果是查询第一页数据，就按照空来进行扫描 scan.withStartRow("".getBytes()); PageFilter pageFilter = new PageFilter(pageSize); scan.setFilter(pageFilter); ResultScanner scanner = table.getScanner(scan); for (Result result : scanner) &#123; byte[] row = result.getRow(); System.out.println(Bytes.toString(row)); &#125; &#125;else&#123; String startRow = ""; //计算我们前两页的数据的最后一条，再加上一条，就是第三页的起始rowkey Scan scan = new Scan(); scan.withStartRow("".getBytes()); PageFilter pageFilter = new PageFilter((pageNum - 1) * pageSize + 1); scan.setFilter(pageFilter); ResultScanner scanner = table.getScanner(scan); for (Result result : scanner) &#123; byte[] row = result.getRow(); startRow = Bytes.toString(row); &#125; //获取第三页的数据 scan.withStartRow(startRow.getBytes()); PageFilter pageFilter1 = new PageFilter(pageSize); scan.setFilter(pageFilter1); ResultScanner scanner1 = table.getScanner(scan); for (Result result : scanner1) &#123; byte[] row = result.getRow(); System.out.println(Bytes.toString(row)); &#125; &#125; &#125; 3 多过滤器综合查询FilterList12345678910111213141516171819202122232425262728293031323334/** * 多过滤器综合查询 * 需求：使用SingleColumnValueFilter查询f1列族，name为刘备的数据，并且同时满足rowkey的前缀以00开头的数据（PrefixFilter） */ @Test public void filterList() throws IOException &#123; SingleColumnValueFilter singleColumnValueFilter = new SingleColumnValueFilter("f1".getBytes(), "name".getBytes(), CompareOperator.EQUAL, "刘备".getBytes()); PrefixFilter prefixFilter = new PrefixFilter("00".getBytes()); //使用filterList来实现多过滤器综合查询 FilterList filterList = new FilterList(singleColumnValueFilter, prefixFilter); Scan scan = new Scan(); scan.setFilter(filterList);//设定为FilterList ResultScanner scanner = table.getScanner(scan); for (Result result : scanner) &#123; List&lt;Cell&gt; cells = result.listCells(); for (Cell cell : cells) &#123; String rowkey = Bytes.toString(cell.getRowArray(), cell.getRowOffset(), cell.getRowLength()); //获取列族名 String familyName = Bytes.toString(cell.getFamilyArray(), cell.getFamilyOffset(), cell.getFamilyLength()); String columnName = Bytes.toString(cell.getQualifierArray(), cell.getQualifierOffset(), cell.getQualifierLength()); if(familyName.equals("f1") &amp;&amp; columnName.equals("id") || columnName.equals("age"))&#123; int value = Bytes.toInt(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()); System.out.println("数据的rowkey为" + rowkey + " 数据的列族名为" + familyName + " 列名为" + columnName + " 列值为" + value); &#125;else&#123; String value = Bytes.toString(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()); System.out.println("数据的rowkey为" + rowkey + " 数据的列族名为" + familyName + " 列名为" + columnName + " 列值为" + value); &#125; &#125; &#125; &#125; 根据rowkey删除数据 12345678/** * 根据rowkey删除某一条数据 */ @Test public void deleteData() throws IOException &#123; Delete delete = new Delete("0007".getBytes()); table.delete(delete); &#125; 删除表操作 123456789101112/** * 删除表操作 */ @Test public void deleteTable() throws IOException &#123; //获取管理员对象 Admin admin = connection.getAdmin(); //禁用表 admin.disableTable(TableName.valueOf("myuser")); //删除表 admin.deleteTable(TableName.valueOf("myuser")); &#125; 更新表操作 123456/** * 更新操作与插入操作是一模一样的，如果rowkey已经存在那么就更新 * 如果rowkey不存在，那么就添加 */ @Test public void updateOperate()&#123;&#125; 表的创建与预分区 12345678910111213141516171819202122232425/** * 通过javaAPI进行HBase的表的创建以及预分区操作 */ @Test public void hbaseSplit() throws IOException &#123; //获取连接 Configuration configuration = HBaseConfiguration.create(); configuration.set("hbase.zookeeper.quorum", "node01:2181,node02:2181,node03:2181"); Connection connection = ConnectionFactory.createConnection(configuration); Admin admin = connection.getAdmin(); //自定义算法，产生一系列Hash散列值存储在二维数组中 byte[][] splitKeys = &#123;&#123;1,2,3,4,5&#125;,&#123;'a','b','c','d','e'&#125;&#125;; //通过HTableDescriptor来实现我们表的参数设置，包括表名，列族等等 HTableDescriptor hTableDescriptor = new HTableDescriptor(TableName.valueOf("staff3")); //添加列族 hTableDescriptor.addFamily(new HColumnDescriptor("f1")); //添加列族 hTableDescriptor.addFamily(new HColumnDescriptor("f2")); admin.createTable(hTableDescriptor,splitKeys); admin.close(); &#125; 十一 Hbase的底层原理系统架构: Client 1 包含访问hbase的接口，client维护着一些cache来加快对hbase的访问，比如regione的位置信息。 Zookeeper 1 保证任何时候，集群中只有一个master 2 存贮所有Region的寻址入口 3 实时监控Region Server的状态，将Region server的上线和下线信息实时通知给Master 4 存储Hbase的schema,包括有哪些table，每个table有哪些column family Master职责 1 为Region server分配region 2 负责region server的负载均衡 3 发现失效的region server并重新分配其上的region 4 HDFS上的垃圾文件回收 5 处理schema更新请求 Region Server职责 1 Region server维护Master分配给它的region，处理对这些region的IO请求 2 Region server负责切分在运行过程中变得过大的region 可以看到，client访问hbase上数据的过程并不需要master参与（寻址访问zookeeper和region server，数据读写访问regione server），master仅仅维护者table和region的元数据信息，负载很低。 HBase的表数据模型Row Key与nosql数据库们一样,row key是用来检索记录的主键。访问hbase table中的行，只有三种方式： 1 通过单个row key访问 2 通过row key的range 3 全表扫描 Row key行键 (Row key)可以是任意字符串(最大长度是 64KB，实际应用中长度一般为 10-100bytes)，在hbase内部，row key保存为字节数组。 Hbase:会对表中的数据按照rowkey排序(字典顺序) 存储时，数据按照Row key的字典序(byte order)排序存储。设计key时，要充分排序存储这个特性，将经常一起读取的行存储放到一起。(位置相关性) 注意： 字典序对int排序的结果是 1,10,100,11,12,13,14,15,16,17,18,19,2,20,21,…,9,91,92,93,94,95,96,97,98,99。要保持整形的自然序，行键必须用0作左填充。 行的一次读写是原子操作 (不论一次读写多少列)。这个设计决策能够使用户很容易的理解程序在对同一个行进行并发更新操作时的行为。 列族Column Familyhbase表中的每个列，都归属与某个列族。列族是表的schema的一部分(而列不是)，必须在使用表之前定义。 列名都以列族作为前缀。例如courses:history ， courses:math 都属于 courses 这个列族。 访问控制、磁盘和内存的使用统计都是在列族层面进行的。 列族越多，在取一行数据时所要参与IO、搜寻的文件就越多，所以，如果没有必要，不要设置太多的列族 列 Column列族下面的具体列，属于某一个ColumnFamily,类似于我们mysql当中创建的具体的列 列是插入数据的时候动态指定的 时间戳HBase中通过row和columns确定的为一个存贮单元称为cell。每个 cell都保存着同一份数据的多个版本。版本通过时间戳来索引。时间戳的类型是 64位整型。时间戳可以由hbase(在数据写入时自动 )赋值，此时时间戳是精确到毫秒的当前系统时间。时间戳也可以由客户显式赋值。如果应用程序要避免数据版本冲突，就必须自己生成具有唯一性的时间戳。每个 cell中，不同版本的数据按照时间倒序排序，即最新的数据排在最前面。 为了避免数据存在过多版本造成的的管理 (包括存贮和索引)负担，hbase提供了两种数据版本回收方式： ² 保存数据的最后n个版本 ² 保存最近一段时间内的版本（设置数据的生命周期TTL）。 用户可以针对每个列族进行设置。 Cell由{row key, column( = + ), version} 唯一确定的单元。 cell中的数据是没有类型的，全部是字节码形式存贮。 VersionNum数据的版本号，每条数据可以有多个版本号，默认值为系统时间戳，类型为Long 物理存储1、整体结构1 Table中的所有行都按照row key的字典序排列。 2 Table 在行的方向上分割为多个Hregion 3 region按大小分割的(默认10G)，每个表一开始只有一个region，随着数据不断插入表，region不断增大，当增大到一个阀值的时候，Hregion就会等分会两个新的Hregion。分出新的 会分一个新的 HRegionServer去管理. 当table中的行不断增多，就会有越来越多的Hregion。 4 Hregion是Hbase中分布式存储和负载均衡的最小单元。最小单元就表示不同的Hregion可以分布在不同的HRegion server上。但一个Hregion是不会拆分到多个server上的。 5 HRegion虽然是负载均衡的最小单元，但并不是物理存储的最小单元。 事实上，HRegion由一个或者多个Store组成，每个store保存一个column family。 每个Strore又由一个memStore和0至多个StoreFile组成。 STORE FILE &amp; HFILE结构StoreFile以HFile格式保存在HDFS上。 首先HFile文件是不定长的，长度固定的只有其中的两块：Trailer和FileInfo。正如图中所示的，Trailer中有指针指向其他数 据块的起始点。 File Info中记录了文件的一些Meta信息，例如：AVG_KEY_LEN, AVG_VALUE_LEN, LAST_KEY, COMPARATOR, MAX_SEQ_ID_KEY等。 Data Index和Meta Index块记录了每个Data块和Meta块的起始点。 Data Block是HBase I/O的基本单元，为了提高效率，HRegionServer中有基于LRU的Block Cache机制。每个Data块的大小可以在创建一个Table的时候通过参数指定，大号的Block有利于顺序Scan，小号Block利于随机查询。 每个Data块除了开头的Magic以外就是一个个KeyValue对拼接而成, Magic内容就是一些随机数字，目的是防止数据损坏。 HFile里面的每个对就是一个简单的数组。但是这个数组里面包含了很多项，并且有固定的结构。 开始是两个固定长度的数值，分别表示Key的长度和Value的长度。紧接着是Key，开始是固定长度的数值，表示RowKey的长度，紧接着是 RowKey，然后是固定长度的数值，表示Family的长度，然后是Family，接着是Qualifier，然后是两个固定长度的数值，表示Time Stamp和Key Type（Put/Delete）。Value部分没有这么复杂的结构，就是纯粹的二进制数据了。 HFile分为六个部分： Data Block 段–保存表中的数据，这部分可以被压缩 Meta Block 段 (可选的)–保存用户自定义的kv对，可以被压缩。 File Info 段–Hfile的元信息，不被压缩，用户也可以在这一部分添加自己的元信息。 Data Block Index 段–Data Block的索引。每条索引的key是被索引的block的第一条记录的key。 Meta Block Index段 (可选的)–Meta Block的索引。 Trailer–这一段是定长的。保存了每一段的偏移量，读取一个HFile时，会首先 读取Trailer，Trailer保存了每个段的起始位置(段的Magic Number用来做安全check)，然后，DataBlock Index会被读取到内存中，这样，当检索某个key时，不需要扫描整个HFile，而只需从内存中找到key所在的block，通过一次磁盘io将整个 block读取到内存中，再找到需要的key。DataBlock Index采用LRU机制淘汰。 HFile的Data Block，Meta Block通常采用压缩方式存储，压缩之后可以大大减少网络IO和磁盘IO，随之而来的开销当然是需要花费cpu进行压缩和解压缩。 目标Hfile的压缩支持两种方式：Gzip，Lzo。 Memstore与storefile一个region由多个store组成，每个store包含一个列族的所有数据 Store包括位于内存的memstore和位于硬盘的storefile 写操作先写入memstore,当memstore中的数据量达到某个阈值，Hregionserver启动flashcache进程写入storefile,每次写入形成单独一个storefile 当storefile大小超过一定阈值后，会把当前的region分割成两个，并由Hmaster分配给相应的HregionServer服务器，实现负载均衡 客户端检索数据时，先在memstore找，找不到再找storefile HLog(WAL log)WAL 意为Write ahead log(http://en.wikipedia.org/wiki/Write-ahead_logging)，类似mysql中的binlog,用来 做灾难恢复时用，Hlog记录数据的所有变更,一旦数据修改，就可以从log中进行恢复. 每个Region Server维护一个Hlog,而不是每个Region一个。这样不同region(来自不同table)的日志会混在一起，这样做的目的是不断追加单个文件相对于同时写多个文件而言，可以减少磁盘寻址次数，因此可以提高对table的写性能。带来的麻烦是，如果一台region server下线，为了恢复其上的region，需要将region server上的log进行拆分，然后分发到其它region server上进行恢复。 HLog文件就是一个普通的Hadoop Sequence File： ² HLog Sequence File 的Key是HLogKey对象，HLogKey中记录了写入数据的归属信息，除了table和region名字外，同时还包括 sequence number和timestamp，timestamp是”写入时间”，sequence number的起始值为0，或者是最近一次存入文件系统中sequence number。 ² HLog Sequece File的Value是HBase的KeyValue对象，即对应HFile中的KeyValue，可参见上文描述。 读写过程读请求过程：HRegionServer保存着meta表以及表数据，要访问表数据，首先Client先去访问zookeeper，从zookeeper里面获取meta表所在的位置信息，即找到这个meta表在哪个HRegionServer上保存着。 接着Client通过刚才获取到的HRegionServer的IP来访问Meta表所在的HRegionServer，从而读取到Meta，进而获取到Meta表中存放的元数据。 Client通过元数据中存储的信息，访问对应的HRegionServer，然后扫描所在HRegionServer的Memstore和Storefile来查询数据 最后HRegionServer把查询到的数据响应给Client。 查看meta表信息 hbase(main):011:0&gt; scan ‘hbase:meta’ 2、写请求过程：Client也是先访问zookeeper，找到Meta表，并获取Meta表元数据。 确定当前将要写入的数据所对应的HRegion和HRegionServer服务器。 Client向该HRegionServer服务器发起写入数据请求，然后HRegionServer收到请求并响应。 Client先把数据写入到HLog，以防止数据丢失。 然后将数据写入到Memstore。 如果HLog和Memstore均写入成功，则这条数据写入成功 如果Memstore达到阈值，会把Memstore中的数据flush到Storefile中。 当Storefile越来越多，会触发Compact合并操作，把过多的Storefile合并成一个大的HFile。 当HFile越来越大，Region也会越来越大，达到阈值后，会触发Split操作，将Region一分为二。 细节描述： hbase使用MemStore和StoreFile存储对表的更新。 数据在更新时首先写入Log(WAL log)和内存(MemStore)中，MemStore中的数据是排序的，当MemStore累计到一定阈值时，就会创建一个新的MemStore，并 且将老的MemStore添加到flush队列，由单独的线程flush到磁盘上，成为一个StoreFile。于此同时，系统会在zookeeper中记录一个redo point，表示这个时刻之前的变更已经持久化了。 当系统出现意外时，可能导致内存(MemStore)中的数据丢失，此时使用Log(WAL log)来恢复checkpoint之后的数据。 StoreFile是只读的，一旦创建后就不可以再修改。因此Hbase的更新其实是不断追加的操作。当一个Store中的StoreFile达到一定的阈值后，就会进行一次合并(minor_compact, major_compact),将对同一个key的修改合并到一起，形成一个大的StoreFile，当StoreFile的大小达到一定阈值后，又会对 StoreFile进行split，等分为两个StoreFile。 由于对表的更新是不断追加的，compact时，需要访问Store中全部的 StoreFile和MemStore，将他们按row key进行合并，由于StoreFile和MemStore都是经过排序的，并且StoreFile带有内存中索引，合并的过程还是比较快。 Region管理 (1) region分配 任何时刻，一个region只能分配给一个region server。master记录了当前有哪些可用的region server。以及当前哪些region分配给了哪些region server，哪些region还没有分配。当需要分配的新的region，并且有一个region server上有可用空间时，master就给这个region server发送一个装载请求，把region分配给这个region server。region server得到请求后，就开始对此region提供服务。 (2) region server上线 master使用zookeeper来跟踪region server状态。当某个region server启动时，会首先在zookeeper上的server目录下建立代表自己的znode。由于master订阅了server目录上的变更消息，当server目录下的文件出现新增或删除操作时，master可以得到来自zookeeper的实时通知。因此一旦region server上线，master能马上得到消息。 (3) region server下线 当region server下线时，它和zookeeper的会话断开，zookeeper而自动释放代表这台server的文件上的独占锁。master就可以确定： 1 region server和zookeeper之间的网络断开了。 2 region server挂了。 无论哪种情况，region server都无法继续为它的region提供服务了，此时master会删除server目录下代表这台region server的znode数据，并将这台region server的region分配给其它还活着的同志。 Master工作机制Ø master上线 master启动进行以下步骤: 1 从zookeeper上获取唯一一个代表active master的锁，用来阻止其它master成为master。 2 扫描zookeeper上的server父节点，获得当前可用的region server列表。 3 和每个region server通信，获得当前已分配的region和region server的对应关系。 4 扫描.META.region的集合，计算得到当前还未分配的region，将他们放入待分配region列表。 Ø master下线 由于master只维护表和region的元数据，而不参与表数据IO的过程，master下线仅导致所有元数据的修改被冻结(无法创建删除表，无法修改表的schema，无法进行region的负载均衡，无法处理region 上下线，无法进行region的合并，唯一例外的是region的split可以正常进行，因为只有region server参与)，表的数据读写还可以正常进行。因此master下线短时间内对整个hbase集群没有影响。 从上线过程可以看到，master保存的信息全是可以冗余信息（都可以从系统其它地方收集到或者计算出来） 因此，一般hbase集群中总是有一个master在提供服务，还有一个以上的‘master’在等待时机抢占它的位置。 十二 hbase三个重要机制1、flush机制1.（hbase.regionserver.global.memstore.size）默认;堆大小的40% regionServer的全局memstore的大小，超过该大小会触发flush到磁盘的操作,默认是堆大小的40%,而且regionserver级别的flush会阻塞客户端读写 2.（hbase.hregion.memstore.flush.size）默认：128M 单个region里memstore的缓存大小，超过那么整个HRegion就会flush, 3.（hbase.regionserver.optionalcacheflushinterval）默认：1h 内存中的文件在自动刷新之前能够存活的最长时间 4.（hbase.regionserver.global.memstore.size.lower.limit）默认：堆大小 0.4 0.95 有时候集群的“写负载”非常高，写入量一直超过flush的量，这时，我们就希望memstore不要超过一定的安全设置。在这种情况下，写操作就要被阻塞一直到memstore恢复到一个“可管理”的大小, 这个大小就是默认值是堆大小 0.4 0.95，也就是当regionserver级别的flush操作发送后,会阻塞客户端写,一直阻塞到整个regionserver级别的memstore的大小为堆得大小乘0.4乘0.95为止 5.（hbase.hregion.preclose.flush.size）默认为：5M 当一个 region 中的 memstore 的大小大于这个值的时候，我们又触发 了 close.会先运行“pre-flush”操作，清理这个需要关闭的memstore，然后 将这个 region 下线。当一个 region 下线了，我们无法再进行任何写操作。 如果一个 memstore 很大的时候，flush 操作会消耗很多时间。”pre-flush” 操作意味着在 region 下线之前，会先把 memstore 清空。这样在最终执行 close 操作的时候，flush 操作会很快。 6.（hbase.hstore.compactionThreshold）默认：超过3个 一个store里面允许存的hfile的个数，超过这个个数会被写到新的一个hfile里面 也即是每个region的每个列族对应的memstore在fulsh为hfile的时候，默认情况下当超过3个hfile的时候就会 对这些文件进行合并重写为一个新文件，设置个数越大可以减少触发合并的时间，但是每次合并的时间就会越长 2 compact机制把小的storeFile文件合并成大的Storefile文件。 清理过期的数据，包括删除的数据 将数据的版本号保存为3个 3、split机制当Region达到阈值，会把过大的Region一分为二。 默认一个HFile达到10Gb的时候就会进行切分]]></content>
      <tags>
        <tag>BigData</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka]]></title>
    <url>%2F2017%2F07%2F16%2FKafka.html</url>
    <content type="text"><![CDATA[Kafka消息队列一消息队列概述1 kafka企业级消息系统kafka企业级消息系统为何使用消息系统 在没有使用消息系统以前，我们对于传统许多业务，以及跨服务器传递消息的时候，会采用串行方式或者并行方法； 与串行的差别是并行的方式可以缩短程序整体处理的时间。 消息系统: 消息系统负责将数据从一个应用程序传送到另一个应用程序，因此应用程序可以专注于数据，但是不必担心 如何共享它。分布式消息系统基于可靠的消息队列的概念。消息在客户端应用程序和消息传递系统之间的异步排队。 有两种类型的消息模式可用 点对点；发布-订阅消息系统 点对点消息系统中，消息被保留在队列中，一个或者多个消费者可以消费队列中的消息，但是特定的消 息只能有最多的一个消费者消费。一旦消费者读取队列中的消息，他就从该队列中消失。该系统的典型应用就是订单处理系统，其中每个订单将有一个订单处理器处理，但多个订单处理器可以同时工作。 大多数的消息系统是基于发布-订阅消息系统 分类 2.1、点对点主要采用的队列的方式，如A-&gt;B 当B消费的队列中的数据，那么队列的数据就会被删除掉【如果B不消费那么就会存在队列中有很多的脏数据】 2.2、发布-订阅发布与订阅主要三大组件 主题：一个消息的分类 发布者：将消息通过主动推送的方式推送给消息系统； 订阅者：可以采用拉、推的方式从消息系统中获取数据 应用场景 3.1、应用解耦将一个大型的任务系统分成若干个小模块，将所有的消息进行统一的管理和存储，因此为了解耦，就会涉及到kafka企业级消息平台 3.2、流量控制秒杀活动当中，一般会因为流量过大，应用服务器挂掉，为了解决这个问题，一般需要在应用前端加上消息队列以控制访问流量。 1、 可以控制活动的人数 可以缓解短时间内流量大使得服务器崩掉 2、 可以通过队列进行数据缓存，后续再进行消费处理 3.3、日志处理日志处理指将消息队列用在日志处理中，比如kafka的应用中，解决大量的日志传输问题； 日志采集工具采集 数据写入kafka中；kafka消息队列负责日志数据的接收，存储，转发功能； 日志处理应用程序：订阅并消费 kafka队列中的数据，进行数据分析。 3.4、消息通讯消息队列一般都内置了高效的通信机制，因此也可以用在纯的消息通讯，比如点对点的消息队列，或者聊天室等。 二 kafka概述kafka是最初由linkedin公司开发的，使用scala语言编写，kafka是一个分布式，分区的，多副本的，多订阅者的日志系统（分布式MQ系统），可以用于搜索日志，监控日志，访问日志等。 kafka目前支持多种客户端的语言：java、python、c++、php等 apache kafka是一个分布式发布-订阅消息系统和一个强大的队列，可以处理大量的数据，并使能够将消息从一个端点传递到另一个端点，kafka适合离线和在线消息消费。kafka消息保留在磁盘上，并在集群内复制以防止数据丢失。kafka构建在zookeeper同步服务之上。它与apache和spark非常好的集成，应用于实时流式数据分析。 其他消息队列: RabbitMQ Redis ZeroMQ ActiveMQ kafka好处: 可靠性：分布式的，分区，复制和容错的。 可扩展性：kafka消息传递系统轻松缩放，无需停机。 耐用性：kafka使用分布式提交日志，这意味着消息会尽可能快速的保存在磁盘上，因此它是持久的。 性能：kafka对于发布和定于消息都具有高吞吐量。即使存储了许多TB的消息，他也爆出稳定的性能。 kafka非常快：保证零停机和零数据丢失。 应用场景 5.1、指标分析kafka 通常用于操作监控数据。这设计聚合来自分布式应用程序的统计信息， 以产生操作的数据集中反馈 5.2、日志聚合解决方法kafka可用于跨组织从多个服务器收集日志，并使他们以标准的合适提供给多个服务器。 5.3、流式处理流式处理框架（spark，storm，ﬂink）重主题中读取数据，对齐进行处理，并将处理后的数据写入新的主题，供 用户和应用程序使用，kafka的强耐久性在流处理的上下文中也非常的有用。 三架构 四大核心: 生产者API允许应用程序发布记录流至一个或者多个kafka的主题（topics）。 消费者API允许应用程序订阅一个或者多个主题，并处理这些主题接收到的记录流。 StreamsAPI允许应用程序充当流处理器（stream processor），从一个或者多个主题获取输入流，并生产一个输出流到一个或 者多个主题，能够有效的变化输入流为输出流。 ConnectorAPI允许构建和运行可重用的生产者或者消费者，能够把kafka主题连接到现有的应用程序或数据系统。例如：一个连 接到关系数据库的连接器可能会获取每个表的变化。 架构关系图 说明：kafka支持消息持久化，消费端为拉模型来拉取数据，消费状态和订阅关系有客户端负责维护，消息消费完 后，不会立即删除，会保留历史消息。因此支持多订阅时，消息只会存储一份就可以了 整体架构 一个典型的kafka集群中包含若干个Producer，若干个Broker，若干个Consumer，以及一个zookeeper集群； kafka通过zookeeper管理集群配置，选举leader，以及在Consumer Group发生变化时进行Rebalance（负载均 衡）；Producer使用push模式将消息发布到Broker；Consumer使用pull模式从Broker中订阅并消费消息。 kafka术语介绍 Broker：kafka集群中包含一个或者多个服务实例，这种服务实例被称为Broker Topic：每条发布到kafka集群的消息都有一个类别，这个类别就叫做Topic Partition：Partition是一个物理上的概念，每个Topic包含一个或者多个Partition Producer：负责发布消息到kafka的Broker中。 Consumer：消息消费者,向kafka的broker中读取消息的客户端 Consumer Group：每一个Consumer属于一个特定的Consumer Group（可以为每个Consumer指定 groupName） kafka中topic说明 1,kafka将消息以topic为单位进行归类 2,topic特指kafka处理的消息源（feeds of messages）的不同分类。 3.topic是一种分类或者发布的一些列记录的名义上的名字。kafka主题始终是支持多用户订阅的；也就是说，一 个主题可以有零个，一个或者多个消费者订阅写入的数据。 4.在kafka集群中，可以有无数的主题。 5.生产者和消费者消费数据一般以主题为单位。更细粒度可以到分区级别。 kafka中分区数 Partitions：分区数：控制topic将分片成多少个log，可以显示指定，如果不指定则会使用 broker（server.properties）中的num.partitions配置的数量。 一个broker服务下，是否可以创建多个分区？ 可以的，broker数与分区数没有关系； 在kafka中，每一个分区会有一个编号：编号从0开始 某一个分区的数据是有序的 如何保证一个主题是有序的: 1一个主题下面只有一个分区即可 topic的Partition数量在创建topic时配置。 Partition数量决定了每个Consumer group中并发消费者的最大数量。 Consumer group A 有两个消费者来读取4个partition中数据；Consumer group B有四个消费者来读取4个 partition中的数据 kafka中的副本数 kafka分区副本数（kafka Partition Replicas) 副本数（replication-factor）：控制消息保存在几个broker（服务器）上，一般情况下等于broker的个数 一个broker服务下，是否可以创建多个副本因子？ ​ 不可以；创建主题时，副本因子应该小于等于可用的broker数。 副本因子操作以分区为单位的。每个分区都有各自的主副本和从副本；主副本叫做leader，从副本叫做 follower（在有多个副本的情况下，kafka会为同一个分区下的分区，设定角色关系：一个leader和N个 follower），处于同步状态的副本叫做in-sync-replicas(ISR);follower通过拉的方式从leader同步数据。消费 者和生产者都是从leader读写数据，不与follower交互。 副本因子的作用：让kafka读取数据和写入数据时的可靠性. 副本因子是包含本身|同一个副本因子不能放在同一个Broker中。 如果某一个分区有三个副本因子，就算其中一个挂掉，那么只会剩下的两个钟，选择一个leader，但不会在其 他的broker中，另启动一个副本（因为在另一台启动的话，存在数据传递，只要在机器之间有数据传递，就 会长时间占用网络IO，kafka是一个高吞吐量的消息系统，这个情况不允许发生）所以不会在零个broker中启 动。 如果所有的副本都挂了，生产者如果生产数据到指定分区的话，将写入不成功。 lsr表示：当前可用的副本 kafka的partition offset 任何发布到此partition的消息都会被直接追加到log文件的尾部，每条消息在文件中的位置称为oﬀset（偏移量）， oﬀset是一个long类型数字，它唯一标识了一条消息，消费者通过（oﬀset，partition，topic）跟踪记录。 kafka分区与消费组之间的关系 消费组： 由一个或者多个消费者组成，同一个组中的消费者对于同一条消息只消费一次。 某一个主题下的分区数，对于消费组来说，应该小于等于该主题下的分区数。如下所示： 123如：某一个主题有4个分区，那么消费组中的消费者应该小于4，而且最好与分区数成整数倍1 2 4同一个分区下的数据，在同一时刻，不能同一个消费组的不同消费者消费 总结：分区数越多，同一时间可以有越多的消费者来进行消费，消费数据的速度就会越快，提高消费的性能 四 集群搭建1 jdk 与zookeeper必须安装2 安装用户如默认用户安装则跳过这一步骤 安装hadoop，会创建一个hadoop用户 安装kafka，创建一个kafka用户 或者 创建bigdata用户，用来安装所有的大数据软件 本例：使用root用户来进行安装 3验证环境保证三台机器的zk服务都正常启动，且正常运行 查看zk的运行装填，保证有一台zk的服务状态为leader，且两台为follower即可 4下载安装包1http://archive.apache.org/dist/kafka/0.10.0.0/kafka_2.11-0.10.0.0.tgz 5上传解压6修改配置文件1234567891011121314151617181920212223242526cd /export/servers/kafka_2.11-0.10.0.0/configvim server.propertiesbroker.id=0num.network.threads=3num.io.threads=8socket.send.buffer.bytes=102400socket.receive.buffer.bytes=102400socket.request.max.bytes=104857600log.dirs=/export/servers/kafka_2.11-0.10.0.0/logsnum.partitions=2num.recovery.threads.per.data.dir=1offsets.topic.replication.factor=1transaction.state.log.replication.factor=1transaction.state.log.min.isr=1log.flush.interval.messages=10000log.flush.interval.ms=1000log.retention.hours=168log.segment.bytes=1073741824log.retention.check.interval.ms=300000zookeeper.connect=node01:2181,node02:2181,node03:2181zookeeper.connection.timeout.ms=6000group.initial.rebalance.delay.ms=0delete.topic.enable=truehost.name=node01 //每台主机不一样 创建数据文件存放目录 1mkdir -p /export/servers/kafka_2.11-0.10.0.0/logs 分发安装包: 123cd /export/servers/scp -r kafka_2.11-0.10.0.0/ node02:$PWDscp -r kafka_2.11-0.10.0.0/ node03:$PWD node02修改: 12345678910111213141516171819202122232425cd /export/servers/kafka_2.11-0.10.0.0/configvim server.propertiesbroker.id=1num.network.threads=3num.io.threads=8socket.send.buffer.bytes=102400socket.receive.buffer.bytes=102400socket.request.max.bytes=104857600log.dirs=/export/servers/kafka_2.11-0.10.0.0/logsnum.partitions=2num.recovery.threads.per.data.dir=1offsets.topic.replication.factor=1transaction.state.log.replication.factor=1transaction.state.log.min.isr=1log.flush.interval.messages=10000log.flush.interval.ms=1000log.retention.hours=168log.segment.bytes=1073741824log.retention.check.interval.ms=300000zookeeper.connect=node01:2181,node02:2181,node03:2181zookeeper.connection.timeout.ms=6000group.initial.rebalance.delay.ms=0delete.topic.enable=truehost.name=node02 node03修改 12345678910111213141516171819202122232425cd /export/servers/kafka_2.11-0.10.0.0/configvim server.propertiesbroker.id=2num.network.threads=3num.io.threads=8socket.send.buffer.bytes=102400socket.receive.buffer.bytes=102400socket.request.max.bytes=104857600log.dirs=/export/servers/kafka_2.11-0.10.0.0/logsnum.partitions=2num.recovery.threads.per.data.dir=1offsets.topic.replication.factor=1transaction.state.log.replication.factor=1transaction.state.log.min.isr=1log.flush.interval.messages=10000log.flush.interval.ms=1000log.retention.hours=168log.segment.bytes=1073741824log.retention.check.interval.ms=300000zookeeper.connect=node01:2181,node02:2181,node03:2181zookeeper.connection.timeout.ms=6000group.initial.rebalance.delay.ms=0delete.topic.enable=truehost.name=node03 启动集群: 注意事项：在kafka启动前，一定要让zookeeper启动起来。 前台启动: node01服务器执行以下命令来启动kafka集群 12cd /export/servers/kafka_2.11-0.10.0.0bin/kafka-server-start.sh config/server.properties 后台启动: node01**执行以下命令将kafka进程启动在后台** 12cd /export/servers/kafka_2.11-0.10.0.0nohup bin/kafka-server-start.sh config/server.properties &gt;/export/log/kafka.log 2&gt;&amp;1 &amp; 停止命令: node01执行以下命令便可以停止kakfa进程 12cd /export/servers/kafka_2.11-0.10.0.0bin/kafka-server-stop.sh 查看启动进程: 1jps 五 集群操作nohup bin/kafka-server-start.sh config/server.properties 2&gt;&amp;1 &amp; 创建topic三分区 两副本 1bin/kafka-topics.sh --create --partitions 3 --replication-factor 2 --topic test --zookeeper node01:2181,node02:2181,node03:2181 查看topic1bin/kafka-topics.sh --list --zookeeper node01:2181,node02:2181,node03:2181 生产数据1bin/kafka-console-producer.sh --broker-list node01:9092,node02:9092,node03:9092 --topic test 消费数据1bin/kafka-console-consumer.sh --from-beginning --topic test --zookeeper node01:2181,node02:2181,node03:2181 查看topic的一些信息1bin/kafka-topics.sh --describe --topic test --zookeeper node01:2181 修改topic的配置属性1bin/kafka-topics.sh --zookeeper node01:2181 --alter --topic test --config flush.messages=1 删除topic1bin/kafka-topics.sh --zookeeper node01:2181 --delete --topic test kafka集群当中JavaAPI操作依赖 12345678910111213141516171819202122232425262728&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &lt;version&gt; 0.10.0.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-streams&lt;/artifactId&gt; &lt;version&gt;0.10.0.0&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt;&lt;build&gt; &lt;plugins&gt; &lt;!-- java编译插件 --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.2&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; kafka集群当中ProducerAPI生产者代码: 12345678910111213141516171819202122232425262728293031323334public class MyProducer &#123; /** * 实现生产数据到kafka test这个topic里面去 * @param args */ public static void main(String[] args) &#123; Properties props = new Properties(); //kafka的机器 props.put("bootstrap.servers", "node01:9092"); //消息确认机制 props.put("acks", "all"); //消息没有发送成功 重试几次 props.put("retries", 0); //消息最大一批次 发送多少条 props.put("batch.size", 16384); //消息每条都进行确认 props.put("linger.ms", 1); //缓存内存大小 props.put("buffer.memory", 33554432); //一下俩个 k和value 进行序列化 和反序列化 props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer"); props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer"); //获取kafakProducer这个类 KafkaProducer&lt;String, String&gt; kafkaProducer = new KafkaProducer&lt;&gt;(props); //使用循环发送消息 for(int i =0;i&lt;100;i++)&#123; // kafkaProducer.send(new ProducerRecord&lt;String, String&gt;("test","mymessage" + i)); kafkaProducer.send(new ProducerRecord&lt;String, String&gt;("mypartition","mymessage" + i)); &#125; //关闭生产者 kafkaProducer.close(); &#125;&#125; kafka集群当中的consumerAPI消费者: 自动提交offset： 123456789101112131415161718192021222324252627282930313233public class AutomaticConsumer &#123; /** * 自动提交offset * @param args */ public static void main(String[] args) &#123; Properties props = new Properties(); props.put("bootstrap.servers", "node01:9092"); props.put("group.id", "test_group"); //消费组 props.put("enable.auto.commit", "true");//允许自动提交offset props.put("auto.commit.interval.ms", "1000");//每隔多久自动提交offset props.put("session.timeout.ms", "30000"); //指定key，value的反序列化类 props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer"); props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer"); KafkaConsumer&lt;String, String&gt; kafkaConsumer = new KafkaConsumer&lt;&gt;(props); //指定消费哪个topic里面的数据 kafkaConsumer.subscribe(Arrays.asList("test")); //使用死循环来消费test这个topic里面的数据 while (true)&#123; //这里面是我们所有拉取到的数据 ConsumerRecords&lt;String, String&gt; consumerRecords = kafkaConsumer.poll(1000); for (ConsumerRecord&lt;String, String&gt; consumerRecord : consumerRecords) &#123; long offset = consumerRecord.offset(); String value = consumerRecord.value(); System.out.println("消息的offset值为"+offset +"消息的value值为"+ value); &#125; &#125; &#125;&#125; 手动提交offset： 12345678910111213141516171819202122232425262728293031323334353637public class MannualConsumer &#123; /** * 实现手动的提交offset * @param args */ public static void main(String[] args) &#123; Properties props = new Properties(); props.put("bootstrap.servers", "node01:9092"); props.put("group.id", "test_group"); props.put("enable.auto.commit", "false"); //禁用自动提交offset，后期我们手动提交offset props.put("auto.commit.interval.ms", "1000"); props.put("session.timeout.ms", "30000"); props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer"); props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer"); KafkaConsumer&lt;String, String&gt; kafkaConsumer = new KafkaConsumer&lt;&gt;(props); kafkaConsumer.subscribe(Arrays.asList("test")); //订阅test这个topic int minBatchSize = 200; //达到200条进行批次的处理，处理完了之后，提交offset List&lt;ConsumerRecord&lt;String, String&gt;&gt; consumerRecordList = new ArrayList&lt;&gt;();//定义一个集合，用于存储我们的ConsumerRecorder while (true)&#123; ConsumerRecords&lt;String, String&gt; consumerRecords = kafkaConsumer.poll(1000); for (ConsumerRecord&lt;String, String&gt; consumerRecord : consumerRecords) &#123; consumerRecordList.add(consumerRecord); &#125; if(consumerRecordList.size() &gt;= minBatchSize)&#123; //如果集合当中的数据大于等于200条的时候，我们批量进行处理 //将这一批次的数据保存到数据库里面去 //insertToDb(consumerRecordList); System.out.println("手动提交offset的值"); //提交offset，表示这一批次的数据全部都处理完了 // kafkaConsumer.commitAsync(); //异步提交offset值 kafkaConsumer.commitSync();//同步提交offset的值 consumerRecordList.clear();//清空集合当中的数据 &#125; &#125; &#125;&#125; offset：offset记录了每个分区里面的消息消费到了哪一条，下一次来的时候，我们继续从上一次的记录接着消费 kafka的streamAPI需求：使用StreamAPI获取test这个topic当中的数据，然后将数据全部转为大写，写入到test2这个topic当中去 第一步：创建一个topicnode01服务器使用以下命令来常见一个topic 名称为test2 12cd /export/servers/kafka_2.11-0.10.0.0/bin/kafka-topics.sh --create --partitions 3 --replication-factor 2 --topic test2 --zookeeper node01:2181,node02:2181,node03:2181 第二步：开发StreamAPI注意：如果程序启动的时候抛出异常，找不到文件夹的路径，需要我们手动的去创建文件夹的路径 1234567891011121314151617181920212223242526public class Stream &#123; /** * 通过streamAPI实现将数据从test里面读取出来，写入到test2里面去 * @param args */ public static void main(String[] args) &#123; Properties properties = new Properties(); properties.put(StreamsConfig.APPLICATION_ID_CONFIG,"bigger"); properties.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG,"node01:9092"); properties.put(StreamsConfig.KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());//key的序列化和反序列化的类 properties.put(StreamsConfig.VALUE_SERDE_CLASS_CONFIG,Serdes.String().getClass()); //获取核心类 KStreamBuilder KStreamBuilder kStreamBuilder = new KStreamBuilder(); //通过KStreamBuilder调用stream方法表示从哪个topic当中获取数据 //调用mapValues方法，表示将每一行value都给取出来 //line表示我们取出来的一行行的数据 //将转成大写的数据，写入到test2这个topic里面去 kStreamBuilder.stream("test").mapValues(line -&gt; line.toString().toUpperCase()).to("test2"); //通过kStreamBuilder可以用于创建KafkaStream 通过kafkaStream来实现流失的编程启动 KafkaStreams kafkaStreams = new KafkaStreams(kStreamBuilder, properties); kafkaStreams.start(); //调用start启动kafka的流 API &#125;&#125; 第三步：生产数据123node01执行以下命令，向test这个topic当中生产数据cd /export/servers/kafka_2.11-0.10.0.0bin/kafka-console-producer.sh --broker-list node01:9092,node02:9092,node03:9092 --topic test 第四步：消费数据 node02执行一下命令消费test2这个topic当中的数据 12cd /export/servers/kafka_2.11-0.10.0.0bin/kafka-console-consumer.sh --from-beginning --topic test2 --zookeeper node01:2181,node02:2181,node03:2181 六 kafka原理一 生产者生产者是一个向kafka Cluster发布记录的客户端；生产者是线程安全的，跨线程共享单个生产者实例通常比具有多个实例更快。 必要条件 生产者要进行生产数据到kafka Cluster中，必要条件有以下三个： 1234#1、地址bootstrap.servers=node01:9092#2、序列化 key.serializer=org.apache.kafka.common.serialization.StringSerializer value.serializer=org.apache.kafka.common.serialization.StringSerializer#3、主题（topic） 需要制定具体的某个topic（order）即可。 生产者写数据 流程: 1、总体流程 Producer连接任意活着的Broker，请求指定Topic，Partion的Leader元数据信息，然后直接与对应的Broker直接连接，发布数据 2、开放分区接口(生产者数据分发策略) 2.1、用户可以指定分区函数，使得消息可以根据key，发送到指定的Partition中。 2.2、kafka在数据生产的时候，有一个数据分发策略。默认的情况使用DefaultPartitioner.class类。 这个类中就定义数据分发的策略。 2.3、如果是用户指定了partition，生产就不会调用DefaultPartitioner.partition()方法 2.4、当用户指定key，使用hash算法。如果key一直不变，同一个key算出来的hash值是个固定值。如果是固定 值，这种hash取模就没有意义。 1Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions 2.5、 当用既没有指定partition也没有key。 1234567/**The default partitioning strategy:&lt;ul&gt;&lt;li&gt;If a partition is specified in the record, use it&lt;li&gt;If no partition is specified but a key is present choose a partition based on a hash of the key&lt;li&gt;If no partition or key is present choose a partition in a round-robin fashion*/ 2.6、数据分发策略的时候，可以指定数据发往哪个partition。当ProducerRecord 的构造参数中有partition的时 候，就可以发送到对应partition上。 1234567891011121314151617181920212223242526272829303132333435363738394041424344public class PartitionProducer &#123; /** * kafka生产数据 通过不同的方式，将数据写入到不同的分区里面去 * * @param args */ public static void main(String[] args) &#123; Properties props = new Properties(); props.put("bootstrap.servers", "node01:9092"); props.put("acks", "all"); props.put("retries", 0); props.put("batch.size", 16384); props.put("linger.ms", 1); props.put("buffer.memory", 33554432); props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer"); props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer"); //配置我们自定义分区类 props.put("partitioner.class","cn.itcast.kafka.partition.MyPartitioner"); //获取kafakProducer这个类 KafkaProducer&lt;String, String&gt; kafkaProducer = new KafkaProducer&lt;&gt;(props); //使用循环发送消息 for(int i =0;i&lt;100;i++)&#123; //分区策略第一种，如果既没有指定分区号，也没有指定数据key，那么就会使用轮询的方式将数据均匀的发送到不同的分区里面去 //ProducerRecord&lt;String, String&gt; producerRecord1 = new ProducerRecord&lt;&gt;("mypartition", "mymessage" + i); //kafkaProducer.send(producerRecord1); //第二种分区策略 如果没有指定分区号，指定了数据key，通过key.hashCode % numPartitions来计算数据究竟会保存在哪一个分区里面 //注意：如果数据key，没有变化 key.hashCode % numPartitions = 固定值 所有的数据都会写入到同一个分区里面去 //ProducerRecord&lt;String, String&gt; producerRecord2 = new ProducerRecord&lt;&gt;("mypartition", "mykey", "mymessage" + i); //kafkaProducer.send(producerRecord2); //第三种分区策略：如果指定了分区号，那么就会将数据直接写入到对应的分区里面去 // ProducerRecord&lt;String, String&gt; producerRecord3 = new ProducerRecord&lt;&gt;("mypartition", 0, "mykey", "mymessage" + i); // kafkaProducer.send(producerRecord3); //第四种分区策略：自定义分区策略。如果不自定义分区规则，那么会将数据使用轮询的方式均匀的发送到各个分区里面去 kafkaProducer.send(new ProducerRecord&lt;String, String&gt;("mypartition","mymessage"+i)); &#125; //关闭生产者 kafkaProducer.close(); &#125;&#125; 自定义分区策略: 123456789101112131415161718192021//对应第四种分区public class MyPartitioner implements Partitioner &#123; /* 这个方法就是确定数据到哪一个分区里面去 直接return 2 表示将数据写入到2号分区里面去 */ @Override public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) &#123; return 2; &#125; @Override public void close() &#123; &#125; @Override public void configure(Map&lt;String, ?&gt; configs) &#123; &#125;&#125; 总体四种: a、可根据主题和内容发送 12345Producer&lt;String, String&gt; producer = new KafkaProducer&lt;String, String&gt;(props);//可根据主题和内容发送producer.send(new ProducerRecord&lt;String, String&gt;("my-topic","具体的数据")); b、根据主题，key、内容发送 12345Producer&lt;String, String&gt; producer = new KafkaProducer&lt;String, String&gt;(props);//可根据主题、key、内容发送producer.send(new ProducerRecord&lt;String, String&gt;("my-topic","key","具体的数据")); c、根据主题、分区、key、内容发送 12345Producer&lt;String, String&gt; producer = new KafkaProducer&lt;String, String&gt;(props);//可根据主题、分区、key、内容发送producer.send(new ProducerRecord&lt;String, String&gt;("my-topic",1,"key","具体的数据")); d、根据主题、分区、时间戳、key，内容发送 1234Producer&lt;String, String&gt; producer = new KafkaProducer&lt;String, String&gt;(props);//可根据主题、分区、时间戳、key、内容发送producer.send(new ProducerRecord&lt;String, String&gt;("my-topic",1,12L,"key","具体的数据")); 总结：1如果指定了数据的分区号，那么数据直接生产到对应的分区里面去 2如果没有指定分区好，出现了数据key，通过key取hashCode来计算数据究竟该落在哪一个分区里面 3如果既没有指定分区号，也没有指定数据的key，使用round-robin轮询 的这种机制来是实现 二 消费者消费者是一个从kafka Cluster中消费数据的一个客户端；该客户端可以处理kafka brokers中的故障问题，并且可以适应在集群内的迁移的topic分区；该客户端还允许消费者组使用消费者组来进行负载均衡。 消费者维持一个TCP的长连接来获取数据，使用后未能正常关闭这些消费者问题会出现，因此消费者不是线程安全 的。 必要条件 12345#1、地址bootstrap.servers=node01:9092#2、序列化 key.serializer=org.apache.kafka.common.serialization.StringSerializer value.serializer=org.apache.kafka.common.serialization.StringSerializer#3、主题（topic） 需要制定具体的某个topic（order）即可。#4、消费者组 group.id=test 一 自动提交offset的值(参考上面) 二 手动提交offset的值(参考上面) 三处理完每一个分区里面的数据，就马上提交这个分区里面的数据 1234567891011121314151617181920212223242526272829303132333435363738public class ConmsumerPartition &#123; /** * 处理完每一个分区里面的数据，就马上提交这个分区里面的数据 * @param args */ public static void main(String[] args) &#123; Properties props = new Properties(); props.put("bootstrap.servers", "node01:9092"); props.put("group.id", "test_group"); props.put("enable.auto.commit", "false"); //禁用自动提交offset，后期我们手动提交offset props.put("auto.commit.interval.ms", "1000"); props.put("session.timeout.ms", "30000"); props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer"); props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer"); KafkaConsumer&lt;String, String&gt; kafkaConsumer = new KafkaConsumer&lt;&gt;(props); kafkaConsumer.subscribe(Arrays.asList("mypartition")); while (true)&#123; //通过while ture进行消费数据 ConsumerRecords&lt;String, String&gt; records = kafkaConsumer.poll(1000); //获取mypartition这个topic里面所有的分区 Set&lt;TopicPartition&gt; partitions = records.partitions(); //循环遍历每一个分区里面的数据，然后将每一个分区里面的数据进行处理，处理完了之后再提交每一个分区里面的offset for (TopicPartition partition : partitions) &#123; //获取每一个分区里面的数据 List&lt;ConsumerRecord&lt;String, String&gt;&gt; records1 = records.records(partition); for (ConsumerRecord&lt;String, String&gt; record : records1) &#123; System.out.println(record.value()+"===="+ record.offset()); &#125; //获取我们分区里面最后一条数据的offset，表示我们已经消费到了这个offset了 long offset = records1.get(records1.size() - 1).offset(); //提交offset //提交我们的offset，并且给offset加1 表示我们下次从没有消费的那一条数据开始消费 kafkaConsumer.commitSync(Collections.singletonMap(partition,new OffsetAndMetadata(offset + 1))); &#125; &#125; &#125;&#125; 四 实现消费一个topic里面某些分区里面的数据 1234567891011121314151617181920212223242526272829303132333435363738public class ConsumerSomePartition &#123; //实现消费一个topic里面某些分区里面的数据 public static void main(String[] args) &#123; Properties props = new Properties(); props.put("bootstrap.servers", "node01:9092,node02:9092,node03:9092"); props.put("group.id", "test_group"); //消费组 props.put("enable.auto.commit", "true");//允许自动提交offset props.put("auto.commit.interval.ms", "1000");//每隔多久自动提交offset props.put("session.timeout.ms", "30000"); //指定key，value的反序列化类 props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer"); props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer"); //获取kafkaConsumer KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props); //通过consumer订阅某一个topic，进行消费.会消费topic里面所有分区的数据 // consumer.subscribe(); //通过调用assign方法实现消费mypartition这个topic里面的0号和1号分区里面的数据 TopicPartition topicPartition0 = new TopicPartition("mypartition", 0); TopicPartition topicPartition1 = new TopicPartition("mypartition", 1); //订阅我们某个topic里面指定分区的数据进行消费 consumer.assign(Arrays.asList(topicPartition0,topicPartition1)); int i =0; while(true)&#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(1000); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; i++; System.out.println("数据值为"+ record.value()+"数据的offset为"+ record.offset()); System.out.println("消费第"+i+"条数据"); &#125; &#125; &#125;&#125; 五 消费者数据丢失-数据重复 说明： 1、已经消费的数据对于kafka来说，会将消费组里面的oﬀset值进行修改，那什么时候进行修改了？是在数据消费 完成之后，比如在控制台打印完后自动提交； 2、提交过程：是通过kafka将oﬀset进行移动到下个message所处的oﬀset的位置。 3、拿到数据后，存储到hbase中或者mysql中，如果hbase或者mysql在这个时候连接不上，就会抛出异常，如果在处理数据的时候已经进行了提交，那么kafka伤的oﬀset值已经进行了修改了，但是hbase或者mysql中没有数据，这个时候就会出现数据丢失。 4、什么时候提交oﬀset值？在Consumer将数据处理完成之后，再来进行oﬀset的修改提交。默认情况下oﬀset是 自动提交，需要修改为手动提交oﬀset值。 5、如果在处理代码中正常处理了，但是在提交oﬀset请求的时候，没有连接到kafka或者出现了故障，那么该次修 改oﬀset的请求是失败的，那么下次在进行读取同一个分区中的数据时，会从已经处理掉的oﬀset值再进行处理一 次，那么在hbase中或者mysql中就会产生两条一样的数据，也就是数据重复 kafka当中数据消费模型: eactly once：消费且仅仅消费一次，可以在事务里面执行kafka的操作 at most once：至多消费一次，数据丢失的问题 at least once ：至少消费一次，数据重复消费的问题 kafka的消费模式：决定了offset值保存在哪里: kafka的highLevel API进行消费：将offset保存在zk当中，每次更新offset的时候，都需要连接zk 以及kafka的lowLevelAP进行消费：保存了消费的状态，其实就是保存了offset，将offset保存在kafka的一个默认的topic里面。kafka会自动的创建一个topic，保存所有其他topic里面的offset在哪里 kafka将数据全部都以文件的方式保存到了文件里面去了。 ###三 kafka的log-存储机制 kafka里面一个topic有多个partition组成，每一个partition里面有多个segment组成，每个segment都由两部分组成，分别是.log文件和.index文件 。一旦.log文件达到1GB的时候，就会生成一个新的segment .log文件：顺序的保存了我们的写入的数据 100000000000000000000.log .index文件：索引文件，使用索引文件，加快kafka数据的查找速度 100000000000000000000.index kafka日志的组成: 1 segment ﬁle组成：由两个部分组成，分别为index ﬁle和data ﬁle，此两个文件一一对应且成对出现； 后缀.index和.log分别表示为segment的索引文件、数据文件。 2 segment文件命名规则：partion全局的第一个segment从0开始，后续每个segment文件名为上一个全局 partion的最大oﬀset（偏移message数）。数值最大为64位long大小，19位数字字符长度，没有数字就用0 填充 3 通过索引信息可以快速定位到message。通过index元数据全部映射到memory，可以避免segment ﬁle的IO磁盘操作； 4 通过索引文件稀疏存储，可以大幅降低index文件元数据占用空间大小。 稀疏索引：为了数据创建索引，但范围并不是为每一条创建，而是为某一个区间创建； 好处：就是可以减少索引值的数量。 不好的地方：找到索引区间之后，要得进行第二次处理。 kafka日志清理 kafka中清理日志的方式有两种：delete和compact。 删除的阈值有两种：过期的时间和分区内总日志大小。 在kafka中，因为数据是存储在本地磁盘中，并没有像hdfs的那样的分布式存储，就会产生磁盘空间不足的情 况，可以采用删除或者合并的方式来进行处理 可以通过时间来删除、合并：默认7天 还可以通过字节大小、合并 总结：查找数据的过程 第一步：通过offset确定数据保存在哪一个segment里面了， 第二部：查找对应的segment里面的index文件 。index文件都是key/value对的。key表示数据在log文件里面的顺序是第几条。value记录了这一条数据在全局的标号。如果能够直接找到对应的offset直接去获取对应的数据即可 如果index文件里面没有存储offset，就会查找offset最近的那一个offset，例如查找offset为7的数据找不到，那么就会去查找offset为6对应的数据，找到之后，再取下一条数据就是offset为7的数据 四 kafka的消息不丢失机制生产者：使用ack机制 有多少个分区，就启动多少个线程来进行同步数据 12345678910111发送数据的方式: 可以采用同步或者异步的方式 同步：发送一批数据给kafka后，等待kafka返回结果 1、生产者等待10s，如果broker没有给出ack相应，就认为失败。 2、生产者重试3次，如果还没有相应，就报错 异步: 发送一批数据给kafka，只是提供一个回调函数 1、先将数据保存在生产者端的buffer中。buffer大小是2万条 2、满足数据阈值或者数量阈值其中的一个条件就可以发送数据。 3、发送一批数据的大小是500条 说明：如果broker迟迟不给ack，而buﬀer又满了，开发者可以设置是否直接清空buﬀer中的数据。 123456ask机制 确认机制服务端返回一个确认码，即ack响应码；ack的响应有三个状态值 0：生产者只负责发送数据，不关心数据是否丢失，响应的状态码为0（丢失的数据，需要再次发送 ） 1：partition的leader收到数据，响应的状态码为1 -1：所有的从节点都收到数据，响应的状态码为-1 说明：如果broker端一直不给ack状态，producer永远不知道是否成功；producer可以设置一个超时时间10s，超 过时间认为失败。 broker：使用partition的副本机制 消费者：使用offset来进行记录 在消费者消费数据的时候，只要每个消费者记录好oﬀset值即可，就能保证数据不丢失。 五 CAP 理论 与kafka中的CAP分布式系统（distributed system）正变得越来越重要，大型网站几乎都是分布式的。 分布式系统的最大难点，就是各个节点的状态如何同步。 为了解决各个节点之间的状态同步问题，在1998年，由加州大学的计算机科学家 Eric Brewer 提出分布式系统的三个指标，分别是 12345 Consistency：一致性Availability：可用性Partition tolerance：分区容错性这三个指标不可能同时做到。这个结论就叫做 CAP 定理 1 Partition tolerance 大多数分布式系统都分布在多个子网络。每个子网络就叫做一个区（partition）。分区容错的意思是，区间通信可能失败。比如，一台服务器放在中国，另一台服务器放在美国，这就是两个区，它们之间可能无法通信 一般来说，分区容错无法避免，因此可以认为 CAP 的 P 总是存在的。即永远可能存在分区容错这个问题 2 Consistency Consistency 中文叫做”一致性”。意思是，写操作之后的读操作，必须返回该值。 如 v0这个数据存在 s1和s2中 用户向s1中 把v0这个值改为v1 则s2中的值也应该改为 v1 3 Availability Availability 中文叫做”可用性”，意思是只要收到用户的请求，服务器就必须给出回应。 用户可以选择向服务器 G1 或 G2 发起读操作。不管是哪台服务器，只要收到请求，就必须告诉用户，到底是 v0 还是 v1，否则就不满足可用性。 kafka中的CAP 应用 kafka为一个分布式消息队列系统,一定满足CAP 定理 ​ kafka满足的是CAP定律当中的CA，其中Partition tolerance通过的是一定的机制尽量的保证分区容错性。 其中C表示的是数据一致性。A表示数据可用性。 kafka首先将数据写入到不同的分区里面去，每个分区又可能有好多个副本，数据首先写入到leader分区里面去，读写的操作都是与leader分区进行通信，保证了数据的一致性原则，也就是满足了Consistency原则。然后kafka通过分区副本机制，来保证了kafka当中数据的可用性。但是也存在另外一个问题，就是副本分区当中的数据与leader当中的数据存在差别的问题如何解决，这个就是Partition tolerance的问题。 kafka为了解决Partition tolerance的问题，使用了ISR的同步策略，来尽最大可能减少Partition tolerance的问题 1234每个leader会维护一个ISR（a set of in-sync replicas，基本同步）列表ISR列表主要的作用就是决定哪些副本分区是可用的，也就是说可以将leader分区里面的数据同步到副本分区里面去，决定一个副本分区是否可用的条件有两个• replica.lag.time.max.ms=10000 副本分区与主分区心跳时间延迟• replica.lag.max.messages=4000 副本分区与主分区消息同步最大差 总结 主分区与副本分区之间的数据同步： 两个指标，一个是副本分区与主分区之间的心跳间隔，超过10S就认为副本分区已经宕机，会将副本分区从ISR当中移除 主分区与副本分区之间的数据同步延迟，默认数据差值是4000条 例如主分区有10000条数据，副本分区同步了3000条，差值是7000 &gt; 4000条，也会将这个副本分区从ISR列表里面移除掉 kafka in zookeeper kafka集群中：包含了很多的broker，但是在这么的broker中也会有一个老大存在；是在kafka节点中的一个临时节 点，去创建相应的数据，这个老大就是 Controller Broker。 Controller Broker职责：管理所有的 kafka 监控及运维在开发工作中，消费在Kafka集群中消息，数据变化是我们关注的问题，当业务前提不复杂时，我们可以使用Kafka 命令提供带有Zookeeper客户端工具的工具，可以轻松完成我们的工作。随着业务的复杂性，增加Group和 Topic，那么我们使用Kafka提供命令工具，已经感到无能为力，那么Kafka监控系统目前尤为重要，我们需要观察 消费者应用的细节。 1、kafka-eagle概述 为了简化开发者和服务工程师维护Kafka集群的工作有一个监控管理工具，叫做 Kafka-eagle。这个管理工具可以很容易地发现分布在集群中的哪些topic分布不均匀，或者是分区在整个集群分布不均匀的的情况。它支持管理多个集群、选择副本、副本重新分配以及创建Topic。同时，这个管理工具也是一个非常好的可以快速浏览这个集群的工具， 2、环境和安装1、环境要求需要安装jdk，启动zk以及kafka的服务 2、安装步骤1、下载源码包kafka-eagle官网： http://download.kafka-eagle.org/ 我们可以从官网上面直接下载最细的安装包即可kafka-eagle-bin-1.3.2.tar.gz这个版本即可 代码托管地址： https://github.com/smartloli/kafka-eagle/releases 2、解压这里我们选择将kafak-eagle安装在第三台 直接将kafka-eagle安装包上传到node03服务器的/export/softwares路径下，然后进行解压 node03服务器执行一下命令进行解压 1234cd /export/softwares/tar -zxf kafka-eagle-bin-1.3.2.tar.gz -C /export/servers/cd /export/servers/kafka-eagle-bin-1.3.2tar -zxf kafka-eagle-web-1.3.2-bin.tar.gz 3、准备数据库kafka-eagle需要使用一个数据库来保存一些元数据信息，我们这里直接使用msyql数据库来保存即可，在node03服务器执行以下命令创建一个mysql数据库即可 进入mysql客户端 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128create database eagle;在这个库中创建表:/* Navicat MySQL Data Transfer Source Server : local Source Server Version : 50616 Source Host : localhost Source Database : ke Target Server Version : 50616 File Encoding : utf-8 Date: 06/23/2017 17:02:12 PM*/use eagle;SET NAMES utf8;SET FOREIGN_KEY_CHECKS = 0;-- ------------------------------ Table structure for `ke_p_role`-- ----------------------------DROP TABLE IF EXISTS `ke_p_role`;CREATE TABLE `ke_p_role` ( `id` tinyint(4) NOT NULL AUTO_INCREMENT, `name` varchar(64) CHARACTER SET utf8 NOT NULL COMMENT 'role name', `seq` tinyint(4) NOT NULL COMMENT 'rank', `description` varchar(128) CHARACTER SET utf8 NOT NULL COMMENT 'role describe', PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=4 DEFAULT CHARSET=utf8;-- ------------------------------ Records of `ke_p_role`-- ----------------------------BEGIN;INSERT INTO `ke_p_role` VALUES ('1', 'Administrator', '1', 'Have all permissions'), ('2', 'Devs', '2', 'Own add or delete'), ('3', 'Tourist', '3', 'Only viewer');COMMIT;-- ------------------------------ Table structure for `ke_resources`-- ----------------------------DROP TABLE IF EXISTS `ke_resources`;CREATE TABLE `ke_resources` ( `resource_id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(255) CHARACTER SET utf8 NOT NULL COMMENT 'resource name', `url` varchar(255) NOT NULL, `parent_id` int(11) NOT NULL, PRIMARY KEY (`resource_id`)) ENGINE=InnoDB AUTO_INCREMENT=17 DEFAULT CHARSET=utf8;-- ------------------------------ Records of `ke_resources`-- ----------------------------BEGIN;INSERT INTO `ke_resources` VALUES ('1', 'System', '/system', '-1'), ('2', 'User', '/system/user', '1'), ('3', 'Role', '/system/role', '1'), ('4', 'Resource', '/system/resource', '1'), ('5', 'Notice', '/system/notice', '1'), ('6', 'Topic', '/topic', '-1'), ('7', 'Message', '/topic/message', '6'), ('8', 'Create', '/topic/create', '6'), ('9', 'Alarm', '/alarm', '-1'), ('10', 'Add', '/alarm/add', '9'), ('11', 'Modify', '/alarm/modify', '9'), ('12', 'Cluster', '/cluster', '-1'), ('13', 'ZkCli', '/cluster/zkcli', '12'), ('14', 'UserDelete', '/system/user/delete', '1'), ('15', 'UserModify', '/system/user/modify', '1'), ('16', 'Mock', '/topic/mock', '6');COMMIT;-- ------------------------------ Table structure for `ke_role_resource`-- ----------------------------DROP TABLE IF EXISTS `ke_role_resource`;CREATE TABLE `ke_role_resource` ( `id` int(11) NOT NULL AUTO_INCREMENT, `role_id` int(11) NOT NULL, `resource_id` int(11) NOT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=19 DEFAULT CHARSET=utf8;-- ------------------------------ Records of `ke_role_resource`-- ----------------------------BEGIN;INSERT INTO `ke_role_resource` VALUES ('1', '1', '1'), ('2', '1', '2'), ('3', '1', '3'), ('4', '1', '4'), ('5', '1', '5'), ('6', '1', '7'), ('7', '1', '8'), ('8', '1', '10'), ('9', '1', '11'), ('10', '1', '13'), ('11', '2', '7'), ('12', '2', '8'), ('13', '2', '13'), ('14', '2', '10'), ('15', '2', '11'), ('16', '1', '14'), ('17', '1', '15'), ('18', '1', '16');COMMIT;-- ------------------------------ Table structure for `ke_trend`-- ----------------------------DROP TABLE IF EXISTS `ke_trend`;CREATE TABLE `ke_trend` ( `cluster` varchar(64) NOT NULL, `key` varchar(64) NOT NULL, `value` varchar(64) NOT NULL, `hour` varchar(2) NOT NULL, `tm` varchar(16) NOT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8;-- ------------------------------ Table structure for `ke_user_role`-- ----------------------------DROP TABLE IF EXISTS `ke_user_role`;CREATE TABLE `ke_user_role` ( `id` int(11) NOT NULL AUTO_INCREMENT, `user_id` int(11) NOT NULL, `role_id` tinyint(4) NOT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8;-- ------------------------------ Records of `ke_user_role`-- ----------------------------BEGIN;INSERT INTO `ke_user_role` VALUES ('1', '1', '1');COMMIT;-- ------------------------------ Table structure for `ke_users`-- ----------------------------DROP TABLE IF EXISTS `ke_users`;CREATE TABLE `ke_users` ( `id` int(11) NOT NULL AUTO_INCREMENT, `rtxno` int(11) NOT NULL, `username` varchar(64) NOT NULL, `password` varchar(128) NOT NULL, `email` varchar(64) NOT NULL, `realname` varchar(128) NOT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8;-- ------------------------------ Records of `ke_users`-- ----------------------------BEGIN;INSERT INTO `ke_users` VALUES ('1', '1000', 'admin', '123456', 'admin@email.com', 'Administrator');COMMIT;SET FOREIGN_KEY_CHECKS = 1; 4、修改kafak-eagle配置文件node03执行以下命令修改kafak-eagle配置文件 1234567891011cd /export/servers/kafka-eagle-bin-1.3.2/kafka-eagle-web-1.3.2/confvim system-config.propertieskafka.eagle.zk.cluster.alias=cluster1,cluster2cluster1.zk.list=node01:2181,node02:2181,node03:2181cluster2.zk.list=node01:2181,node02:2181,node03:2181kafka.eagle.driver=com.mysql.jdbc.Driverkafka.eagle.url=jdbc:mysql://node03:3306/eaglekafka.eagle.username=rootkafka.eagle.password=123456 5、配置环境变量kafka-eagle必须配置环境变量，node03服务器执行以下命令来进行配置环境变量 123456vim /etc/profileexport KE_HOME=/export/servers/kafka-eagle-bin-1.3.2/kafka-eagle-web-1.3.2export PATH=:$KE_HOME/bin:$PATHsource /etc/profile 6、启动kafka-eaglenode03执行以下界面启动kafka-eagle 123cd /export/servers/kafka-eagle-bin-1.3.2/kafka-eagle-web-1.3.2/binchmod u+x ke.sh./ke.sh start stop status 7、主界面访问kafka-eagle 123http://node03:8048/ke用户名：admin密码：123456]]></content>
      <tags>
        <tag>BigData</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ClouderaManager]]></title>
    <url>%2F2017%2F07%2F15%2FClouderaManager.html</url>
    <content type="text"><![CDATA[ClouderaManagercm 管理工具,用来安装cdh&lt;hadoop,zookeeper,habse…….&gt; cdh (cloudera distribute hadoop&lt;hadoop,zookeeper,hbase……&gt;) 版本:5.14.0 Cloudera Manager是cloudera公司提供的一种大数据的解决方案，可以通过ClouderaManager管理界面来对我们的集群进行安装和操作，提供了良好的UI界面交互，使得我们管理集群不用熟悉任何的linux技术，只需要通过网页浏览器就可以实现我们的集群的操作和管理，让我们使用和管理集群更加的方便。 1、ClouderaManager整体架构 Cloudera Manager的核心是Cloudera Manager Server。Server托管Admin Console Web Server和应用程序逻辑。它负责安装软件、配置、启动和停止服务以及管理运行服务的群集。 解释： · Agent：安装在每台主机上。它负责启动和停止进程，解压缩配置，触发安装和监控主机 · Management Service：执行各种监控、报警和报告功能的一组角色的服务。 · Database：存储配置和监控信息 · Cloudera Repository：可供Cloudera Manager分配的软件的存储库（repo库） · Client：用于与服务器进行交互的接口： · Admin Console：管理员控制台 · API：开发人员使用 API可以创建自定义的Cloudera Manager应用程序 Cloudera Management ServiceCloudera Management Service 可作为一组角色实施各种管理功能 · Activity Monitor：收集有关服务运行的活动的信息 · Host Monitor：收集有关主机的运行状况和指标信息 · Service Monitor：收集有关服务的运行状况和指标信息 · Event Server：聚合组件的事件并将其用于警报和搜索 · Alert Publisher ：为特定类型的事件生成和提供警报 · Reports Manager：生成图表报告，它提供用户、用户组的目录的磁盘使用率、磁盘、io等历史视图 信号检测默认情况下，Agent 每隔 15 秒向 Cloudera Manager Server 发送一次检测信号。但是，为了减少用户延迟，在状态变化时会提高频率。 状态管理· 模型状态捕获什么进程应在何处运行以及具有什么配置 · 运行时状态是哪些进程正在何处运行以及正在执行哪些命令（例如，重新平衡 HDFS 或执行备份/灾难恢复计划或滚动升级或停止） · 当您更新配置（例如Hue Server Web 端口）时，您即更新了模型状态。但是，如果 Hue 在更新时正在运行，则它仍将使用旧端口。当出现这种不匹配情况时，角色会标记为具有”过时的配置”。要重新同步，您需重启角色（这会触发重新生成配置和重启进程） · 特殊情况如果要加入一些clouder manager控制台没有的属性时候都在高级里面嵌入 服务器和客户端配置· 如使用HDFS，文件 /etc/hadoop/conf/hdfs-site.xml 仅包含与 HDFS 客户端相关的配置 · 而 HDFS 角色实例（例如，NameNode 和 DataNode）会从/var/run/cloudera-scm-agent/process/unique-process-name下的每个进程专用目录获取它们的配置 进程管理· 在 Cloudera Manager 管理的群集中，只能通过 Cloudera Manager 启动或停止服务。ClouderaManager 使用一种名为 supervisord的开源进程管理工具，它会重定向日志文件，通知进程失败，为合适用户设置调用进程的有效用户 ID 等等 · Cloudera Manager 支持自动重启崩溃进程。如果一个角色实例在启动后反复失败，Cloudera Manager还会用不良状态标记该实例 · 特别需要注意的是，停止 Cloudera Manager 和 Cloudera Manager Agent 不会停止群集；所有正在运行的实例都将保持运行 · Agent 的一项主要职责是启动和停止进程。当 Agent 从检测信号检测到新进程时，Agent 会在/var/run/cloudera-scm-agent 中为它创建一个目录，并解压缩配置 · Agent 受到监控，属于 Cloudera Manager 的主机监控的一部分：如果 Agent 停止检测信号，主机将被标记为运行状况不良 主机管理· Cloudera Manager 自动将作为群集中的托管主机身份：JDK、Cloudera Manager Agent、CDH、Impala、Solr 等参与所需的所有软件部署到主机 · Cloudera Manager 提供用于管理参与主机生命周期的操作以及添加和删除主机的操作 · Cloudera Management Service Host Monitor 角色执行运行状况检查并收集主机度量，以使您可以监控主机的运行状况和性能 安全· 身份验证 · Hadoop中身份验证的目的仅仅是证明用户或服务确实是他或她所声称的用户或服务，通常，企业中的身份验证通过单个分布式系统（例如，轻型目录访问协议 (LDAP) 目录）进行管理。LDAP身份验证包含由各种存储系统提供支持的简单用户名/密码服务 · Hadoop 生态系统的许多组件会汇总到一起来使用 Kerberos 身份验证并提供用于在 LDAP 或 AD 中管理和存储凭据的选项 · 授权 CDH 当前提供以下形式的访问控制： · 适用于目录和文件的传统 POSIX 样式的权限 · 适用于 HDFS 的扩展的访问控制列表 (ACL) · Apache HBase 使用 ACL 来按列、列族和列族限定符授权各种操作 (READ, WRITE, CREATE, ADMIN) · 使用 Apache Sentry 基于角色进行访问控制 · 加密 · 需要获得企业版的Cloudera（Cloudera Navigator 许可） 2、clouderaManager环境安装前准备准备两台虚拟机，其中一台作为我们的主节点，安装我们的ClouderaManager Server与ClouderaManager agent，另外一台作为我们的从节点只安装我们的clouderaManager agent 机器规划如下 服务器IP 192.168.52.100 192.168.52.110 主机名 node01.hadoop.com node02.hadoop.com 主机名与IP地址映射 是 是 防火墙 关闭 关闭 selinux 关闭 关闭 jdk 安装 安装 ssh免密码登录 是 是 mysql数据库 否 是 服务器内存 16G 8G 所有机器统一两个路径 12mkdir -p /export/softwares/mkdir -p /export/servers/ 2.1、两台机器更改主机名第一台机器更改主机名 123vim /etc/sysconfig/networkNETWORKING=yesHOSTNAME=node01.hadoop.com 第二台机器更改主机名 123vim /etc/sysconfig/networkNETWORKING=yesHOSTNAME=node02.hadoop.com 2.2、更改主机名与IP地址的映射两台机器更改hosts文件 123vim /etc/hosts192.168.52.100 node01.hadoop.com node01192.168.52.110 node02.hadoop.com node02 2.3、两台机器关闭防火墙12service iptables stopchkconfig iptables off 2.4、两台机器关闭selinux12vim /etc/selinux/configSELINUX=disabled 2.5、两台机器安装jdk将我们的jdk的压缩包上传到node01.hadoop.com的/export/softwares路径下 12cd /export/softwares/tar -zxvf jdk-8u141-linux-x64.tar.gz -C /export/servers/ 配置环境变量 123456vim /etc/profileexport JAVA_HOME=/export/servers/jdk1.8.0_141export PATH=:$JAVA_HOME/bin:PATHsource /etc/profile 第二台机器同样安装jdk即可 2.6、两台机器实现SSH免密码登录第一步：两台器生成公钥与私钥两台机器上面执行以下命令，然后按下三个回车键即可生成公钥与私钥 1ssh-keygen -t rsa 第二步：两台机器将公钥拷贝到同一个文件当中去两台机器执行以下命令 1ssh-copy-id node01.hadoop.com 第三步：拷贝authorized_keys到其他机器第一台机器上将authorized_keys拷贝到第二台机器 1scp /root/.ssh/authorized_keys node02.hadoop.com:/root/.ssh/ 2.7、第二台机器安装mysql数据库通过yum源，在线安装mysql 12345yum install mysql mysql-server mysql-devel/etc/init.d/mysqld start/usr/bin/mysql_secure_installation grant all privileges on . to 'root'@'%' identified by '123456' with grant option; flush privileges; 2.8、解除linux系统打开文件最大数量的限制两台机器都需要执行 1vi /etc/security/limits.conf 添加以下内容 1234567* soft noproc 11000* hard noproc 11000* soft nofile 65535* hard nofile 65535 2.9、设置linux交换区内存两台机器都要执行 执行命令 1echo 10 &gt; /proc/sys/vm/swappiness 并编辑文件sysctl.conf： 12345vim /etc/sysctl.conf添加或修改vm.swappiness = 0 两台机器都要执行： 12echo never &gt; /sys/kernel/mm/transparent_hugepage/defragecho never &gt; /sys/kernel/mm/transparent_hugepage/enabled 并编辑文件rc.local ： 1234vim /etc/rc.localecho never &gt; /sys/kernel/mm/transparent_hugepage/defragecho never &gt; /sys/kernel/mm/transparent_hugepage/enabled 2.10、两台机器时钟同步两台机器需要进行时钟同步操作，保证两台机器时间相同 3、clouderaManager安装资源下载第一步：下载安装资源并上传到服务器我们这里安装CM5.14.0这个版本，需要下载以下这些资源，一共是四个文件即可 下载cm5的压缩包 下载地址：http://archive.cloudera.com/cm5/cm/5/ 具体文件地址： http://archive.cloudera.com/cm5/cm/5/cloudera-manager-el6-cm5.14.0_x86_64.tar.gz 下载cm5的parcel包 下载地址： http://archive.cloudera.com/cdh5/parcels/ 第一个文件具体下载地址： http://archive.cloudera.com/cdh5/parcels/5.14.0/CDH-5.14.0-1.cdh5.14.0.p0.24-el6.parcel 第二个文件具体下载地址： http://archive.cloudera.com/cdh5/parcels/5.14.0/CDH-5.14.0-1.cdh5.14.0.p0.24-el6.parcel.sha1 第三个文件具体下载地址： http://archive.cloudera.com/cdh5/parcels/5.14.0/manifest.json 将这四个安装包都上传到第一台机器的/opt/softwares路径下 第二步：解压压缩包到指定路径解压CM安装包到/opt路径下去 123cd /export/softwarestar -zxvf cloudera-manager-el6-cm5.14.0_x86_64.tar.gz -C /opt/ 第三步：将我们的parcel包的三个文件拷贝到对应路径将我们的parcel包含三个文件，拷贝到/opt/cloudera/parcel-repo路径下面去，并记得有个文件需要重命名 123cd /export/softwares/cp CDH-5.14.0-1.cdh5.14.0.p0.24-el6.parcel CDH-5.14.0-1.cdh5.14.0.p0.24-el6.parcel.sha1 manifest.json /opt/cloudera/parcel-repo/ 重命名标黄的这个文件 123cd /opt/cloudera/parcel-repo/mv CDH-5.14.0-1.cdh5.14.0.p0.24-el6.parcel.sha1 CDH-5.14.0-1.cdh5.14.0.p0.24-el6.parcel.sha 第四步：所有节点添加普通用户并给与sudo权限在node01机器上面添加普通用户并赋予sudo权限 执行以下命令创建普通用户cloudera-scm 1useradd --system --home=/opt/cm-5.14.0/run/cloudera-scm-server --no-create-home --shell=/bin/false --comment "Cloudera SCM User" cloudera-scm 赋予cloudera-scm普通用户的sudo权限 123vim /etc/sudoerscloudera-scm ALL=(ALL) NOPASSWD: ALL 第五步：更改主节点的配置文件node01机器上面更改配置文件 123vim /opt/cm-5.14.0/etc/cloudera-scm-agent/config.iniserver_host=node01.hadoop.com 第六步：将/opt目录下的安装包发放到其他机器将第一台机器的安装包发放到其他机器 123cd /optscp -r cloudera/ cm-5.14.0/ node02:/opt 第七步：创建一些数据库备用node02机器上面创建数据库 123456789101112131415hive 数据库create database hive DEFAULT CHARSET utf8 COLLATE utf8_general_ci;集群监控数据库create database amon DEFAULT CHARSET utf8 COLLATE utf8_general_ci;hue 数据库create database hue DEFAULT CHARSET utf8 COLLATE utf8_general_ci;oozie 数据库create database oozie DEFAULT CHARSET utf8 COLLATE utf8_general_ci; 第八步：准备数据库连接的驱动包在所有机器上面都准备一份数据库的连接驱动jar包放到/usr/share/java路径下 准备一份mysql的驱动连接包，放到/usr/share/java路径下去 123456789cd /export/softwares/wget https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.45.tar.gztar -zxvf mysql-connector-java-5.1.45.tar.gzcd /export/softwares/mysql-connector-java-5.1.45cp mysql-connector-java-5.1.45-bin.jar /usr/share/java/mysql-connector-java.jar 拷贝驱动包到第二台机器 123cd /usr/share/javascp mysql-connector-java.jar node02:$PWD 第九步：为clouderaManager创建数据库1/opt/cm-5.14.0/share/cmf/schema/scm_prepare_database.sh mysql -hnode02 -uroot -p123456 --scm-host node01 scm root 123456 命令说明：/opt/cm-5.14.0/share/cmf/schema/scm_prepare_database.sh 数据库类型 -h数据库主机 –u数据库用户名 –p数据库密码 –scm-host cm主机 数据库名称 用户名 密码 第十步：启动服务主节点启动clouderaManager Server与ClouderaManager agent服务 123/opt/cm-5.14.0/etc/init.d/cloudera-scm-server start/opt/cm-5.14.0/etc/init.d/cloudera-scm-agent start 从节点node02启动ClouderaManager agent服务 第十一步：浏览器页面访问http://node01:7180/cmf/login 默认用户名admin 密码 admin]]></content>
      <tags>
        <tag>BigData</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kudu]]></title>
    <url>%2F2017%2F07%2F14%2Fkudu.html</url>
    <content type="text"><![CDATA[KuDu一概述背景介绍在KUDU之前，大数据主要以两种方式存储； （1）静态数据： 以 HDFS 引擎作为存储引擎，适用于高吞吐量的离线大数据分析场景。 这类存储的局限性是数据无法进行随机的读写。 （2）动态数据： 以 HBase、Cassandra 作为存储引擎，适用于大数据随机读写场景。 局限性是批量读取吞吐量远不如 HDFS，不适用于批量数据分析的场景。 从上面分析可知，这两种数据在存储方式上完全不同，进而导致使用场景完全不同，但在真实的场景中，边界可能没有那么清晰，面对既需要随机读写，又需要批量分析的大数据场景，该如何选择呢？ 这个场景中，单种存储引擎无法满足业务需求，我们需要通过多种大数据工具组合来满足这一需求 如上图所示，数据实时写入 HBase，实时的数据更新也在 HBase 完成，为了应对 OLAP 需求，我们定时将 HBase 数据写成静态的文件（如：Parquet）导入到 OLAP 引擎（如：Impala、hive）。这一架构能满足既需要随机读写，又可以支持 OLAP 分析的场景，但他有如下缺点： (1)架构复杂。从架构上看，数据在HBase、消息队列、HDFS 间流转，涉及环节太多，运维成本很高。并且每个环节需要保证高可用，都需要维护多个副本，存储空间也有一定的浪费。最后数据在多个系统上，对数据安全策略、监控等都提出了挑战。 (2)时效性低。数据从HBase导出成静态文件是周期性的，一般这个周期是一天（或一小时），在时效性上不是很高。 (3)难以应对后续的更新。真实场景中，总会有数据是延迟到达的。如果这些数据之前已经从HBase导出到HDFS，新到的变更数据就难以处理了，一个方案是把原有数据应用上新的变更后重写一遍，但这代价又很高。 为了解决上述架构的这些问题，KUDU应运而生。KUDU的定位是Fast Analytics on Fast Data，是一个既支持随机读写、又支持 OLAP 分析的大数据存储引擎。 KUDU 是一个折中的产品，在 HDFS 和 HBase 这两个偏科生中平衡了随机读写和批量分析的性能。从 KUDU 的诞生可以说明一个观点：底层的技术发展很多时候都是上层的业务推动的，脱离业务的技术很可能是空中楼阁。 kudu是什么 是一个大数据存储引擎 用于大数据的存储，结合其他软件开展数据分析。 汲取了hdfs中高吞吐数据的能力和hbase中高随机读写数据的能力 既满足有传统OLAP分析 又满足于随机读写访问数据 kudu来自于cloudera 后来贡献给了apache kudu应用场景 适用于那些既有随机访问，也有批量数据扫描的复合场景 高计算量的场景 使用了高性能的存储设备，包括使用更多的内存 支持数据更新，避免数据反复迁移 支持跨地域的实时数据备份和查询 二架构 kudu集群是主从架构 主角色 master ：管理集群 管理元数据 从角色 tablet server：负责最终数据的存储 对外提供数据读写能力 里面存储的都是一个个tablet kudu tablet 是kudu表中的数据水平分区 一个表可以划分成为多个tablet(类似于hbase region) tablet中主键是不重复连续的 所有tablet加起来就是一个table的所有数据 tablet在存储的时候 会进行冗余存放 设置多个副本 在一个tablet所有冗余中 任意时刻 一个是leader 其他的冗余都是follower 与HDFS和HBase相似，Kudu使用单个的Master节点，用来管理集群的元数据，并且使用任意数量的Tablet Server（类似HBase中的RegionServer角色）节点用来存储实际数据。可以部署多个Master节点来提高容错性。 1． Table表（Table）是数据库中用来存储数据的对象，是有结构的数据集合。kudu中的表具有schema（纲要）和全局有序的primary key（主键）。kudu中一个table会被水平分成多个被称之为tablet的片段。 2． Tablet一个 tablet 是一张 table连续的片段，tablet是kudu表的水平分区，类似于HBase的region。每个tablet存储着一定连续range的数据（key），且tablet两两间的range不会重叠。一张表的所有tablet包含了这张表的所有key空间。 tablet 会冗余存储。放置到多个 tablet server上，并且在任何给定的时间点，其中一个副本被认为是leader tablet,其余的被认之为follower tablet。每个tablet都可以进行数据的读请求，但只有Leader tablet负责写数据请求。 3． Tablet Servertablet server集群中的小弟，负责数据存储，并提供数据读写服务 一个 tablet server 存储了table表的tablet，向kudu client 提供读取数据服务。对于给定的 tablet，一个tablet server 充当 leader，其他 tablet server 充当该 tablet 的 follower 副本。 只有 leader服务写请求，然而 leader 或 followers 为每个服务提供读请求 。一个 tablet server 可以服务多个 tablets ，并且一个 tablet 可以被多个 tablet servers 服务着。 4． Master Server集群中的老大，负责集群管理、元数据管理等功能。 三 kudu安装1节点规划 节点 kudu-master kudu-tserver node01 是 是 node02 是 是 node03 是 是 本次配置node01 和node02 不配置 kudu-master 2本地yum源配置 配过了在 node03上 3 安装KUDU 服务器 安装命令 node01 yum install -y kudu kudu-tserver kudu-client0 kudu-client-devel node02 yum install -y kudu kudu-tserver kudu-client0 kudu-client-devel node03 yum install -y kudu kudu-master kudu-tserver kudu-client0 kudu-client-devel 12345yum install kudu # Kudu的基本包yum install kudu-master # KuduMaster yum install kudu-tserver # KuduTserver yum install kudu-client0 #Kudu C ++客户端共享库yum install kudu-client-devel # Kudu C ++客户端共享库 SDK 4 kudu节点配置 安装完成之后。 需要在所有节点的/etc/kudu/conf目录下有两个文件：master.gflagfile和tserver.gflagfile。 1.1． 修改master.gflagfile123456789# cat /etc/kudu/conf/master.gflagfile# Do not modify these two lines. If you wish to change these variables,# modify them in /etc/default/kudu-master.--fromenv=rpc_bind_addresses--fromenv=log_dir--fs_wal_dir=/export/servers/kudu/master--fs_data_dirs=/export/servers/kudu/master--master_addresses=node03:7051 若为集node01:7051,node02:7051,node03:7051 若为单节点 则这句注释掉若为单节点 且没注释掉 则启动报错 1.2 修改tserver.gflagfile1234567# Do not modify these two lines. If you wish to change these variables,# modify them in /etc/default/kudu-tserver.--fromenv=rpc_bind_addresses--fromenv=log_dir--fs_wal_dir=/export/servers/kudu/tserver--fs_data_dirs=/export/servers/kudu/tserver--tserver_master_addrs=node03:7051 若为集node01:7051,node02:7051,node03:7051 1.3修改 /etc/default/kudu-master123export FLAGS_log_dir=/var/log/kudu#每台机器的master地址要与主机名一致,这里是在node03上export FLAGS_rpc_bind_addresses=node03:7051 1.4修改 /etc/default/kudu-tserver123export FLAGS_log_dir=/var/log/kudu#每台机器的tserver地址要与主机名一致，这里是在node03上export FLAGS_rpc_bind_addresses=node03:7050 kudu默认用户就是KUDU，所以需要将/export/servers/kudu权限修改成kudu： 12mkdir /export/servers/kuduchown -R kudu:kudu /export/servers/kudu (如果使用的是普通的用户，那么最好配置sudo权限)/etc/sudoers文件中添加： kudu集群的启动与关闭 1 ntp服务的安装 启动的时候要注意时间同步 安装ntp服务 1yum -y install ntp 设置开机启动 12service ntpd start chkconfig ntpd on 可以在每台服务器执行 1/etc/init.d/ntpd restart 123456启动service kudu-master startservice kudu-tserver start关闭service kudu-master stopservice kudu-tserver stop 12345kudu的web管理界面。http://master主机名:8051可以查看每个机器上master相关信息。http://node03:8051/masters 一定为8051 不为7051tserver 的web地址 http://node03:8051/tablet-servers 安装属于事项 1.1给普通用户授予sudo出错12sudo: /etc/sudoers is world writable解决方式：``pkexec chmod 555 /etc/sudoers 1.2 启动kudu的时候报错12345678910Failed to start Kudu Master Server. Return value: 1 [FAILED]去日志文件中查看：Service unavailable: Cannot initialize clock: Errorreading clock. Clock consideredunsynchronized解决：第一步：首先检查是否有安装ntp：如果没有安装则使用以下命令安装：yum -y install ntp第二步：设置随机启动：service ntpd startchkconfig ntpd on 1.3 启动过程中报错123456Invalid argument: Unable to initialize catalog manager: Failed to initialize systablesasync: on-disk master list解决：（1）：停掉master和tserver（2）：删除掉之前所有的/export/servers/kudu/master/*和/export/servers/kudu/tserver/* 1.4 启动过程中报错1234error: Could not create new FS layout: unable to create file system roots: unable towrite instance metadata: Call to mkstemp() failed on name template/export/servers/kudu/master/instance.kudutmp.XXXXXX: Permission denied (error 13)这是因为kudu默认使用kudu权限进行执行，可能遇到文件夹的权限不一致情况，更改文件夹权限即可 四 Java操作kudu###1 创建maven工程 导入依赖 123456789101112&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.kudu&lt;/groupId&gt; &lt;artifactId&gt;kudu-client&lt;/artifactId&gt; &lt;version&gt;1.6.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 2 初始化方法1234567891011121314151617181920public class TestKudu &#123; //声明全局变量 KuduClient后期通过它来操作kudu表 private KuduClient kuduClient; //指定kuduMaster地址 private String kuduMaster; //指定表名 private String tableName; @Before public void init()&#123; //初始化操作 kuduMaster="node03:7051"; //指定表名 tableName="student"; KuduClient.KuduClientBuilder kuduClientBuilder = new KuduClient.KuduClientBuilder(kuduMaster); //设置客户端与kudu集群socket超时时间 kuduClientBuilder.defaultSocketReadTimeoutMs(10000); kuduClient=kuduClientBuilder.build(); &#125; 3 创建表1234567891011121314151617181920212223242526/** * 创建表 */@Testpublic void createTable() throws KuduException &#123; //判断表是否存在，不存在就构建 if(!kuduClient.tableExists(tableName))&#123; //构建创建表的schema信息-----就是表的字段和类型 ArrayList&lt;ColumnSchema&gt; columnSchemas = new ArrayList&lt;ColumnSchema&gt;(); columnSchemas.add(new ColumnSchema.ColumnSchemaBuilder("id", Type.INT32).key(true).build()); columnSchemas.add(new ColumnSchema.ColumnSchemaBuilder("name", Type.STRING).build()); columnSchemas.add(new ColumnSchema.ColumnSchemaBuilder("age", Type.INT32).build()); columnSchemas.add(new ColumnSchema.ColumnSchemaBuilder("sex", Type.INT32).build()); Schema schema = new Schema(columnSchemas); //指定创建表的相关属性 CreateTableOptions options = new CreateTableOptions(); ArrayList&lt;String&gt; partitionList = new ArrayList&lt;String&gt;(); //指定kudu表的分区字段是什么 partitionList.add("id"); // 按照 id.hashcode % 分区数 = 分区号 options.addHashPartitions(partitionList,6); kuduClient.createTable(tableName,schema,options); &#125;&#125; 4 插入数据123456789101112131415161718192021222324 /** * 向表加载数据 */ @Test public void insertTable() throws KuduException &#123; //向表加载数据需要一个kuduSession对象 KuduSession kuduSession = kuduClient.newSession(); //设置提交数据为自动flush kuduSession.setFlushMode(SessionConfiguration.FlushMode.AUTO_FLUSH_SYNC); //需要使用kuduTable来构建Operation的子类实例对象 就是打开本次操作的表名 KuduTable kuduTable = kuduClient.openTable(tableName);//此处需要kudutable 来构建operation的子类实例对象 此处 为insert for(int i=1;i&lt;=10;i++)&#123; Insert insert = kuduTable.newInsert(); PartialRow row = insert.getRow(); row.addInt("id",i); row.addString("name","zhangsan-"+i); row.addInt("age",20+i); row.addInt("sex",i%2); kuduSession.apply(insert);//最后实现执行数据的加载操作 &#125; &#125; 5 查询数据123456789101112131415161718192021222324252627282930313233/** * 查询表的数据结果 */ @Test public void queryData() throws KuduException &#123; //构建一个查询的扫描器 在扫描器中指定 表名 KuduScanner.KuduScannerBuilder kuduScannerBuilder = kuduClient.newScannerBuilder(kuduClient.openTable(tableName)); //创建集合 用于存储扫描的字段信息 ArrayList&lt;String&gt; columnsList = new ArrayList&lt;String&gt;(); columnsList.add("id"); columnsList.add("name"); columnsList.add("age"); columnsList.add("sex"); kuduScannerBuilder.setProjectedColumnNames(columnsList); // 调用build方法执行扫描,返回结果集 KuduScanner kuduScanner = kuduScannerBuilder.build(); //遍历 while (kuduScanner.hasMoreRows())&#123; RowResultIterator rowResults = kuduScanner.nextRows(); while (rowResults.hasNext())&#123; RowResult row = rowResults.next(); //拿到的是一行数据 int id = row.getInt("id"); String name = row.getString("name"); int age = row.getInt("age"); int sex = row.getInt("sex"); System.out.println("id="+id+" name="+name+" age="+age+" sex="+sex); &#125; &#125; &#125; 6 修改数据1234567891011121314151617181920212223/** * 修改表的数据 */ @Test public void updateData() throws KuduException &#123; //修改表的数据需要一个kuduSession对象 KuduSession kuduSession = kuduClient.newSession(); //设置提交数据为自动flush kuduSession.setFlushMode(SessionConfiguration.FlushMode.AUTO_FLUSH_SYNC); //需要使用kuduTable来构建Operation的子类实例对象 KuduTable kuduTable = kuduClient.openTable(tableName); //Update update = kuduTable.newUpdate(); //如果id不存在 ,就什么也不操作 Upsert upsert = kuduTable.newUpsert(); //如果id存在就表示修改，不存在就新增 PartialRow row = upsert.getRow(); row.addInt("id",100); row.addString("name","zhangsan-100"); row.addInt("age",100); row.addInt("sex",0); kuduSession.apply(upsert);//最后实现执行数据的修改操作 &#125; 7删除数据12345678910111213141516171819/** * 删除数据 */ @Test public void deleteData() throws KuduException &#123; //删除表的数据需要一个kuduSession对象 KuduSession kuduSession = kuduClient.newSession(); kuduSession.setFlushMode(SessionConfiguration.FlushMode.AUTO_FLUSH_SYNC); //需要使用kuduTable来构建Operation的子类实例对象 KuduTable kuduTable = kuduClient.openTable(tableName); Delete delete = kuduTable.newDelete(); PartialRow row = delete.getRow(); row.addInt("id",100); kuduSession.apply(delete);//最后实现执行数据的删除操作 &#125; 8 删除表12345678@Testpublic void dropTable() throws KuduException &#123; if(kuduClient.tableExists(tableName))&#123; kuduClient.deleteTable(tableName); &#125;&#125; 9 kudu的分区方式为了提供可扩展性，Kudu 表被划分为称为 tablet 的单元，并分布在许多 tablet servers 上。行总是属于单个tablet 。将行分配给 tablet 的方法由在表创建期间设置的表的分区决定。 kudu提供了3种分区方式。 1 范围分区Range Partitioning范围分区可以根据存入数据的数据量，均衡的存储到各个机器上，防止机器出现负载不均衡现象. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758/** * 测试分区： * RangePartition */ @Test public void testRangePartition() throws KuduException &#123; //设置表的schema LinkedList&lt;ColumnSchema&gt; columnSchemas = new LinkedList&lt;ColumnSchema&gt;(); columnSchemas.add(new Column("id", Type.INT32,true)); columnSchemas.add(new Column("WorkId", Type.INT32,false)); columnSchemas.add(new Column("Name", Type.STRING,false)); columnSchemas.add(new Column("Gender", Type.STRING,false)); columnSchemas.add(new Column("Photo", Type.STRING,false)); //创建schema Schema schema = new Schema(columnSchemas); //创建表时提供的所有选项 CreateTableOptions tableOptions = new CreateTableOptions(); //设置副本数 //tableOptions.setNumReplicas(1); //设置范围分区的规则 LinkedList&lt;String&gt; parcols = new LinkedList&lt;String&gt;(); parcols.add("id"); //设置按照那个字段进行range分区 tableOptions.setRangePartitionColumns(parcols); /** * range * 0 &lt; value &lt; 10 * 10 &lt;= value &lt; 20 * 20 &lt;= value &lt; 30 * ........ * 80 &lt;= value &lt; 90 * */ int count=0; for(int i =0;i&lt;10;i++)&#123; //范围开始 PartialRow lower = schema.newPartialRow(); lower.addInt("id",count); //范围结束 PartialRow upper = schema.newPartialRow(); count +=10; upper.addInt("id",count); //设置每一个分区的范围 tableOptions.addRangePartition(lower,upper); &#125; try &#123; kuduClient.createTable("t-range-partition",schema,tableOptions); &#125; catch (KuduException e) &#123; e.printStackTrace(); &#125; kuduClient.close(); &#125; 2 哈希分区Hash Partitioning 为kudu的默认分区哈希分区通过哈希值将行分配到许多 buckets (存储桶 )之一； 哈希分区是一种有效的策略，当不需要对表进行有序访问时。哈希分区对于在 tablet 之间随机散布这些功能是有效的，这有助于减轻热点和 tablet 大小不均匀。 12345678910111213141516171819202122232425262728293031323334/** * 测试分区： * hash分区 */ @Test public void testHashPartition() throws KuduException &#123; //设置表的schema LinkedList&lt;ColumnSchema&gt; columnSchemas = new LinkedList&lt;ColumnSchema&gt;(); columnSchemas.add(new Column("id", Type.INT32,true)); columnSchemas.add(new Column("WorkId", Type.INT32,false)); columnSchemas.add(new Column("Name", Type.STRING,false)); columnSchemas.add(new Column("Gender", Type.STRING,false)); columnSchemas.add(new Column("Photo", Type.STRING,false)); //创建schema Schema schema = new Schema(columnSchemas); //创建表时提供的所有选项 CreateTableOptions tableOptions = new CreateTableOptions(); //设置副本数 tableOptions.setNumReplicas(1); //设置范围分区的规则 LinkedList&lt;String&gt; parcols = new LinkedList&lt;String&gt;(); parcols.add("id"); //设置按照那个字段进行range分区 tableOptions.addHashPartitions(parcols,6); try &#123; kuduClient.createTable("t-hash-partition",schema,tableOptions); &#125; catch (KuduException e) &#123; e.printStackTrace(); &#125; kuduClient.close(); &#125; 3 多级分区Multilevel PartitioningKudu 允许一个表在单个表上组合多级分区。 当正确使用时，多级分区可以保留各个分区类型的优点，同时减少每个分区的缺点 如 范围分区 为5个 hash分区为3个 则多级分区为15个 (即在范围分区里面有进行了hash分区) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253/** * 测试分区： * 多级分区 * Multilevel Partition * 混合使用hash分区和range分区 * * 哈希分区有利于提高写入数据的吞吐量，而范围分区可以避免tablet无限增长问题， * hash分区和range分区结合，可以极大的提升kudu的性能 */ @Test public void testMultilevelPartition() throws KuduException &#123; //设置表的schema LinkedList&lt;ColumnSchema&gt; columnSchemas = new LinkedList&lt;ColumnSchema&gt;(); columnSchemas.add(new Column("id", Type.INT32,true)); columnSchemas.add(new Column("WorkId", Type.INT32,false)); columnSchemas.add(new Column("Name", Type.STRING,false)); columnSchemas.add(new Column("Gender", Type.STRING,false)); columnSchemas.add(new Column("Photo", Type.STRING,false)); //创建schema Schema schema = new Schema(columnSchemas); //创建表时提供的所有选项 CreateTableOptions tableOptions = new CreateTableOptions(); //设置副本数 //tableOptions.setNumReplicas(1); //设置范围分区的规则 LinkedList&lt;String&gt; parcols = new LinkedList&lt;String&gt;(); parcols.add("id"); //hash分区 tableOptions.addHashPartitions(parcols,5); //range分区 int count=0; for(int i=0;i&lt;10;i++)&#123; //指定上界 PartialRow lower = schema.newPartialRow(); lower.addInt("id",count); count+=10; //指定下界 PartialRow upper = schema.newPartialRow(); upper.addInt("id",count); tableOptions.addRangePartition(lower,upper); &#125; try &#123; kuduClient.createTable("t-multilevel-partition",schema,tableOptions); &#125; catch (KuduException e) &#123; e.printStackTrace(); &#125; kuduClient.close(); &#125; 五 kudu集成impala1 impala的配置修改可选项 若配置以后写的时候指定 master地址即可 在每一个服务器的impala的配置文件中添加如下配置。 1vim /etc/default/impala 在IMPALA_SERVER_ARGS下添加： 1-kudu_master_hosts=node-1:7051,node-2:7051,node-3:7051 2 创建kudu表需要先启动hdfs、hive、kudu、impala。使用impala的shell控制台。 1impala-shell 内部表:impala中删除,kudu里也会删除 外部表: impala中删除,kudu中不会删除. 2.1 内部表 内部表由Impala管理，当您从Impala中删除时，数据和表确实被删除。当您使用Impala创建新表时，它通常是内部表。 1234567891011121314CREATE TABLE my_first_table(id BIGINT,name STRING,PRIMARY KEY(id))PARTITION BY HASH PARTITIONS 16 STORED AS KUDUTBLPROPERTIES ('kudu.master_addresses' = 'node1:7051,node2:7051,node3:7051', //这种就是没有配置以上的 指定地址'kudu.table_name' = 'my_first_table'); 在 CREATE TABLE 语句中，必须首先列出构成主键的列。 2.2 外部表 外部表（创建者CREATE EXTERNAL TABLE）不受Impala管理，并且删除此表不会将表从其源位置（此处为Kudu）丢弃。相反，它只会去除Impala和Kudu之间的映射。这是Kudu提供的用于将现有表映射到Impala的语法。 首先使用java创建kudu表： 1234567891011121314151617181920212223242526272829303132333435363738394041424344public class CreateTable &#123; private static ColumnSchema newColumn(String name, Type type, boolean iskey) &#123; ColumnSchema.ColumnSchemaBuilder column = new ColumnSchema.ColumnSchemaBuilder(name, type); column.key(iskey); return column.build(); &#125; public static void main(String[] args) throws KuduException &#123; // master地址 final String masteraddr = "node1,node2,node3"; // 创建kudu的数据库链接 KuduClient client = new KuduClient.KuduClientBuilder(masteraddr).defaultSocketReadTimeoutMs(6000).build(); // 设置表的schema List&lt;ColumnSchema&gt; columns = new LinkedList&lt;ColumnSchema&gt;(); columns.add(newColumn("CompanyId", Type.INT32, true)); columns.add(newColumn("WorkId", Type.INT32, false)); columns.add(newColumn("Name", Type.STRING, false)); columns.add(newColumn("Gender", Type.STRING, false)); columns.add(newColumn("Photo", Type.STRING, false)); Schema schema = new Schema(columns); //创建表时提供的所有选项 CreateTableOptions options = new CreateTableOptions(); // 设置表的replica备份和分区规则 List&lt;String&gt; parcols = new LinkedList&lt;String&gt;(); parcols.add("CompanyId"); //设置表的备份数 options.setNumReplicas(1); //设置range分区 options.setRangePartitionColumns(parcols); //设置hash分区和数量 options.addHashPartitions(parcols, 3); try &#123; client.createTable("person", schema, options); &#125; catch (KuduException e) &#123; e.printStackTrace(); &#125; client.close(); &#125;&#125; 使用impala创建外部表 ， 将kudu的表映射到impala上。 123456在impala-shell中执行CREATE EXTERNAL TABLE `person` STORED AS KUDUTBLPROPERTIES( 'kudu.table_name' = 'person', 'kudu.master_addresses' = 'node1:7051,node2:7051,node3:7051') 3 使用impala对kudu进行DML1 插入数据 impala 允许使用标准 SQL 语句将数据插入 Kudu 。 首先建表： 123456789101112在impala-shell中CREATE TABLE my_first_table1(id BIGINT,name STRING,PRIMARY KEY(id))PARTITION BY HASH PARTITIONS 16STORED AS KUDU TBLPROPERTIES( 'kudu.table_name' = 'person1', 'kudu.master_addresses' = 'node1:7051,node2:7051,node3:7051'); 此示例插入单个行： 1INSERT INTO my_first_table VALUES (50, "zhangsan"); 此示例插入3行： 1INSERT INTO my_first_table VALUES (1, "john"), (2, "jane"), (3, "jim"); 批量导入数据： 从 Impala 和 Kudu 的角度来看，通常表现最好的方法通常是使用 Impala 中的 SELECT FROM 语句导入数据。 1INSERT INTO my_first_table SELECT * FROM temp1; 2 更新数据 1UPDATE my_first_table SET name="xiaowang" where id =1 ; 3 删除数据 1delete from my_first_table where id =2; 4 更改表属性 4.1重命名impala表 1ALTER TABLE PERSON RENAME TO person_temp; 4.2重新命名内部表的基础kudu表 4.1.1创建内部表 123456789101112131415CREATE TABLE kudu_student(CompanyId INT,WorkId INT,Name STRING,Gender STRING,Photo STRING,PRIMARY KEY(CompanyId))PARTITION BY HASH PARTITIONS 16STORED AS KUDUTBLPROPERTIES ('kudu.master_addresses' = 'node1:7051,node2:7051,node3:7051','kudu.table_name' = 'student'); ​ 4.1.2如果表是内部表，则可以通过更改 kudu.table_name 属性重命名底层的 Kudu 表。 1ALTER TABLE kudu_student SET TBLPROPERTIES('kudu.table_name' = 'new_student'); 4.3将外部表重新映射kudu表 如果用户在使用过程中发现其他应用程序重新命名了kudu表，那么此时的外部表需要重新映射到kudu上。 首先创建一个外部表： 123456CREATE EXTERNAL TABLE external_table STORED AS KUDU TBLPROPERTIES ( 'kudu.master_addresses' = 'node1:7051,node2:7051,node3:7051', 'kudu.table_name' = 'person'); 重新映射外部表，指向不同的kudu表： 12ALTER TABLE external_tableSET TBLPROPERTIES('kudu.table_name' = 'hashTable') 上面的操作是：将external_table映射的PERSON表重新指向hashTable表。 4.4 更改kudu master地址 12ALTER TABLE my_tableSET TBLPROPERTIES('kudu.master_addresses' = 'kudu-new-master.example.com:7051'); 4.5 将内部表改为外部表 1ALTER TABLE my_table SET TBLPROPERTIES('EXTERNAL' = 'TRUE'); ##4 impala使用Java操作kudu 对于impala而言，开发人员是可以通过JDBC连接impala的，有了JDBC，开发人员可以通过impala来间接操作 kudu。 1 引入依赖123456789101112131415161718192021222324252627282930313233343536373839404142434445 &lt;!--impala的jdbc操作--&gt; &lt;dependency&gt; &lt;groupId&gt;com.cloudera&lt;/groupId&gt; &lt;artifactId&gt;ImpalaJDBC41&lt;/artifactId&gt; &lt;version&gt;2.5.42&lt;/version&gt; &lt;/dependency&gt; &lt;!--Caused by : ClassNotFound : thrift.protocol.TPro--&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.thrift&lt;/groupId&gt; &lt;artifactId&gt;libfb303&lt;/artifactId&gt; &lt;version&gt;0.9.3&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;/dependency&gt; &lt;!--Caused by : ClassNotFound : thrift.protocol.TPro--&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.thrift&lt;/groupId&gt; &lt;artifactId&gt;libthrift&lt;/artifactId&gt; &lt;version&gt;0.9.3&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hive&lt;/groupId&gt; &lt;artifactId&gt;hive-jdbc&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.apache.hive&lt;/groupId&gt; &lt;artifactId&gt;hive-service-rpc&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;org.apache.hive&lt;/groupId&gt; &lt;artifactId&gt;hive-service&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;version&gt;1.1.0&lt;/version&gt; &lt;/dependency&gt; &lt;!--导入hive--&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hive&lt;/groupId&gt; &lt;artifactId&gt;hive-service&lt;/artifactId&gt; &lt;version&gt;1.1.0&lt;/version&gt; &lt;/dependency&gt; 2 jdbc连接impala操作kudu使用JDBC连接impala操作kudu，与JDBC连接mysql做更重增删改查基本一样。 创建实体类 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657package cn.itcast.impala.impala;public class Person &#123; private int companyId; private int workId; private String name; private String gender; private String photo; public Person(int companyId, int workId, String name, String gender, String photo) &#123; this.companyId = companyId; this.workId = workId; this.name = name; this.gender = gender; this.photo = photo; &#125; public int getCompanyId() &#123; return companyId; &#125; public void setCompanyId(int companyId) &#123; this.companyId = companyId; &#125; public int getWorkId() &#123; return workId; &#125; public void setWorkId(int workId) &#123; this.workId = workId; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public String getGender() &#123; return gender; &#125; public void setGender(String gender) &#123; this.gender = gender; &#125; public String getPhoto() &#123; return photo; &#125; public void setPhoto(String photo) &#123; this.photo = photo; &#125;&#125; JDBC连接impala对kudu进行增删改查123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214package cn.itcast.impala.impala;import java.sql.*;public class Contants &#123; private static String JDBC_DRIVER="com.cloudera.impala.jdbc41.Driver"; private static String CONNECTION_URL="jdbc:impala://node1:21050/default;auth=noSasl"; //定义数据库连接 static Connection conn=null; //定义PreparedStatement对象 static PreparedStatement ps=null; //定义查询的结果集 static ResultSet rs= null; //数据库连接 public static Connection getConn()&#123; try &#123; Class.forName(JDBC_DRIVER); conn=DriverManager.getConnection(CONNECTION_URL); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return conn; &#125; //创建一个表 public static void createTable()&#123; conn=getConn(); String sql="CREATE TABLE impala_kudu_test" + "(" + "companyId BIGINT," + "workId BIGINT," + "name STRING," + "gender STRING," + "photo STRING," + "PRIMARY KEY(companyId)" + ")" + "PARTITION BY HASH PARTITIONS 16 " + "STORED AS KUDU " + "TBLPROPERTIES (" + "'kudu.master_addresses' = 'node1:7051,node2:7051,node3:7051'," + "'kudu.table_name' = 'impala_kudu_test'" + ");"; try &#123; ps = conn.prepareStatement(sql); ps.execute(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125; //查询数据 public static ResultSet queryRows()&#123; try &#123; //定义执行的sql语句 String sql="select * from impala_kudu_test"; ps = getConn().prepareStatement(sql); rs= ps.executeQuery(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; return rs; &#125; //打印结果 public static void printRows(ResultSet rs)&#123; /** private int companyId; private int workId; private String name; private String gender; private String photo; */ try &#123; while (rs.next())&#123; //获取表的每一行字段信息 int companyId = rs.getInt("companyId"); int workId = rs.getInt("workId"); String name = rs.getString("name"); String gender = rs.getString("gender"); String photo = rs.getString("photo"); System.out.print("companyId:"+companyId+" "); System.out.print("workId:"+workId+" "); System.out.print("name:"+name+" "); System.out.print("gender:"+gender+" "); System.out.println("photo:"+photo); &#125; &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125;finally &#123; if(ps!=null)&#123; try &#123; ps.close(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125; if(conn !=null)&#123; try &#123; conn.close(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; //插入数据 public static void insertRows(Person person)&#123; conn=getConn(); String sql="insert into table impala_kudu_test(companyId,workId,name,gender,photo) values(?,?,?,?,?)"; try &#123; ps=conn.prepareStatement(sql); //给占位符？赋值 ps.setInt(1,person.getCompanyId()); ps.setInt(2,person.getWorkId()); ps.setString(3,person.getName()); ps.setString(4,person.getGender()); ps.setString(5,person.getPhoto()); ps.execute(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125;finally &#123; if(ps !=null)&#123; try &#123; //关闭 ps.close(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125; if(conn !=null)&#123; try &#123; //关闭 conn.close(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; //更新数据 public static void updateRows(Person person)&#123; //定义执行的sql语句 String sql="update impala_kudu_test set workId="+person.getWorkId()+ ",name='"+person.getName()+"' ,"+"gender='"+person.getGender()+"' ,"+ "photo='"+person.getPhoto()+"' where companyId="+person.getCompanyId(); try &#123; ps= getConn().prepareStatement(sql); ps.execute(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125;finally &#123; if(ps !=null)&#123; try &#123; //关闭 ps.close(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125; if(conn !=null)&#123; try &#123; //关闭 conn.close(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; //删除数据 public static void deleteRows(int companyId)&#123; //定义sql语句 String sql="delete from impala_kudu_test where companyId="+companyId; try &#123; ps =getConn().prepareStatement(sql); ps.execute(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125; //删除表 public static void dropTable() &#123; String sql="drop table if exists impala_kudu_test"; try &#123; ps =getConn().prepareStatement(sql); ps.execute(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 代码测试运行1234567891011121314151617181920212223242526272829package cn.itcast.impala.impala;import java.sql.Connection;public class ImpalaJdbcClient &#123; public static void main(String[] args) &#123; Connection conn = Contants.getConn(); //创建一个表 Contants.createTable(); //插入数据 Contants.insertRows(new Person(1,100,"lisi","male","lisi-photo")); //查询表的数据 ResultSet rs = Contants.queryRows(); Contants.printRows(rs); //更新数据 Contants.updateRows(new Person(1,200,"zhangsan","male","zhangsan-photo")); //删除数据 Contants.deleteRows(1); //删除表 Contants.dropTable(); &#125;&#125; 六 Apache KUDU的原理1． table与schemaKudu设计是面向结构化存储的，因此，Kudu的表需要用户在建表时定义它的Schema信息，这些Schema信息包含：列定义（含类型），Primary Key定义（用户指定的若干个列的有序组合）。数据的唯一性，依赖于用户所提供的Primary Key中的Column组合的值的唯一性。Kudu提供了Alter命令来增删列，但位于Primary Key中的列是不允许删除的。 从用户角度来看，Kudu是一种存储结构化数据表的存储系统。在一个Kudu集群中可以定义任意数量的table，每个table都需要预先定义好schema。每个table的列数是确定的，每一列都需要有名字和类型，每个表中可以把其中一列或多列定义为主键。这么看来，Kudu更像关系型数据库，而不是像HBase、Cassandra和MongoDB这些NoSQL数据库。不过Kudu目前还不能像关系型数据一样支持二级索引。 Kudu使用确定的列类型，而不是类似于NoSQL的“everything is byte”。带来好处：确定的列类型使Kudu可以进行类型特有的编码,可以提供元数据给其他上层查询工具。 2 kudu底层数据模型Kudu的底层数据文件的存储，未采用HDFS这样的较高抽象层次的分布式文件系统，而是自行开发了一套可基于Table/Tablet/Replica视图级别的底层存储系统。 这套实现基于如下的几个设计目标： • 可提供快速的列式查询 • 可支持快速的随机更新 • 可提供更为稳定的查询性能保障 一张table会分成若干个tablet，每个tablet包括MetaData元信息及若干个RowSet。 RowSet包含一个MemRowSet及若干个DiskRowSet，DiskRowSet中包含一个BloomFile、Ad_hoc Index、BaseData、DeltaMem及若干个RedoFile和UndoFile。 MemRowSet：用于新数据insert及已在MemRowSet中的数据的更新，一个MemRowSet写满后会将数据刷到磁盘形成若干个DiskRowSet。默认是1G或者或者120S。 DiskRowSet：用于老数据的变更，后台定期对DiskRowSet做compaction，以删除没用的数据及合并历史数据，减少查询过程中的IO开销。 BloomFile：根据一个DiskRowSet中的key生成一个bloom filter，用于快速模糊定位某个key是否在DiskRowSet中。 Ad_hocIndex：是主键的索引，用于定位key在DiskRowSet中的具体哪个偏移位置。 BaseData是MemRowSet flush下来的数据，按列存储，按主键有序。 UndoFile是基于BaseData之前时间的历史数据，通过在BaseData上apply UndoFile中的记录，可以获得历史数据。 RedoFile是基于BaseData之后时间的变更记录，通过在BaseData上apply RedoFile中的记录，可获得较新的数据。 DeltaMem用于DiskRowSet中数据的变更，先写到内存中，写满后flush到磁盘形成RedoFile。 REDO与UNDO与关系型数据库中的REDO与UNDO日志类似（在关系型数据库中，REDO日志记录了更新后的数据，可以用来恢复尚未写入Data File的已成功事务更新的数据。而UNDO日志用来记录事务更新之前的数据，可以用来在事务失败时进行回滚） MemRowSets可以对比理解成HBase中的MemStore, 而DiskRowSets可理解成HBase中的HFile。 MemRowSets中的数据被Flush到磁盘之后，形成DiskRowSets。 DisRowSets中的数据，按照32MB大小为单位，按序划分为一个个的DiskRowSet。 DiskRowSet中的数据按照Column进行组织，与Parquet类似。 这是Kudu可支持一些分析性查询的基础。每一个Column的数据被存储在一个相邻的数据区域，而这个数据区域进一步被细分成一个个的小的Page单元，与HBase File中的Block类似，对每一个Column Page可采用一些Encoding算法，以及一些通用的Compression算法。 既然可对Column Page可采用Encoding以及Compression算法，那么，对单条记录的更改就会比较困难了。 前面提到了Kudu可支持单条记录级别的更新/删除，是如何做到的？ 与HBase类似，也是通过增加一条新的记录来描述这次更新/删除操作的。DiskRowSet是不可修改了，那么 KUDU 要如何应对数据的更新呢？在KUDU中，把DiskRowSet分为了两部分：base data、delta stores。base data 负责存储基础数据，delta stores负责存储 base data 中的变更数据. 如上图所示，数据从MemRowSet 刷到磁盘后就形成了一份 DiskRowSet（只包含 base data），每份 DiskRowSet 在内存中都会有一个对应的 DeltaMemStore，负责记录此 DiskRowSet 后续的数据变更（更新、删除）。DeltaMemStore 内部维护一个 B-树索引，映射到每个 row_offset 对应的数据变更。DeltaMemStore 数据增长到一定程度后转化成二进制文件存储到磁盘，形成一个 DeltaFile，随着 base data 对应数据的不断变更，DeltaFile 逐渐增长。 3 tablet发现过程当创建Kudu客户端时，其会从主服务器上获取tablet位置信息，然后直接与服务于该tablet的服务器进行交谈。 为了优化读取和写入路径，客户端将保留该信息的本地缓存，以防止他们在每个请求时需要查询主机的tablet位置信息。随着时间的推移，客户端的缓存可能会变得过时，并且当写入被发送到不再是tablet领导者的tablet服务器时，则将被拒绝。然后客户端将通过查询主服务器发现新领导者的位置来更新其缓存。 4 kudu写流程当 Client 请求写数据时，先根据主键从Master Server中获取要访问的目标 Tablets，然后到依次对应的Tablet获取数据。 因为KUDU表存在主键约束，所以需要进行主键是否已经存在的判断，这里就涉及到之前说的索引结构对读写的优化了。一个Tablet中存在很多个RowSets，为了提升性能，我们要尽可能地减少要扫描的RowSets数量。 首先，我们先通过每个 RowSet 中记录的主键的（最大最小）范围，过滤掉一批不存在目标主键的RowSets，然后在根据RowSet中的布隆过滤器，过滤掉确定不存在目标主键的 RowSets，最后再通过RowSets中的 B-树索引，精确定位目标主键是否存在。 如果主键已经存在，则报错（主键重复），否则就进行写数据（写 MemRowSet）。 5kudu读流程数据读取过程大致如下：先根据要扫描数据的主键范围，定位到目标的Tablets，然后读取Tablets 中的RowSets。 在读取每个RowSet时，先根据主键过滤要scan范围，然后加载范围内的base data，再找到对应的delta stores，应用所有变更，最后union上MemRowSet中的内容，返回数据给Client。 6kudu更新流程数据更新的核心是定位到待更新数据的位置，这块与写入的时候类似，就不展开了，等定位到具体位置后，然后将变更写到对应的delta store 中。]]></content>
      <tags>
        <tag>BigData</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hue]]></title>
    <url>%2F2017%2F07%2F13%2FHue.html</url>
    <content type="text"><![CDATA[Hue一概述HUE=Hadoop User Experience Hue是一个开源的Apache Hadoop UI系统，由Cloudera Desktop演化而来，最后Cloudera公司将其贡献给Apache基金会的Hadoop社区，它是基于Python Web框架Django实现的。 用于与其他各个框架做整合,提供一个web 界面可以供我们去操作其他的大数据框架. 其实就是一个与其他大数据框架做整合的工具,他本身并不提供任何功能,这些功能由他整合的框架完成 二架构第一个UI界面：主要是提供我们web界面供我们使用的第二个hue server：就是一个tomcat的服务第三个hue DB: hue的数据库，主要用于保存一起我们提交的任务 核心功能: · SQL编辑器，支持Hive, Impala, MySQL, Oracle, PostgreSQL, SparkSQL, Solr SQL, Phoenix… · 搜索引擎Solr的各种图表 · Spark和Hadoop的友好界面支持 · 支持调度系统Apache Oozie，可进行workflow的编辑、查看 HUE提供的这些功能相比Hadoop生态各组件提供的界面更加友好，但是一些需要debug的场景可能还是需要使用原生系统才能更加深入的找到错误的原因。 HUE中查看Oozie workflow时，也可以很方便的看到整个workflow的DAG图，不过在最新版本中已经将DAG图去掉了，只能看到workflow中的action列表和他们之间的跳转关系，想要看DAG图的仍然可以使用oozie原生的界面系统查看。 支持的框架: 1，访问HDFS和文件浏览 2，通过web调试和开发hive以及数据结果展示 3，查询solr和结果展示，报表生成 4，通过web调试和开发impala交互式SQL Query 5，spark调试和开发 7，oozie任务的开发，监控，和工作流协调调度 8，Hbase数据查询和修改，数据展示 9，Hive的元数据（metastore）查询 10，MapReduce任务进度查看，日志追踪 11，创建和提交MapReduce，Streaming，Java job任务 12，Sqoop2的开发和调试 13，Zookeeper的浏览和编辑 14，数据库（，，，）的查询和展示 Hue是一个友好的界面集成框架，可以集成我们各种学习过的以及将要学习的框架，一个界面就可以做到查看以及执行所有的框架. 三 hue 的安装安装方式: 1 .RPM 2.tar.gz 安装 3.clouder mananger 安装 这里使用tar.gz安装 1 下载解压 1tar -zxvf hue-3.9.0-cdh5.14.0.tar.gz -C ../servers/ 2 编译安装启动 2.1 linux安装依赖包 1yum install ant asciidoc cyrus-sasl-devel cyrus-sasl-gssapi cyrus-sasl-plain gcc gcc-c++ krb5-devel libffi-devel libxml2-devel libxslt-devel make mysql mysql-devel openldap-devel python-devel sqlite-devel gmp-devel 2.2 配值 hue 12cd /hue/desktop/confvim hue.ini 通用配置: 在标签 [desktop] 12345678secret_key=jFE93j;2[290-eiw.KEiwN2s3[&apos;d;/.q[eIW^y#e=+Iei*@Mn&lt;qW5ohttp_host=node03.hadoop.comis_hue_4=truetime_zone=Asia/Shanghaiserver_user=rootserver_group=rootdefault_user=rootdefault_hdfs_superuser=root #配置使用mysql作为hue的存储数据库,大概在hue.ini的587行左右 [database] 123456engine=mysqlhost=node03.hadoop.comport=3306user=rootpassword=123456name=hue 2.3 配置mysql数据库 1create database hue default character set utf8 default collate utf8_general_ci; 注意实际工作中需要为hue 这个数据库创建对应的用户并分配权限,此处不用创建 1grant all on hue.* to 'hue'@'%' identified by 'hue'; 2.4 准备编译 12cd /huemake apps 2.5 linux 添加普通用户 12useradd huepasswd hue 2.6 启动hue 12cd /huebulid/env/bin/supervisor 2.7 页面访问 http://node03:8888 第一次访问的时候，需要设置管理员用户和密码 我们这里的管理员的用户名与密码尽量保持与我们安装hadoop的用户名和密码一致， 我们安装hadoop的用户名与密码分别是root *** 初次登录使用root用户，密码为** 四 hue与其他框架的集成1hue 与hadoop的集成1.1 hue 与hdfs的集成注意修改完HDFS相关配置后，需要把配置scp给集群中每台机器，重启hdfs集群。 修改 hadoop 中的core-site.xml 12345678910&lt;!--允许通过httpfs方式访问hdfs的主机名 --&gt;&lt;property&gt;&lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt;&lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;!—允许通过httpfs方式访问hdfs的用户组 --&gt;&lt;property&gt;&lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt;&lt;value&gt;*&lt;/value&gt;&lt;/property&gt; 修改hadoop 中的hdfs-site.xml 1234&lt;property&gt; &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt; 修改hue.ini文件 12345678910cd /export/servers/hue-3.9.0-cdh5.14.0/desktop/confvim hue.ini[[hdfs_clusters]] [[[default]]]fs_defaultfs=hdfs://node-1:9000webhdfs_url=http://node-1:50070/webhdfs/v1hadoop_hdfs_home= /export/servers/hadoop-2.7.5hadoop_bin=/export/servers/hadoop-2.7.5/binhadoop_conf_dir=/export/servers/hadoop-2.7.5/etc/hadoop 重启hdfs与hue 2 与yarn集成修改hue.ini 文件 1234567[[yarn_clusters]] [[[default]]] resourcemanager_host=node-1 resourcemanager_port=8032 submit_to=True resourcemanager_api_url=http://node-1:8088 history_server_api_url=http://node-1:19888 开启yarn日志聚集服务 修改Hadoop中的 yarn-site.xml 12345678&lt;property&gt; ##是否启用日志聚集功能。&lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; ##设置日志保留时间，单位是秒。&lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt;&lt;value&gt;106800&lt;/value&gt;&lt;/property&gt; 重启yarn 与hue 2 hue集成hive如果需要配置hue与hive的集成，我们需要启动hive的metastore服务以及hiveserver2服务（impala需要hive的metastore服务，hue需要hvie的hiveserver2服务）。 1 修改hue.ini文件 1234567891011[beeswax] hive_server_host=node-1 hive_server_port=10000 hive_conf_dir=/export/servers/hive/conf server_conn_timeout=120 auth_username=root auth_password=123456[metastore] #允许使用hive创建数据库表等操作 enable_new_create_table=true 2 启动hIive 和hue 去node-1机器上启动hive的metastore以及hiveserver2服务 123cd /export/servers/hivenohup bin/hive --service metastore &amp;nohup bin/hive --service hiveserver2 &amp; 重新启动hue。 12cd /export/servers/hue-3.9.0-cdh5.14.0/build/env/bin/supervisor 3 hue 集成MySQL1 修改hue.ini文件 需要把mysql的注释给去掉。 大概位于1546行 1234567[[[mysql]]] nice_name="My SQL DB" engine=mysql host=node-1 port=3306 user=root password=hadoop 2重启hue 4 hue集成Oozie1修改hue.ini 文件 12345678910[liboozie] # The URL where the Oozie service runs on. This is required in order for # users to submit jobs. Empty value disables the config check. oozie_url=http://node-1:11000/oozie # Requires FQDN in oozie_url if enabled ## security_enabled=false # Location on HDFS where the workflows/coordinator are deployed when submitted. remote_deployement_dir=/user/root/oozie_works 12345678910111213141516171819202122232425[oozie] # Location on local FS where the examples are stored. # local_data_dir=/export/servers/oozie-4.1.0-cdh5.14.0/examples/apps # Location on local FS where the data for the examples is stored. # sample_data_dir=/export/servers/oozie-4.1.0-cdh5.14.0/examples/input-data # Location on HDFS where the oozie examples and workflows are stored. # Parameters are $TIME and $USER, e.g. /user/$USER/hue/workspaces/workflow-$TIME # remote_data_dir=/user/root/oozie_works/examples/apps # Maximum of Oozie workflows or coodinators to retrieve in one API call. oozie_jobs_count=100 # Use Cron format for defining the frequency of a Coordinator instead of the old frequency number/unit. enable_cron_scheduling=true # Flag to enable the saved Editor queries to be dragged and dropped into a workflow. enable_document_action=true # Flag to enable Oozie backend filtering instead of doing it at the page level in Javascript. Requires Oozie 4.3+. enable_oozie_backend_filtering=true # Flag to enable the Impala action. enable_impala_action=true 123456789101112[filebrowser] # Location on local filesystem where the uploaded archives are temporary stored. archive_upload_tempdir=/tmp # Show Download Button for HDFS file browser. show_download_button=true # Show Upload Button for HDFS file browser. show_upload_button=true # Flag to enable the extraction of a uploaded archive in HDFS. enable_extract_uploaded_archive=true 启动hue和oozie 启动hue进程 12cd /export/servers/hue-3.9.0-cdh5.14.0build/env/bin/supervisor 启动oozie进程 12cd /export/servers/oozie-4.1.0-cdh5.14.0bin/oozied.sh start 5 hue集成impala修改配置文件 1234[impala] server_host=node-3 server_port=21050 impala_conf_dir=/etc/impala/conf 启动impala 和hue]]></content>
      <tags>
        <tag>BigData</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Oozie]]></title>
    <url>%2F2017%2F07%2F12%2FOozie.html</url>
    <content type="text"><![CDATA[Oozie一 概述 是一个工作流调度软件 本身属于cloudera 后来贡献给了apache oozie目的根据一个定义DAG（有向无环图）执行工作流程 oozie本身的配置是一种xml格式的配置文件 oozie跟hue配合使用将会很方便 oozie特点：顺序执行 周期重复定时 可视化 追踪结果 二 架构Oozie Client：提供命令行、java api、rest等方式，对Oozie的工作流流程的提交、启动、运行等操作； Oozie WebApp：即 Oozie Server,本质是一个java应用。可以使用内置的web容器，也可以使用外置的web容器； Hadoop Cluster：底层执行Oozie编排流程的各个hadoop生态圈组件；launcher Job(single map task no reduce task) 和actual program(mr hive pig sqark etc) oozie各种类型任务提交底层依赖于mr程序 首先启动一个没有reducetak的mr 通过这个mr把各个不同类型的任务提交到具体的集群上执行 三 原理​ Oozie对工作流的编排，是基于workflow.xml文件来完成的。用户预先将工作流执行规则定制于workflow.xml文件中，并在job.properties配置相关的参数，然后由Oozie Server向MR提交job来启动工作流。 1 流程节点Control Flow Nodes：控制工作流执行路径，包括start，end，kill，decision判断的节点，fork 插值 分开执行不同的任务 并行执行,join 合并。 Action Nodes：决定每个操作执行的任务类型，包括MapReduce、java、hive、shell等。 上述两种类型结合起来 就可以描绘出一个工作流的DAG图。 2 工作流类型 workflow 基本类型的工作流 只会按照定义顺序执行 无定时触发和条件触发 coordinator Coordinator将多个工作流Job组织起来，称为Coordinator Job，并指定触发时间和频率，还可以配置数据集、并发数等，类似于在工作流外部增加了一个协调器来管理这些工作流的工作流Job的运行。 bundle 针对coordinator的批处理工作流。Bundle将多个Coordinator管理起来，这样我们只需要一个Bundle提交即可。 四 Apache Oozie 安装版本问题：Apache官方提供的是源码包 需要自己结合hadoop生态圈软件环境进行编译 兼容性问题特别难以处理 因此可以使用第三方商业公司编译好 Cloudera（CDH） 1 修改Hadoop的配置1.1 配置httpfs服务修改Hadoop的配置文件core-site.xml： 12345678&lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt; hadoop.proxyuser.root.hosts允许通过httpfs方式访问hdfs的主机名、域名； hadoop.proxyuser.root.groups允许访问的客户端的用户组 1.2 配置jobhistory服务修改Hadoop的配置文件mapred-site.xml: 12345678910111213141516171819202122&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;node01:10020&lt;/value&gt; &lt;description&gt;MapReduce JobHistory Server IPC host:port&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;node01:19888&lt;/value&gt; &lt;description&gt;MapReduce JobHistory Server Web UI host:port&lt;/description&gt;&lt;/property&gt;&lt;!-- 配置运行过的日志存放在hdfs上的存放路径 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.done-dir&lt;/name&gt; &lt;value&gt;/export/data/history/done&lt;/value&gt;&lt;/property&gt;&lt;!-- 配置正在运行中的日志在hdfs上的存放路径 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.intermediate-done-dir&lt;/name&gt; &lt;value&gt;/export/data/history/done_intermediate&lt;/value&gt;&lt;/property&gt; 1.3 重启hadoop集群相关服务启动history-server mr-jobhistory-daemon.sh start historyserver 停止history-server mr-jobhistory-daemon.sh stop historyserver 通过浏览器访问Hadoop Jobhistory的WEBUI 1http://node01:19888 2 解压 Oozie的安装包例如： 123456oozie的安装包上传到/export/softwarestar -zxvf oozie-4.1.0-cdh5.14.0.tar.gz -C ../servers/解压hadooplibs到与oozie平行的目录cd /export/servers/oozie-4.1.0-cdh5.14.0tar -zxvf oozie-hadooplibs-4.1.0-cdh5.14.0.tar.gz -C ../ 3 添加相关依赖123456789101112oozie的安装路径下创建libext目录cd /export/servers/oozie-4.1.0-cdh5.14.0mkdir -p libext拷贝hadoop依赖包到libextcd /export/servers/oozie-4.1.0-cdh5.14.0cp -ra hadooplibs/hadooplib-2.6.0-cdh5.14.0.oozie-4.1.0-cdh5.14.0/* libext/上传mysql的驱动包到libextmysql-connector-java-5.1.32.jar添加ext-2.2.zip压缩包到libextext-2.2.zip 4 修改oozie-site.xml文件oozie默认使用的是UTC的时区，需要在oozie-site.xml当中配置时区为GMT+0800时区 123456789101112131415161718192021222324252627282930313233cd /export/servers/oozie-4.1.0-cdh5.14.0/confvim oozie-site.xml &lt;property&gt; &lt;name&gt;oozie.service.JPAService.jdbc.driver&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;oozie.service.JPAService.jdbc.url&lt;/name&gt; &lt;value&gt;jdbc:mysql://node03:3306/oozie&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;oozie.service.JPAService.jdbc.username&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;oozie.service.JPAService.jdbc.password&lt;/name&gt; &lt;value&gt;123456&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;oozie.processing.timezone&lt;/name&gt; &lt;value&gt;GMT+0800&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;oozie.service.coord.check.maximum.frequency&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;oozie.service.HadoopAccessorService.hadoop.configurations&lt;/name&gt; &lt;value&gt;*=/export/servers/hadoop-2.7.5/etc/hadoop&lt;/value&gt; &lt;/property&gt; 5 初始化MySQL相关信息上传oozie的解压后目录的下的yarn.tar.gz到hdfs目录 1bin/oozie-setup.sh sharelib create -fs hdfs://node01:8020 -locallib oozie-sharelib-4.1.0-cdh5.14.0-yarn.tar.gz 本质上就是将这些jar包解压到了hdfs上面的路径下面去 创建mysql数据库 12mysql -uroot -pcreate database oozie; 初始化创建oozie的数据库表 12cd /export/servers/oozie-4.1.0-cdh5.14.0bin/oozie-setup.sh db create -run -sqlfile oozie.sql 6 打包项目生成war包12cd /export/servers/oozie-4.1.0-cdh5.14.0bin/oozie-setup.sh prepare-war 7 配置oozie 环境变量1234567vim /etc/profileexport OOZIE_HOME=/export/servers/oozie-4.1.0-cdh5.14.0export OOZIE_URL=http://node01.hadoop.com:11000/oozieexport PATH=$PATH:$OOZIE_HOME/binsource /etc/profile 8 启动关闭oozie服务12345启动命令cd /export/servers/oozie-4.1.0-cdh5.14.0bin/oozied.sh start 关闭命令bin/oozied.sh stop 注意： 1oozie-4.1.0-cdh5.14.0/oozie-server/temp/oozie.pid 启动的时候产生的 pid文件，如果是kill方式关闭进程 则需要删除该文件重新启动，否则再次启动会报错。启动的时候产生的 pid文件，如果是kill方式关闭进程 则需要删除该文件重新启动，否则再次启动会报错。 9 浏览器web ui 页面1http://node01:11000/oozie 10 解决oozie 页面显示异常页面访问的时候，发现oozie使用的还是GMT的时区，与我们现在的时区相差一定的时间，所以需要调整一个js的获取时区的方法，将其改成我们现在的时区。 修改js当中的时区问题 12cd oozie-server/webapps/oozievim oozie-console.js 1234function getTimeZone() &#123; Ext.state.Manager.setProvider(new Ext.state.CookieProvider()); return Ext.state.Manager.get("TimezoneId","GMT+0800");&#125; 重启oozie服务 123cd /export/servers/oozie-4.1.0-cdh5.14.0bin/oozied.sh stopbin/oozied.sh start 五 Oozie的实战oozie安装好了之后，需要测试oozie的功能是否完整好使，官方已经给自带带了各种测试案例，可以通过官方提供的各种案例来学习oozie的使用，后续也可以把这些案例作为模板在企业实际中使用。 先把官方提供的各种案例给解压出来 12cd /export/servers/oozie-4.1.0-cdh5.14.0tar -zxvf oozie-examples.tar.gz 创建统一的工作目录，便于集中管理oozie。企业中可任意指定路径。这里直接在oozie的安装目录下面创建工作目录 12cd /export/servers/oozie-4.1.0-cdh5.14.0mkdir oozie_works 1 ． 优化更新hadoop相关配置1.1 yarn 容器资源 分配属性 yarn-site.xml 123456789101112131415161718192021222324252627282930&lt;!—节点最大可用内存，结合实际物理内存调整 --&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt; &lt;value&gt;3072&lt;/value&gt;&lt;/property&gt;&lt;!—每个容器可以申请内存资源的最小值，最大值 --&gt;&lt;property&gt; &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt; &lt;value&gt;1024&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt; &lt;value&gt;3072&lt;/value&gt;&lt;/property&gt;&lt;!—修改为Fair公平调度，动态调整资源，避免yarn上任务等待（多线程执行） --&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler&lt;/value&gt;&lt;/property&gt;&lt;!—Fair调度时候是否开启抢占功能 --&gt;&lt;property&gt; &lt;name&gt;yarn.scheduler.fair.preemption&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;!—超过多少开始抢占，默认0.8--&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.fair.preemption.cluster-utilization-threshold&lt;/name&gt; &lt;value&gt;1.0&lt;/value&gt; &lt;/property&gt; 1.2mapreduce资源申请配置 设置mapreduce.map.memory.mb和mapreduce.reduce.memory.mb配置 否则Oozie读取的默认配置 -1, 提交给yarn的时候会抛异常Invalid resource request, requested memory &lt; 0, or requested memory &gt; max configured, requestedMemory=-1, maxMemory=8192 mapred-site.xml 123456789&lt;!—单个maptask、reducetask可申请内存大小 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.map.memory.mb&lt;/name&gt; &lt;value&gt;1024&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.reduce.memory.mb&lt;/name&gt; &lt;value&gt;1024&lt;/value&gt;&lt;/property&gt; scp给其他机器 重启集群 （hadoop ） oozie ##1． Oozie调度shell脚本 把shell的任务模板拷贝到oozie的工作目录当中去 12cd /export/servers/oozie-4.1.0-cdh5.14.0cp -r examples/apps/shell/ oozie_works/ 准备待调度的shell脚本文件 12cd /export/servers/oozie-4.1.0-cdh5.14.0vim oozie_works/shell/hello.sh 注意：这个脚本一定要是在我们oozie工作路径下的shell路径下的位置 #!/bin/bash echo “hello world” &gt;&gt; /export/servers/hello_oozie.txt 修改job.properties 123456789cd /export/servers/oozie-4.1.0-cdh5.14.0/oozie_works/shellvim job.propertiesnameNode=hdfs://node01:8020jobTracker=node01:8032queueName=defaultexamplesRoot=oozie_worksoozie.wf.application.path=$&#123;nameNode&#125;/user/$&#123;user.name&#125;/$&#123;examplesRoot&#125;/shellEXEC=hello.sh jobTracker：在hadoop2当中，jobTracker这种角色已经没有了，只有resourceManager，这里给定resourceManager的IP及端口即可。 queueName：提交mr任务的队列名； examplesRoot：指定oozie的工作目录； oozie.wf.application.path：指定oozie调度资源存储于hdfs的工作路径； EXEC：指定执行任务的名称。 修改workflow.xml 1234567891011121314151617181920212223242526272829303132333435&lt;workflow-app xmlns="uri:oozie:workflow:0.4" name="shell-wf"&gt;&lt;start to="shell-node"/&gt;&lt;action name="shell-node"&gt; &lt;shell xmlns="uri:oozie:shell-action:0.2"&gt; &lt;job-tracker&gt;$&#123;jobTracker&#125;&lt;/job-tracker&gt; &lt;name-node&gt;$&#123;nameNode&#125;&lt;/name-node&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapred.job.queue.name&lt;/name&gt; &lt;value&gt;$&#123;queueName&#125;&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; &lt;exec&gt;$&#123;EXEC&#125;&lt;/exec&gt; &lt;file&gt;/user/root/oozie_works/shell/$&#123;EXEC&#125;#$&#123;EXEC&#125;&lt;/file&gt; &lt;capture-output/&gt; &lt;/shell&gt; &lt;ok to="end"/&gt; &lt;error to="fail"/&gt;&lt;/action&gt;&lt;decision name="check-output"&gt; &lt;switch&gt; &lt;case to="end"&gt; $&#123;wf:actionData('shell-node')['my_output'] eq 'Hello Oozie'&#125; &lt;/case&gt; &lt;default to="fail-output"/&gt; &lt;/switch&gt;&lt;/decision&gt;&lt;kill name="fail"&gt; &lt;message&gt;Shell action failed, error message[$&#123;wf:errorMessage(wf:lastErrorNode())&#125;]&lt;/message&gt;&lt;/kill&gt;&lt;kill name="fail-output"&gt; &lt;message&gt;Incorrect output, expected [Hello Oozie] but was [$&#123;wf:actionData('shell-node')['my_output']&#125;]&lt;/message&gt;&lt;/kill&gt;&lt;end name="end"/&gt;&lt;/workflow-app&gt; 1.1． 上传调度任务到hdfs注意：上传的hdfs目录为/user/root，因为hadoop启动的时候使用的是root用户，如果hadoop启动的是其他用户，那么就上传到/user/其他用户 12cd /export/servers/oozie-4.1.0-cdh5.14.0hdfs dfs -put oozie_works/ /user/root 通过oozie的命令来执行调度任务 123cd /export/servers/oozie-4.1.0-cdh5.14.0bin/oozie job -oozie http://node01:11000/oozie -config oozie_works/shell/job.properties -run 杀死任务流: 1bin/oozie job -oozie http://node01:11000/oozie -kill job的ID ##2． Oozie调度hive 2.1 准备模板12cd /export/servers/oozie-4.1.0-cdh5.14.0cp -ra examples/apps/hive2/ oozie_works/ 2.2 修改模板 1234567891011121314修改job.propertiescd /export/servers/oozie-4.1.0-cdh5.14.0/oozie_works/hive2vim job.propertiesnameNode=hdfs://node01:8020jobTracker=node01:8032queueName=defaultjdbcURL=jdbc:hive2://node01:10000/defaultexamplesRoot=oozie_worksoozie.use.system.libpath=true# 配置我们文件上传到hdfs的保存路径 实际上就是在hdfs 的/user/root/oozie_works/hive2这个路径下oozie.wf.application.path=$&#123;nameNode&#125;/user/$&#123;user.name&#125;/$&#123;examplesRoot&#125;/hive2 修改workflow.xml（实际上无修改） 1234567891011121314151617181920212223242526272829303132&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;workflow-app xmlns="uri:oozie:workflow:0.5" name="hive2-wf"&gt; &lt;start to="hive2-node"/&gt; &lt;action name="hive2-node"&gt; &lt;hive2 xmlns="uri:oozie:hive2-action:0.1"&gt; &lt;job-tracker&gt;$&#123;jobTracker&#125;&lt;/job-tracker&gt; &lt;name-node&gt;$&#123;nameNode&#125;&lt;/name-node&gt; &lt;prepare&gt; &lt;delete path="$&#123;nameNode&#125;/user/$&#123;wf:user()&#125;/$&#123;examplesRoot&#125;/output-data/hive2"/&gt; &lt;mkdir path="$&#123;nameNode&#125;/user/$&#123;wf:user()&#125;/$&#123;examplesRoot&#125;/output-data"/&gt; &lt;/prepare&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapred.job.queue.name&lt;/name&gt; &lt;value&gt;$&#123;queueName&#125;&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; &lt;jdbc-url&gt;$&#123;jdbcURL&#125;&lt;/jdbc-url&gt; &lt;script&gt;script.q&lt;/script&gt; &lt;param&gt;INPUT=/user/$&#123;wf:user()&#125;/$&#123;examplesRoot&#125;/input-data/table&lt;/param&gt; &lt;param&gt;OUTPUT=/user/$&#123;wf:user()&#125;/$&#123;examplesRoot&#125;/output-data/hive2&lt;/param&gt; &lt;/hive2&gt; &lt;ok to="end"/&gt; &lt;error to="fail"/&gt; &lt;/action&gt; &lt;kill name="fail"&gt; &lt;message&gt;Hive2 (Beeline) action failed, error message[$&#123;wf:errorMessage(wf:lastErrorNode())&#125;]&lt;/message&gt; &lt;/kill&gt; &lt;end name="end"/&gt;&lt;/workflow-app&gt; 编辑hivesql文件 1vim script.q 12345DROP TABLE IF EXISTS test;CREATE EXTERNAL TABLE test (a INT) STORED AS TEXTFILE LOCATION '$&#123;INPUT&#125;';insert into test values(10);insert into test values(20);insert into test values(30); 上传调度任务到hdfs 12cd /export/servers/oozie-4.1.0-cdh5.14.0/oozie_workshdfs dfs -put hive2/ /user/root/oozie_works/ 执行调度任务 oozie调度hive脚本 首先必须保证hiveserve2服务是启动正常的，如果配置metastore服务，要首先启动metastore 123在hive目录下执行nohup hive --service metastore &amp;nohup hive --service hiveserver2 &amp; 测试 hive2 12beeline! connect jdbc:hive2//node01:10000 看能否进去 前提node01上的hiveserve2服务是启动正常的 12cd /export/servers/oozie-4.1.0-cdh5.14.0bin/oozie job -oozie http://node01:11000/oozie -config oozie_works/hive2/job.properties -run 3 oozie 调度 MapReduce1准备模板 准备mr程序的待处理数据。用hadoop自带的MR程序来运行wordcount。 准备数据上传到HDFS的/oozie/input路径下去 12hdfs dfs -mkdir -p /oozie/inputhdfs dfs -put wordcount.txt /oozie/input 拷贝MR的任务模板 12cd /export/servers/oozie-4.1.0-cdh5.14.0cp -ra examples/apps/map-reduce/ oozie_works/ 删掉MR任务模板lib目录下自带的jar包 12cd /export/servers/oozie-4.1.0-cdh5.14.0/oozie_works/map-reduce/librm -rf oozie-examples-4.1.0-cdh5.14.0.jar 拷贝官方自带mr程序jar包到对应目录 12cp /export/servers/hadoop-2.7.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.5.jar /export/servers/oozie-4.1.0-cdh5.14.0/oozie_works/map-reduce/lib/ 2 修改模板 修改job.properties 12345678910cd /export/servers/oozie-4.1.0-cdh5.14.0/oozie_works/map-reducevim job.propertiesnameNode=hdfs://node01:8020jobTracker=node01:8032queueName=defaultexamplesRoot=oozie_worksoozie.wf.application.path=$&#123;nameNode&#125;/user/$&#123;user.name&#125;/$&#123;examplesRoot&#125;/map-reduce/workflow.xmloutputDir=/oozie/outputinputdir=/oozie/input 修改workflow.xml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103cd /export/servers/oozie-4.1.0-cdh5.14.0/oozie_works/map-reducevim workflow.xml&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;workflow-app xmlns="uri:oozie:workflow:0.5" name="map-reduce-wf"&gt; &lt;start to="mr-node"/&gt; &lt;action name="mr-node"&gt; &lt;map-reduce&gt; &lt;job-tracker&gt;$&#123;jobTracker&#125;&lt;/job-tracker&gt; &lt;name-node&gt;$&#123;nameNode&#125;&lt;/name-node&gt; &lt;prepare&gt; &lt;delete path="$&#123;nameNode&#125;/$&#123;outputDir&#125;"/&gt; &lt;/prepare&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapred.job.queue.name&lt;/name&gt; &lt;value&gt;$&#123;queueName&#125;&lt;/value&gt; &lt;/property&gt; &lt;!-- &lt;property&gt; &lt;name&gt;mapred.mapper.class&lt;/name&gt; &lt;value&gt;org.apache.oozie.example.SampleMapper&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.reducer.class&lt;/name&gt; &lt;value&gt;org.apache.oozie.example.SampleReducer&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.map.tasks&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.input.dir&lt;/name&gt; &lt;value&gt;/user/$&#123;wf:user()&#125;/$&#123;examplesRoot&#125;/input-data/text&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.output.dir&lt;/name&gt; &lt;value&gt;/user/$&#123;wf:user()&#125;/$&#123;examplesRoot&#125;/output-data/$&#123;outputDir&#125;&lt;/value&gt; &lt;/property&gt; --&gt; &lt;!-- 开启使用新的API来进行配置 --&gt; &lt;property&gt; &lt;name&gt;mapred.mapper.new-api&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.reducer.new-api&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定MR的输出key的类型 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.job.output.key.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.io.Text&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定MR的输出的value的类型--&gt; &lt;property&gt; &lt;name&gt;mapreduce.job.output.value.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.io.IntWritable&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定输入路径 --&gt; &lt;property&gt; &lt;name&gt;mapred.input.dir&lt;/name&gt; &lt;value&gt;$&#123;nameNode&#125;/$&#123;inputdir&#125;&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定输出路径 --&gt; &lt;property&gt; &lt;name&gt;mapred.output.dir&lt;/name&gt; &lt;value&gt;$&#123;nameNode&#125;/$&#123;outputDir&#125;&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定执行的map类 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.job.map.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.examples.WordCount$TokenizerMapper&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定执行的reduce类 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.job.reduce.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.examples.WordCount$IntSumReducer&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置map task的个数 --&gt; &lt;property&gt; &lt;name&gt;mapred.map.tasks&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; &lt;/map-reduce&gt; &lt;ok to="end"/&gt; &lt;error to="fail"/&gt; &lt;/action&gt; &lt;kill name="fail"&gt; &lt;message&gt;Map/Reduce failed, error message[$&#123;wf:errorMessage(wf:lastErrorNode())&#125;]&lt;/message&gt; &lt;/kill&gt; &lt;end name="end"/&gt;&lt;/workflow-app&gt; 3 上传调度任务到hdfs 1cd /export/servers/oozie-4.1.0-cdh5.14.0/oozie_works hdfs dfs -put map-reduce/ /user/root/oozie_works/ 4 执行任务 12cd /export/servers/oozie-4.1.0-cdh5.14.0bin/oozie job -oozie http://node01:11000/oozie -config oozie_works/map-reduce/job.properties -run 注意: 需要在workflow.xml中开启使用新版的 api hadoop2.x 4 oozie 任务串联在实际工作当中，肯定会存在多个任务需要执行，并且存在上一个任务的输出结果作为下一个任务的输入数据这样的情况，所以我们需要在workflow.xml配置文件当中配置多个action，实现多个任务之间的相互依赖关系。 需求：首先执行一个shell脚本，执行完了之后再执行一个MR的程序，最后再执行一个hive的程序。 1 准备工作目录 12cd /export/servers/oozie-4.1.0-cdh5.14.0/oozie_worksmkdir -p sereval-actions 2 准备调度文件 将之前的hive，shell， MR的执行，进行串联成到一个workflow当中。 1234cd /export/servers/oozie-4.1.0-cdh5.14.0/oozie_workscp hive2/script.q sereval-actions/cp shell/hello.sh sereval-actions/cp -ra map-reduce/lib sereval-actions/ 3修改配置模板 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161cd /export/servers/oozie-4.1.0-cdh5.14.0/oozie_works/sereval-actionsvim workflow.xml&lt;workflow-app xmlns="uri:oozie:workflow:0.4" name="shell-wf"&gt;&lt;start to="shell-node"/&gt;&lt;action name="shell-node"&gt; &lt;shell xmlns="uri:oozie:shell-action:0.2"&gt; &lt;job-tracker&gt;$&#123;jobTracker&#125;&lt;/job-tracker&gt; &lt;name-node&gt;$&#123;nameNode&#125;&lt;/name-node&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapred.job.queue.name&lt;/name&gt; &lt;value&gt;$&#123;queueName&#125;&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; &lt;exec&gt;$&#123;EXEC&#125;&lt;/exec&gt; &lt;!-- &lt;argument&gt;my_output=Hello Oozie&lt;/argument&gt; --&gt; &lt;file&gt;/user/root/oozie_works/sereval-actions/$&#123;EXEC&#125;#$&#123;EXEC&#125;&lt;/file&gt; &lt;capture-output/&gt; &lt;/shell&gt; &lt;ok to="mr-node"/&gt; &lt;error to="mr-node"/&gt;&lt;/action&gt;&lt;action name="mr-node"&gt; &lt;map-reduce&gt; &lt;job-tracker&gt;$&#123;jobTracker&#125;&lt;/job-tracker&gt; &lt;name-node&gt;$&#123;nameNode&#125;&lt;/name-node&gt; &lt;prepare&gt; &lt;delete path="$&#123;nameNode&#125;/$&#123;outputDir&#125;"/&gt; &lt;/prepare&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapred.job.queue.name&lt;/name&gt; &lt;value&gt;$&#123;queueName&#125;&lt;/value&gt; &lt;/property&gt; &lt;!-- &lt;property&gt; &lt;name&gt;mapred.mapper.class&lt;/name&gt; &lt;value&gt;org.apache.oozie.example.SampleMapper&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.reducer.class&lt;/name&gt; &lt;value&gt;org.apache.oozie.example.SampleReducer&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.map.tasks&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.input.dir&lt;/name&gt; &lt;value&gt;/user/$&#123;wf:user()&#125;/$&#123;examplesRoot&#125;/input-data/text&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.output.dir&lt;/name&gt; &lt;value&gt;/user/$&#123;wf:user()&#125;/$&#123;examplesRoot&#125;/output-data/$&#123;outputDir&#125;&lt;/value&gt; &lt;/property&gt; --&gt; &lt;!-- 开启使用新的API来进行配置 --&gt; &lt;property&gt; &lt;name&gt;mapred.mapper.new-api&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.reducer.new-api&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定MR的输出key的类型 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.job.output.key.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.io.Text&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定MR的输出的value的类型--&gt; &lt;property&gt; &lt;name&gt;mapreduce.job.output.value.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.io.IntWritable&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定输入路径 --&gt; &lt;property&gt; &lt;name&gt;mapred.input.dir&lt;/name&gt; &lt;value&gt;$&#123;nameNode&#125;/$&#123;inputdir&#125;&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定输出路径 --&gt; &lt;property&gt; &lt;name&gt;mapred.output.dir&lt;/name&gt; &lt;value&gt;$&#123;nameNode&#125;/$&#123;outputDir&#125;&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定执行的map类 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.job.map.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.examples.WordCount$TokenizerMapper&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定执行的reduce类 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.job.reduce.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.examples.WordCount$IntSumReducer&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置map task的个数 --&gt; &lt;property&gt; &lt;name&gt;mapred.map.tasks&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; &lt;/map-reduce&gt; &lt;ok to="hive2-node"/&gt; &lt;error to="fail"/&gt; &lt;/action&gt; &lt;action name="hive2-node"&gt; &lt;hive2 xmlns="uri:oozie:hive2-action:0.1"&gt; &lt;job-tracker&gt;$&#123;jobTracker&#125;&lt;/job-tracker&gt; &lt;name-node&gt;$&#123;nameNode&#125;&lt;/name-node&gt; &lt;prepare&gt; &lt;delete path="$&#123;nameNode&#125;/user/$&#123;wf:user()&#125;/$&#123;examplesRoot&#125;/output-data/hive2"/&gt; &lt;mkdir path="$&#123;nameNode&#125;/user/$&#123;wf:user()&#125;/$&#123;examplesRoot&#125;/output-data"/&gt; &lt;/prepare&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapred.job.queue.name&lt;/name&gt; &lt;value&gt;$&#123;queueName&#125;&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; &lt;jdbc-url&gt;$&#123;jdbcURL&#125;&lt;/jdbc-url&gt; &lt;script&gt;script.q&lt;/script&gt; &lt;param&gt;INPUT=/user/$&#123;wf:user()&#125;/$&#123;examplesRoot&#125;/input-data/table&lt;/param&gt; &lt;param&gt;OUTPUT=/user/$&#123;wf:user()&#125;/$&#123;examplesRoot&#125;/output-data/hive2&lt;/param&gt; &lt;/hive2&gt; &lt;ok to="end"/&gt; &lt;error to="fail"/&gt; &lt;/action&gt;&lt;decision name="check-output"&gt; &lt;switch&gt; &lt;case to="end"&gt; $&#123;wf:actionData('shell-node')['my_output'] eq 'Hello Oozie'&#125; &lt;/case&gt; &lt;default to="fail-output"/&gt; &lt;/switch&gt;&lt;/decision&gt;&lt;kill name="fail"&gt; &lt;message&gt;Shell action failed, error message[$&#123;wf:errorMessage(wf:lastErrorNode())&#125;]&lt;/message&gt;&lt;/kill&gt;&lt;kill name="fail-output"&gt; &lt;message&gt;Incorrect output, expected [Hello Oozie] but was [$&#123;wf:actionData('shell-node')['my_output']&#125;]&lt;/message&gt;&lt;/kill&gt;&lt;end name="end"/&gt;&lt;/workflow-app&gt; job.properties配置文件 1234567891011nameNode=hdfs://node01:8020jobTracker=node01:8032queueName=defaultexamplesRoot=oozie_worksEXEC=hello.shoutputDir=/oozie/outputinputdir=/oozie/inputjdbcURL=jdbc:hive2://node01:10000/defaultoozie.use.system.libpath=true# 配置我们文件上传到hdfs的保存路径 实际上就是在hdfs 的/user/root/oozie_works/sereval-actions这个路径下oozie.wf.application.path=$&#123;nameNode&#125;/user/$&#123;user.name&#125;/$&#123;examplesRoot&#125;/sereval-actions/workflow.xml 4 上传任务调度到hdfs 12cd /export/servers/oozie-4.1.0-cdh5.14.0/oozie_works/hdfs dfs -put sereval-actions/ /user/root/oozie_works/ 5执行任务调度 12cd /export/servers/oozie-4.1.0-cdh5.14.0/bin/oozie job -oozie http://node01:11000/oozie -config oozie_works/sereval-actions/job.properties -run 通过action节点 成功失败控制执行的流程 如果上一个action成功 跳转到下一个action 这样就可以变成首尾相连的串联任务 5 定时调度在oozie当中，主要是通过Coordinator 来实现任务的定时调度， Coordinator 模块主要通过xml来进行配置即可。 Coordinator 的调度主要可以有两种实现方式 第一种：基于时间的定时任务调度： oozie基于时间的调度主要需要指定三个参数，第一个起始时间，第二个结束时间，第三个调度频率； 第二种：基于数据的任务调度， 这种是基于数据的调度，只要在有了数据才会触发调度任务。 ###1 准备配置模板 第一步：拷贝定时任务的调度模板 12cd /export/servers/oozie-4.1.0-cdh5.14.0cp -r examples/apps/cron oozie_works/cron-job 第二步：拷贝我们的hello.sh脚本 12cd /export/servers/oozie-4.1.0-cdh5.14.0/oozie_workscp shell/hello.sh cron-job/ 2修改配制模板修改job.properties 1234567891011121314cd /export/servers/oozie-4.1.0-cdh5.14.0/oozie_works/cron-jobvim job.propertiesnameNode=hdfs://node01:8020jobTracker=node01:8032queueName=defaultexamplesRoot=oozie_worksoozie.coord.application.path=$&#123;nameNode&#125;/user/$&#123;user.name&#125;/$&#123;examplesRoot&#125;/cron-job/coordinator.xml#start：必须设置为未来时间，否则任务失败start=2019-05-22T19:20+0800end=2019-08-22T19:20+0800EXEC=hello.shworkflowAppUri=$&#123;nameNode&#125;/user/$&#123;user.name&#125;/$&#123;examplesRoot&#125;/cron-job/workflow.xml 修改coordinator.xml 123456789101112131415161718192021222324252627282930vim coordinator.xml&lt;!-- oozie的frequency 可以支持很多表达式，其中可以通过定时每分，或者每小时，或者每天，或者每月进行执行，也支持可以通过与linux的crontab表达式类似的写法来进行定时任务的执行 例如frequency 也可以写成以下方式 frequency="10 9 * * *" 每天上午的09:10:00开始执行任务 frequency="0 1 * * *" 每天凌晨的01:00开始执行任务 --&gt;&lt;coordinator-app name="cron-job" frequency="$&#123;coord:minutes(1)&#125;" start="$&#123;start&#125;" end="$&#123;end&#125;" timezone="GMT+0800" xmlns="uri:oozie:coordinator:0.4"&gt; &lt;action&gt; &lt;workflow&gt; &lt;app-path&gt;$&#123;workflowAppUri&#125;&lt;/app-path&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;jobTracker&lt;/name&gt; &lt;value&gt;$&#123;jobTracker&#125;&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;nameNode&lt;/name&gt; &lt;value&gt;$&#123;nameNode&#125;&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;queueName&lt;/name&gt; &lt;value&gt;$&#123;queueName&#125;&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; &lt;/workflow&gt; &lt;/action&gt;&lt;/coordinator-app&gt; 修改workflow.xml 1234567891011121314151617181920212223242526vim workflow.xml &lt;workflow-app xmlns="uri:oozie:workflow:0.5" name="one-op-wf"&gt; &lt;start to="action1"/&gt; &lt;action name="action1"&gt; &lt;shell xmlns="uri:oozie:shell-action:0.2"&gt; &lt;job-tracker&gt;$&#123;jobTracker&#125;&lt;/job-tracker&gt; &lt;name-node&gt;$&#123;nameNode&#125;&lt;/name-node&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapred.job.queue.name&lt;/name&gt; &lt;value&gt;$&#123;queueName&#125;&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; &lt;exec&gt;$&#123;EXEC&#125;&lt;/exec&gt; &lt;!-- &lt;argument&gt;my_output=Hello Oozie&lt;/argument&gt; --&gt; &lt;file&gt;/user/root/oozie_works/cron-job/$&#123;EXEC&#125;#$&#123;EXEC&#125;&lt;/file&gt; &lt;capture-output/&gt; &lt;/shell&gt; &lt;ok to="end"/&gt; &lt;error to="end"/&gt;&lt;/action&gt; &lt;end name="end"/&gt;&lt;/workflow-app&gt; 3上传调度任务到hdfs12cd /export/servers/oozie-4.1.0-cdh5.14.0/oozie_workshdfs dfs -put cron-job/ /user/root/oozie_works/ 4 执行调度12cd /export/servers/oozie-4.1.0-cdh5.14.0bin/oozie job -oozie http://node01:11000/oozie -config oozie_works/cron-job/job.properties –run oozie基于时间的定时 主要是需要coordinator来配合workflow进行周期性的触发执行 需要注意时间的格式问题 时区的问题]]></content>
      <tags>
        <tag>BigData</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Impala]]></title>
    <url>%2F2017%2F07%2F11%2FImpala.html</url>
    <content type="text"><![CDATA[一 简介​ impala来自于cloudera，后来贡献给了apache ​ impala是cloudera提供的一款高效率的sql查询工具，提供实时的查询效果，官方测试性能比hive快10到100倍，其sql查询比sparkSQL还要更加快速，号称是当前大数据领域最快的查询sql工具， ​ impala是参照谷歌的新三篇论文（Caffeine–网络搜索引擎、Pregel–分布式图计算、Dremel–交互式分析工具）当中的Dremel实现而来，其中旧三篇论文分别是（BigTable，GFS，MapReduce）分别对应我们即将学的HBase和已经学过的HDFS以及MapReduce。 ​ impala是基于hive并使用内存进行计算，兼顾数据仓库，具有实时，批处理，多并发等优点。 1impala与hive的关系:impala工作底层执行依赖于hive 与hive共用一套元数据存储。在使用impala的时候，必须保证hive服务是正常可靠的，至少metastore开启。 impala是基于hive的大数据分析查询引擎，直接使用hive的元数据库metadata，意味着impala元数据都存储在hive的metastore当中，并且impala兼容hive的绝大多数sql语法。所以需要安装impala的话，必须先安装hive，保证hive安装成功，并且还需要启动hive的metastore服务。 Hive元数据包含用Hive创建的database、table等元信息。元数据存储在关系型数据库中，如Derby、MySQL等。 客户端连接metastore服务，metastore再去连接MySQL数据库来存取元数据。有了metastore服务，就可以有多个客户端同时连接，而且这些客户端不需要知道MySQL数据库的用户名和密码，只需要连接metastore 服务即可。 Hive适合于长时间的批处理查询分析，而Impala适合于实时交互式SQL查询。可以先使用hive进行数据转换处理，之后使用Impala在Hive处理后的结果数据集上进行快速的数据分析。 2 impala与hive的异同相同: 数据表元数据、ODBC/JDBC驱动、SQL语法、灵活的文件格式、存储资源池等 不同: impala最大的跟hive的不同在于 不在把sql编译成mr程序执行 编译成执行计划树 impala的运行分为前端和后端 前端为java实现 后端为c++实现 impala的优化技术:使用LLVM产生运行代码，针对特定查询生成特定代码，同时使用Inline的方式减少函数调用的开销，加快执行效率。(C++特性) 充分利用可用的硬件指令（SSE4.2）。 更好的IO调度，Impala知道数据块所在的磁盘位置能够更好的利用多磁盘的优势，同时Impala支持直接数据块读取和本地代码计算checksum。 通过选择合适数据存储格式可以得到最好性能（Impala支持多种存储格式）。 最大使用内存，中间结果不写磁盘，及时通过网络以stream的方式传递。 执行计划:Hive: 依赖于MapReduce执行框架，执行计划分成 map-&gt;shuffle-&gt;reduce-&gt;map-&gt;shuffle-&gt;reduce…的模型。如果一个Query会 被编译成多轮MapReduce，则会有更多的写中间结果。由于MapReduce执行框架本身的特点，过多的中间过程会增加整个Query的执行时间。 Impala: 把执行计划表现为一棵完整的执行计划树，可以更自然地分发执行计划到各个Impalad执行查询，而不用像Hive那样把它组合成管道型的 map-&gt;reduce模式，以此保证Impala有更好的并发性和避免不必要的中间sort与shuffle。 数据流:Hive: 采用推的方式，每一个计算节点计算完成后将数据主动推给后续节点。 Impala: 采用拉的方式，后续节点通过getNext主动向前面节点要数据，以此方式数据可以流式的返回给客户端，且只要有1条数据被处理完，就可以立即展现出来，而不用等到全部处理完成，更符合SQL交互式查询使用。 内存使用:Hive: 在执行过程中如果内存放不下所有数据，则会使用外存，以保证Query能顺序执行完。每一轮MapReduce结束，中间结果也会写入HDFS中，同样由于MapReduce执行架构的特性，shuffle过程也会有写本地磁盘的操作。 Impala: 1.0.1，而不会利用外存，以后版本应该会进行改进。这使用得目前处理会受到一定的限制，最好还是与配合使用 调度:Hive: 任务调度依赖于Hadoop的调度策略。 Impala: 调度由自己完成，目前只有一种调度器simple-schedule，它会尽量满足数据的局部性，扫描数据的进程尽量靠近数据本身所在的物理机器。调度器 目前还比较简单，在SimpleScheduler::GetBackend中可以看到，现在还没有考虑负载，网络IO状况等因素进行调度。但目前 Impala已经有对执行过程的性能统计分析，应该以后版本会利用这些统计信息进行调度吧。 容错:Hive: 依赖于Hadoop的容错能力。 Impala: 在查询过程中，没有容错逻辑，如果在执行过程中发生故障，则直接返回错误（这与Impala的设计有关，因为Impala定位于实时查询，一次查询失败， 再查一次就好了，再查一次的成本很低）。 使用地:Hive: 复杂的批处理查询任务，数据转换任务。 Impala：实时数据分析，因为不支持UDF，能处理的问题域有一定的限制，与Hive配合使用,对Hive的结果数据集进行实时分析。 架构:Impala主要由Impalad、 State Store、Catalogd和CLI组成。 impala 可以集群部署 Impalad(impala server):可以部署多个不同机器上，通常与datanode部署在同一个节点 方便数据本地计算，负责具体执行本次查询sql的impalad称之为Coordinator。每个impala server都可以对外提供服务。 impala state store:主要是保存impalad的状态信息 监视其健康状态 impala catalogd :metastore维护的网关 负责跟hive 的metastore进行交互 同步hive的元数据到impala自己的元数据中。 CLI:用户操作impala的方式（impala shell、jdbc、hue） impala 查询处理流程 impalad分为java前端（接受解析sql编译成执行计划树），c++后端（负责具体的执行计划树操作） impala sql—-&gt;impalad（Coordinator）—-&gt;调用java前端编译sql成计划树——&gt;以Thrift数据格式返回给C++后端——&gt;根据执行计划树、数据位于路径（libhdfs和hdfs交互）、impalad状态分配执行计划 查询—–&gt;汇总查询结果—–&gt;返回给java前端—-&gt;用户cli 跟hive不同就在于整个执行中已经没有了mapreduce程序的存在 二安装部署前提集群提前安装好Hadoop和hive hive安装包scp在所有需要安装impala的节点上，因为impala需要引用hive的依赖包。 hadoop框架需要支持C程序访问接口，Hadoop/native，如果有该路径下有这么文件，就证明支持C接口。 1 下载安装包和依赖包 由于impala没有提供tar包进行安装，只提供了rpm包。因此在安装impala的时候，需要使用rpm包来进行安装。rpm包只有cloudera公司提供了，所以去cloudera公司网站进行下载rpm包即可。 但是另外一个问题，impala的rpm包依赖非常多的其他的rpm包，可以一个个的将依赖找出来，也可以将所有的rpm包下载下来，制作成我们本地yum源来进行安装。这里就选择制作本地的yum源来进行安装。 所以首先需要下载到所有的rpm包，下载地址如下 1http://archive.cloudera.com/cdh5/repo-as-tarball/5.14.0/cdh5.14.0-centos6.tar.gz 2 配置本地yam源 1.1． 上传安装包解压使用sftp的方式把安装包大文件上传到服务器/cloudera_data目录下。 12cd /cloudera_datatar -zxvf cdh5.14.0-centos6.tar.gz 1.2． 配置本地yum源信息安装Apache Server服务器 123yum -y install httpdservice httpd startchkconfig httpd on 配置本地yum源的文件 123456789cd /etc/yum.repos.dvim localimp.repo [localimp]name=localimpbaseurl=http://node-3/cdh5.14.0/gpgcheck=0enabled=1 创建apache httpd的读取链接 1ln -s /cloudera_data/cdh/5.14.0 /var/www/html/cdh5.14.0 确保**linux的Selinux**关闭 通过浏览器访问本地yum源，如果出现下述页面则成功。 1node03/cdh5.14.0 将本地yum源配置文件localimp.repo发放到所有需要安装impala的节点。 123cd /etc/yum.repos.d/scp localimp.repo node02:$PWDscp localimp.repo node01:$PWD 3 安装 impala 节点规划 服务名称 从节点 从节点 主节点 impala-catalog Node-3 impala-state-store Node-3 impala-server(impalad) Node-1 Node-2 Node-3 1.1． 主节点安装在规划的主节点**node-3**执行以下命令进行安装： 1yum install -y impala impala-server impala-state-store impala-catalog impala-shell 1.2． 从节点安装在规划的从节点**node-1、node-2**执行以下命令进行安装： 1yum install -y impala-server 配置hive和Hadoop 1 hive 1234567891011121314151617181920212223242526272829303132333435363738394041vim /export/servers/hive/conf/hive-site.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://node-1:3306/hive?createDatabaseIfNotExist=true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;hadoop&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.cli.print.current.db&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.cli.print.header&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 绑定运行hiveServer2的主机host,默认localhost --&gt; &lt;property&gt; &lt;name&gt;hive.server2.thrift.bind.host&lt;/name&gt; &lt;value&gt;node-1&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定hive metastore服务请求的uri地址 --&gt; &lt;property&gt; &lt;name&gt;hive.metastore.uris&lt;/name&gt; &lt;value&gt;thrift://node-1:9083&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.metastore.client.socket.timeout&lt;/name&gt; &lt;value&gt;3600&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 将hive安装包cp给其他两个机器。 123cd /export/servers/scp -r hive/ node-2:$PWDscp -r hive/ node-3:$PWD 1.1． 修改hadoop配置所有节点创建下述文件夹 1mkdir -p /var/run/hdfs-sockets 修改所有节点的hdfs-site.xml添加以下配置，修改完之后重启hdfs集群生效 1vim etc/hadoop/hdfs-site.xml dfs.client.read.shortcircuit 打开DFSClient本地读取数据的控制， dfs.domain.socket.path是Datanode和DFSClient之间沟通的Socket的本地路径。 1 把更新hadoop的配置文件，scp给其他机器。 123cd /export/servers/hadoop-2.7.5/etc/hadoopscp -r hdfs-site.xml node-2:$PWDscp -r hdfs-site.xml node-3:$PWD 注意：root用户不需要下面操作，普通用户需要这一步操作。 给这个文件夹赋予权限，如果用的是普通用户hadoop，那就直接赋予普通用户的权限，例如： 1chown -R hadoop:hadoop /var/run/hdfs-sockets/ 因为这里直接用的root用户，所以不需要赋权限了。 1.1． 重启hadoop、hive在node-1上执行下述命令分别启动hive metastore服务和hadoop。 12345678910111213141516171819202122cd /export/servers/hivenohup bin/hive --service metastore &amp;nohup bin/hive --service hiveserver2 &amp;清空日志文件：cat /dev/null &gt; nohup.out启动hiveserver2 的服务 nohup bin/hive --service hiveserver2 &amp;再启动metastore服务nohup bin/hive --service metastore &amp;如果先启动hiveserver2成功再启动metastore报错怎么办？？先把hiveserver2的服务杀死，然后先启动metastore 再启动hiveserver2 如果先启动metastore成功再启动hiveserver2报错怎么办？？先把metastore的服务杀死，然后先启动hiveserver2 再启动metastore cd /export/servers/hadoop-2.7.5/sbin/stop-dfs.sh | sbin/start-dfs.sh 1.2． 复制hadoop、hive配置文件impala的配置目录为/etc/impala/conf，这个路径下面需要把core-site.xml，hdfs-site.xml以及hive-site.xml。 所有节点执行以下命令 123cp -r /export/servers/hadoop-2.7.5/etc/hadoop/core-site.xml /etc/impala/conf/core-site.xmlcp -r /export/servers/hadoop-2.7.5/etc/hadoop/hdfs-site.xml /etc/impala/conf/hdfs-site.xmlcp -r /export/servers/hive/conf/hive-site.xml /etc/impala/conf/hive-site.xml 1． 修改impala配置1.1． 修改impala默认配置所有节点更改impala默认配置文件 123vim /etc/default/impalaIMPALA_CATALOG_SERVICE_HOST=node-3IMPALA_STATE_STORE_HOST=node-3 1.2． 添加mysql驱动通过配置/etc/default/impala中可以发现已经指定了mysql驱动的位置名字。 ​ 使用软链接指向该路径即可（3台机器都需要执行） 1ln -s /export/servers/hive/lib/mysql-connector-java-5.1.32.jar /usr/share/java/mysql-connector-java.jar 1.3． 修改bigtop配置修改bigtop的java_home路径（3台机器） 12vim /etc/default/bigtop-utilsexport JAVA_HOME=/export/servers/jdk1.8.0_65 1． 启动、关闭impala服务主节点node-3启动以下三个服务进程 123service impala-state-store startservice impala-catalog startservice impala-server start 从节点启动node-1与node-2启动impala-server 1service impala-server start 查看impala进程是否存在 ` ps -ef | grep impala启动之后所有关于impala的日志默认都在**/var/log/impala** 如果需要关闭impala服务 把命令中的start该成stop即可。注意如果关闭之后进程依然驻留，可以采取下述方式删除。正常情况下是随着关闭消失的。 解决方式： 访问impalad的管理界面node03:25000 访问statestored的管理界面node03:25010 四 impala的shell参数]]></content>
      <tags>
        <tag>BigData</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Azkaban]]></title>
    <url>%2F2017%2F07%2F10%2FAzkaban.html</url>
    <content type="text"><![CDATA[# Azkaban 一 概述是由领英推出的一款开源免费的工作流调度器软件. 特点: 功能强大 可以调度几乎所有软件的执行（command） 配置简单 job配置文件 提供了web页面使用 提供模块化和可插拔的插件机制，原生支持command、Java、Hive、Pig、Hadoop java语言开发 源码清晰可见 可以进行二次开发 工作流概述: ​ 工作流（Workflow），指“业务过程的部分或整体在计算机应用环境下的自动化”。 ​ 一个完整的数据分析系统通常都是由多个前后依赖的模块组合构成的：数据采集、数据预处理、数据分析、数据展示等。各个模块单元之间存在时间先后依赖关系，且存在着周期性重复。为了很好地组织起这样的复杂执行计划，需要一个工作流调度系统来调度执行。 实现方式: ​ 简单的任务调度：直接使用linux的crontab来定义,但是缺点也是比较明显，无法设置依赖。 ​ 复杂的任务调度：自主开发调度平台，使用开源调度系统，比如azkaban、Apache Oozie、Cascading、Hamake等。其中知名度比较高的是Apache Oozie，但是其配置工作流的过程是编写大量的XML配置，而且代码复杂度比较高，不易于二次开发。 工作流之间的对比: 特性 Hamake Oozie Azkaban Cascading 工作流描述语言 XML XML (xPDL based) text file with key/value pairs Java API 依赖机制 data-driven explicit explicit explicit 是否要web容器 No Yes Yes No 进度跟踪 console/log messages web page web page Java API Hadoop job调度支持 no yes yes yes 运行模式 command line utility daemon daemon API Pig支持 yes yes yes yes 事件通知 no no no yes 需要安装 no yes yes no 支持的hadoop版本 0.18+ 0.20+ currently unknown 0.18+ 重试支持 no workflownode evel yes yes 运行任意命令 yes yes yes yes Amazon EMR支持 yes no currently unknown yes ##二 Azkaban调度器 ###1原理架构: mysql服务器: 存储元数据，如项目名称、项目描述、项目权限、任务状态、SLA规则等 AzkabanWebServer: 对外提供web服务，使用户可以通过web页面管理。职责包括项目管理、权限授权、任务调度、监控executor。 AzkabanExecutorServer: 负责具体的工作流的提交、执行。 2部署方式(3种) 单节点模式：web、executor在同一个进程 适用于测试体验 使用自带的H2数据库 two-server: web、executor在不同的进程中 可以使用第三方数据库 mutil-executor-server: web、executor在不同的机器上 可以部署多个executor服务器 可使用第三方数据库 3源码编译1.Azkaban3.x在安装前需要自己编译成二进制包。并且提前安装好Maven,Ant,Node等软件 2 编译环境 123yum install –y gityum install –y gcc-c++ 3 下载源码解析 123wget https://github.com/azkaban/azkaban/archive/3.51.0.tar.gztar -zxvf 3.51.0.tar.gz cd ./azkaban-3.51.0/ 4 编译源码 1./gradlew build installDist -x test Gradle是一个基于ApacheAnt和ApacheMaven的项目自动化构建工具。-x test 跳过测试。（注意联网下载jar可能会失败、慢） 5 编译后安装包路径 solo-server模式安装包路径 1azkaban-solo-server/build/distributions/ two-server模式和multiple-executor模式web-server安装包路径 1azkaban-web-server/build/distributions/ two-server模式和multiple-executor模式exec-server安装包路径 1azkaban-exec-server/build/distributions/ 数据库相关安装包路径 1azkaban-db/build/distributions/ ##三 安装部署 1单节点模式123456789mkdir /export/servers/azkabantar -zxvf azkaban-solo-server-0.1.0-SNAPSHOT.tar.gz –C /export/servers/azkaban/vim conf/azkaban.propertiesdefault.timezone.id=Asia/Shanghai #修改时区vim plugins/jobtypes/commonprivate.properties添加：memCheck.enabled=falseazkaban默认需要3G的内存，剩余内存不足则会报异常 启动验证: 123cd azkaban-solo-server-0.1.0-SNAPSHOT/bin/start-solo.sh注:启动/关闭必须进到azkaban-solo-server-0.1.0-SNAPSHOT/目录下。 AzkabanSingleServer(对于Azkaban solo‐server模式，Exec Server和Web Server在同一个进程中) 登录页面: 12http://node01:8081/默认密码和用户名为 azkaban 测试: 创建两个文件one.job ,two.job,内容如下，打包成zip包。 12345678910##one.jobcat one.job type=command ##命令类型 command=echo &quot;this is job one&quot;##two.job cat two.job type=command dependencies=one ## 依赖于one.job one.job 执行完后 在执行自己的 command=echo &quot;this is job two&quot; Create Project=&gt;Upload zip包=&gt;execute flow执行一步步操作即可。 上传zip压缩包 选择调度schduler或者立即执行executor工程。 2 two server 模式节点规划: 12node03 mysqlnode02 web-server和exec-server不同进程 node03 上mysql配置初始化: 1234567tar -zxvf azkaban-db-0.1.0-SNAPSHOT.tar.gz –C /export/servers/azkaban/Mysql上创建对应的库、增加权限、创建表mysql&gt; CREATE DATABASE azkaban_two_server; #创建数据库mysql&gt; use azkaban_two_server;mysql&gt; source /export/servers/azkaban/azkaban-db-0.1.0-SNAPSHOT/create-all-sql-0.1.0-SNAPSHOT.sql;#加载初始化sql创建表 node02上 配置 12tar -zxvf azkaban-web-server-0.1.0-SNAPSHOT.tar.gz –C /export/servers/azkaban/tar -zxvf azkaban-exec-server-0.1.0-SNAPSHOT.tar.gz –C /export/servers/azkaban/ ###2.1web配置: 生成ssl证书： 服务器证书,将ssl证书安装在网站服务器上可以实现网站身份和数据加密传输双重功能 1keytool -keystore keystore -alias jetty -genkey -keyalg RSA 运行此命令后,会提示输入当前生成keystore的密码及相应信息,输入的密码请记住(所有密码统一以123456输入)。 完成上述工作后,将在当前目录生成keystore证书文件,将keystore拷贝到 azkaban web服务器根目录中。 配置 web服务器的 conf/azkaban.properties： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768# Azkaban Personalization Settingsazkaban.name=Testazkaban.label=My Local Azkabanazkaban.color=#FF3601azkaban.default.servlet.path=/indexweb.resource.dir=web/# 1要修改的地方default.timezone.id=Asia/Shanghai# Azkaban UserManager classuser.manager.class=azkaban.user.XmlUserManageruser.manager.xml.file=conf/azkaban-users.xml# Loader for projectsexecutor.global.properties=conf/global.propertiesazkaban.project.dir=projects# Velocity dev modevelocity.dev.mode=false# Azkaban Jetty server properties.# 2要修改的地方jetty.use.ssl=truejetty.ssl.port=8443jetty.maxThreads=25jetty.port=8081# Azkaban Executor settings#3要修改的地方executor.host=localhostexecutor.port=12321# KeyStore for SSL ssl相关配置 注意密码和证书路径# 4要修改的地方jetty.keystore=keystorejetty.password=123456jetty.keypassword=123456jetty.truststore=keystorejetty.trustpassword=123456# mail settingsmail.sender=mail.host=# User facing web server configurations used to construct the user facing server URLs. They are useful when there is a reverse proxy between Azkaban web servers and users.# enduser -&gt; myazkabanhost:443 -&gt; proxy -&gt; localhost:8081# when this parameters set then these parameters are used to generate email links.# if these parameters are not set then jetty.hostname, and jetty.port(if ssl configured jetty.ssl.port) are used.# azkaban.webserver.external_hostname=myazkabanhost.com# azkaban.webserver.external_ssl_port=443# azkaban.webserver.external_port=8081job.failure.email=job.success.email=lockdown.create.projects=falsecache.directory=cache# JMX statsjetty.connector.stats=trueexecutor.connector.stats=true# Azkaban mysql settings by default. Users should configure their own username and password.#5要修改的地方database.type=mysqlmysql.port=3306mysql.host=node03mysql.database=azkaban_two_servermysql.user=rootmysql.password=123456mysql.numconnections=100#Multiple Executorazkaban.use.multiple.executors=true# 6修改的地方 注释掉这一行 放弃检查内存#azkaban.executorselector.filters=StaticRemainingFlowSize,MinimumFreeMemory,CpuStatusazkaban.executorselector.comparator.NumberOfAssignedFlowComparator=1azkaban.executorselector.comparator.Memory=1azkaban.executorselector.comparator.LastDispatched=1azkaban.executorselector.comparator.CpuUsage=1 在web 的根目录下创建 目录: 1mkdir -p plugins/jobtypes 在这个目录下: 1234567vim commonprivate.properties#本地库azkaban.native.lib=falseexecute.as.user=false#内存检测memCheck.enabled=false 2.2 exec配置exec服务器下:conf/azkaban.properties： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556# Azkaban Personalization Settingsazkaban.name=Testazkaban.label=My Local Azkabanazkaban.color=#FF3601azkaban.default.servlet.path=/indexweb.resource.dir=web/# 1要修改default.timezone.id=Asia/Shanghai# Azkaban UserManager classuser.manager.class=azkaban.user.XmlUserManageruser.manager.xml.file=conf/azkaban-users.xml# Loader for projectsexecutor.global.properties=conf/global.propertiesazkaban.project.dir=projects# Velocity dev modevelocity.dev.mode=false# Azkaban Jetty server properties.jetty.use.ssl=falsejetty.maxThreads=25jetty.port=8081# Where the Azkaban web server is located # 2要修改的地方azkaban.webserver.url=https://node02:8443# mail settingsmail.sender=mail.host=# User facing web server configurations used to construct the user facing server URLs. They are useful when there is a reverse proxy between Azkaban web servers and users.# enduser -&gt; myazkabanhost:443 -&gt; proxy -&gt; localhost:8081# when this parameters set then these parameters are used to generate email links.# if these parameters are not set then jetty.hostname, and jetty.port(if ssl configured jetty.ssl.port) are used.# azkaban.webserver.external_hostname=myazkabanhost.com# azkaban.webserver.external_ssl_port=443# azkaban.webserver.external_port=8081job.failure.email=job.success.email=lockdown.create.projects=falsecache.directory=cache# JMX statsjetty.connector.stats=trueexecutor.connector.stats=true# Azkaban plugin settingsazkaban.jobtype.plugin.dir=plugins/jobtypes# Azkaban mysql settings by default. Users should configure their own username and password.# 3要修改的地方database.type=mysqlmysql.port=3306mysql.host=node03mysql.database=azkaban_two_servermysql.user=rootmysql.password=123456mysql.numconnections=100# Azkaban Executor settingsexecutor.maxThreads=50# 4要修改的地方 添加executor.port=12321executor.flow.threads=30 2.3启动:先启动exec-server 在根目录下 bin/start-exec.sh 再启动web-server。 在根目录下 bin/start-web.sh 启动webServer之后进程失败消失，可通过安装包根目录下对应启动日志进行排查。 解决: 需要手动激活executor 到exec服务器的根目录下 向executor.port 发送一个executor 将其action改为activate 1curl -G "node02:$(&lt;./executor.port)/executor?action=activate" &amp;&amp; echo 重启web即可 即可在页面测试 测试阶段可能一直处于running状态 解决: 在exec服务器下 cd 到 plugins/jobtypes 123vim commonprivate.properties加入以下配置:memCheck.enabled=false 重启exec和web 并重新向executor.port 发送一个executor 将其action改为activate 1curl -G "node02:$(&lt;./executor.port)/executor?action=activate" &amp;&amp; echo 特点: 该模式的特点是web服务器和executor服务器分别位于不同的进程中 使用第三方的数据库进行数据的保存 ：mysql 注意事项: 先对mysql进行初始化操作 配置azkaban.properties 注意时区 mysql相关 ssl 启动时候注意需要自己手动的激活executor服务器 在根目录下启动 如果启动出错 通过安装包根目录下的日志进行判断 访问的页面https 3multiple-executor模式部署multiple-executor模式是多个executor Server分布在不同服务器上，只需要将azkaban-exec-server安装包拷贝到不同机器上即可组成分布式。 节点配置: 123node03 mysqlnode02 web-server和 exec-servernode01 exec-server 1 scp executor server安装包到node01 前提 node01 和node02 有相同的目录结构(元数据存在的路径)才可以使用 pwd 否则回传到root路径下 1scp -r azkaban-exec-server-0.1.0-SNAPSHOT/ node01:$PWD 启动: 先启动exec 分别启动node01 和node02上的exec 在向executor.port 发送一个executor 将其action改为activate 1curl -G "node02:$(&lt;./executor.port)/executor?action=activate" &amp;&amp; echo 1curl -G "node01:$(&lt;./executor.port)/executor?action=activate" &amp;&amp; echo 在启动web即可 所谓的 multiple-executor指的是可以在多个机器上分别部署executor服务器 相当于做了一个负载均衡 特别注意：executor启动（包括重启）的时候 默认不会激活 需要自己手动激活 对应的mysql中的表executors active ：0 表示未激活 1表示激活 可以自己手动修改数据提交激活 也可以使用官方的命令请求激活 1curl -G "node01:$(&lt;./executor.port)/executor?action=activate" &amp;&amp; echo 五 实战 理论上任何一款软件，只有可以通过shell command执行 都可以转化成为azkaban的调度执行 type=command command = sh xxx.sh 1． shell command调度 创建job描述文件 vi command.job 123#command.jobtype=commandcommand=sh hello.sh 123hello.sh 内容#!/bin/bashdate &gt; /root/hello.txt 将hello.sh和command.job一起打包为zip 上传azkaban 2． job依赖调度123# foo.jobtype=commandcommand=echo foo 1234# bar.jobtype=commanddependencies=foocommand=echo bar 123type=commanddependencies=barcommand=echo baidu 资源打包 zip上传: 若前一个未成功 后一个不会执行 3 hdfs的调度123# fs.jobtype=commandcommand=sh hdfs.sh 12#!/bin/bash/export/servers/hadoop-2.7.5/bin/hadoop fs -mkdir /azaz666 打包压缩 zip 通过azkaban的web管理平台创建project并上传job压缩包 启动执行该job 4 MAPREDUCE任务调度123# mrwc.jobtype=commandcommand=/home/hadoop/apps/hadoop-2.6.1/bin/hadoop jar hadoop-mapreduce-examples-2.6.1.jar wordcount /wordcount/input /wordcount/azout 将jar包和job文件打包zip压缩 上传 5hivehive脚本 test.sql 123456use default;drop table aztest;create table aztest(id int,name string) row format delimited fields terminated by ',';load data inpath '/aztest/hiveinput' into table aztest;create table azres as select * from aztest;insert overwrite directory '/aztest/hiveoutput' select count(1) from aztest; 123# hivef.jobtype=commandcommand=/home/hadoop/apps/hive/bin/hive -f 'test.sql' 6定时任务调度页面中选择schedule进行设置]]></content>
      <tags>
        <tag>BigData</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sqoop]]></title>
    <url>%2F2017%2F07%2F09%2Fsqoop.html</url>
    <content type="text"><![CDATA[Sqoop一 简介Apache Sqoop是在Hadoop生态体系和RDBMS体系之间传送数据的一种工具。 ​ Sqoop工作机制是将导入或导出命令翻译成mapreduce程序来实现。在翻译出的mapreduce中主要是对inputformat和outputformat进行定制。 Hadoop生态系统包括：HDFS、Hive、Hbase等 RDBMS体系包括(关系型数据库)：Mysql、Oracle、DB2等 Sqoop也可以理解为：“SQL 到 Hadoop 和 Hadoop 到SQL”。 站在Apache的立场数据可以分为导入和导出: Import：数据导入。RDBMS—–&gt;Hadoop Export：数据导出。Hadoop—-&gt;RDBMS 安装前提: 安装过java和Hadoop 版本 1.4.6 解压及安装 配置sqoop中的conf中的 mv sqoop-env-template.sh sqoop-env.sh vi sqoop-env.sh 1234export HADOOP_COMMON_HOME= /export/servers/hadoop-2.7.5 export HADOOP_MAPRED_HOME= /export/servers/hadoop-2.7.5export HIVE_HOME= /export/servers/hive##还可以配置hbase等 把数据库的驱动加入 sqoop的lib中 测试: 1bin/sqoop list-databases --connect jdbc:mysql://node03:3306/ --username root --password 123456 本命令会列出所有(mysql)(orole)等的数据库。 二 sqoop的导入1 从数据库导入hdfs mysql的地址尽量不要使用localhost 请使用ip或者host 如果不指定 导入到hdfs默认分隔符是 “,” 可以通过– fields-terminated-by ‘\ t‘ 指定具体的分隔符 如果表的数据比较大 可以并行启动多个maptask执行导入操作，如果表没有主键，请指定根据哪个字段进行切分 全量导入 若是换行 每行结尾必须加 \ 否则报错 123456bin/sqoop import \--connect jdbc:mysql://node03:3306/userdb \--username root \--password 123456 \--target-dir /sqoopresult1 \ ##指定存在hdfs的目录--table emp --m 1 ###指定要导入的表 并指定要运行几个MapTask --m 1 指定分隔符导入 sqoop的默认分隔符为 “,” –fields-terminated-by ‘\t’ \ ##指定存在hdfs 上的文件的分隔符 1234567bin/sqoop import \--connect jdbc:mysql://node03:3306/userdb \--username root \--password 123456 \--target-dir /sqoopresout2 \--fields-terminated-by '\t' \--table emp --m 1 指定分割字段和启动几个MapReduce sqoop命令中，–split-byid通常配合-m 10参数使用。用于指定根据哪个字段进行划分并启动多少个maptask。 12345678bin/sqoop import \--connect jdbc:mysql://node03:3306/userdb \--username root \--password 123456 \--target-dir /sqoopresult214 \--fields-terminated-by '\t' \--split-by id \ ##指定分割字段--table emp --m 2 ##--m 2 启动两个maptask ##2从数据库导入hive 全量导入 1将表结构复制到hive中hive 中的数据库为test 必须存在 emp_add_sp表 可以不存在 123456bin/sqoop create-hive-table \--connect jdbc:mysql://node03:3306/userdb \--username root \--password 123456 \--table emp_add \--hive-table test.emp_add_sp 从关系数据库导入文件到hive中 12345678bin/sqoop import \--connect jdbc:mysql://node03:3306/userdb \--username root \--password 123456 \--table emp_add \--hive-table test.emp_add_sp \--hive-import \--m 1 2直接从数据库导入数据到hive中 包括表结构和数据不用指定hive的表名,会根据数据库的表名自动创建 若test库不存在会在root下创建表 12345678bin/sqoop import \--connect jdbc:mysql://node03:3306/userdb \--username root \--password 123456 \--table emp_conn \--hive-import \--m 1 \--hive-database test ##3导入表数据子集(导入hdfs) 1where条件过滤导入–where可以指定从关系数据库导入数据时的查询条件。它执行在数据库服务器相应的SQL查询，并将结果存储在HDFS的目标目录。 12345678bin/sqoop import \--connect jdbc:mysql://node03:3306/userdb \--username root \--password 123456 \--where "city='sec-bad'" \--target-dir /wherequery \--table emp_add \--m 1 ###2query查询过滤导入 使用 query sql 语句来进行查找不能加参数–table ;并且必须要添加 where 条件;并且 where 条件后面必须带一个$CONDITIONS 这个字符串;并且这个 sql 语句必须用单引号，不能用双引号; 123456789bin/sqoop import \--connect jdbc:mysql://node03:3306/userdb \--username root \--password 123456 \--target-dir /query1 \--query 'select id,name,deg from emp where id&gt;1203 and $CONDITIONS' \--split-by id \--fields-terminated-by '\001' \--m 2 sqoop命令中–split-by id通常配合-m 10参数使用。首先sqoop会向关系型数据库比如mysql发送一个命令:select max(id),min(id) from test。然后会把max、min之间的区间平均分为10分，最后10个并行的map去找数据库，导数据就正式开始。 4增量导入1 Append,模式就是追加导入,在原有的基础上 根据数值类型字段进行追加导入 大于指定的last-value 例子 先把数据库数据导入 123456bin/sqoop import \--connect jdbc:mysql://node03:3306/userdb \--username root \--password 123456 \--target-dir /appendresult \--table emp --m 1 在数据库emp 中在插入两条数据 12insert into `userdb`.`emp` (`id`, `name`, `deg`, `salary`, `dept`) values ('1206', 'allen', 'admin', '30000', 'tp');insert into `userdb`.`emp` (`id`, `name`, `deg`, `salary`, `dept`) values ('1207', 'woon', 'admin', '40000', 'tp'); 执行追加导入 –incremental append \ 增量导入的模式–check-column id \ 数值类型字段进行追加导入–last-value 1205 最后字段值 123456789bin/sqoop import \--connect jdbc:mysql://node03:3306/userdb \--username root \--password 123456 \--table emp --m 1 \--target-dir /appendresult \--incremental append \--check-column id \--last-value 1205 3 lastmodified模式1append模式(附加) lastmodified 根据时间戳类型字段进行追加 大于等于指定的last-value 注意在lastmodified 模式下 还分为两种情形：append merge-key 关于lastmodified 中的两种模式： append 只会追加增量数据到一个新的文件中 并且会产生数据的重复问题 因为默认是从指定的last-value 大于等于其值的数据开始导入 merge-key 把增量的数据合并到一个文件中 处理追加增量数据之外 如果之前的数据有变化修改 也可以进行修改操作 底层相当于进行了一次完整的mr作业。数据不会重复。 数据库建表: 12create table customertest(id int,name varchar(20),last_mod timestamp default current_timestamp on update current_timestamp);此处的时间戳设置为在数据的产生和更新时都会发生改变. 插入数据(一个一个插,可以保证last_mod 字段不一样) 12345insert into customertest(id,name) values(1,'neil');insert into customertest(id,name) values(2,'jack');insert into customertest(id,name) values(3,'martin');insert into customertest(id,name) values(4,'tony');insert into customertest(id,name) values(5,'eric'); 执行命令导入hdfs: 123456bin/sqoop import \--connect jdbc:mysql://node03:3306/userdb \--username root \--password 123456 \--target-dir /lastmodifiedresult \--table customertest --m 1 在mysql中在插入一条数据 1insert into customertest(id,name) values(6,'james') 使用增量导入: 三兄弟:两种导入都要写 –check-column last_mod \–incremental lastmodified \–last-value “2019-06-05 15:52:58” \ 1234567891011bin/sqoop import \--connect jdbc:mysql://node03:3306/userdb \--username root \--password 123456 \--table customertest \--target-dir /lastmodifiedresult \--check-column last_mod \--incremental lastmodified \--last-value "2019-06-05 15:52:58" \--m 1 \--append #追加方式 merge-key 和append 查看结果发现: 此处已经会导入我们最后插入的一条记录,但是我们却发现此处插入了2条数据，这是为什么呢？这是因为采用lastmodified模式去处理增量时，会将大于等于last-value值的数据当做增量插入 注意:使用lastmodified模式进行增量处理要指定增量数据是以append模式(附加)还是merge-key(合并)模式添加 2merge-key模式(合并) 数据库操作: 1update customertest set name = 'Neil' where id = 1; 增量导入: 1234567891011bin/sqoop import \--connect jdbc:mysql://node03:3306/userdb \--username root \--password 123456 \--table customertest \--target-dir /lastmodifiedresult \--check-column last_mod \--incremental lastmodified \--last-value "2019-06-05 15:52:58" \--m 1 \--merge-key id 由于merge-key模式是进行了一次完整的mapreduce操作，因此最终我们在lastmodifiedresult文件夹下可以看到生成的为part-r-00000这样的文件，会发现id=1的name已经得到修改，同时新增了id=6的数据。 三sqoop导出将数据从Hadoop生态体系导出到RDBMS数据库导出前，目标表必须存在于目标数据库中。 export有三种模式： 默认操作: 是从将文件中的数据使用INSERT语句插入到表中。若为空表底层为insert一条一条的插入 更新模式：Sqoop将生成UPDATE替换数据库中现有记录的语句。底层为updata 调用模式：Sqoop将为每条记录创建一个存储过程调用。 配置参数: 导出文件的分隔符 如果不指定 默认以“,”去切割读取数据文件 –input-fields-terminated-by 如果文件的字段顺序和表中顺序不一致 需要–columns 指定 多个字段之间以”,” 导出的时候需要指定导出数据的目的 export-dir 和导出到目标的表名或者存储过程名 针对空字符串类型和非字符串类型的转换 “\n” 1 默认模式导出HDFS数据到mysql默认情况下，sqoop export将每行输入记录转换成一条INSERT语句，添加到目标数据库表中。如果数据库中的表具有约束条件（例如，其值必须唯一的主键列）并且已有数据存在，则必须注意避免插入违反这些约束条件的记录。如果INSERT语句失败，导出过程将失败。此模式主要用于将记录导出到可以接收这些结果的空表中。通常用于全表数据导出。 导出时可以是将Hive表中的全部记录或者HDFS数据（可以是全部字段也可以部分字段）导出到Mysql目标表。 1准备hdfs数据 在HDFS文件系统中“/emp/”目录的下创建一个文件emp_data.txt： 1234561201,gopal,manager,50000,TP1202,manisha,preader,50000,TP1203,kalil,php dev,30000,AC1204,prasanth,php dev,30000,AC1205,kranthi,admin,20000,TP1206,satishp,grpdes,20000,GR 2手动创建数据库中的表 1234567mysql&gt; USE userdb;mysql&gt; CREATE TABLE employee ( id INT NOT NULL PRIMARY KEY, name VARCHAR(20), deg VARCHAR(20), salary INT, dept VARCHAR(10)); 3执行导出命令 123456bin/sqoop export \--connect jdbc:mysql://node03:3306/userdb \--username root \--password 123456 \--table employee \--export-dir /emp/ 若是数据库中字段与emp_data.txt文件中字段类型一致,上述做法可以 若不一致 当导出数据文件和目标表字段列顺序完全一致的时候上述做法可以 若不一致。以逗号为间隔选择和排列各个列。加一下配置 –columns id,name,deg,salary,dept \ 指定emp_data.txt 中个字段的名字 1234567bin/sqoop export \--connect jdbc:mysql://node03:3306/userdb \--username root \--password 123456 \--table employee1 \--columns id,name,deg,salary,dept \--export-dir /emp/ –input-fields-terminated-by ‘\t’ 指定文件中的分隔符 –columns 选择列并控制它们的排序。当导出数据文件和目标表字段列顺序完全一致的时候可以不写。否则以逗号为间隔选择和排列各个列。没有被包含在–columns后面列名或字段要么具备默认值，要么就允许插入空值。否则数据库会拒绝接受sqoop导出的数据，导致Sqoop作业失败 –export-dir 导出目录，在执行导出的时候，必须指定这个参数，同时需要具备–table或–call参数两者之一，–table是指的导出数据库当中对应的表， –call是指的某个存储过程。 –input-null-string –input-null-non-string 如果没有指定第一个参数，对于字符串类型的列来说，“NULL”这个字符串就回被翻译成空值，如果没有使用第二个参数，无论是“NULL”字符串还是说空字符串也好，对于非字符串类型的字段来说，这两个类型的空串都会被翻译成空值。比如： –input-null-string “\N” –input-null-non-string “\N” 2更新导出（updateonly模式）– update-key，更新标识，即根据某个字段进行更新，例如id，可以指定多个更新标识的字段，多个字段之间用逗号分隔。 – updatemod，指定updateonly（默认模式），仅仅更新已存在的数据记录，不会插入新纪录。 在HDFS文件系统中“/updateonly_1/”目录的下创建一个文件updateonly_1.txt：1201,gopal,manager,500001202,manisha,preader,500001203,kalil,php dev,30000 手动创建mysql中的目标表 123456mysql&gt; USE userdb;mysql&gt; CREATE TABLE updateonly ( id INT NOT NULL PRIMARY KEY, name VARCHAR(20), deg VARCHAR(20), salary INT); 先执行全部导出操作： 123456bin/sqoop export \--connect jdbc:mysql://node03:3306/userdb \--username root \--password 123456 \--table updateonly \--export-dir /updateonly_1/ 新增一个文件updateonly_2.txt：修改了前三条数据并且新增了一条记录1201,gopal,manager,12121202,manisha,preader,13131203,kalil,php dev,14141204,allen,java,1515 hadoop fs -put updateonly_2.txt /updateonly_2 执行更新导出： 12345678bin/sqoop export \--connect jdbc:mysql://node03:3306/userdb \--username root \--password 123456 \--table updateonly \--export-dir /updateonly_2/ \--update-key id \--update-mode updateonly 3 更新导出（allowinsert模式）– update-key，更新标识，即根据某个字段进行更新，例如id，可以指定多个更新标识的字段，多个字段之间用逗号分隔。 – updatemod，指定allowinsert，更新已存在的数据记录，同时插入新纪录。实质上是一个insert &amp; update的操作。 在HDFS “/allowinsert_1/”目录的下创建一个文件allowinsert_1.txt：1201,gopal,manager,500001202,manisha,preader,500001203,kalil,php dev,30000 手动创建mysql中的目标表 123456mysql&gt; USE userdb;mysql&gt; CREATE TABLE allowinsert ( id INT NOT NULL PRIMARY KEY, name VARCHAR(20), deg VARCHAR(20), salary INT); 先执行全部导出操作 123456bin/sqoop export \--connect jdbc:mysql://node03:3306/userdb \--username root \--password 123456 \--table allowinsert \--export-dir /allowinsert_1/ allowinsert_2.txt。修改了前三条数据并且新增了一条记录。上传至/ allowinsert_2/目录下：1201,gopal,manager,12121202,manisha,preader,13131203,kalil,php dev,14141204,allen,java,1515 执行更新导出 1234567bin/sqoop export \--connect jdbc:mysql://node03:3306/userdb \--username root --password 123456 \--table allowinsert \--export-dir /allowinsert_2/ \--update-key id \--update-mode allowinsert 四sqoop job 作业1 job 语法12345$ sqoop job (generic-args) (job-args) [-- [subtool-name] (subtool-args)]$ sqoop-job (generic-args) (job-args) [-- [subtool-name] (subtool-args)] 2 创建job在这里，我们创建一个名为jobtest，这可以从RDBMS表的数据导入到HDFS作业。 下面的命令用于创建一个从DB数据库的emp表导入到HDFS文件的作业。 1234567bin/sqoop job --create jobtest -- import --connect jdbc:mysql://node03:3306/userdb \--username root \--password 123456 \--target-dir /sqoopresult333 \--table emp --m 1注意import前要有空格 3 验证job–list’ 参数是用来验证保存的作业。下面的命令用来验证保存Sqoop作业的列表。 1bin/sqoop job --list 4检查job‘–show’ 参数用于检查或验证特定的工作，及其详细信息。以下命令和样本输出用来验证一个名为jobtest的作业。 1bin/sqoop job --show jobtest 5 执行job‘–exec’ 选项用于执行保存的作业。下面的命令用于执行保存的作业称为jobtest。 1bin/sqoop job --exec jobtest 6免密执行jobsqoop在创建job时，使用–password-file参数，可以避免输入mysql密码，如果使用–password将出现警告，并且每次都要手动输入密码才能执行job，sqoop规定密码文件必须存放在HDFS上，并且权限必须是400。 并且检查sqoop的sqoop-site.xml是否存在如下配置： 1234567891011&lt;property&gt; &lt;name&gt;sqoop.metastore.client.record.password&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;description&gt;If true, allow saved passwords in the metastore. &lt;/description&gt;&lt;/property&gt; 12345bin/sqoop job --create jobtest -- import --connect jdbc:mysql://node03:3306/userdb \--username root \--password-file /input/sqoop/pwd/mysqltest.pwd \--target-dir /sqoopresult333 \--table emp --m 1 五 sqoop1与sqoop2的区别1 版本号对比两代之间是两个完全不同的版本，不兼容 两代之间是两个完全不同的版本，不兼容sqoop1：1.4.x sqoop2：1.99.x 2 sqoop2的改进(1) 引入sqoop server，集中化管理connector等(2) 多种访问方式：CLI,Web UI，REST API(3) 引入基于角色 的安全机制\ 3 功能性对比 功能 sqoop1 sqoop2 用于所有主要 RDBMS 的连接器 支持 不支持解决办法： 使用已在以下数据库上执行测试的通用 JDBC 连接器： Microsoft SQL Server 、 PostgreSQL 、 MySQL 和 Oracle 。 Kerberos 安全集成 支持 不支持 数据从 RDBMS 传输至 Hive 或 HBase 支持 不支持 解决办法： 按照此两步方法操作。 将数据从 RDBMS 导入 HDFS 在 Hive 中使用相应的工具和命令（例如 LOAD DATA 语句），手动将数据载入 Hive 或 HBase 数据从 Hive 或 HBase 传输至 RDBMS 不支持 解决办法： 按照此两步方法操作。 从 Hive 或 HBase 将数据提取至 HDFS （作为文本或 Avro 文件） 使用 Sqoop 将上一步的输出导出至 RDBMS 不支持 按照与 Sqoop 1 相同的解决方法操作 4 架构对比1 sqoop1在架构上：sqoop1使用sqoop客户端直接提交的方式访问方式：CLI控制台方式进行访问安全性：命令或脚本中指定用户数据库名及密码 2 sqoop2版本号为1.99x为sqoop2在架构上：sqoop2引入了sqoop server，对connector实现了集中的管理访问方式：REST API、 JAVA API、 WEB UI以及CLI控制台方式进行访问 CLI方式访问，会通过交互过程界面，输入的密码信息丌被看到，同时Sqoop2引入基亍角色的安全机制，Sqoop2比Sqoop多了一个Server端。 5 两者的优缺点比较sqoop1: 123sqoop1优点架构部署简单 sqoop1的缺点命令行方式容易出错，格式紧耦合，无法支持所有数据类型，安全机制不够完善，例如密码暴漏， 安装需要root权限，connector必须符合JDBC模型 sqoop2 123sqoop2的优点多种交互方式，命令行，web UI，rest API，conncetor集中化管理，所有的链接安装在sqoop server上，完善权限管理机制，connector规范化，仅仅负责数据的读写。 它引入了基于角色的安全机制，管理员可以在sqoopServer上， 配置不同的角色ssqoop2的缺点，架构稍复杂，配置部署更繁琐。]]></content>
      <tags>
        <tag>BigData</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flume]]></title>
    <url>%2F2017%2F07%2F08%2Fflume.html</url>
    <content type="text"><![CDATA[​ Apache Flume 一 概述​ flume是一个高可用,高可靠的,分布式的海量日志采集,聚合和传输的软件.特别指的是数据流转的过程，或者说是数据搬运的过程。把数据从一个存储介质通过flume传递到另一个存储介质中。 ​ 核心是把数据从数据源(source)收集过来,再将收集的数据送到指定的目的地(sink).为保证一定能送到目的地,再送到目的地之前,会先缓存数据(channel),等到数据到达目的地后,flume删除缓存. ​ flume支持定义各类数据发送方,用于收集各种类型数据,同时，Flume支持定制各种数据接受方，用于最终存储数据。一般的采集需求，通过对flume的简单配置即可实现。针对特殊场景也具备良好的自定义扩展能力。因此，flume可以适用于大部分的日常数据采集场景。 二 运行机制 ​ 1 核心组件 source ：用于对接各个不同的数据源 sink：用于对接各个不同存储数据的目的地（数据下沉地） channel：用于中间临时存储缓存数据 2 机制 flume本身是java程序 在需要采集数据机器上启动 —–&gt;agent进程 agent进程里面包含了：source sink channel 在flume中，数据被包装成event 真是的数据是放在event body中event是flume中最小的数据单元 3 架构 简单架构 只需要部署一个agent进程即可 复杂架构 多个agent之间的串联 相当于大家手拉手共同完成数据的采集传输工作 在串联的架构中没有主从之分 大家的地位都是一样的 三 安装部署​ 上传安装包到数据源所在节点上然后解压 tar -zxvf apache-flume-1.8.0-bin.tar.gz然后进入flume的目录，修改conf下的flume-env.sh，在里面配置JAVA_HOME flume开发步骤 在conf中,就是根据业务需求编写采集方案配置文件 文件名见名知意 通常以souce——sink.conf 具体需要描述清楚sink source channel组件配置信息 结合官网配置 启动命令: 12345bin/flume-ng agent --conf conf --conf-file conf/netcat-logger.conf --name a1 -Dflume.root.logger=INFO,console 命令完整版bin/flume-ng agent -c ./conf -f ./conf/spool-hdfs.conf -n a1 -Dflume.root.logger=INFO,console 命令精简版--conf指定配置文件的目录--conf-file指定采集方案路径--name agent进程名字 要跟采集方案中保持一致 四 测试环境:在conf目录下配置:vi netcat-logger.conf 123456789101112131415161718192021# 定义这个agent中各组件的名字a1.sources = r1a1.sinks = k1a1.channels = c1# 描述和配置source组件：r1a1.sources.r1.type = netcata1.sources.r1.bind = localhosta1.sources.r1.port = 44444# 描述和配置sink组件：k1a1.sinks.k1.type = logger# 描述和配置channel组件，此处使用是内存缓存的方式a1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100# 描述和配置source channel sink之间的连接关系a1.sources.r1.channels = c1a1.sinks.k1.channel = c1 启动agent去采集数据: 1bin/flume-ng agent --conf conf --conf-file conf/netcat-logger.conf --name a1 -Dflume.root.logger=INFO,console 测试: 先要往agent采集监听的端口上发送数据，让agent有数据可采。 随便在一个能跟agent节点联网的机器上： telnetanget-hostname port （telnet localhost 44444） 五 采集目录到hdfs:需求:服务器的某特定目录下，会不断产生新的文件，每当有新文件出现，就需要把文件采集到HDFS中去 l 采集源，即source——监控文件目录 : spooldir l 下沉目标，即sink——HDFS文件系统 : hdfs sink l source和sink之间的传递通道——channel，可用file channel 也可以用内存channel 编写配置文件:vim spooldir-hdfs.conf 12345678910111213141516171819202122232425262728293031323334# Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1# Describe/configure the source##注意：不能往监控目中重复丢同名文件a1.sources.r1.type = spooldira1.sources.r1.spoolDir = /root/logsa1.sources.r1.fileHeader = true# Describe the sink 目的地a1.sinks.k1.type = hdfsa1.sinks.k1.hdfs.path = /flume/events/%y-%m-%d/%H%M/a1.sinks.k1.hdfs.filePrefix = events-a1.sinks.k1.hdfs.round = truea1.sinks.k1.hdfs.roundValue = 10a1.sinks.k1.hdfs.roundUnit = minutea1.sinks.k1.hdfs.rollInterval = 3a1.sinks.k1.hdfs.rollSize = 20a1.sinks.k1.hdfs.rollCount = 5a1.sinks.k1.hdfs.batchSize = 1a1.sinks.k1.hdfs.useLocalTimeStamp = true#生成的文件类型，默认是Sequencefile，可用DataStream，则为普通文本a1.sinks.k1.hdfs.fileType = DataStream# Use a channel which buffers events in memory a1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1 参数解释(sink): 1234567891011 roll控制写入hdfs文件 以何种方式进行滚动 a1.sinks.k1.hdfs.rollInterval = 3 以时间间隔 a1.sinks.k1.hdfs.rollSize = 20 以文件大小 a1.sinks.k1.hdfs.rollCount = 5 event的个数 以event个数如果三个都配置 谁先满足谁触发滚动如果不想以某种属性滚动 设置为0即可 是否开启时间上的舍弃 控制文件夹以多少时间间隔滚动 以下述为例：就会每10分钟生成一个文件夹 a1.sinks.k1.hdfs.round = true a1.sinks.k1.hdfs.roundValue = 10 a1.sinks.k1.hdfs.roundUnit = minute Channel参数解释： capacity：默认该通道中最大的可以存储的event数量 trasactionCapacity：每次最大可以从source中拿到或者送到sink中的event数量 注意: 注意其监控的文件夹下面不能有同名文件的产生 如果有 报错且罢工 后续就不再进行数据的监视采集了 在企业中通常给文件追加时间戳命名的方式保证文件不会重名 启动同上: 测试 :往源目录里存入文件 六 采集文件到HDFS需求:比如业务系统使用log4j生成的日志，日志内容不断增加，需要把追加到日志文件中的数据实时采集到hdfs l 采集源，即source——监控文件内容更新 : exec ‘tail-F file’ l 下沉目标，即sink——HDFS文件系统 : hdfssink l Source和sink之间的传递通道——channel，可用file channel 也可以用 内存channel 编写配置文件: 123456789101112131415161718192021222324252627282930313233# Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1# Describe/configure the sourcea1.sources.r1.type = execa1.sources.r1.command = tail -F /root/logs/test.loga1.sources.r1.channels = c1# Describe the sinka1.sinks.k1.type = hdfsa1.sinks.k1.hdfs.path = /flume/tailout/%y-%m-%d/%H%M/a1.sinks.k1.hdfs.filePrefix = events-a1.sinks.k1.hdfs.round = truea1.sinks.k1.hdfs.roundValue = 10a1.sinks.k1.hdfs.roundUnit = minutea1.sinks.k1.hdfs.rollInterval = 3a1.sinks.k1.hdfs.rollSize = 20a1.sinks.k1.hdfs.rollCount = 5a1.sinks.k1.hdfs.batchSize = 1a1.sinks.k1.hdfs.useLocalTimeStamp = true#生成的文件类型，默认是Sequencefile，可用DataStream，则为普通文本a1.sinks.k1.hdfs.fileType = DataStream# Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1 参数解析: · rollInterval 默认值：30 hdfssink间隔多长将临时文件滚动成最终目标文件，单位：秒； 如果设置成0，则表示不根据时间来滚动文件； 注：滚动（roll）指的是，hdfs sink将临时文件重命名成最终目标文件，并新打开一个临时文件来写入数据； rollsize 默认值：1024 当临时文件达到该大小（单位：bytes）时，滚动成目标文件； 如果设置成0，则表示不根据临时文件大小来滚动文件； rootcount 默认值：10 当events数据达到该数量时候，将临时文件滚动成目标文件； 如果设置成0，则表示不根据events数据来滚动文件； round 默认值：false 是否启用时间上的“舍弃”，这里的“舍弃”，类似于“四舍五入”。 roundvalue 默认值：1 时间上进行“舍弃”的值； roundunit 默认值：seconds 时间上进行“舍弃”的单位，包含：second,minute,hour 示例： a1.sinks.k1.hdfs.path= /flume/events/%y-%m-%d/%H%M/%S a1.sinks.k1.hdfs.round= true a1.sinks.k1.hdfs.roundValue= 10 a1.sinks.k1.hdfs.roundUnit= minute 当时间为2015-10-16 17:38:59时候，hdfs.path依然会被解析为： /flume/events/20151016/17:30/00 因为设置的是舍弃10分钟内的时间，因此，该目录每10分钟新生成一个。 启动同上: 测试:exec source 可以执行指定的linux command 把命令的结果作为数据进行收集 12while true; do date &gt;&gt; /root/logs/test.log;done使用该脚本模拟数据实时变化的过程 七 Flume的load-balance、failover1 负载均衡 所谓的负载均衡 用于解决一个进程或者程序处理不了所有请求 多个进程一起处理的场景 同一个请求只能交给一个进行处理 避免数据重复 如何分配请求就涉及到了负载均衡的算法：轮询（round_robin） 随机（random） 权重 例如三节点配置: 主节点:vim exec-avro.conf 1234567891011121314151617181920212223242526272829303132333435363738394041424344#agent1 nameagent1.channels = c1agent1.sources = r1agent1.sinks = k1 k2#set gruopagent1.sinkgroups = g1#set channelagent1.channels.c1.type = memoryagent1.channels.c1.capacity = 1000agent1.channels.c1.transactionCapacity = 100# set sourcesagent1.sources.r1.channels = c1agent1.sources.r1.type = execagent1.sources.r1.command = tail -F /root/logs/123.log# set sink1agent1.sinks.k1.channel = c1agent1.sinks.k1.type = avroagent1.sinks.k1.hostname = node02agent1.sinks.k1.port = 52020# set sink2agent1.sinks.k2.channel = c1agent1.sinks.k2.type = avroagent1.sinks.k2.hostname = node03agent1.sinks.k2.port = 52020#set sink groupagent1.sinkgroups.g1.sinks = k1 k2#set failoveragent1.sinkgroups.g1.processor.type = load_balanceagent1.sinkgroups.g1.processor.backoff = true #如果开启，则将失败的sink放入黑名单# round_robin 轮询 还支持random 随机agent1.sinkgroups.g1.processor.selector = round_robin agent1.sinkgroups.g1.processor.selector.maxTimeOut=10000 #在黑名单放置的超时时间，超时结束时，若仍然无法接收，则超时时间呈指数增长#启动bin/flume-ng agent -c conf -f conf/exec-avro.conf -n agent1 -Dflume.root.logger=INFO,console 第二节点:vim avro-logger.conf 123456789101112131415161718192021222324# Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1# Describe/configure the sourcea1.sources.r1.type = avroa1.sources.r1.channels = c1a1.sources.r1.bind = node02a1.sources.r1.port = 52020# Describe the sinka1.sinks.k1.type = logger# Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1#启动bin/flume-ng agent -c conf -f conf/avro-logger.conf -n a1 -Dflume.root.logger=INFO,console 第三节点:vim avro-logger.conf 123456789101112131415161718192021222324252627# Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1# Describe/configure the sourcea1.sources.r1.type = avroa1.sources.r1.channels = c1a1.sources.r1.bind = node03a1.sources.r1.port = 52020# Describe the sinka1.sinks.k1.type = logger# Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1#启动bin/flume-ng agent -c conf -f conf/avro-logger.conf -n a1 -Dflume.root.logger=INFO,console flume串联跨网络传输数据 avro sink avro source 使用上述两个组件指定绑定的端口ip 就可以满足数据跨网络的传递 通常用于flume串联架构中 flume串联启动 通常从远离数据源的那一级开始启动 2 flume failover 容错又称之为故障转移 容忍错误的发生。 通常用于解决单点故障 给容易出故障的地方设置备份 备份越多 容错能力越强 但是资源的浪费越严重 具体流程类似loadbalance，但是内部处理机制与load balance完全不同。 ​ Failover Sink Processor维护一个优先级Sink组件列表，只要有一个Sink组件可用，Event就被传递到下一个组件。故障转移机制的作用是将失败的Sink降级到一个池，在这些池中它们被分配一个冷却时间，随着故障的连续，在重试之前冷却时间增加。一旦Sink成功发送一个事件，它将恢复到活动池。 Sink具有与之相关的优先级，数量越大，优先级越高。 ​ 例如，具有优先级为100的sink在优先级为80的Sink之前被激活。如果在发送事件时汇聚失败，则接下来将尝试下一个具有最高优先级的Sink发送事件。如果没有指定优先级，则根据在配置中指定Sink的顺序来确定优先级。 主节点: vim exec-avro.conf 1234567891011121314151617181920212223242526272829303132333435363738394041#agent1 nameagent1.channels = c1agent1.sources = r1agent1.sinks = k1 k2#set gruopagent1.sinkgroups = g1#set channelagent1.channels.c1.type = memoryagent1.channels.c1.capacity = 1000agent1.channels.c1.transactionCapacity = 100# set sourcesagent1.sources.r1.channels = c1agent1.sources.r1.type = execagent1.sources.r1.command = tail -F /root/logs/456.log# set sink1agent1.sinks.k1.channel = c1agent1.sinks.k1.type = avroagent1.sinks.k1.hostname = node02agent1.sinks.k1.port = 52020# set sink2agent1.sinks.k2.channel = c1agent1.sinks.k2.type = avroagent1.sinks.k2.hostname = node03agent1.sinks.k2.port = 52020#set sink groupagent1.sinkgroups.g1.sinks = k1 k2#set failoveragent1.sinkgroups.g1.processor.type = failoveragent1.sinkgroups.g1.processor.priority.k1 = 10 #优先级值, 绝对值越大表示优先级越高agent1.sinkgroups.g1.processor.priority.k2 = 1agent1.sinkgroups.g1.processor.maxpenalty = 10000 #失败的Sink的最大回退期（millis）#启动bin/flume-ng agent -c conf -f conf/exec-avro.conf -n agent1 -Dflume.root.logger=INFO,console 第二节点与第三节点配置 同load-balance的 八 flume拦截器1 静态拦截器 日志的采集与汇总需求: A、B两台日志服务机器实时生产日志主要类型为access.log、nginx.log、web.log 把A、B 机器中的access.log、nginx.log、web.log 采集汇总到C机器上然后统一收集到hdfs中 数据流程处理分析: 在第一台节点上配置 vim exec_source_avro_sink.conf 123456789101112131415161718192021222324252627282930313233343536373839404142# Name the components on this agent a1.sources = r1 r2 r3a1.sinks = k1a1.channels = c1# Describe/configure the sourcea1.sources.r1.type = execa1.sources.r1.command = tail -F /root/logs/access.loga1.sources.r1.interceptors = i1a1.sources.r1.interceptors.i1.type = statica1.sources.r1.interceptors.i1.key = typea1.sources.r1.interceptors.i1.value = accessa1.sources.r2.type = execa1.sources.r2.command = tail -F /root/logs/nginx.loga1.sources.r2.interceptors = i2a1.sources.r2.interceptors.i2.type = statica1.sources.r2.interceptors.i2.key = typea1.sources.r2.interceptors.i2.value = nginxa1.sources.r3.type = execa1.sources.r3.command = tail -F /root/logs/web.loga1.sources.r3.interceptors = i3a1.sources.r3.interceptors.i3.type = statica1.sources.r3.interceptors.i3.key = typea1.sources.r3.interceptors.i3.value = web# Describe the sinka1.sinks.k1.type = avroa1.sinks.k1.hostname = node02a1.sinks.k1.port = 41414# Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity = 2000a1.channels.c1.transactionCapacity = 100# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sources.r2.channels = c1a1.sources.r3.channels = c1a1.sinks.k1.channel = c1 在第三台上配置vim avro_source_hdfs_sink.conf 123456789101112131415161718192021222324252627282930313233343536373839404142434445#定义agent名， source、channel、sink的名称a1.sources = r1a1.sinks = k1a1.channels = c1#定义sourcea1.sources.r1.type = avroa1.sources.r1.bind = node02a1.sources.r1.port =41414#添加时间拦截器a1.sources.r1.interceptors = i1a1.sources.r1.interceptors.i1.type = org.apache.flume.interceptor.TimestampInterceptor$Builder#定义channelsa1.channels.c1.type = memorya1.channels.c1.capacity = 20000a1.channels.c1.transactionCapacity = 10000#定义sinka1.sinks.k1.type = hdfsa1.sinks.k1.hdfs.path=hdfs://node01:9000/source/logs/%&#123;type&#125;/%Y%m%da1.sinks.k1.hdfs.filePrefix =eventsa1.sinks.k1.hdfs.fileType = DataStreama1.sinks.k1.hdfs.writeFormat = Text#时间类型#a1.sinks.k1.hdfs.useLocalTimeStamp = true#生成的文件不按条数生成a1.sinks.k1.hdfs.rollCount = 0#生成的文件不按时间生成a1.sinks.k1.hdfs.rollInterval = 20#生成的文件按大小生成a1.sinks.k1.hdfs.rollSize = 10485760#批量写入hdfs的个数a1.sinks.k1.hdfs.batchSize = 20flume操作hdfs的线程数（包括新建，写入等）a1.sinks.k1.hdfs.threadsPoolSize=10#操作hdfs超时时间a1.sinks.k1.hdfs.callTimeout=30000#组装source、channel、sinka1.sources.r1.channels = c1a1.sinks.k1.channel = c1 使用静态拦截器前后对比: 1234567如果没有使用静态拦截器Event: &#123; headers:&#123;&#125; body: 36 Sun Jun 2 18:26 &#125;使用静态拦截器之后 自己添加kv标识对Event: &#123; headers:&#123;type=access&#125; body: 36 Sun Jun 2 18:26 &#125;Event: &#123; headers:&#123;type=nginx&#125; body: 36 Sun Jun 2 18:26 &#125;Event: &#123; headers:&#123;type=web&#125; body: 36 Sun Jun 2 18:26 &#125; 后续在存放数据的时候可以使用flume的规则语法获取到拦截器添加的kv内容 1%&#123;type&#125; 模拟数据实时产生: 123while true; do echo &quot;access access.....&quot; &gt;&gt; /root/logs/access.log;sleep 0.5;donewhile true; do echo &quot;web web.....&quot; &gt;&gt; /root/logs/web.log;sleep 0.5;donewhile true; do echo &quot;nginx nginx.....&quot; &gt;&gt; /root/logs/nginx.log;sleep 0.5;done 配置完成后: 配置完成之后，在服务器A上的/root/data有数据文件access.log、nginx.log、web.log。 先启动服务器C上的flume，启动命令在flume安装目录下执行 ： bin/flume-ng agent -c conf -f conf/avro_source_hdfs_sink.conf -name a1 -Dflume.root.logger=DEBUG,console 然后在启动服务器上的A，启动命令 在flume安装目录下执行 ： bin/flume-ng agent -c conf -f conf/exec_source_avro_sink.conf -name a1 -Dflume.root.logger=DEBUG,console 2 自定义拦截器拦截器简介: ​ Flume有各种自带的拦截器，比如：TimestampInterceptor、HostInterceptor、RegexExtractorInterceptor等，通过使用不同的拦截器，实现不同的功能。但是以上的这些拦截器，不能改变原有日志数据的内容或者对日志信息添加一定的处理逻辑，当一条日志信息有几十个甚至上百个字段的时候，在传统的Flume处理下，收集到的日志还是会有对应这么多的字段，也不能对你想要的字段进行对应的处理。 自定义拦截器: 根据实际业务的需求，为了更好的满足数据在应用层的处理，通过自定义Flume拦截器，过滤掉不需要的字段，并对指定字段加密处理，将源数据进行预处理。减少了数据的传输量，降低了存储的开销. 分为两部分; 1 编写java代码,自定义拦截器 pom.xml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;cn.baidu.cloud&lt;/groupId&gt; &lt;artifactId&gt;example-flume-intercepter&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flume&lt;/groupId&gt; &lt;artifactId&gt;flume-ng-sdk&lt;/artifactId&gt; &lt;version&gt;1.8.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flume&lt;/groupId&gt; &lt;artifactId&gt;flume-ng-core&lt;/artifactId&gt; &lt;version&gt;1.8.0&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.0&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;!-- &lt;verbal&gt;true&lt;/verbal&gt;--&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt; &lt;version&gt;3.1.1&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;shade&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;filters&gt; &lt;filter&gt; &lt;artifact&gt;*:*&lt;/artifact&gt; &lt;excludes&gt; &lt;exclude&gt;META-INF/*.SF&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.DSA&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.RSA&lt;/exclude&gt; &lt;/excludes&gt; &lt;/filter&gt; &lt;/filters&gt; &lt;transformers&gt; &lt;transformer implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer"&gt; &lt;mainClass&gt;&lt;/mainClass&gt; &lt;/transformer&gt; &lt;/transformers&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; java代码: 定义一个类CustomParameterInterceptor实现Interceptor接口。 在CustomParameterInterceptor类中定义变量，这些变量是需要到Flume的配置文件中进行配置使用的。每一行字段间的分隔符(fields_separator)、通过分隔符分隔后，所需要列字段的下标（indexs）、多个下标使用的分隔符（indexs_separator)、多个下标使用的分隔符（indexs_separator)。 添加CustomParameterInterceptor的有参构造方法。并对相应的变量进行处理 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import com.google.common.base.Charsets;import org.apache.flume.Context;import org.apache.flume.Event;import org.apache.flume.interceptor.Interceptor;import java.security.MessageDigest;import java.security.NoSuchAlgorithmException;import java.util.ArrayList;import java.util.List;import java.util.regex.Matcher;import java.util.regex.Pattern;import static cn.baidu.interceptor.CustomParameterInterceptor.Constants.*;/** * Created by itcast */public class CustomParameterInterceptor implements Interceptor&#123; /** The field_separator.指明每一行字段的分隔符 */ private final String fields_separator; /** The indexs.通过分隔符分割后，指明需要那列的字段 下标*/ private final String indexs; /** The indexs_separator. 多个下标的分隔符*/ private final String indexs_separator; /** The encrypted_field_index. 需要加密的字段下标*/ private final String encrypted_field_index; /** * */ public CustomParameterInterceptor( String fields_separator, String indexs, String indexs_separator,String encrypted_field_index) &#123; String f = fields_separator.trim(); String i = indexs_separator.trim(); this.indexs = indexs; this.encrypted_field_index=encrypted_field_index.trim(); if (!f.equals("")) &#123; f = UnicodeToString(f); &#125; this.fields_separator =f; if (!i.equals("")) &#123; i = UnicodeToString(i); &#125; this.indexs_separator = i; &#125; \t 制表符 的UNicode编码格式为(‘\u0009’) 将配置文件中传过来的unicode编码进行转换为字符串。 写具体的要处理的逻辑intercept()方法，一个是单个处理的，一个是批量处理。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172public static String UnicodeToString(String str) &#123; Pattern pattern = Pattern.compile("(\\\\u(\\p&#123;XDigit&#125;&#123;4&#125;))"); Matcher matcher = pattern.matcher(str); char ch; while (matcher.find()) &#123; ch = (char) Integer.parseInt(matcher.group(2), 16); str = str.replace(matcher.group(1), ch + ""); &#125; return str; &#125; /* * @see org.apache.flume.interceptor.Interceptor#intercept(org.apache.flume.Event) */ public Event intercept(Event event) &#123; if (event == null) &#123; return null; &#125; try &#123; String line = new String(event.getBody(), Charsets.UTF_8); String[] fields_spilts = line.split(fields_separator); String[] indexs_split = indexs.split(indexs_separator); String newLine=""; for (int i = 0; i &lt; indexs_split.length; i++) &#123; int parseInt = Integer.parseInt(indexs_split[i]); //对加密字段进行加密 if(!"".equals(encrypted_field_index)&amp;&amp;encrypted_field_index.equals(indexs_split[i]))&#123; newLine+=StringUtils.GetMD5Code(fields_spilts[parseInt]); &#125;else&#123; newLine+=fields_spilts[parseInt]; &#125; if(i!=indexs_split.length-1)&#123; newLine+=fields_separator; &#125; &#125; event.setBody(newLine.getBytes(Charsets.UTF_8)); return event; &#125; catch (Exception e) &#123; return event; &#125; &#125; /* * @see org.apache.flume.interceptor.Interceptor#intercept(java.util.List) */ public List&lt;Event&gt; intercept(List&lt;Event&gt; events) &#123; List&lt;Event&gt; out = new ArrayList&lt;Event&gt;(); for (Event event : events) &#123; Event outEvent = intercept(event); if (outEvent != null) &#123; out.add(outEvent); &#125; &#125; return out; &#125; /* * @see org.apache.flume.interceptor.Interceptor#initialize() */ public void initialize() &#123; // TODO Auto-generated method stub &#125; /* * @see org.apache.flume.interceptor.Interceptor#close() */ public void close() &#123; // TODO Auto-generated method stub &#125; 接口中定义了一个内部接口Builder，在configure方法中，进行一些参数配置。并给出，在flume的conf中没配置一些参数时，给出其默认值。通过其builder方法，返回一个CustomParameterInterceptor对象。 123456789101112131415161718192021222324252627282930313233public static class Builder implements Interceptor.Builder &#123; /** The fields_separator.指明每一行字段的分隔符 */ private String fields_separator; /** The indexs.通过分隔符分割后，指明需要那列的字段 下标*/ private String indexs; /** The indexs_separator. 多个下标下标的分隔符*/ private String indexs_separator; /** The encrypted_field. 需要加密的字段下标*/ private String encrypted_field_index; /* * @see org.apache.flume.conf.Configurable#configure(org.apache.flume.Context) */ public void configure(Context context) &#123; fields_separator = context.getString(FIELD_SEPARATOR, DEFAULT_FIELD_SEPARATOR); indexs = context.getString(INDEXS, DEFAULT_INDEXS); indexs_separator = context.getString(INDEXS_SEPARATOR, DEFAULT_INDEXS_SEPARATOR); encrypted_field_index= context.getString(ENCRYPTED_FIELD_INDEX, DEFAULT_ENCRYPTED_FIELD_INDEX); &#125; /* * @see org.apache.flume.interceptor.Interceptor.Builder#build() */ public Interceptor build() &#123; return new CustomParameterInterceptor(fields_separator, indexs, indexs_separator,encrypted_field_index); &#125; &#125; 1234567891011121314151617181920212223242526272829303132333435/** * The Class Constants. * */public static class Constants &#123; /** The Constant FIELD_SEPARATOR. */ public static final String FIELD_SEPARATOR = "fields_separator"; /** The Constant DEFAULT_FIELD_SEPARATOR. */ public static final String DEFAULT_FIELD_SEPARATOR =" "; /** The Constant INDEXS. */ public static final String INDEXS = "indexs"; /** The Constant DEFAULT_INDEXS. */ public static final String DEFAULT_INDEXS = "0"; /** The Constant INDEXS_SEPARATOR. */ public static final String INDEXS_SEPARATOR = "indexs_separator"; /** The Constant DEFAULT_INDEXS_SEPARATOR. */ public static final String DEFAULT_INDEXS_SEPARATOR = ","; /** The Constant ENCRYPTED_FIELD_INDEX. */ public static final String ENCRYPTED_FIELD_INDEX = "encrypted_field_index"; /** The Constant DEFAUL_TENCRYPTED_FIELD_INDEX. */ public static final String DEFAULT_ENCRYPTED_FIELD_INDEX = ""; /** The Constant PROCESSTIME. */ public static final String PROCESSTIME = "processTime"; /** The Constant PROCESSTIME. */ public static final String DEFAULT_PROCESSTIME = "a";&#125; 定义一个静态类，类中封装MD5加密方法 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253/** * 字符串md5加密 */ public static class StringUtils &#123; // 全局数组 private final static String[] strDigits = &#123; "0", "1", "2", "3", "4", "5", "6", "7", "8", "9", "a", "b", "c", "d", "e", "f" &#125;; // 返回形式为数字跟字符串 private static String byteToArrayString(byte bByte) &#123; int iRet = bByte; // System.out.println("iRet="+iRet); if (iRet &lt; 0) &#123; iRet += 256; &#125; int iD1 = iRet / 16; int iD2 = iRet % 16; return strDigits[iD1] + strDigits[iD2]; &#125; // 返回形式只为数字 private static String byteToNum(byte bByte) &#123; int iRet = bByte; System.out.println("iRet1=" + iRet); if (iRet &lt; 0) &#123; iRet += 256; &#125; return String.valueOf(iRet); &#125; // 转换字节数组为16进制字串 private static String byteToString(byte[] bByte) &#123; StringBuffer sBuffer = new StringBuffer(); for (int i = 0; i &lt; bByte.length; i++) &#123; sBuffer.append(byteToArrayString(bByte[i])); &#125; return sBuffer.toString(); &#125; public static String GetMD5Code(String strObj) &#123; String resultString = null; try &#123; resultString = new String(strObj); MessageDigest md = MessageDigest.getInstance("MD5"); // md.digest() 该函数返回值为存放哈希值结果的byte数组 resultString = byteToString(md.digest(strObj.getBytes())); &#125; catch (NoSuchAlgorithmException ex) &#123; ex.printStackTrace(); &#125; return resultString; &#125; &#125; &#125; 通过以上步骤，自定义拦截器的代码开发已完成，然后打包成jar，放到Flume的根目录下的lib中 新增配置文件: vim spool-interceptor-hdfs.conf 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849a1.channels = c1a1.sources = r1a1.sinks = s1#channela1.channels.c1.type = memorya1.channels.c1.capacity=1000a1.channels.c1.transactionCapacity=200#sourcea1.sources.r1.channels = c1a1.sources.r1.type = spooldira1.sources.r1.spoolDir = /root/logs4/a1.sources.r1.batchSize= 50a1.sources.r1.inputCharset = UTF-8a1.sources.r1.interceptors =i1 i2a1.sources.r1.interceptors.i1.type =cn.baidu.interceptor.CustomParameterInterceptor$Buildera1.sources.r1.interceptors.i1.fields_separator=\\u0009a1.sources.r1.interceptors.i1.indexs =0,1,3,5,6a1.sources.r1.interceptors.i1.indexs_separator =\\u002ca1.sources.r1.interceptors.i1.encrypted_field_index =0a1.sources.r1.interceptors.i2.type = org.apache.flume.interceptor.TimestampInterceptor$Builder#sinka1.sinks.s1.channel = c1a1.sinks.s1.type = hdfsa1.sinks.s1.hdfs.path =hdfs://node01:9000/intercept/%Y%m%da1.sinks.s1.hdfs.filePrefix = itcasra1.sinks.s1.hdfs.fileSuffix = .data1.sinks.s1.hdfs.rollSize = 10485760a1.sinks.s1.hdfs.rollInterval =20a1.sinks.s1.hdfs.rollCount = 0a1.sinks.s1.hdfs.batchSize = 2a1.sinks.s1.hdfs.round = truea1.sinks.s1.hdfs.roundUnit = minutea1.sinks.s1.hdfs.threadsPoolSize = 25a1.sinks.s1.hdfs.useLocalTimeStamp = truea1.sinks.s1.hdfs.minBlockReplicas = 1a1.sinks.s1.hdfs.fileType =DataStreama1.sinks.s1.hdfs.writeFormat = Texta1.sinks.s1.hdfs.callTimeout = 60000a1.sinks.s1.hdfs.idleTimeout =60//启动bin/flume-ng agent -c conf -f conf/spool-interceptor-hdfs.conf -n a1 -Dflume.root.logger=INFO,console #九 自定义组件 1flume自定义source说明:​ Source是负责接收数据到Flume Agent的组件。Source组件可以处理各种类型、各种格式的日志数据，包括avro、thrift、exec、jms、spooling directory、netcat、sequence generator、syslog、http、legacy。官方提供的source类型已经很多，但是有时候并不能满足实际开发当中的需求，此时我们就需要根据实际需求自定义某些source。 如：实时监控MySQL，从MySQL中获取数据传输到HDFS或者其他存储框架，所以此时需要我们自己实现MySQLSource。 官方也提供了自定义source的接口： 官网说明：https://flume.apache.org/FlumeDeveloperGuide.html#source 实现原理:根据官方说明自定义mysqlsource需要继承AbstractSource类并实现Configurable和PollableSource接口。 实现相应方法： getBackOffSleepIncrement() //此处暂不用 getMaxBackOffSleepInterval() //此处暂不用 configure(Context context) //初始化context process() //获取数据（从mysql获取数据，业务处理比较复杂，所以我们定义一个专门的类——QueryMysql来处理跟mysql的交互），封装成event并写入channel，这个方法被循环调用 stop() //关闭相关的资源 ###实现: 创建数据库及表 1234567891011121314151617181920212223242526272829303132CREATE DATABASE `mysqlsource`;USE `mysqlsource`;/*Table structure for table `flume_meta` */DROP TABLEIF EXISTS `flume_meta`;CREATE TABLE `flume_meta` ( `source_tab` VARCHAR (255) NOT NULL, `currentIndex` VARCHAR (255) NOT NULL, PRIMARY KEY (`source_tab`)) ENGINE = INNODB DEFAULT CHARSET = utf8;/*Table structure for table `student` */DROP TABLEIF EXISTS `student`;CREATE TABLE `student` ( `id` INT (11) NOT NULL AUTO_INCREMENT, `name` VARCHAR (255) NOT NULL, PRIMARY KEY (`id`)) ENGINE = INNODB AUTO_INCREMENT = 5 DEFAULT CHARSET = utf8;/*Data for the table `student` */INSERT INTO `student` (`id`, `name`)VALUES (1, 'zhangsan'), (2, 'lisi'), (3, 'wangwu'), (4, 'zhaoliu'); pom.xml 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;cn.baidu.cloud&lt;/groupId&gt; &lt;artifactId&gt;example-flume&lt;/artifactId&gt; &lt;version&gt;1.1&lt;/version&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flume&lt;/groupId&gt; &lt;artifactId&gt;flume-ng-core&lt;/artifactId&gt; &lt;version&gt;1.8.0&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.38&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-lang3&lt;/artifactId&gt; &lt;version&gt;3.6&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;1.16.22&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.0&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;!-- &lt;verbal&gt;true&lt;/verbal&gt;--&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt; &lt;version&gt;3.1.1&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;shade&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;filters&gt; &lt;filter&gt; &lt;artifact&gt;*:*&lt;/artifact&gt; &lt;excludes&gt; &lt;exclude&gt;META-INF/*.SF&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.DSA&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.RSA&lt;/exclude&gt; &lt;/excludes&gt; &lt;/filter&gt; &lt;/filters&gt; &lt;transformers&gt; &lt;transformer implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer"&gt; &lt;mainClass&gt;&lt;/mainClass&gt; &lt;/transformer&gt; &lt;/transformers&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 自定义QueryMySql工具类 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249package cn.baidu.flumesource;import org.apache.flume.Context;import org.apache.flume.conf.ConfigurationException;import org.apache.http.ParseException;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.sql.*;import java.util.ArrayList;import java.util.List;import java.util.Properties;@Data@Getter@Setterpublic class QueryMySql &#123; private static final Logger LOG = LoggerFactory.getLogger(QueryMySql.class); private int runQueryDelay, //两次查询的时间间隔 startFrom, //开始id currentIndex, //当前id recordSixe = 0, //每次查询返回结果的条数 maxRow; //每次查询的最大条数 private String table, //要操作的表 columnsToSelect, //用户传入的查询的列 customQuery, //用户传入的查询语句 query, //构建的查询语句 defaultCharsetResultSet;//编码集 //上下文，用来获取配置文件 private Context context; //为定义的变量赋值（默认值），可在flume任务的配置文件中修改 private static final int DEFAULT_QUERY_DELAY = 10000; private static final int DEFAULT_START_VALUE = 0; private static final int DEFAULT_MAX_ROWS = 2000; private static final String DEFAULT_COLUMNS_SELECT = "*"; private static final String DEFAULT_CHARSET_RESULTSET = "UTF-8"; private static Connection conn = null; private static PreparedStatement ps = null; private static String connectionURL, connectionUserName, connectionPassword; //加载静态资源 static &#123; Properties p = new Properties(); try &#123; p.load(QueryMySql.class.getClassLoader().getResourceAsStream("jdbc.properties")); connectionURL = p.getProperty("dbUrl"); connectionUserName = p.getProperty("dbUser"); connectionPassword = p.getProperty("dbPassword"); Class.forName(p.getProperty("dbDriver")); &#125; catch (Exception e) &#123; LOG.error(e.toString()); &#125; &#125; //获取JDBC连接 private static Connection InitConnection(String url, String user, String pw) &#123; try &#123; Connection conn = DriverManager.getConnection(url, user, pw); if (conn == null) throw new SQLException(); return conn; &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; return null; &#125; //构造方法 QueryMySql(Context context) throws ParseException &#123; //初始化上下文 this.context = context; //有默认值参数：获取flume任务配置文件中的参数，读不到的采用默认值 this.columnsToSelect = context.getString("columns.to.select", DEFAULT_COLUMNS_SELECT); this.runQueryDelay = context.getInteger("run.query.delay", DEFAULT_QUERY_DELAY); this.startFrom = context.getInteger("start.from", DEFAULT_START_VALUE); this.defaultCharsetResultSet = context.getString("default.charset.resultset", DEFAULT_CHARSET_RESULTSET); //无默认值参数：获取flume任务配置文件中的参数 this.table = context.getString("table"); this.customQuery = context.getString("custom.query"); connectionURL = context.getString("connection.url"); connectionUserName = context.getString("connection.user"); connectionPassword = context.getString("connection.password"); conn = InitConnection(connectionURL, connectionUserName, connectionPassword); //校验相应的配置信息，如果没有默认值的参数也没赋值，抛出异常 checkMandatoryProperties(); //获取当前的id currentIndex = getStatusDBIndex(startFrom); //构建查询语句 query = buildQuery(); &#125; //校验相应的配置信息（表，查询语句以及数据库连接的参数） private void checkMandatoryProperties() &#123; if (table == null) &#123; throw new ConfigurationException("property table not set"); &#125; if (connectionURL == null) &#123; throw new ConfigurationException("connection.url property not set"); &#125; if (connectionUserName == null) &#123; throw new ConfigurationException("connection.user property not set"); &#125; if (connectionPassword == null) &#123; throw new ConfigurationException("connection.password property not set"); &#125; &#125; //构建sql语句 private String buildQuery() &#123; String sql = ""; //获取当前id currentIndex = getStatusDBIndex(startFrom); LOG.info(currentIndex + ""); if (customQuery == null) &#123; sql = "SELECT " + columnsToSelect + " FROM " + table; &#125; else &#123; sql = customQuery; &#125; StringBuilder execSql = new StringBuilder(sql); //以id作为offset if (!sql.contains("where")) &#123; execSql.append(" where "); execSql.append("id").append("&gt;").append(currentIndex); return execSql.toString(); &#125; else &#123; int length = execSql.toString().length(); return execSql.toString().substring(0, length - String.valueOf(currentIndex).length()) + currentIndex; &#125; &#125; //执行查询 List&lt;List&lt;Object&gt;&gt; executeQuery() &#123; try &#123; //每次执行查询时都要重新生成sql，因为id不同 customQuery = buildQuery(); //存放结果的集合 List&lt;List&lt;Object&gt;&gt; results = new ArrayList&lt;&gt;(); if (ps == null) &#123; // ps = conn.prepareStatement(customQuery); &#125; ResultSet result = ps.executeQuery(customQuery); while (result.next()) &#123; //存放一条数据的集合（多个列） List&lt;Object&gt; row = new ArrayList&lt;&gt;(); //将返回结果放入集合 for (int i = 1; i &lt;= result.getMetaData().getColumnCount(); i++) &#123; row.add(result.getObject(i)); &#125; results.add(row); &#125; LOG.info("execSql:" + customQuery + "\nresultSize:" + results.size()); return results; &#125; catch (SQLException e) &#123; LOG.error(e.toString()); // 重新连接 conn = InitConnection(connectionURL, connectionUserName, connectionPassword); &#125; return null; &#125; //将结果集转化为字符串，每一条数据是一个list集合，将每一个小的list集合转化为字符串 List&lt;String&gt; getAllRows(List&lt;List&lt;Object&gt;&gt; queryResult) &#123; List&lt;String&gt; allRows = new ArrayList&lt;&gt;(); if (queryResult == null || queryResult.isEmpty()) return allRows; StringBuilder row = new StringBuilder(); for (List&lt;Object&gt; rawRow : queryResult) &#123; Object value = null; for (Object aRawRow : rawRow) &#123; value = aRawRow; if (value == null) &#123; row.append(","); &#125; else &#123; row.append(aRawRow.toString()).append(","); &#125; &#125; allRows.add(row.toString()); row = new StringBuilder(); &#125; return allRows; &#125; //更新offset元数据状态，每次返回结果集后调用。必须记录每次查询的offset值，为程序中断续跑数据时使用，以id为offset void updateOffset2DB(int size) &#123; //以source_tab做为KEY，如果不存在则插入，存在则更新（每个源表对应一条记录） String sql = "insert into flume_meta(source_tab,currentIndex) VALUES('" + this.table + "','" + (recordSixe += size) + "') on DUPLICATE key update source_tab=values(source_tab),currentIndex=values(currentIndex)"; LOG.info("updateStatus Sql:" + sql); execSql(sql); &#125; //执行sql语句 private void execSql(String sql) &#123; try &#123; ps = conn.prepareStatement(sql); LOG.info("exec::" + sql); ps.execute(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125; //获取当前id的offset private Integer getStatusDBIndex(int startFrom) &#123; //从flume_meta表中查询出当前的id是多少 String dbIndex = queryOne("select currentIndex from flume_meta where source_tab='" + table + "'"); if (dbIndex != null) &#123; return Integer.parseInt(dbIndex); &#125; //如果没有数据，则说明是第一次查询或者数据表中还没有存入数据，返回最初传入的值 return startFrom; &#125; //查询一条数据的执行语句(当前id) private String queryOne(String sql) &#123; ResultSet result = null; try &#123; ps = conn.prepareStatement(sql); result = ps.executeQuery(); while (result.next()) &#123; return result.getString(1); &#125; &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; return null; &#125; //关闭相关资源 void close() &#123; try &#123; ps.close(); conn.close(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 自定义MySqlSource主类 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485package cn.baidu.flumesource;import org.apache.flume.Context;import org.apache.flume.Event;import org.apache.flume.EventDeliveryException;import org.apache.flume.PollableSource;import org.apache.flume.conf.Configurable;import org.apache.flume.event.SimpleEvent;import org.apache.flume.source.AbstractSource;import org.slf4j.Logger;import java.util.ArrayList;import java.util.HashMap;import java.util.List;import static org.slf4j.LoggerFactory.*;public class MySqlSource extends AbstractSource implements Configurable, PollableSource &#123; //打印日志 private static final Logger LOG = getLogger(MySqlSource.class); //定义sqlHelper private QueryMySql sqlSourceHelper; @Override public long getBackOffSleepIncrement() &#123; return 0; &#125; @Override public long getMaxBackOffSleepInterval() &#123; return 0; &#125; @Override public void configure(Context context) &#123; //初始化 sqlSourceHelper = new QueryMySql(context); &#125; @Override public PollableSource.Status process() throws EventDeliveryException &#123; try &#123; //查询数据表 List&lt;List&lt;Object&gt;&gt; result = sqlSourceHelper.executeQuery(); //存放event的集合 List&lt;Event&gt; events = new ArrayList&lt;&gt;(); //存放event头集合 HashMap&lt;String, String&gt; header = new HashMap&lt;&gt;(); //如果有返回数据，则将数据封装为event if (!result.isEmpty()) &#123; List&lt;String&gt; allRows = sqlSourceHelper.getAllRows(result); Event event = null; for (String row : allRows) &#123; event = new SimpleEvent(); event.setBody(row.getBytes()); event.setHeaders(header); events.add(event); &#125; //将event写入channel this.getChannelProcessor().processEventBatch(events); //更新数据表中的offset信息 sqlSourceHelper.updateOffset2DB(result.size()); &#125; //等待时长 Thread.sleep(sqlSourceHelper.getRunQueryDelay()); return Status.READY; &#125; catch (InterruptedException e) &#123; LOG.error("Error procesing row", e); return Status.BACKOFF; &#125; &#125; @Override public synchronized void stop() &#123; LOG.info("Stopping sql source &#123;&#125; ...", getName()); try &#123; //关闭资源 sqlSourceHelper.close(); &#125; finally &#123; super.stop(); &#125; &#125;&#125; 使用maven对工程进行打包，需要将mysql的依赖包一起打到jar包里，然后将打包好的jar包放到flume的lib目录下。 配置文件: vim mysqlsource.conf 1234567891011121314151617181920212223242526a1.sources = r1a1.sinks = k1a1.channels = c1# Describe/configure the sourcea1.sources.r1.type = cn.baidu.flumesource.MySqlSourcea1.sources.r1.connection.url = jdbc:mysql://node01:3306/mysqlsourcea1.sources.r1.connection.user = roota1.sources.r1.connection.password = hadoopa1.sources.r1.table = studenta1.sources.r1.columns.to.select = *a1.sources.r1.incremental.column.name = ida1.sources.r1.incremental.value = 0a1.sources.r1.run.query.delay=3000# Describe the sinka1.sinks.k1.type = logger# Describe the channela1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1 启动: 1bin/flume-ng agent -c conf -fconf/mysqlsource.conf -n a1 -Dflume.root.logger=INFO,console 2flume自定义sink###说明: 同自定义source类似，对于某些sink如果没有我们想要的，我们也可以自定义sink实现将数据保存到我们想要的地方去，例如kafka，或者mysql，或者文件等等都可以 需求：从网络端口当中发送数据，自定义sink，使用sink从网络端口接收数据，然后将数据保存到本地文件当中去。 pom.xml 同自定义source 自定义mysink 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758package cn.baidu.flumesink;import org.apache.commons.io.FileUtils;import org.apache.flume.*;import org.apache.flume.conf.Configurable;import org.apache.flume.sink.AbstractSink;import java.io.*;public class MySink extends AbstractSink implements Configurable &#123; private Context context ; private String filePath = ""; private String fileName = ""; private File fileDir; //这个方法会在初始化调用，主要用于初始化我们的Context，获取我们的一些配置参数 @Override public void configure(Context context) &#123; try &#123; this.context = context; filePath = context.getString("filePath"); fileName = context.getString("fileName"); fileDir = new File(filePath); if(!fileDir.exists())&#123; fileDir.mkdirs(); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; //这个方法会被反复调用 @Override public Status process() throws EventDeliveryException &#123; Event event = null; Channel channel = this.getChannel(); Transaction transaction = channel.getTransaction(); transaction.begin(); while(true)&#123; event = channel.take(); if(null != event)&#123; break; &#125; &#125; byte[] body = event.getBody(); String line = new String(body); try &#123; FileUtils.write(new File(filePath+File.separator+fileName),line,true); transaction.commit(); &#125; catch (IOException e) &#123; transaction.rollback(); e.printStackTrace(); return Status.BACKOFF; &#125;finally &#123; transaction.close(); &#125; return Status.READY; &#125;&#125; 将代码使用打包插件，打成jar包，注意一定要将commons-langs这个依赖包打进去，放到flume的lib目录下 配置conf文件 vim filesink.conf 12345678910111213141516171819a1.sources = r1a1.sinks = k1a1.channels = c1# Describe/configure the sourcea1.sources.r1.type = netcata1.sources.r1.bind = node01a1.sources.r1.port = 5678a1.sources.r1.channels = c1# # Describe the sinka1.sinks.k1.type = cn.baidu.flumesink.MySinka1.sinks.k1.filePath=/export/serversa1.sinks.k1.fileName=filesink.txt# # Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100# # Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1 启动 1bin/flume-ng agent -c conf -f conf/filesink.conf -n a1 -Dflume.root.logger=INFO,console Telnet node01 5678 连接到机器端口上输入数据]]></content>
      <tags>
        <tag>BigData</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive]]></title>
    <url>%2F2017%2F07%2F07%2FHive.html</url>
    <content type="text"><![CDATA[Hivehive中所有查询除了select from table 其他都要通过MapReduce方式执行 即使只有一行一列 如不是select from table 也要查询8,9秒. 1. 数据仓库1.1. 基本概念英文名称为Data Warehouse，可简写为DW或DWH。数据仓库的目的是构建面向分析的集成化数据环境，为企业提供决策支持（Decision Support）。 数据仓库是存数据的，企业的各种数据往里面存，主要目的是为了分析有效数据，后续会基于它产出供分析挖掘的数据，或者数据应用需要的数据，如企业的分析性报告和各类报表等。 可以理解为：面向分析的存储系统。 1.2. 主要特征数据仓库是面向主题的（Subject-Oriented ）、集成的（Integrated）、非易失的（Non-Volatile）和时变的（Time-Variant ）数据集合，用以支持管理决策。 1.2.1. 面向主题 数据仓库是面向主题的,数据仓库通过一个个主题域将多个业务系统的数据加载到一起，为了各个主题（如：用户、订单、商品等）进行分析而建，操作型数据库是为了支撑各种业务而建立。 1.2.2. 集成性数据仓库会将不同源数据库中的数据汇总到一起,数据仓库中的综合数据不能从原有的数据库系统直接得到。因此在数据进入数据仓库之前，必然要经过统一与整合，这一步是数据仓库建设中最关键、最复杂的一步(ETL)，要统一源数据中所有矛盾之处，如字段的同名异义、异名同义、单位不统一、字长不一致，等等。 1.2.3. 非易失性操作型数据库主要服务于日常的业务操作，使得数据库需要不断地对数据实时更新，以便迅速获得当前最新数据，不至于影响正常的业务运作。 在数据仓库中只要保存过去的业务数据，不需要每一笔业务都实时更新数据仓库，而是根据商业需要每隔一段时间把一批较新的数据导入数据仓库。数据仓库的数据反映的是一段相当长的时间内历史数据的内容，是不同时点的数据库的集合，以及基于这些快照进行统计、综合和重组的导出数据。数据仓库中的数据一般仅执行查询操作，很少会有删除和更新。但是需定期加载和刷新数据。 1.2.4. 时变性数据仓库包含各种粒度的历史数据。数据仓库中的数据可能与某个特定日期、星期、月份、季度或者年份有关。数据仓库的目的是通过分析企业过去一段时间业务的经营状况，挖掘其中隐藏的模式。虽然数据仓库的用户不能修改数据，但并不是说数据仓库的数据是永远不变的。分析的结果只能反映过去的情况，当业务变化后，挖掘出的模式会失去时效性。因此数据仓库的数据需要定时更新，以适应决策的需要。 1.3. 数据库与数据仓库的区别数据库与数据仓库的区别实际讲的是 OLTP 与 OLAP 的区别。 操作型处理，叫联机事务处理 OLTP（On-Line Transaction Processing，），也可以称面向交易的处理系统，它是针对具体业务在数据库联机的日常操作，通常对少数记录进行查询、修改。用户较为关心操作的响应时间、数据的安全性、完整性和并发支持的用户数等问题。传统的数据库系统作为数据管理的主要手段，主要用于操作型处理。 分析型处理，叫联机分析处理 OLAP（On-Line Analytical Processing）一般针对某些主题的历史数据进行分析，支持 管理决策。 首先要明白，数据仓库的出现，并不是要取代数据库。 数据库是面向事务的设计，数据仓库是面向主题设计的。 数据库一般存储业务数据，数据仓库存储的一般是历史数据。 数据库设计是尽量避免冗余，一般针对某一业务应用进行设计，比如一张简单的User表，记录用户名、密码等简单数据即可，符合业务应用，但是不符合分析。数据仓库在设计是有意引入冗余，依照分析需求，分析维度、分析指标进行设计。 数据库是为捕获数据而设计，数据仓库是为分析数据而设计。 数据仓库，是在数据库已经大量存在的情况下，为了进一步挖掘数据资源、为了决策需要而产生的，它决不是所谓的“大型数据库”。 1.4. 数仓的分层架构按照数据流入流出的过程，数据仓库架构可分为三层——源数据、数据仓库、数据应用。 数据仓库的数据来源于不同的源数据，并提供多样的数据应用，数据自下而上流入数据仓库后向上层开放应用，而数据仓库只是中间集成化数据管理的一个平台。 源数据层（ODS）：此层数据无任何更改，直接沿用外围系统数据结构和数据，不对外开放；为临时存储层，是接口数据的临时存储区域，为后一步的数据处理做准备。 数据仓库层（DW）：也称为细节层，DW层的数据应该是一致的、准确的、干净的数据，即对源系统数据进行了清洗（去除了杂质）后的数据。 数据应用层（DA或APP）：前端应用直接读取的数据源；根据报表、专题分析需求而计算生成的数据。 数据仓库从各数据源获取数据及在数据仓库内的数据转换和流动都可以认为是ETL（抽取Extra, 转化Transfer, 装载Load）的过程，ETL是数据仓库的流水线，也可以认为是数据仓库的血液，它维系着数据仓库中数据的新陈代谢，而数据仓库日常的管理和维护工作的大部分精力就是保持ETL的正常和稳定。 为什么要对数据仓库分层？用空间换时间，通过大量的预处理来提升应用系统的用户体验（效率），因此数据仓库会存在大量冗余的数据；不分层的话，如果源业务系统的业务规则发生变化将会影响整个数据清洗过程，工作量巨大。 通过数据分层管理可以简化数据清洗的过程，因为把原来一步的工作分到了多个步骤去完成，相当于把一个复杂的工作拆成了多个简单的工作，把一个大的黑盒变成了一个白盒，每一层的处理逻辑都相对简单和容易理解，这样我们比较容易保证每一个步骤的正确性，当数据发生错误的时候，往往我们只需要局部调整某个步骤即可。 1.5. 数仓的元数据管理元数据（Meta Date），主要记录数据仓库中模型的定义、各层级间的映射关系、监控数据仓库的数据状态及ETL的任务运行状态。一般会通过元数据资料库（Metadata Repository）来统一地存储和管理元数据，其主要目的是使数据仓库的设计、部署、操作和管理能达成协同和一致。 元数据是数据仓库管理系统的重要组成部分，元数据管理是企业级数据仓库中的关键组件，贯穿数据仓库构建的整个过程，直接影响着数据仓库的构建、使用和维护。 构建数据仓库的主要步骤之一是ETL。这时元数据将发挥重要的作用，它定义了源数据系统到数据仓库的映射、数据转换的规则、数据仓库的逻辑结构、数据更新的规则、数据导入历史记录以及装载周期等相关内容。数据抽取和转换的专家以及数据仓库管理员正是通过元数据高效地构建数据仓库。 用户在使用数据仓库时，通过元数据访问数据，明确数据项的含义以及定制报表。 数据仓库的规模及其复杂性离不开正确的元数据管理，包括增加或移除外部数据源，改变数据清洗方法，控制出错的查询以及安排备份等。 元数据可分为技术元数据和业务元数据。技术元数据为开发和管理数据仓库的IT 人员使用，它描述了与数据仓库开发、管理和维护相关的数据，包括数据源信息、数据转换描述、数据仓库模型、数据清洗与更新规则、数据映射和访问权限等。而业务元数据为管理层和业务分析人员服务，从业务角度描述数据，包括商务术语、数据仓库中有什么数据、数据的位置和数据的可用性等，帮助业务人员更好地理解数据仓库中哪些数据是可用的以及如何使用。 由上可见，元数据不仅定义了数据仓库中数据的模式、来源、抽取和转换规则等，而且是整个数据仓库系统运行的基础，元数据把数据仓库系统中各个松散的组件联系起来，组成了一个有机的整体。 2. Hive 的基本概念2.1. Hive 简介什么是 HiveHive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供类SQL查询功能。 其本质是将SQL转换为MapReduce的任务进行运算，底层由HDFS来提供数据的存储，说白了hive可以理解为一个将SQL转换为MapReduce的任务的工具，甚至更进一步可以说hive就是一个MapReduce的客户端 为什么使用 Hive 采用类SQL语法去操作数据，提供快速开发的能力。 避免了去写MapReduce，减少开发人员的学习成本。 功能扩展很方便。 2.2. Hive 架构 用户接口： 包括CLI、JDBC/ODBC、WebGUI。其中，CLI(command line interface)为shell命令行；JDBC/ODBC是Hive的JAVA实现，与传统数据库JDBC类似；WebGUI是通过浏览器访问Hive。 元数据存储： 通常是存储在关系数据库如mysql/derby中。Hive 将元数据存储在数据库中。Hive 中的元数据包括表的名字，表的列和分区及其属性，表的属性（是否为外部表等），表的数据所在目录等。 解释器、编译器、优化器、执行器: 完成HQL 查询语句从词法分析、语法分析、编译、优化以及查询计划的生成。生成的查询计划存储在HDFS 中，并在随后有MapReduce 调用执行。 2.3. Hive 与 Hadoop 的关系Hive利用HDFS存储数据，利用MapReduce查询分析数据 2.4. Hive与传统数据库对比hive用于海量数据的离线数据分析 总结：hive具有sql数据库的外表，但应用场景完全不同，hive只适合用来做批量数据统计分析 2.5. Hive 的安装这里我们选用hive的版本是2.1.1下载地址为：http://archive.apache.org/dist/hive/hive-2.1.1/apache-hive-2.1.1-bin.tar.gz 下载之后，将我们的安装包上传到第三台机器的/export/softwares目录下面去 第一步：上传并解压安装包将我们的hive的安装包上传到第三台服务器的/export/softwares路径下，然后进行解压 12cd /export/softwares/tar -zxvf apache-hive-2.1.1-bin.tar.gz -C ../servers/ 第二步：安装mysql第一步：在线安装mysql相关的软件包 yum install mysql mysql-server mysql-devel 第二步：启动mysql的服务 /etc/init.d/mysqld start 第三步：通过mysql安装自带脚本进行设置 /usr/bin/mysql_secure_installation 第四步：进入mysql的客户端然后进行授权 grant all privileges on *.* to &#39;root&#39;@&#39;%&#39; identified by &#39;123456&#39; with grant option; flush privileges; 第三步：修改hive的配置文件修改hive-env.sh 12cd /export/servers/apache-hive-2.1.1-bin/confcp hive-env.sh.template hive-env.sh 12HADOOP_HOME=/export/servers/hadoop-2.7.5export HIVE_CONF_DIR=/export/servers/apache-hive-2.1.1-bin/conf 修改hive-site.xml 12cd /export/servers/apache-hive-2.1.1-bin/confvim hive-site.xml 1234567891011121314151617181920212223242526272829303132&lt;?xml version="1.0" encoding="UTF-8" standalone="no"?&gt;&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;&lt;configuration&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;123456&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://node03:3306/hive?createDatabaseIfNotExist=true&amp;amp;useSSL=false&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.metastore.schema.verification&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;datanucleus.schema.autoCreateAll&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.server2.thrift.bind.host&lt;/name&gt; &lt;value&gt;node03&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 第四步：添加mysql的连接驱动包到hive的lib目录下hive使用mysql作为元数据存储，必然需要连接mysql数据库，所以我们添加一个mysql的连接驱动包到hive的安装目录下，然后就可以准备启动hive了 将我们准备好的mysql-connector-java-5.1.38.jar 这个jar包直接上传到/export/servers/apache-hive-2.1.1-bin/lib 这个目录下即可 至此，hive的安装部署已经完成，接下来我们来看下hive的三种交互方式 第五步：配置hive的环境变量node03服务器执行以下命令配置hive的环境变量 1sudo vim /etc/profile 12export HIVE_HOME=/export/servers/apache-hive-2.1.1-binexport PATH=:$HIVE_HOME/bin:$PATH 2.6. Hive 的交互方式第一种交互方式 bin/hive12cd /export/servers/apache-hive-2.1.1-bin/bin/hive 创建一个数据库 1create database if not exists mytest; 第二种交互方式：使用sql语句或者sql脚本进行交互不进入hive的客户端直接执行hive的hql语句 12cd /export/servers/apache-hive-2.1.1-binbin/hive -e "create database if not exists mytest;" 或者我们可以将我们的hql语句写成一个sql脚本然后执行 12cd /export/serversvim hive.sql 123create database if not exists mytest;use mytest;create table stu(id int,name string); 通过hive -f 来执行我们的sql脚本 1bin/hive -f /export/servers/hive.sql 3. Hive 的基本操作3.1 数据库操作3.1.1 创建数据库12create database if not exists myhive;use myhive; 说明：hive的表存放位置模式是由hive-site.xml当中的一个属性指定的 12&lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;&lt;value&gt;/user/hive/warehouse&lt;/value&gt; 3.1.2 创建数据库并指定位置1create database myhive2 location '/myhive2'; 3.1.3 设置数据库键值对信息数据库可以有一些描述性的键值对信息，在创建时添加： 1create database foo with dbproperties ('owner'='itcast', 'date'='20190120'); 查看数据库的键值对信息： 1describe database extended foo; 修改数据库的键值对信息： 1alter database foo set dbproperties ('owner'='itheima'); 3.1.4 查看数据库更多详细信息1desc database extended myhive2; 3.1.5 删除数据库删除一个空数据库，如果数据库下面有数据表，那么就会报错 1drop database myhive2; 强制删除数据库，包含数据库下面的表一起删除 1drop database myhive cascade; 3.2 数据库表操作3.2.1 创建表的语法:12345678910create [external] table [if not exists] table_name (col_name data_type [comment '字段描述信息']col_name data_type [comment '字段描述信息'])[comment '表的描述信息'][partitioned by (col_name data_type,...)][clustered by (col_name,col_name,...)][sorted by (col_name [asc|desc],...) into num_buckets buckets][row format row_format][storted as ....][location '指定表的路径'] 说明： create table 创建一个指定名字的表。如果相同名字的表已经存在，则抛出异常；用户可以用 IF NOT EXISTS 选项来忽略这个异常。 external 可以让用户创建一个外部表，在建表的同时指定一个指向实际数据的路径（LOCATION），Hive 创建内部表时，会将数据移动到数据仓库指向的路径；若创建外部表，仅记录数据所在的路径，不对数据的位置做任何改变。在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据。 comment 表示注释,默认不能使用中文 partitioned by 表示使用表分区,一个表可以拥有一个或者多个分区，每一个分区单独存在一个目录下 . clustered by对于每一个表分文件， Hive可以进一步组织成桶，也就是说桶是更为细粒度的数据范围划分。Hive也是 针对某一列进行桶的组织。 sorted by 指定排序字段和排序规则 row format ​ 指定表文件字段分隔符 storted as指定表文件的存储格式, 常用格式:SEQUENCEFILE, TEXTFILE, RCFILE,如果文件数据是纯文本，可以使用 STORED AS TEXTFILE。如果数据需要压缩，使用 storted as SEQUENCEFILE。 location 指定表文件的存储路径 3.2.2 内部表的操作创建表时,如果没有使用external关键字,则该表是内部表（managed table） Hive建表字段类型 分类 类型 描述 字面量示例 原始类型 BOOLEAN true/false TRUE TINYINT 1字节的有符号整数, -128~127 1Y SMALLINT 2个字节的有符号整数，-32768~32767 1S INT 4个字节的带符号整数 1 BIGINT 8字节带符号整数 1L FLOAT 4字节单精度浮点数 1.0 DOUBLE 8字节双精度浮点数 1.0 DEICIMAL 任意精度的带符号小数 1.0 STRING 字符串，变长 “a”,’b’ VARCHAR 变长字符串 “a”,’b’ CHAR 固定长度字符串 “a”,’b’ BINARY 字节数组 无法表示 TIMESTAMP 时间戳，毫秒值精度 122327493795 DATE 日期 ‘2016-03-29’ INTERVAL 时间频率间隔 复杂类型 ARRAY 有序的的同类型的集合 array(1,2) MAP key-value,key必须为原始类型，value可以任意类型 map(‘a’,1,’b’,2) STRUCT 字段集合,类型可以不同 struct(‘1’,1,1.0), named_stract(‘col1’,’1’,’col2’,1,’clo3’,1.0) UNION 在有限取值范围内的一个值 create_union(1,’a’,63) 建表入门: 1234use myhive;create table stu(id int,name string);insert into stu values (1,"zhangsan"); #插入数据select * from stu; 创建表并指定字段之间的分隔符 1create table if not exists stu2(id int ,name string) row format delimited fields terminated by '\t'; 创建表并指定表文件的存放路径 1create table if not exists stu2(id int ,name string) row format delimited fields terminated by '\t' location '/user/stu2'; 根据查询结果创建表 1create table stu3 as select * from stu2; # 通过复制表结构和表内容创建新表 根据已经存在的表结构创建表 1create table stu4 like stu; 查询表的详细信息 1desc formatted stu2; . 删除表 1drop table stu4; 3.2.3 外部表的操作外部表说明 外部表因为是指定其他的hdfs路径的数据加载到表当中来，所以hive表会认为自己不完全独占这份数据，所以删除hive表的时候，数据仍然存放在hdfs当中，不会删掉. 内部表和外部表的使用场景 每天将收集到的网站日志定期流入HDFS文本文件。在外部表（原始日志表）的基础上做大量的统计分析，用到的中间表、结果表使用内部表存储，数据通过SELECT+INSERT进入内部表。 操作案例 分别创建老师与学生表外部表，并向表中加载数据 创建老师表 1create external table teacher (t_id string,t_name string) row format delimited fields terminated by '\t'; 创建学生表 1create external table student (s_id string,s_name string,s_birth string , s_sex string ) row format delimited fields terminated by '\t'; 加载数据 1load data local inpath '/export/servers/hivedatas/student.csv' into table student; 加载数据并覆盖已有数据 1load data local inpath '/export/servers/hivedatas/student.csv' overwrite into table student; 从hdfs文件系统向表中加载数据（需要提前将数据上传到hdfs文件系统） 1234cd /export/servers/hivedatashdfs dfs -mkdir -p /hivedatashdfs dfs -put techer.csv /hivedatas/load data inpath '/hivedatas/techer.csv' into table teacher; 3.2.4 分区表的操作在大数据中，最常用的一种思想就是分治，我们可以把大的文件切割划分成一个个的小的文件，这样每次操作一个小的文件就会很容易了，同样的道理，在hive当中也是支持这种思想的，就是我们可以把大的数据，按照每月，或者天进行切分成一个个的小的文件,存放在不同的文件夹中. 创建分区表语法 1create table score(s_id string,c_id string, s_score int) partitioned by (month string) row format delimited fields terminated by '\t'; 创建一个表带多个分区 1create table score2 (s_id string,c_id string, s_score int) partitioned by (year string,month string,day string) row format delimited fields terminated by '\t'; 加载数据到分区表中 1load data local inpath '/export/servers/hivedatas/score.csv' into table score partition (month='201806'); 加载数据到多分区表中 1load data local inpath '/export/servers/hivedatas/score.csv' into table score2 partition(year='2018',month='06',day='01'); 多分区表联合查询(使用 union all) 1select * from score where month = '201806' union all select * from score where month = '201806'; 查看分区 1show partitions score; 添加一个分区 1alter table score add partition(month='201805'); 删除分区 1alter table score drop partition(month = '201806'); 3.2.5 分区表综合练习需求描述： 现在有一个文件score.csv文件，存放在集群的这个目录下/scoredatas/month=201806，这个文件每天都会生成，存放到对应的日期文件夹下面去，文件别人也需要公用，不能移动。需求，创建hive对应的表，并将数据加载到表中，进行数据统计分析，且删除表之后，数据不能删除 数据准备： 12hdfs dfs -mkdir -p /scoredatas/month=201806hdfs dfs -put score.csv /scoredatas/month=201806/ 创建外部分区表，并指定文件数据存放目录 1create external table score4(s_id string, c_id string,s_score int) partitioned by (month string) row format delimited fields terminated by '\t' location '/scoredatas'; 进行表的修复(建立表与数据文件之间的一个关系映射) 1msck repair table score4; 3.2.6 分桶表操作 分桶，就是将数据按照指定的字段进行划分到多个文件当中去,分桶就是MapReduce中的分区. 开启 Hive 的分桶功能 1set hive.enforce.bucketing=true; 设置 Reduce 个数 1set mapreduce.job.reduces=3; 创建分桶表 1create table course (c_id string,c_name string,t_id string) clustered by(c_id) into 3 buckets row format delimited fields terminated by '\t'; 桶表的数据加载，由于通标的数据加载通过hdfs dfs -put文件或者通过load data均不好使，只能通过insert overwrite 创建普通表，并通过insert overwriter的方式将普通表的数据通过查询的方式加载到桶表当中去 创建普通表 1create table course_common (c_id string,c_name string,t_id string) row format delimited fields terminated by '\t'; 普通表中加载数据 1load data local inpath '/export/servers/hivedatas/course.csv' into table course_common; 通过insert overwrite给桶表中加载数据 1insert overwrite table course select * from course_common cluster by(c_id); 3.3 修改表结构重命名: 1alter table old_table_name rename to new_table_name; 把表score4修改成score5 1alter table score4 rename to score5; 增加/修改列信息: 查询表结构 1desc score5; 添加列 1alter table score5 add columns (mycol string, mysco int); 更新列 1alter table score5 change column mysco mysconew int; 删除表 1drop table score5; 1.8. hive表中加载数据 直接向分区表中插入数据 123create table score3 like score;insert into table score3 partition(month ='201807') values ('001','002','100'); 通过查询插入数据 通过load方式加载数据 1load data local inpath '/export/servers/hivedatas/score.csv' overwrite into table score partition(month='201806'); 通过查询方式加载数据 12create table score4 like score;insert overwrite table score4 partition(month = '201806') select s_id,c_id,s_score from score; 4. Hive 查询语法4.1. SELECT12345678SELECT [ALL | DISTINCT] select_expr, select_expr, ...FROM table_reference[WHERE where_condition][GROUP BY col_list [HAVING condition]][CLUSTER BY col_list| [DISTRIBUTE BY col_list] [SORT BY| ORDER BY col_list]][LIMIT number] order by 会对输入做全局排序，因此只有一个reducer，会导致当输入规模较大时，需要较长的计算时间。 sort by不是全局排序，其在数据进入reducer前完成排序。因此，如果用sort by进行排序，并且设置mapred.reduce.tasks&gt;1，则sort by只保证每个reducer的输出有序，不保证全局有序。 distribute by(字段)根据指定的字段将数据分到不同的reducer，且分发算法是hash散列。 cluster by(字段) 除了具有distribute by的功能外，还会对该字段进行排序. 因此，如果distribute 和sort字段是同一个时，此时，cluster by = distribute by + sort by 4.2. 查询语法全表查询 1select * from score; 选择特定列 1select s_id ,c_id from score; 列别名 1）重命名一个列。2）便于计算。3）紧跟列名，也可以在列名和别名之间加入关键字‘AS’ 1select s_id as myid ,c_id from score; 4.3. 常用函数 求总行数（count） 1select count(1) from score; 求分数的最大值（max） 1select max(s_score) from score; 求分数的最小值（min） 1select min(s_score) from score; 求分数的总和（sum） 1select sum(s_score) from score; 求分数的平均值（avg） 1select avg(s_score) from score; 4.4. LIMIT语句典型的查询会返回多行数据。LIMIT子句用于限制返回的行数。 1select * from score limit 3; 4.5. WHERE语句 使用WHERE 子句，将不满足条件的行过滤掉。 WHERE 子句紧随 FROM 子句。 案例实操 查询出分数大于60的数据 1select * from score where s_score &gt; 60; 比较运算符 操作符 支持的数据类型 描述 A=B 基本数据类型 如果A等于B则返回TRUE，反之返回FALSE A&lt;=&gt;B 基本数据类型 如果A和B都为NULL，则返回TRUE，其他的和等号（=）操作符的结果一致，如果任一为NULL则结果为NULL A&lt;&gt;B, A!=B 基本数据类型 A或者B为NULL则返回NULL；如果A不等于B，则返回TRUE，反之返回FALSE A&lt;B 基本数据类型 A或者B为NULL，则返回NULL；如果A小于B，则返回TRUE，反之返回FALSE A&lt;=B 基本数据类型 A或者B为NULL，则返回NULL；如果A小于等于B，则返回TRUE，反之返回FALSE A&gt;B 基本数据类型 A或者B为NULL，则返回NULL；如果A大于B，则返回TRUE，反之返回FALSE A&gt;=B 基本数据类型 A或者B为NULL，则返回NULL；如果A大于等于B，则返回TRUE，反之返回FALSE A [NOT] BETWEEN B AND C 基本数据类型 如果A，B或者C任一为NULL，则结果为NULL。如果A的值大于等于B而且小于或等于C，则结果为TRUE，反之为FALSE。如果使用NOT关键字则可达到相反的效果。 A IS NULL 所有数据类型 如果A等于NULL，则返回TRUE，反之返回FALSE A IS NOT NULL 所有数据类型 如果A不等于NULL，则返回TRUE，反之返回FALSE IN(数值1, 数值2) 所有数据类型 使用 IN运算显示列表中的值 A [NOT] LIKE B STRING 类型 B是一个SQL下的简单正则表达式，如果A与其匹配的话，则返回TRUE；反之返回FALSE。B的表达式说明如下：‘x%’表示A必须以字母‘x’开头，‘%x’表示A必须以字母’x’结尾，而‘%x%’表示A包含有字母’x’,可以位于开头，结尾或者字符串中间。如果使用NOT关键字则可达到相反的效果。 A RLIKE B, A REGEXP B STRING 类型 B是一个正则表达式，如果A与其匹配，则返回TRUE；反之返回FALSE。匹配使用的是JDK中的正则表达式接口实现的，因为正则也依据其中的规则。例如，正则表达式必须和整个字符串A相匹配，而不是只需与其字符串匹配。 查询分数等于80的所有的数据 1select * from score where s_score = 80; 查询分数在80到100的所有数据 1select * from score where s_score between 80 and 100; 查询成绩为空的所有数据 1select * from score where s_score is null; 查询成绩是80和90的数据 1select * from score where s_score in(80,90); 4.6. LIKE 和 RLIKE 使用LIKE运算选择类似的值 选择条件可以包含字符或数字: 12% 代表零个或多个字符(任意个字符)。_ 代表一个字符。 RLIKE子句是Hive中这个功能的一个扩展，其可以通过Java的正则表达式这个更强大的语言来指定匹配条件。 案例实操 查找以8开头的所有成绩 1select * from score where s_score like '8%'; 查找第二个数值为9的所有成绩数据 1select * from score where s_score like '_9%'; 查找s_id中含1的数据 1select * from score where s_id rlike '[1]'; # like '%1%' 4.7. 逻辑运算符 操作符 含义 AND 逻辑并 OR 逻辑或 NOT 逻辑否 查询成绩大于80，并且s_id是01的数据 1select * from score where s_score &gt;80 and s_id = '01'; 查询成绩大于80，或者s_id 是01的数 1select * from score where s_score &gt; 80 or s_id = '01'; 查询s_id 不是 01和02的学生 1select * from score where s_id not in ('01','02'); 4.8. 分组GROUP BY 语句GROUP BY语句通常会和聚合函数一起使用，按照一个或者多个列队结果进行分组，然后对每个组执行聚合操作。案例实操： 计算每个学生的平均分数 1select s_id ,avg(s_score) from score group by s_id; 计算每个学生最高成绩 1select s_id ,max(s_score) from score group by s_id; HAVING 语句 having与where不同点 where针对表中的列发挥作用，查询数据；having针对查询结果中的列发挥作用，筛选数据。 where后面不能写分组函数，而having后面可以使用分组函数。 having只用于group by分组统计语句。 案例实操： 求每个学生的平均分数 1select s_id ,avg(s_score) from score group by s_id; 求每个学生平均分数大于85的人 1select s_id ,avg(s_score) avgscore from score group by s_id having avgscore &gt; 85; 4.9. JOIN 语句4.9.1. 等值 JOINHive支持通常的SQL JOIN语句，但是只支持等值连接，不支持非等值连接。 案例操作: 查询分数对应的姓名 1select s.s_id,s.s_score,stu.s_name,stu.s_birth from score s join student stu on s.s_id = stu.s_id; 4.9.2. 表的别名 好处 使用别名可以简化查询。 使用表名前缀可以提高执行效率。 案例实操 合并老师与课程表 1select * from techer t join course c on t.t_id = c.t_id; 4.9.3. 内连接内连接：只有进行连接的两个表中都存在与连接条件相匹配的数据才会被保留下来。 1select * from techer t inner join course c on t.t_id = c.t_id; 4.9.4. 左外连接左外连接：JOIN操作符左边表中符合WHERE子句的所有记录将会被返回。查询老师对应的课程 1select * from techer t left join course c on t.t_id = c.t_id; 4.9.5. 右外连接右外连接：JOIN操作符右边表中符合WHERE子句的所有记录将会被返回。 1select * from teacher t right join course c on t.t_id = c.t_id; 4.9.6. 多表连接注意：连接 n个表，至少需要n-1个连接条件。例如：连接三个表，至少需要两个连接条件。 多表连接查询，查询老师对应的课程，以及对应的分数，对应的学生 1234567select * from teacher tleft join course con t.t_id = c.t_idleft join score son s.c_id = c.c_idleft join student stuon s.s_id = stu.s_id; 大多数情况下，Hive会对每对JOIN连接对象启动一个MapReduce任务。本例中会首先启动一个MapReduce job对表techer和表course进行连接操作，然后会再启动一个MapReduce job将第一个MapReduce job的输出和表score;进行连接操作。 4.10. 排序4.10.1. 全局排序Order By：全局排序，一个reduce 使用 ORDER BY 子句排序ASC（ascend）: 升序（默认）DESC（descend）: 降序 ORDER BY 子句在SELECT语句的结尾。 案例实操 查询学生的成绩，并按照分数降序排列 1SELECT * FROM student s LEFT JOIN score sco ON s.s_id = sco.s_id ORDER BY sco.s_score DESC; 查询学生的成绩，并按照分数升序排列 1SELECT * FROM student s LEFT JOIN score sco ON s.s_id = sco.s_id ORDER BY sco.s_score asc; 4.10.2. 按照别名排序按照分数的平均值排序 1select s_id ,avg(s_score) avg from score group by s_id order by avg; 4.10.3. 多个列排序按照学生id和平均成绩进行排序 1select s_id ,avg(s_score) avg from score group by s_id order by s_id,avg; 4.10.4. 每个MapReduce内部排序（Sort By）局部排序Sort By：每个MapReduce内部进行排序，对全局结果集来说不是排序。 设置reduce个数 1set mapreduce.job.reduces=3; 查看设置reduce个数 1set mapreduce.job.reduces; 查询成绩按照成绩降序排列 1select * from score sort by s_score; 将查询结果导入到文件中（按照成绩降序排列） 1insert overwrite local directory '/export/servers/hivedatas/sort' select * from score sort by s_score; 4.10.5. 分区排序（DISTRIBUTE BY）Distribute By：类似MR中partition，进行分区，结合sort by使用。 注意，Hive要求DISTRIBUTE BY语句要写在SORT BY语句之前。 对于distribute by进行测试，一定要分配多reduce进行处理，否则无法看到distribute by的效果。 案例实操：先按照学生id进行分区，再按照学生成绩进行排序。 设置reduce的个数，将我们对应的s_id划分到对应的reduce当中去 1set mapreduce.job.reduces=7; 通过distribute by 进行数据的分区 1insert overwrite local directory '/export/servers/hivedatas/sort' select * from score distribute by s_id sort by s_score; 4.10.6. CLUSTER BY当distribute by和sort by字段相同时，可以使用cluster by方式。 cluster by除了具有distribute by的功能外还兼具sort by的功能。但是排序只能是倒序排序，不能指定排序规则为ASC或者DESC。 以下两种写法等价 12select * from score cluster by s_id;select * from score distribute by s_id sort by s_id; 5.Hive Shell参数5.1 Hive命令行语法结构 1bin/hive [-hiveconf x=y]* [&lt;-i filename&gt;]* [&lt;-f filename&gt;|&lt;-e query-string&gt;] [-S] 说明： 1、 -i 从文件初始化HQL。 2、 -e从命令行执行指定的HQL 3、 -f 执行HQL脚本 4、 -v 输出执行的HQL语句到控制台 5、 -p connect to Hive Server on port number 6、 -hiveconf x=y Use this to set hive/hadoop configuration variables. 设置hive运行时候的参数配置 5.2 Hive参数配置方式开发Hive应用时，不可避免地需要设定Hive的参数。设定Hive的参数可以调优HQL代码的执行效率，或帮助定位问题。 对于一般参数，有以下三种设定方式： 配置文件 命令行参数 参数声明 配置文件：Hive的配置文件包括 用户自定义配置文件：$HIVE_CONF_DIR/hive-site.xml 默认配置文件： $HIVE_CONF_DIR/hive-default.xml 用户自定义配置会覆盖默认配置。 另外，Hive也会读入Hadoop的配置，因为Hive是作为Hadoop的客户端启动的，Hive的配置会覆盖Hadoop的配置。 配置文件的设定对本机启动的所有Hive进程都有效。 命令行参数：启动Hive（客户端或Server方式）时，可以在命令行添加-hiveconf param=value来设定参数，例如： 1bin/hive -hiveconf hive.root.logger=INFO,console 这一设定对本次启动的Session（对于Server方式启动，则是所有请求的Sessions）有效。 参数声明：可以在HQL中使用SET关键字设定参数，例如： 1set mapred.reduce.tasks=100; 这一设定的作用域也是session级的。 上述三种设定方式的优先级依次递增。即参数声明覆盖命令行参数，命令行参数覆盖配置文件设定。注意某些系统级的参数，例如log4j相关的设定，必须用前两种方式设定，因为那些参数的读取在Session建立以前已经完成了。 参数声明 &gt; 命令行参数 &gt; 配置文件参数（hive） 6. Hive 函数6.1. 内置函数内容较多，见《Hive官方文档》 1https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF 查看系统自带的函数 1hive&gt; show functions; 显示自带的函数的用法 1hive&gt; desc function upper; 详细显示自带的函数的用法 1hive&gt; desc function extended upper; 4:常用内置函数 1234567891011#字符串连接函数： concat select concat('abc','def’,'gh');#带分隔符字符串连接函数： concat_ws select concat_ws(',','abc','def','gh');#cast类型转换 select cast(1.5 as int);#get_json_object(json 解析函数，用来处理json，必须是json格式) select get_json_object('&#123;"name":"jack","age":"20"&#125;','$.name');#URL解析函数 select parse_url('http://facebook.com/path1/p.php?k1=v1&amp;k2=v2#Ref1', 'HOST');#explode：把map集合中每个键值对或数组中的每个元素都单独生成一行的形式 6.2. 自定义函数6.2.1 概述: Hive 自带了一些函数，比如：max/min等，当Hive提供的内置函数无法满足你的业务处理需要时，此时就可以考虑使用用户自定义函数(UDF). 根据用户自定义函数类别分为以下三种： UDF（User-Defined-Function） 一进一出 UDAF（User-Defined Aggregation Function） 聚集函数，多进一出 类似于：count/max/min UDTF（User-Defined Table-Generating Functions） 一进多出 如 lateral view explore() 编程步骤： 继承org.apache.hadoop.hive.ql.UDF 需要实现evaluate函数；evaluate函数支持重载； 注意事项 UDF必须要有返回类型，可以返回null，但是返回类型不能为void； UDF中常用Text/LongWritable等类型，不推荐使用java类型； 6.2.2 UDF 开发实例Step 1 创建 Maven 工程1234567891011121314151617181920212223242526272829&lt;dependencies&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.hive/hive-exec --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hive&lt;/groupId&gt; &lt;artifactId&gt;hive-exec&lt;/artifactId&gt; &lt;version&gt;2.7.5&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-common --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;2.7.5&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.0&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; Step 2 开发 Java 类集成 UDF12345678910public class MyUDF extends UDF&#123; public Text evaluate(final Text str)&#123; String tmp_str = str.toString(); if(str != null &amp;&amp; !tmp_str.equals(""))&#123; String str_ret = tmp_str.substring(0, 1).toUpperCase() + tmp_str.substring(1); return new Text(str_ret); &#125; return new Text(""); &#125;&#125; Step 3 项目打包，并上传到hive的lib目录下 Step 4 添加jar包重命名我们的jar包名称 12cd /export/servers/apache-hive-2.7.5-bin/libmv original-day_10_hive_udf-1.0-SNAPSHOT.jar my_upper.jar hive的客户端添加我们的jar包 1add jar /export/servers/apache-hive-2.7.5-bin/lib/my_upper.jar; Step 5 设置函数与我们的自定义函数关联1create temporary function my_upper as 'cn.itcast.udf.ItcastUDF'; Step 6 使用自定义函数1select my_upper('abc'); 7.hive的数据压缩在实际工作当中，hive当中处理的数据，一般都需要经过压缩，前期我们在学习hadoop的时候，已经配置过hadoop的压缩，我们这里的hive也是一样的可以使用压缩来节省我们的MR处理的网络带宽 7.1 MR支持的压缩编码 压缩格式 工具 算法 文件扩展名 是否可切分 DEFAULT 无 DEFAULT .deflate 否 Gzip gzip DEFAULT .gz 否 bzip2 bzip2 bzip2 .bz2 是 LZO lzop LZO .lzo 否 LZ4 无 LZ4 .lz4 否 Snappy 无 Snappy .snappy 否 为了支持多种压缩/解压缩算法，Hadoop引入了编码/解码器，如下表所示 压缩格式 对应的编码/解码器 DEFLATE org.apache.hadoop.io.compress.DefaultCodec gzip org.apache.hadoop.io.compress.GzipCodec bzip2 org.apache.hadoop.io.compress.BZip2Codec LZO com.hadoop.compression.lzo.LzopCodec LZ4 org.apache.hadoop.io.compress.Lz4Codec Snappy org.apache.hadoop.io.compress.SnappyCodec 压缩性能的比较 压缩算法 原始文件大小 压缩文件大小 压缩速度 解压速度 gzip 8.3GB 1.8GB 17.5MB/s 58MB/s bzip2 8.3GB 1.1GB 2.4MB/s 9.5MB/s LZO 8.3GB 2.9GB 49.3MB/s 74.6MB/s http://google.github.io/snappy/ On a single core of a Core i7 processor in 64-bit mode, Snappy compresses at about 250 MB/sec or more and decompresses at about 500 MB/sec or more. 7.2 压缩配置参数要在Hadoop中启用压缩，可以配置如下参数（mapred-site.xml文件中）： 参数 默认值 阶段 建议 io.compression.codecs （在core-site.xml中配置） org.apache.hadoop.io.compress.DefaultCodec, org.apache.hadoop.io.compress.GzipCodec, org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.Lz4Codec 输入压缩 Hadoop使用文件扩展名判断是否支持某种编解码器 mapreduce.map.output.compress false mapper输出 这个参数设为true启用压缩 mapreduce.map.output.compress.codec org.apache.hadoop.io.compress.DefaultCodec mapper输出 使用LZO、LZ4或snappy编解码器在此阶段压缩数据 mapreduce.output.fileoutputformat.compress false reducer输出 这个参数设为true启用压缩 mapreduce.output.fileoutputformat.compress.codec org.apache.hadoop.io.compress. DefaultCodec reducer输出 使用标准工具或者编解码器，如gzip和bzip2 mapreduce.output.fileoutputformat.compress.type RECORD reducer输出 SequenceFile输出使用的压缩类型：NONE和BLOCK 7.3 开启Map输出阶段压缩开启map输出阶段压缩可以减少job中map和Reduce task间数据传输量。具体配置如下： 案例实操： 1）开启hive中间传输数据压缩功能 1set hive.exec.compress.intermediate=true; 2）开启mapreduce中map输出压缩功能 1set mapreduce.map.output.compress=true; 3）设置mapreduce中map输出数据的压缩方式 1set mapreduce.map.output.compress.codec= org.apache.hadoop.io.compress.SnappyCodec; 4）执行查询语句 1select count(1) from score; 7.4 开启Reduce输出阶段压缩当Hive将输出写入到表中时，输出内容同样可以进行压缩。属性hive.exec.compress.output控制着这个功能。用户可能需要保持默认设置文件中的默认值false，这样默认的输出就是非压缩的纯文本文件了。用户可以通过在查询语句或执行脚本中设置这个值为true，来开启输出结果压缩功能。 案例实操： 1）开启hive最终输出数据压缩功能 1set hive.exec.compress.output=true; 2）开启mapreduce最终输出数据压缩 1set mapreduce.output.fileoutputformat.compress=true; 3）设置mapreduce最终数据输出压缩方式 1set mapreduce.output.fileoutputformat.compress.codec = org.apache.hadoop.io.compress.SnappyCodec; 4）设置mapreduce最终数据输出压缩为块压缩 1set mapreduce.output.fileoutputformat.compress.type=BLOCK; 5）测试一下输出结果是否是压缩文件 1insert overwrite local directory '/export/servers/snappy' select * from score distribute by s_id sort by s_id desc; 8.hive的数据存储格式Hive支持的存储数的格式主要有：TEXTFILE（行式存储） 、SEQUENCEFILE(行式存储)、ORC（列式存储）、PARQUET（列式存储）。 8.1 列式存储和行式存储 上图左边为逻辑表，右边第一个为行式存储，第二个为列式存储。 行存储的特点： 查询满足条件的一整行数据的时候，列存储则需要去每个聚集的字段找到对应的每个列的值，行存储只需要找到其中一个值，其余的值都在相邻地方，所以此时行存储查询的速度更快。 列存储的特点： 因为每个字段的数据聚集存储，在查询只需要少数几个字段的时候，能大大减少读取的数据量；每个字段的数据类型一定是相同的，列式存储可以针对性的设计更好的设计压缩算法。 TEXTFILE和SEQUENCEFILE的存储格式都是基于行存储的； ORC和PARQUET是基于列式存储的。 8.2 常用数据存储格式TEXTFILE格式 默认格式，数据不做压缩，磁盘开销大，数据解析开销大。可结合Gzip、Bzip2使用. ORC格式 Orc (Optimized Row Columnar)是hive 0.11版里引入的新的存储格式。 可以看到每个Orc文件由1个或多个stripe组成，每个stripe250MB大小，每个Stripe里有三部分组成，分别是Index Data,Row Data,Stripe Footer： indexData：某些列的索引数据 rowData :真正的数据存储 StripFooter：stripe的元数据信息 PARQUET格式 Parquet是面向分析型业务的列式存储格式，由Twitter和Cloudera合作开发， Parquet文件是以二进制方式存储的，所以是不可以直接读取的，文件中包括该文件的数据和元数据，因此Parquet格式文件是自解析的。 通常情况下，在存储Parquet数据的时候会按照Block大小设置行组的大小，由于一般情况下每一个Mapper任务处理数据的最小单位是一个Block，这样可以把每一个行组由一个Mapper任务处理，增大任务执行并行度。Parquet文件的格式如下图所示。 9. 文件存储格式与数据压缩结合9.1 压缩比和查询速度对比1）TextFile （1）创建表，存储数据格式为TEXTFILE 1234567891011create table log_text (track_time string,url string,session_id string,referer string,ip string,end_user_id string,city_id string)ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'STORED AS TEXTFILE ; （2）向表中加载数据 1load data local inpath '/export/servers/hivedatas/log.data' into table log_text ; （3）查看表中数据大小 1dfs -du -h /user/hive/warehouse/myhive.db/log_text; 2）ORC （1）创建表，存储数据格式为ORC 1234567891011create table log_orc(track_time string,url string,session_id string,referer string,ip string,end_user_id string,city_id string)ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'STORED AS orc ; （2）向表中加载数据 1insert into table log_orc select * from log_text ; （3）查看表中数据大小 1dfs -du -h /user/hive/warehouse/myhive.db/log_orc; 3）Parquet （1）创建表，存储数据格式为parquet 1234567891011create table log_parquet(track_time string,url string,session_id string,referer string,ip string,end_user_id string,city_id string)ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'STORED AS PARQUET ; （2）向表中加载数据 1insert into table log_parquet select * from log_text ; （3）查看表中数据大小 1dfs -du -h /user/hive/warehouse/myhive.db/log_parquet; 存储文件的压缩比总结： ORC &gt; Parquet &gt; textFile 4)存储文件的查询速度测试： 1）TextFile hive (default)&gt; select count(1) from log_text; Time taken: 21.54 seconds, Fetched: 1 row(s) 2）ORC hive (default)&gt; select count(1) from log_orc; Time taken: 20.867 seconds, Fetched: 1 row(s) 3）Parquet hive (default)&gt; select count(1) from log_parquet; Time taken: 22.922 seconds, Fetched: 1 row(s) 存储文件的查询速度总结： ORC &gt; TextFile &gt; Parquet 9.2 ORC存储指定压缩方式官网：https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC ORC存储方式的压缩： Key Default Notes orc.compress ZLIB high level compression (one of NONE, ZLIB, SNAPPY) orc.compress.size 262,144 number of bytes in each compression chunk orc.stripe.size 67,108,864 number of bytes in each stripe orc.row.index.stride 10,000 number of rows between index entries (must be &gt;= 1000) orc.create.index true whether to create row indexes orc.bloom.filter.columns “” comma separated list of column names for which bloom filter should be created orc.bloom.filter.fpp 0.05 false positive probability for bloom filter (must &gt;0.0 and &lt;1.0) 1）创建一个非压缩的的ORC存储方式 （1）建表语句 1234567891011create table log_orc_none(track_time string,url string,session_id string,referer string,ip string,end_user_id string,city_id string)ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'STORED AS orc tblproperties ("orc.compress"="NONE"); （2）插入数据 1insert into table log_orc_none select * from log_text ; （3）查看插入后数据 1dfs -du -h /user/hive/warehouse/myhive.db/log_orc_none; 2）创建一个SNAPPY压缩的ORC存储方式 （1）建表语句 1234567891011create table log_orc_snappy(track_time string,url string,session_id string,referer string,ip string,end_user_id string,city_id string)ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'STORED AS orc tblproperties ("orc.compress"="SNAPPY"); （2）插入数据 1insert into table log_orc_snappy select * from log_text ; （3）查看插入后数据 1dfs -du -h /user/hive/warehouse/myhive.db/log_orc_snappy ; 9.3 存储方式和压缩总结：​ 在实际的项目开发当中，hive表的数据存储格式一般选择：orc或parquet。压缩方式一般选择snappy。 10.hive调优10.1 Fetch抓取Hive中对某些情况的查询可以不必使用MapReduce计算。例如：SELECT * FROM score;在这种情况下，Hive可以简单地读取score对应的存储目录下的文件，然后输出查询结果到控制台。通过设置hive.fetch.task.conversion参数,可以控制查询语句是否走MapReduce. 案例实操： 1）把hive.fetch.task.conversion设置成none，然后执行查询语句，都会执行mapreduce程序。 12345set hive.fetch.task.conversion=none;select * from score;select s_score from score;select s_score from score limit 3; 2）把hive.fetch.task.conversion设置成more，然后执行查询语句，如下查询方式都不会执行mapreduce程序。 12345set hive.fetch.task.conversion=more;select * from score;select s_score from score;select s_score from score limit 3; 10.2 本地模式大多数的Hadoop Job是需要Hadoop提供的完整的可扩展性来处理大数据集的。不过，有时Hive的输入数据量是非常小的。在这种情况下，为查询触发执行任务时消耗可能会比实际job的执行时间要多的多。对于大多数这种情况，Hive可以通过本地模式在单台机器上处理所有的任务。对于小数据集，执行时间可以明显被缩短。 用户可以通过设置hive.exec.mode.local.auto的值为true，来让Hive在适当的时候自动启动这个优化。 案例实操： 1）开启本地模式，并执行查询语句 12set hive.exec.mode.local.auto=true; select * from score cluster by s_id; 2）关闭本地模式，并执行查询语句 12set hive.exec.mode.local.auto=false; select * from score cluster by s_id; 10.3 MapJoin如果不指定MapJoin或者不符合MapJoin的条件，那么Hive解析器会在Reduce阶段完成join,容易发生数据倾斜。可以用MapJoin把小表全部加载到内存在map端进行join，避免reducer处理。 1）开启MapJoin参数设置： （1）设置自动选择Mapjoin 1set hive.auto.convert.join = true; （2）大表小表的阈值设置（默认25M以下认为是小表）： 1set hive.mapjoin.smalltable.filesize=25123456; 10.4 Group By 默认情况下，Map阶段同一Key数据分发给一个reduce，当一个key数据过大时就倾斜了。并不是所有的聚合操作都需要在Reduce端完成，很多聚合操作都可以先在Map端进行部分聚合，最后在Reduce端得出最终结果。 开启Map端聚合参数设置 （1）是否在Map端进行聚合，默认为True 1set hive.map.aggr = true; （2）在Map端进行聚合操作的条目数目 1set hive.groupby.mapaggr.checkinterval = 100000; （3）有数据倾斜的时候进行负载均衡（默认是false） 1set hive.groupby.skewindata = true; 当选项设定为 true，生成的查询计划会有两个MR Job。 第一个MR Job中，Map的输出结果会随机分布到Reduce中，每个Reduce做部分聚合操作，并输出结果，这样处理的结果是相同的Group By Key有可能被分发到不同的Reduce中，从而达到负载均衡的目的； 第二个MR Job再根据预处理的数据结果按照Group By Key分布到Reduce中（这个过程可以保证相同的Group By Key被分布到同一个Reduce中），最后完成最终的聚合操作。 10.5 Count(distinct)数据量小的时候无所谓，数据量大的情况下，由于COUNT DISTINCT操作需要用一个Reduce Task来完成，这一个Reduce需要处理的数据量太大，就会导致整个Job很难完成，一般COUNT DISTINCT使用先GROUP BY再COUNT的方式替换： 12select count(distinct s_id) from score;select count(s_id) from (select id from score group by s_id) a; 虽然会多用一个Job来完成，但在数据量大的情况下，这个绝对是值得的。 10.6 笛卡尔积尽量避免笛卡尔积，即避免join的时候不加on条件，或者无效的on条件，Hive只能使用1个reducer来完成笛卡尔积。 10.7 动态分区调整往hive分区表中插入数据时，hive提供了一个动态分区功能，其可以基于查询参数的位置去推断分区的名称，从而建立分区。使用Hive的动态分区，需要进行相应的配置。 Hive的动态分区是以第一个表的分区规则，来对应第二个表的分区规则，将第一个表的所有分区，全部拷贝到第二个表中来，第二个表在加载数据的时候，不需要指定分区了，直接用第一个表的分区即可 10.7.1 开启动态分区参数设置（1）开启动态分区功能（默认true，开启） 1set hive.exec.dynamic.partition=true; （2）设置为非严格模式（动态分区的模式，默认strict，表示必须指定至少一个分区为静态分区，nonstrict模式表示允许所有的分区字段都可以使用动态分区。） 1set hive.exec.dynamic.partition.mode=nonstrict; （3）在所有执行MR的节点上，最大一共可以创建多少个动态分区。 1set hive.exec.max.dynamic.partitions=1000; （4）在每个执行MR的节点上，最大可以创建多少个动态分区。该参数需要根据实际的数据来设定。 1set hive.exec.max.dynamic.partitions.pernode=100 （5）整个MR Job中，最大可以创建多少个HDFS文件。 ​ 在linux系统当中，每个linux用户最多可以开启1024个进程，每一个进程最多可以打开2048个文件，即持有2048个文件句柄，下面这个值越大，就可以打开文件句柄越大 1set hive.exec.max.created.files=100000; （6）当有空分区生成时，是否抛出异常。一般不需要设置。 1set hive.error.on.empty.partition=false; 10.7.2 案例操作需求：将ori中的数据按照时间(如：20111231234568)，插入到目标表ori_partitioned的相应分区中。 （1）准备数据原表 1234567create table ori_partitioned(id bigint, time bigint, uid string, keyword string, url_rank int, click_num int, click_url string) PARTITIONED BY (p_time bigint) row format delimited fields terminated by '\t';load data local inpath '/export/servers/hivedatas/small_data' into table ori_partitioned partition (p_time='20111230000010');load data local inpath '/export/servers/hivedatas/small_data' into table ori_partitioned partition (p_time='20111230000011'); （2）创建目标分区表 1create table ori_partitioned_target(id bigint, time bigint, uid string, keyword string, url_rank int, click_num int, click_url string) PARTITIONED BY (p_time STRING) row format delimited fields terminated by '\t' （3）向目标分区表加载数据 如果按照之前介绍的往指定一个分区中Insert数据，那么这个需求很不容易实现。这时候就需要使用动态分区来实现。 123INSERT overwrite TABLE ori_partitioned_target PARTITION (p_time)SELECT id, time, uid, keyword, url_rank, click_num, click_url, p_timeFROM ori_partitioned; 注意：在SELECT子句的最后几个字段，必须对应前面PARTITION (p_time)中指定的分区字段，包括顺序。 (4)查看分区 1show partitions ori_partitioned_target; 10.8 并行执行Hive会将一个查询转化成一个或者多个阶段。这样的阶段可以是MapReduce阶段、抽样阶段、合并阶段、limit阶段。或者Hive执行过程中可能需要的其他阶段。默认情况下，Hive一次只会执行一个阶段。不过，某个特定的job可能包含众多的阶段，而这些阶段可能并非完全互相依赖的，也就是说有些阶段是可以并行执行的，这样可能使得整个job的执行时间缩短。不过，如果有更多的阶段可以并行执行，那么job可能就越快完成。 ​ 通过设置参数hive.exec.parallel值为true，就可以开启并发执行。不过，在共享集群中，需要注意下，如果job中并行阶段增多，那么集群利用率就会增加。 1set hive.exec.parallel = true; 当然，得是在系统资源比较空闲的时候才有优势，否则，没资源，并行也起不来。 10.9 严格模式Hive提供了一个严格模式，可以防止用户执行那些可能意向不到的不好的影响的查询。 ​ 通过设置属性hive.mapred.mode值为默认是非严格模式nonstrict 。开启严格模式需要修改hive.mapred.mode值为strict，开启严格模式可以禁止3种类型的查询。 12set hive.mapred.mode = strict; #开启严格模式set hive.mapred.mode = nostrict; #开启非严格模式 1）对于分区表，在where语句中必须含有分区字段作为过滤条件来限制范围，否则不允许执行。换句话说，就是用户不允许扫描所有分区。进行这个限制的原因是，通常分区表都拥有非常大的数据集，而且数据增加迅速。没有进行分区限制的查询可能会消耗令人不可接受的巨大资源来处理这个表。 2）对于使用了order by语句的查询，要求必须使用limit语句。因为order by为了执行排序过程会将所有的结果数据分发到同一个Reducer中进行处理，强制要求用户增加这个LIMIT语句可以防止Reducer额外执行很长一段时间。 3）限制笛卡尔积的查询。对关系型数据库非常了解的用户可能期望在执行JOIN查询的时候不使用ON语句而是使用where语句，这样关系数据库的执行优化器就可以高效地将WHERE语句转化成那个ON语句。不幸的是，Hive并不会执行这种优化，因此，如果表足够大，那么这个查询就会出现不可控的情况。 10.10 JVM重用JVM重用是Hadoop调优参数的内容，其对Hive的性能具有非常大的影响，特别是对于很难避免小文件的场景或task特别多的场景，这类场景大多数执行时间都很短。 Hadoop的默认配置通常是使用派生JVM来执行map和Reduce任务的。这时JVM的启动过程可能会造成相当大的开销，尤其是执行的job包含有成百上千task任务的情况。JVM重用可以使得JVM实例在同一个job中重新使用N次。N的值可以在Hadoop的mapred-site.xml文件中进行配置。通常在10-20之间，具体多少需要根据具体业务场景测试得出。 我们也可以在hive当中通过 1set mapred.job.reuse.jvm.num.tasks=10; 这个设置来设置我们的jvm重用 这个功能的缺点是，开启JVM重用将一直占用使用到的task插槽，以便进行重用，直到任务完成后才能释放。如果某个“不平衡的”job中有某几个reduce task执行的时间要比其他Reduce task消耗的时间多的多的话，那么保留的插槽就会一直空闲着却无法被其他的job使用，直到所有的task都结束了才会释放。 10.11 推测执行在分布式集群环境下，因为程序Bug（包括Hadoop本身的bug），负载不均衡或者资源分布不均等原因，会造成同一个作业的多个任务之间运行速度不一致，有些任务的运行速度可能明显慢于其他任务（比如一个作业的某个任务进度只有50%，而其他所有任务已经运行完毕），则这些任务会拖慢作业的整体执行进度。为了避免这种情况发生，Hadoop采用了推测执行（Speculative Execution）机制，它根据一定的法则推测出“拖后腿”的任务，并为这样的任务启动一个备份任务，让该任务与原始任务同时处理同一份数据，并最终选用最先成功运行完成任务的计算结果作为最终结果。 设置开启推测执行参数： 123set mapred.map.tasks.speculative.execution=trueset mapred.reduce.tasks.speculative.execution=trueset hive.mapred.reduce.tasks.speculative.execution=true; 关于调优这些推测执行变量，还很难给一个具体的建议。如果用户对于运行时的偏差非常敏感的话，那么可以将这些功能关闭掉。如果用户因为输入数据量很大而需要执行长时间的map或者Reduce task的话，那么启动推测执行造成的浪费是非常巨大大。]]></content>
      <tags>
        <tag>BigData</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Yarn资源调度]]></title>
    <url>%2F2017%2F07%2F06%2FYarn%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6.html</url>
    <content type="text"><![CDATA[Yarn资源调度详解1.yarn的介绍：​ yarn是hadoop集群当中的资源管理系统模块，从hadoop2.0开始引入yarn模块,yarn可为各类计算框架提供资源的管理和调度,主要用于管理集群当中的资源（主要是服务器的各种硬件资源，包括CPU，内存，磁盘，网络IO等）以及调度运行在yarn上面的各种任务。 yarn核心出发点是为了分离资源管理与作业监控，实现分离的做法是拥有一个全局的资源管理（ResourceManager，RM），以及每个应用程序对应一个的应用管理器（ApplicationMaster，AM） ​ 总结一句话就是说：yarn主要就是为了调度资源，管理任务等 其调度分为两个层级来说： 一级调度管理： ​ 计算资源管理(CPU,内存，网络IO，磁盘) 二级调度管理： ​ 任务内部的计算模型管理 (AppMaster的任务精细化管理) yarn的官网文档说明： http://hadoop.apache.org/docs/r2.7.5/hadoop-yarn/hadoop-yarn-site/YARN.html yarn集群的监控管理界面： http://node01:8088/cluster jobHistoryServer查看界面： http://node01:19888/jobhistory 2.Yarn的主要组件介绍与作用 YARN总体上是Master/Slave结构，主要由ResourceManager、NodeManager、 ApplicationMaster和Container等几个组件构成。 ResourceManager(RM)负责处理客户端请求,对各NM上的资源进行统一管理和调度。给ApplicationMaster分配空闲的Container 运行并监控其运行状态。主要由两个组件构成：调度器和应用程序管理器： 调度器(Scheduler)：调度器根据容量、队列等限制条件，将系统中的资源分配给各个正在运行的应用程序。调度器仅根据各个应用程序的资源需求进行资源分配，而资源分配单位是Container。Shceduler不负责监控或者跟踪应用程序的状态。总之，调度器根据应用程序的资源要求，以及集群机器的资源情况，为应用程序分配封装在Container中的资源。 应用程序管理器(Applications Manager)：应用程序管理器负责管理整个系统中所有应用程序，包括应用程序提交、与调度器协商资源以启动ApplicationMaster 、监控ApplicationMaster运行状态并在失败时重新启动等，跟踪分给的Container的进度、状态也是其职责。 NodeManager (NM)NodeManager 是每个节点上的资源和任务管理器。它会定时地向ResourceManager汇报本节点上的资源使用情况和各个Container的运行状态；同时会接收并处理来自ApplicationMaster 的Container 启动/停止等请求。 ApplicationMaster (AM)：用户提交的应用程序均包含一个ApplicationMaster ，负责应用的监控，跟踪应用执行状态，重启失败任务等。ApplicationMaster是应用框架，它负责向ResourceManager协调资源，并且与NodeManager协同工作完成Task的执行和监控。 Container：Container是YARN中的资源抽象，它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等，当ApplicationMaster向ResourceManager申请资源时，ResourceManager为ApplicationMaster 返回的资源便是用Container 表示的。 3.yarn的架构和工作流程 4.yarn的调度器yarn我们都知道主要是用于做资源调度，任务分配等功能的，那么在hadoop当中，究竟使用什么算法来进行任务调度就需要我们关注了，hadoop支持好几种任务的调度方式，不同的场景需要使用不同的任务调度器. 第一种调度器：FIFO Scheduler（队列调度）把任务按提交的顺序排成一个队列，这是一个先进先出队列，在进行资源分配的时候，先给队列中最头上的任务进行分配资源，待最头上任务需求满足后再给下一个分配，以此类推。 FIFO Scheduler是最简单也是最容易理解的调度器，也不需要任何配置，但它并不适用于共享集群。大的任务可能会占用所有集群资源，这就导致其它任务被阻塞。 第二种调度器：Capacity Scheduler（容量调度器，apache版本默认使用的调度器）Capacity 调度器允许多个组织共享整个集群，每个组织可以获得集群的一部分计算能力。通过为每个组织分配专门的队列，然后再为每个队列分配一定的集群资源，这样整个集群就可以通过设置多个队列的方式给多个组织提供服务了。除此之外，队列内部又可以垂直划分，这样一个组织内部的多个成员就可以共享这个队列资源了，在一个队列内部，资源的调度是采用的是先进先出(FIFO)策略。 第三种调度器：Fair Scheduler（公平调度器，CDH版本的hadoop默认使用的调度器）Fair调度器的设计目标是为所有的应用分配公平的资源（对公平的定义可以通过参数来设置）。公平调度在也可以在多个队列间工作。举个例子，假设有两个用户A和B，他们分别拥有一个队列。当A启动一个job而B没有任务时，A会获得全部集群资源；当B启动一个job后，A的job会继续运行，不过一会儿之后两个任务会各自获得一半的集群资源。如果此时B再启动第二个job并且其它job还在运行，则它将会和B的第一个job共享B这个队列的资源，也就是B的两个job会用于四分之一的集群资源，而A的job仍然用于集群一半的资源，结果就是资源最终在两个用户之间平等的共享 使用哪种调度器取决于yarn-site.xml当中的 yarn.resourcemanager.scheduler.class 这个属性的配置 5.关于yarn常用参数设置设置container分配最小内存 yarn.scheduler.minimum-allocation-mb 1024 给应用程序container分配的最小内存 设置container分配最大内存 yarn.scheduler.maximum-allocation-mb 8192 给应用程序container分配的最大内存 设置每个container的最小虚拟内核个数yarn.scheduler.minimum-allocation-vcores 1 每个container默认给分配的最小的虚拟内核个数 设置每个container的最大虚拟内核个数 yarn.scheduler.maximum-allocation-vcores 32 每个container可以分配的最大的虚拟内核的个数 设置NodeManager可以分配的内存大小yarn.nodemanager.resource.memory-mb 8192 nodemanager可以分配的最大内存大小，默认8192Mb 定义每台机器的内存使用大小 yarn.nodemanager.resource.memory-mb 8192 定义交换区空间可以使用的大小 交换区空间就是讲一块硬盘拿出来做内存使用,这里指定的是nodemanager的2.1倍 yarn.nodemanager.vmem-pmem-ratio 2.1]]></content>
      <tags>
        <tag>BigData</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce2]]></title>
    <url>%2F2017%2F07%2F06%2FMapReduce2.html</url>
    <content type="text"><![CDATA[MapReduce详解1 .MapReduce的运行机制详解全流程: 1.1:MapTask 工作机制简单概述：inputFile通过split被逻辑切分为多个split文件，通过Record按行读取内容给map（用户自己实现的）进行处理，数据被map处理结束之后交给OutputCollector收集器，对其结果key进行分区（默认使用hash分区），然后写入buffer，每个map task都有一个内存缓冲区，存储着map的输出结果，当缓冲区快满的时候需要将缓冲区的数据以一个临时文件的方式存放到磁盘，当整个map task结束后再对磁盘中这个map task产生的所有临时文件做合并，生成最终的正式输出文件，然后等待reduce task来拉数据 详细步骤 读取数据组件 InputFormat (默认 TextInputFormat) 会通过 getSplits 方法对输入目录中文件进行逻辑切片规划得到 block, 有多少个 block就对应启动多少个 MapTask。 将输入文件切分为 block 之后, 由 RecordReader 对象 (默认是LineRecordReader) 进行读取, 以 \n 作为分隔符, 读取一行数据, 返回 &lt;key，value&gt;. Key 表示每行首字符偏移值, Value 表示这一行文本内容 读取 block 返回 &lt;key,value&gt;, 进入用户自己继承的 Mapper 类中，执行用户重写的 map 函数, RecordReader 读取一行这里调用一次 Mapper 逻辑结束之后, 将 Mapper 的每条结果通过 context.write 进行collect数据收集. 在 collect 中, 会先对其进行分区处理，默认使用 HashPartitioner MapReduce 提供 Partitioner 接口, 它的作用就是根据 Key 或 Value 及 Reducer 的数量来决定当前的这对输出数据最终应该交由哪个 Reduce task 处理, 默认对 Key Hash 后再以 Reducer 数量取模. 默认的取模方式只是为了平均 Reducer 的处理能力, 如果用户自己对 Partitioner 有需求, 可以订制并设置到 Job 上 接下来, 会将数据写入内存, 内存中这片区域叫做环形缓冲区, 缓冲区的作用是批量收集 Mapper 结果, 减少磁盘 IO 的影响. 我们的 Key/Value 对以及 Partition 的结果都会被写入缓冲区. 当然, 写入之前，Key 与 Value 值都会被序列化成字节数组 环形缓冲区其实是一个数组, 数组中存放着 Key, Value 的序列化数据和 Key, Value 的元数据信息, 包括 Partition, Key 的起始位置, Value 的起始位置以及 Value 的长度，环形结构是一个抽象概念 缓冲区是有大小限制, 默认是 100MB，当 Mapper 的输出结果很多时, 就可能会撑爆内存, 所以需要在一定条件下将缓冲区中的数据临时写入磁盘, 然后重新利用这块缓冲区， 这个从内存往磁盘写数据的过程被称为 Spill, 中文可译为溢写，这个溢写是由单独线程来完成, 不影响往缓冲区写 Mapper 结果的线程. 溢写线程启动时不应该阻止 Mapper 的结果输出, 所以整个缓冲区有个溢写的比例 spill.percent，这个比例默认是 0.8, 也就是当缓冲区的数据已经达到阈值 buffer size * spill percent = 100MB * 0.8 = 80MB, 溢写线程启动, 锁定这 80MB 的内存, 执行溢写过程. Mapper 的输出结果还可以往剩下的 20MB 内存中写, 互不影响 当溢写线程启动后, 需要对这 80MB 空间内的 Key 做排序 (Sort). 排序是 MapReduce 模型默认的行为, 这里的排序也是对序列化的字节做的排序 如果 Job 设置过 Combiner, 那么现在就是使用 Combiner 的时候了. 将有相同 Key 的 Key/Value 对的 Value 加起来, 减少溢写到磁盘的数据量. Combiner 会优化 MapReduce 的中间结果, 所以它在整个模型中会多次使用 那哪些场景才能使用 Combiner 呢? 从这里分析, Combiner 的输出是 Reducer 的输入, Combiner 绝不能改变最终的计算结果. Combiner 只应该用于那种 Reduce 的输入 Key/Value 与输出 Key/Value 类型完全一致, 且不影响最终结果的场景. 比如累加, 最大值等. Combiner 的使用一定得慎重, 如果用好, 它对 Job 执行效率有帮助, 反之会影响 Reducer 的最终结果 合并溢写文件, 每次溢写会在磁盘上生成一个临时文件 (写之前判断是否有 Combiner), 如果 Mapper 的输出结果真的很大, 有多次这样的溢写发生, 磁盘上相应的就会有多个临时文件存在. 当整个数据处理结束之后开始对磁盘中的临时文件进行 Merge 合并, 因为最终的文件只有一个, 写入磁盘, 并且为这个文件提供了一个索引文件, 以记录每个reduce对应数据的偏移量 配置 配置 默认值 解释 mapreduce.task.io.sort.mb 100 设置环型缓冲区的内存值大小 mapreduce.map.sort.spill.percent 0.8 设置溢写的比例 mapreduce.cluster.local.dir ${hadoop.tmp.dir}/mapred/local 溢写数据目录 mapreduce.task.io.sort.factor 10 设置一次合并多少个溢写文件 1.2 :ReduceTask 工作机制Reduce 大致分为 copy、sort、reduce 三个阶段，重点在前两个阶段。copy 阶段包含一个 eventFetcher 来获取已完成的 map 列表，由 Fetcher 线程去 copy 数据，在此过程中会启动两个 merge 线程，分别为 inMemoryMerger 和 onDiskMerger，分别将内存中的数据 merge 到磁盘和将磁盘中的数据进行 merge。待数据 copy 完成之后，copy 阶段就完成了，开始进行 sort 阶段，sort 阶段主要是执行 finalMerge 操作，纯粹的 sort 阶段，完成之后就是 reduce 阶段，调用用户定义的 reduce 函数进行处理 详细步骤 Copy阶段，简单地拉取数据。Reduce进程启动一些数据copy线程(Fetcher)，通过HTTP方式请求maptask获取属于自己的文件。 Merge阶段。这里的merge如map端的merge动作，只是数组中存放的是不同map端copy来的数值。Copy过来的数据会先放入内存缓冲区中，这里的缓冲区大小要比map端的更为灵活。merge有三种形式：内存到内存；内存到磁盘；磁盘到磁盘。默认情况下第一种形式不启用。当内存中的数据量到达一定阈值，就启动内存到磁盘的merge。与map 端类似，这也是溢写的过程，这个过程中如果你设置有Combiner，也是会启用的，然后在磁盘中生成了众多的溢写文件。第二种merge方式一直在运行，直到没有map端的数据时才结束，然后启动第三种磁盘到磁盘的merge方式生成最终的文件。 合并排序。把分散的数据合并成一个大的数据后，还会再对合并后的数据排序。 对排序后的键值对调用reduce方法，键相等的键值对调用一次reduce方法，每次调用会产生零个或者多个键值对，最后把这些输出的键值对写入到HDFS文件中。 1.3:Shuffle 过程map 阶段处理的数据如何传递给 reduce 阶段，是 MapReduce 框架中最关键的一个流程，这个流程就叫 shuffleshuffle: 洗牌、发牌 ——（核心机制：数据分区，排序，分组，规约，合并等过程） shuffle 是 Mapreduce 的核心，它分布在 Mapreduce 的 map 阶段和 reduce 阶段。一般把从 Map 产生输出开始到 Reduce 取得数据作为输入之前的过程称作 shuffle。 Collect阶段：将 MapTask 的结果输出到默认大小为 100M 的环形缓冲区，保存的是 key/value，Partition 分区信息等。 Spill阶段：当内存中的数据量达到一定的阀值的时候，就会将数据写入本地磁盘，在将数据写入磁盘之前需要对数据进行一次排序的操作，如果配置了 combiner，还会将有相同分区号和 key 的数据进行排序。 Merge阶段：把所有溢出的临时文件进行一次合并操作，以确保一个 MapTask 最终只产生一个中间数据文件。 Copy阶段：ReduceTask 启动 Fetcher 线程到已经完成 MapTask 的节点上复制一份属于自己的数据，这些数据默认会保存在内存的缓冲区中，当内存的缓冲区达到一定的阀值的时候，就会将数据写到磁盘之上。 Merge阶段：在 ReduceTask 远程复制数据的同时，会在后台开启两个线程对内存到本地的数据文件进行合并操作。 Sort阶段：在对数据进行合并的同时，会进行排序操作，由于 MapTask 阶段已经对数据进行了局部的排序，ReduceTask 只需保证 Copy 的数据的最终整体有效性即可。Shuffle 中的缓冲区大小会影响到 mapreduce 程序的执行效率，原则上说，缓冲区越大，磁盘io的次数越少，执行速度就越快缓冲区的大小可以通过参数调整, 参数：mapreduce.task.io.sort.mb 默认100M 2. 案例: Reduce 端实现 JOIN 2.1. 需求 假如数据量巨大，两表的数据是以文件的形式存储在 HDFS 中, 需要用 MapReduce 程序来实现以下 SQL 查询运算 12&gt; select a.id,a.date,b.name,b.category_id,b.price from t_order a left join t_product b on a.pid = b.id&gt; 商品表 id pname category_id price P0001 小米5 1000 2000 P0002 锤子T1 1000 3000 订单数据表 id date pid amount 1001 20150710 P0001 2 1002 20150710 P0002 3 2.2 实现步骤通过将关联的条件作为map输出的key，将两表满足join条件的数据并携带数据所来源的文件信息，发往同一个reduce task，在reduce中进行数据的串联 Step 1: 定义 Mapper1234567891011121314151617181920212223242526public class ReduceJoinMapper extends Mapper&lt;LongWritable,Text,Text,Text&gt; &#123; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; //1:判断数据来自哪个文件 FileSplit fileSplit = (FileSplit) context.getInputSplit(); String fileName = fileSplit.getPath().getName(); if(fileName.equals("product.txt"))&#123; //数据来自商品表 //2:将K1和V1转为K2和V2,写入上下文中 String[] split = value.toString().split(","); String productId = split[0]; context.write(new Text(productId), value); &#125;else&#123; //数据来自订单表 //2:将K1和V1转为K2和V2,写入上下文中 String[] split = value.toString().split(","); String productId = split[2]; context.write(new Text(productId), value); &#125; &#125;&#125; Step 2: 定义 Reducer1234567891011121314151617181920212223242526272829public class ReduceJoinMapper extends Mapper&lt;LongWritable,Text,Text,Text&gt; &#123; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; //1:判断数据来自哪个文件 FileSplit fileSplit = (FileSplit) context.getInputSplit(); String fileName = fileSplit.getPath().getName(); if(fileName.equals("product.txt"))&#123; //数据来自商品表 //2:将K1和V1转为K2和V2,写入上下文中 String[] split = value.toString().split(","); String productId = split[0]; context.write(new Text(productId), value); &#125;else&#123; //数据来自订单表 //2:将K1和V1转为K2和V2,写入上下文中 String[] split = value.toString().split(","); String productId = split[2]; context.write(new Text(productId), value); &#125; &#125;&#125; Step 3: 定义主类123456789101112131415161718public class ReduceJoinReducer extends Reducer&lt;Text,Text,Text,Text&gt; &#123; @Override protected void reduce(Text key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException &#123; //1:遍历集合,获取V3 (first +second) String first = ""; String second = ""; for (Text value : values) &#123; if(value.toString().startsWith("p"))&#123; first = value.toString(); &#125;else&#123; second += value.toString(); &#125; &#125; //2:将K3和V3写入上下文中 context.write(key, new Text(first+"\t"+second)); &#125;&#125; 3. 案例: Map端实现 JOIN 3.1 概述​ 适用于关联表中有小表的情形. ​ 使用分布式缓存,可以将小表分发到所有的map节点，这样，map节点就可以在本地对自己所读到的大表数据进行join并输出最终结果，可以大大提高join操作的并发度，加快处理速度 3.2 实现步骤 先在mapper类中预先定义好小表，进行join 引入实际场景中的解决方案：一次加载数据库或者用 Step 1：定义Mapper1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public class MapJoinMapper extends Mapper&lt;LongWritable,Text,Text,Text&gt;&#123; private HashMap&lt;String, String&gt; map = new HashMap&lt;&gt;(); //第一件事情:将分布式缓存的小表数据读取到本地Map集合(只需要做一次) @Override protected void setup(Context context) throws IOException, InterruptedException &#123; //1:获取分布式缓存文件列表 URI[] cacheFiles = context.getCacheFiles(); //2:获取指定的分布式缓存文件的文件系统(FileSystem) FileSystem fileSystem = FileSystem.get(cacheFiles[0], context.getConfiguration()); //3:获取文件的输入流 FSDataInputStream inputStream = fileSystem.open(new Path(cacheFiles[0])); //4:读取文件内容, 并将数据存入Map集合 //4.1 将字节输入流转为字符缓冲流FSDataInputStream ---&gt;BufferedReader BufferedReader bufferedReader = new BufferedReader(new InputStreamReader(inputStream)); //4.2 读取小表文件内容,以行位单位,并将读取的数据存入map集合 String line = null; while((line = bufferedReader.readLine()) != null)&#123; String[] split = line.split(","); map.put(split[0], line); &#125; //5:关闭流 bufferedReader.close(); fileSystem.close(); &#125; //第二件事情:对大表的处理业务逻辑,而且要实现大表和小表的join操作 @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; //1:从行文本数据中获取商品的id: p0001 , p0002 得到了K2 String[] split = value.toString().split(","); String productId = split[2]; //K2 //2:在Map集合中,将商品的id作为键,获取值(商品的行文本数据) ,将value和值拼接,得到V2 String productLine = map.get(productId); String valueLine = productLine+"\t"+value.toString(); //V2 //3:将K2和V2写入上下文中 context.write(new Text(productId), new Text(valueLine)); &#125;&#125; Step 2：定义主类123456789101112131415161718192021222324252627282930313233343536public class JobMain extends Configured implements Tool&#123; @Override public int run(String[] args) throws Exception &#123; //1:获取job对象 Job job = Job.getInstance(super.getConf(), "map_join_job"); //2:设置job对象(将小表放在分布式缓存中) //将小表放在分布式缓存中 // DistributedCache.addCacheFile(new URI("hdfs://node01:8020/cache_file/product.txt"), super.getConf()); job.addCacheFile(new URI("hdfs://node01:8020/cache_file/product.txt")); //第一步:设置输入类和输入的路径 job.setInputFormatClass(TextInputFormat.class); TextInputFormat.addInputPath(job, new Path("file:///D:\\input\\map_join_input")); //第二步:设置Mapper类和数据类型 job.setMapperClass(MapJoinMapper.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(Text.class); //第八步:设置输出类和输出路径 job.setOutputFormatClass(TextOutputFormat.class); TextOutputFormat.setOutputPath(job, new Path("file:///D:\\out\\map_join_out")); //3:等待任务结束 boolean bl = job.waitForCompletion(true); return bl ? 0 :1; &#125; public static void main(String[] args) throws Exception &#123; Configuration configuration = new Configuration(); //启动job任务 int run = ToolRunner.run(configuration, new JobMain(), args); System.exit(run); &#125;&#125; 4. 案例:求共同好友分析图: 4.1 需求分析以下是qq的好友列表数据，冒号前是一个用户，冒号后是该用户的所有好友（数据中的好友关系是单向的） 1234567891011121314A:B,C,D,F,E,OB:A,C,E,KC:A,B,D,E,I D:A,E,F,LE:B,C,D,M,LF:A,B,C,D,E,O,MG:A,C,D,E,FH:A,C,D,E,OI:A,OJ:B,OK:A,C,DL:D,E,FM:E,F,GO:A,H,I,J 求出哪些人两两之间有共同好友，及他俩的共同好友都有谁？ 4.2 实现步骤第一步：代码实现Mapper类 123456789101112131415public class Step1Mapper extends Mapper&lt;LongWritable,Text,Text,Text&gt; &#123; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; //1:以冒号拆分行文本数据: 冒号左边就是V2 String[] split = value.toString().split(":"); String userStr = split[0]; //2:将冒号右边的字符串以逗号拆分,每个成员就是K2 String[] split1 = split[1].split(","); for (String s : split1) &#123; //3:将K2和v2写入上下文中 context.write(new Text(s), new Text(userStr)); &#125; &#125;&#125; Reducer类: 1234567891011121314public class Step1Reducer extends Reducer&lt;Text,Text,Text,Text&gt; &#123; @Override protected void reduce(Text key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException &#123; //1:遍历集合,并将每一个元素拼接,得到K3 StringBuffer buffer = new StringBuffer(); for (Text value : values) &#123; buffer.append(value.toString()).append("-"); &#125; //2:K2就是V3 //3:将K3和V3写入上下文中 context.write(new Text(buffer.toString()), key); &#125;&#125; JobMain: 12345678910111213141516171819202122232425262728293031323334353637383940414243public class JobMain extends Configured implements Tool &#123; @Override public int run(String[] args) throws Exception &#123; //1:获取Job对象 Job job = Job.getInstance(super.getConf(), "common_friends_step1_job"); //2:设置job任务 //第一步:设置输入类和输入路径 job.setInputFormatClass(TextInputFormat.class); TextInputFormat.addInputPath(job, new Path("file:///D:\\input\\common_friends_step1_input")); //第二步:设置Mapper类和数据类型 job.setMapperClass(Step1Mapper.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(Text.class); //第三,四,五,六 //第七步:设置Reducer类和数据类型 job.setReducerClass(Step1Reducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(Text.class); //第八步:设置输出类和输出的路径 job.setOutputFormatClass(TextOutputFormat.class); TextOutputFormat.setOutputPath(job, new Path("file:///D:\\out\\common_friends_step1_out")); //3:等待job任务结束 boolean bl = job.waitForCompletion(true); return bl ? 0: 1; &#125; public static void main(String[] args) throws Exception &#123; Configuration configuration = new Configuration(); //启动job任务 int run = ToolRunner.run(configuration, new JobMain(), args); System.exit(run); &#125;&#125; 第二步：代码实现Mapper类 1234567891011121314151617181920212223242526272829303132333435363738394041424344public class Step2Mapper extends Mapper&lt;LongWritable,Text,Text,Text&gt; &#123; /* K1 V1 0 A-F-C-J-E- B ---------------------------------- K2 V2 A-C B A-E B A-F B C-E B */ @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; //1:拆分行文本数据,结果的第二部分可以得到V2 String[] split = value.toString().split("\t"); String friendStr =split[1]; //2:继续以'-'为分隔符拆分行文本数据第一部分,得到数组 String[] userArray = split[0].split("-"); //3:对数组做一个排序 Arrays.sort(userArray); //4:对数组中的元素进行两两组合,得到K2 /* A-E-C -----&gt; A C E A C E A C E */ for (int i = 0; i &lt;userArray.length -1 ; i++) &#123; for (int j = i+1; j &lt; userArray.length ; j++) &#123; //5:将K2和V2写入上下文中 context.write(new Text(userArray[i] +"-"+userArray[j]), new Text(friendStr)); &#125; &#125; &#125;&#125; Reducer类: 1234567891011121314public class Step2Reducer extends Reducer&lt;Text,Text,Text,Text&gt; &#123; @Override protected void reduce(Text key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException &#123; //1:原来的K2就是K3 //2:将集合进行遍历,将集合中的元素拼接,得到V3 StringBuffer buffer = new StringBuffer(); for (Text value : values) &#123; buffer.append(value.toString()).append("-"); &#125; //3:将K3和V3写入上下文中 context.write(key, new Text(buffer.toString())); &#125;&#125; JobMain: 123456789101112131415161718192021222324252627282930313233343536373839public class JobMain extends Configured implements Tool &#123; @Override public int run(String[] args) throws Exception &#123; //1:获取Job对象 Job job = Job.getInstance(super.getConf(), "common_friends_step2_job"); //2:设置job任务 //第一步:设置输入类和输入路径 job.setInputFormatClass(TextInputFormat.class); TextInputFormat.addInputPath(job, new Path("file:///D:\\out\\common_friends_step1_out")); //第二步:设置Mapper类和数据类型 job.setMapperClass(Step2Mapper.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(Text.class); //第三,四,五,六 //第七步:设置Reducer类和数据类型 job.setReducerClass(Step2Reducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(Text.class); //第八步:设置输出类和输出的路径 job.setOutputFormatClass(TextOutputFormat.class); TextOutputFormat.setOutputPath(job, new Path("file:///D:\\out\\common_friends_step2_out")); //3:等待job任务结束 boolean bl = job.waitForCompletion(true); return bl ? 0: 1; &#125; public static void main(String[] args) throws Exception &#123; Configuration configuration = new Configuration(); //启动job任务 int run = ToolRunner.run(configuration, new JobMain(), args); System.exit(run); &#125;&#125; 1. 自定义InputFormat合并小文件 1.1 需求无论hdfs还是mapreduce，对于小文件都有损效率，实践中，又难免面临处理大量小文件的场景，此时，就需要有相应解决方案 1.2 分析小文件的优化无非以下几种方式： 1、 在数据采集的时候，就将小文件或小批数据合成大文件再上传HDFS 2、 在业务处理之前，在HDFS上使用mapreduce程序对小文件进行合并 3、 在mapreduce处理时，可采用combineInputFormat提高效率 1.3 实现本节实现的是上述第二种方式 程序的核心机制： 自定义一个InputFormat 改写RecordReader，实现一次读取一个完整文件封装为KV 在输出时使用SequenceFileOutPutFormat输出合并文件 代码如下： 自定义InputFromat123456789101112131415161718public class MyInputFormat extends FileInputFormat&lt;NullWritable,BytesWritable&gt; &#123; @Override public RecordReader&lt;NullWritable, BytesWritable&gt; createRecordReader(InputSplit inputSplit, TaskAttemptContext taskAttemptContext) throws IOException, InterruptedException &#123; //1:创建自定义RecordReader对象 MyRecordReader myRecordReader = new MyRecordReader(); //2:将inputSplit和context对象传给MyRecordReader myRecordReader.initialize(inputSplit, taskAttemptContext); return myRecordReader; &#125; /* 设置文件是否可以被切割 */ @Override protected boolean isSplitable(JobContext context, Path filename) &#123; return false; &#125;&#125; 自定义RecordReader12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273public class MyRecordReader extends RecordReader&lt;NullWritable,BytesWritable&gt;&#123; private Configuration configuration = null; private FileSplit fileSplit = null; private boolean processed = false; private BytesWritable bytesWritable = new BytesWritable(); private FileSystem fileSystem = null; private FSDataInputStream inputStream = null; //进行初始化工作 @Override public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext) throws IOException, InterruptedException &#123; //获取文件的切片 fileSplit= (FileSplit)inputSplit; //获取Configuration对象 configuration = taskAttemptContext.getConfiguration(); &#125; //该方法用于获取K1和V1 /* K1: NullWritable V1: BytesWritable */ @Override public boolean nextKeyValue() throws IOException, InterruptedException &#123; if(!processed)&#123; //1:获取源文件的字节输入流 //1.1 获取源文件的文件系统 (FileSystem) fileSystem = FileSystem.get(configuration); //1.2 通过FileSystem获取文件字节输入流 inputStream = fileSystem.open(fileSplit.getPath()); //2:读取源文件数据到普通的字节数组(byte[]) byte[] bytes = new byte[(int) fileSplit.getLength()]; IOUtils.readFully(inputStream, bytes, 0, (int)fileSplit.getLength()); //3:将字节数组中数据封装到BytesWritable ,得到v1 bytesWritable.set(bytes, 0, (int)fileSplit.getLength()); processed = true; return true; &#125; return false; &#125; //返回K1 @Override public NullWritable getCurrentKey() throws IOException, InterruptedException &#123; return NullWritable.get(); &#125; //返回V1 @Override public BytesWritable getCurrentValue() throws IOException, InterruptedException &#123; return bytesWritable; &#125; //获取文件读取的进度 @Override public float getProgress() throws IOException, InterruptedException &#123; return 0; &#125; //进行资源释放 @Override public void close() throws IOException &#123; inputStream.close(); fileSystem.close(); &#125;&#125; Mapper类:1234567891011public class SequenceFileMapper extends Mapper&lt;NullWritable,BytesWritable,Text,BytesWritable&gt; &#123; @Override protected void map(NullWritable key, BytesWritable value, Context context) throws IOException, InterruptedException &#123; //1:获取文件的名字,作为K2 FileSplit fileSplit = (FileSplit) context.getInputSplit(); String fileName = fileSplit.getPath().getName(); //2:将K2和V2写入上下文中 context.write(new Text(fileName), value); &#125;&#125; 主类:1234567891011121314151617181920212223242526272829303132333435363738public class JobMain extends Configured implements Tool &#123; @Override public int run(String[] args) throws Exception &#123; //1:获取job对象 Job job = Job.getInstance(super.getConf(), "sequence_file_job"); //2:设置job任务 //第一步:设置输入类和输入的路径 job.setInputFormatClass(MyInputFormat.class); MyInputFormat.addInputPath(job, new Path("file:///D:\\input\\myInputformat_input")); //第二步:设置Mapper类和数据类型 job.setMapperClass(SequenceFileMapper.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(BytesWritable.class); //第七步: 不需要设置Reducer类,但是必须设置数据类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(BytesWritable.class); //第八步:设置输出类和输出的路径 job.setOutputFormatClass(SequenceFileOutputFormat.class); SequenceFileOutputFormat.setOutputPath(job, new Path("file:///D:\\out\\myinputformat_out")); //3:等待job任务执行结束 boolean bl = job.waitForCompletion(true); return bl ? 0 : 1; &#125; public static void main(String[] args) throws Exception &#123; Configuration configuration = new Configuration(); int run = ToolRunner.run(configuration, new JobMain(), args); System.exit(run); &#125;&#125; 2. 自定义outputFormat 2.1 需求现在有一些订单的评论数据，需求，将订单的好评与差评进行区分开来，将最终的数据分开到不同的文件夹下面去，数据内容参见资料文件夹，其中数据第九个字段表示好评，中评，差评。0：好评，1：中评，2：差评 2.2 分析程序的关键点是要在一个mapreduce程序中根据数据的不同输出两类结果到不同目录，这类灵活的输出需求可以通过自定义outputformat来实现 2.3 实现实现要点： 1、 在mapreduce中访问外部资源 2、 自定义outputformat，改写其中的recordwriter，改写具体输出数据的方法write() 第一步：自定义MyOutputFormatMyOutputFormat类: 1234567891011121314public class MyOutputFormat extends FileOutputFormat&lt;Text,NullWritable&gt; &#123; @Override public RecordWriter&lt;Text, NullWritable&gt; getRecordWriter(TaskAttemptContext taskAttemptContext) throws IOException, InterruptedException &#123; //1:获取目标文件的输出流(两个) FileSystem fileSystem = FileSystem.get(taskAttemptContext.getConfiguration()); FSDataOutputStream goodCommentsOutputStream = fileSystem.create(new Path("file:///D:\\out\\good_comments\\good_comments.txt")); FSDataOutputStream badCommentsOutputStream = fileSystem.create(new Path("file:///D:\\out\\bad_comments\\bad_comments.txt")); //2:将输出流传给MyRecordWriter MyRecordWriter myRecordWriter = new MyRecordWriter(goodCommentsOutputStream,badCommentsOutputStream); return myRecordWriter; &#125;&#125; MyRecordReader类: 1234567891011121314151617181920212223242526272829303132333435363738394041424344public class MyRecordWriter extends RecordWriter&lt;Text,NullWritable&gt; &#123; private FSDataOutputStream goodCommentsOutputStream; private FSDataOutputStream badCommentsOutputStream; public MyRecordWriter() &#123; &#125; public MyRecordWriter(FSDataOutputStream goodCommentsOutputStream, FSDataOutputStream badCommentsOutputStream) &#123; this.goodCommentsOutputStream = goodCommentsOutputStream; this.badCommentsOutputStream = badCommentsOutputStream; &#125; /** * * @param text 行文本内容 * @param nullWritable * @throws IOException * @throws InterruptedException */ @Override public void write(Text text, NullWritable nullWritable) throws IOException, InterruptedException &#123; //1:从行文本数据中获取第9个字段 String[] split = text.toString().split("\t"); String numStr = split[9]; //2:根据字段的值,判断评论的类型,然后将对应的数据写入不同的文件夹文件中 if(Integer.parseInt(numStr) &lt;= 1)&#123; //好评或者中评 goodCommentsOutputStream.write(text.toString().getBytes()); goodCommentsOutputStream.write("\r\n".getBytes()); &#125;else&#123; //差评 badCommentsOutputStream.write(text.toString().getBytes()); badCommentsOutputStream.write("\r\n".getBytes()); &#125; &#125; @Override public void close(TaskAttemptContext taskAttemptContext) throws IOException, InterruptedException &#123; IOUtils.closeStream(goodCommentsOutputStream); IOUtils.closeStream(badCommentsOutputStream); &#125;&#125; 第二步：自定义Mapper类123456public class MyOutputFormatMapper extends Mapper&lt;LongWritable,Text,Text,NullWritable&gt; &#123; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; context.write(value, NullWritable.get()); &#125;&#125; 第三步:主类JobMain1234567891011121314151617181920212223242526272829303132public class JobMain extends Configured implements Tool &#123; @Override public int run(String[] args) throws Exception &#123; //1:获取job对象 Job job = Job.getInstance(super.getConf(), "myoutputformat_job"); //2:设置job任务 //第一步:设置输入类和输入的路径 job.setInputFormatClass(TextInputFormat.class); TextInputFormat.addInputPath(job, new Path("file:///D:\\input\\myoutputformat_input")); //第二步:设置Mapper类和数据类型 job.setMapperClass(MyOutputFormatMapper.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(NullWritable.class); //第八步:设置输出类和输出的路径 job.setOutputFormatClass(MyOutputFormat.class); MyOutputFormat.setOutputPath(job, new Path("file:///D:\\out\\myoutputformat_out")); //3:等待任务结束 boolean bl = job.waitForCompletion(true); return bl ? 0 : 1; &#125; public static void main(String[] args) throws Exception &#123; Configuration configuration = new Configuration(); int run = ToolRunner.run(configuration, new JobMain(), args); System.exit(run); &#125;&#125; 3. 自定义分组求取topN 分组是mapreduce当中reduce端的一个功能组件，主要的作用是决定哪些数据作为一组，调用一次reduce的逻辑，默认是每个不同的key，作为多个不同的组，每个组调用一次reduce逻辑，我们可以自定义分组实现不同的key作为同一个组，调用一次reduce逻辑 3.1 需求有如下订单数据 订单id 商品id 成交金额 Order_0000001 Pdt_01 222.8 Order_0000001 Pdt_05 25.8 Order_0000002 Pdt_03 522.8 Order_0000002 Pdt_04 122.4 Order_0000002 Pdt_05 722.4 Order_0000003 Pdt_01 222.8 现在需要求出每一个订单中成交金额最大的一笔交易 3.2 分析1、利用“订单id和成交金额”作为key，可以将map阶段读取到的所有订单数据按照id分区，按照金额排序，发送到reduce 2、在reduce端利用分组将订单id相同的kv聚合成组，然后取第一个即是最大值 3.3 实现第一步:定义OrderBean定义一个OrderBean，里面定义两个字段，第一个字段是我们的orderId，第二个字段是我们的金额（注意金额一定要使用Double或者DoubleWritable类型，否则没法按照金额顺序排序） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051public class OrderBean implements WritableComparable&lt;OrderBean&gt;&#123; private String orderId; private Double price; public String getOrderId() &#123; return orderId; &#125; public void setOrderId(String orderId) &#123; this.orderId = orderId; &#125; public Double getPrice() &#123; return price; &#125; public void setPrice(Double price) &#123; this.price = price; &#125; @Override public String toString() &#123; return orderId + "\t" + price; &#125; //指定排序规则 @Override public int compareTo(OrderBean orderBean) &#123; //先比较订单ID,如果订单ID一致,则排序订单金额(降序) int i = this.orderId.compareTo(orderBean.orderId); if(i == 0)&#123; i = this.price.compareTo(orderBean.price) * -1; &#125; return i; &#125; //实现对象的序列化 @Override public void write(DataOutput out) throws IOException &#123; out.writeUTF(orderId); out.writeDouble(price); &#125; //实现对象反序列化 @Override public void readFields(DataInput in) throws IOException &#123; this.orderId = in.readUTF(); this.price = in.readDouble(); &#125;&#125; 第二步: 定义Mapper类123456789101112131415public class GroupMapper extends Mapper&lt;LongWritable,Text,OrderBean,Text&gt; &#123; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; //1:拆分行文本数据,得到订单的ID,订单的金额 String[] split = value.toString().split("\t"); //2:封装OrderBean,得到K2 OrderBean orderBean = new OrderBean(); orderBean.setOrderId(split[0]); orderBean.setPrice(Double.valueOf(split[2])); //3:将K2和V2写入上下文中 context.write(orderBean, value); &#125;&#125; 第三步:自定义分区自定义分区，按照订单id进行分区，把所有订单id相同的数据，都发送到同一个reduce中去 123456789101112131415public class OrderPartition extends Partitioner&lt;OrderBean,Text&gt; &#123; //分区规则: 根据订单的ID实现分区 /** * * @param orderBean K2 * @param text V2 * @param i ReduceTask个数 * @return 返回分区的编号 */ @Override public int getPartition(OrderBean orderBean, Text text, int i) &#123; return (orderBean.getOrderId().hashCode() &amp; 2147483647) % i; &#125;&#125; 第四步:自定义分组按照我们自己的逻辑进行分组，通过比较相同的订单id，将相同的订单id放到一个组里面去，进过分组之后当中的数据，已经全部是排好序的数据，我们只需要取前topN即可 123456789101112131415161718// 1: 继承WriteableComparatorpublic class OrderGroupComparator extends WritableComparator &#123; // 2: 调用父类的有参构造 public OrderGroupComparator() &#123; super(OrderBean.class,true); &#125; //3: 指定分组的规则(重写方法) @Override public int compare(WritableComparable a, WritableComparable b) &#123; //3.1 对形参做强制类型转换 OrderBean first = (OrderBean)a; OrderBean second = (OrderBean)b; //3.2 指定分组规则 return first.getOrderId().compareTo(second.getOrderId()); &#125;&#125; 第五步:定义Reducer类1234567891011121314public class GroupReducer extends Reducer&lt;OrderBean,Text,Text,NullWritable&gt; &#123; @Override protected void reduce(OrderBean key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException &#123; int i = 0; //获取集合中的前N条数据 for (Text value : values) &#123; context.write(value, NullWritable.get()); i++; if(i &gt;= 1)&#123; break; &#125; &#125; &#125;&#125; 第六步:程序main函数入口123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public class JobMain extends Configured implements Tool &#123; @Override public int run(String[] args) throws Exception &#123; //1:获取Job对象 Job job = Job.getInstance(super.getConf(), "mygroup_job"); //2:设置job任务 //第一步:设置输入类和输入路径 job.setInputFormatClass(TextInputFormat.class); TextInputFormat.addInputPath(job, new Path("file:///D:\\input\\mygroup_input")); //第二步:设置Mapper类和数据类型 job.setMapperClass(GroupMapper.class); job.setMapOutputKeyClass(OrderBean.class); job.setMapOutputValueClass(Text.class); //第三,四,五,六 //设置分区 job.setPartitionerClass(OrderPartition.class); //设置分组 job.setGroupingComparatorClass(OrderGroupComparator.class); //第七步:设置Reducer类和数据类型 job.setReducerClass(GroupReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(NullWritable.class); //第八步:设置输出类和输出的路径 job.setOutputFormatClass(TextOutputFormat.class); TextOutputFormat.setOutputPath(job, new Path("file:///D:\\out\\mygroup_out")); //3:等待job任务结束 boolean bl = job.waitForCompletion(true); return bl ? 0: 1; &#125; public static void main(String[] args) throws Exception &#123; Configuration configuration = new Configuration(); //启动job任务 int run = ToolRunner.run(configuration, new JobMain(), args); System.exit(run); &#125;&#125;]]></content>
      <tags>
        <tag>BigData</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce]]></title>
    <url>%2F2017%2F07%2F06%2FMapReduce.html</url>
    <content type="text"><![CDATA[hadoop的核心 MapReduce1. MapReduce 介绍MapReduce思想在生活中处处可见。或多或少都曾接触过这种思想。MapReduce的思想核心是“分而治之”，适用于大量复杂的任务处理场景（大规模数据处理场景）。 Map负责“分”，即把复杂的任务分解为若干个“简单的任务”来并行处理。可以进行拆分的前提是这些小任务可以并行计算，彼此间几乎没有依赖关系。 Reduce负责“合”，即对map阶段的结果进行全局汇总。 MapReduce运行在yarn集群 ResourceManager NodeManager 这两个阶段合起来正是MapReduce思想的体现。 还有一个比较形象的语言解释MapReduce: 我们要数图书馆中的所有书。你数1号书架，我数2号书架。这就是“Map”。我们人越多，数书就更快。 现在我们到一起，把所有人的统计数加在一起。这就是“Reduce”。 1.1. MapReduce 设计构思MapReduce是一个分布式运算程序的编程框架，核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并发运行在Hadoop集群上。 MapReduce设计并提供了统一的计算框架，为程序员隐藏了绝大多数系统层面的处理细节。为程序员提供一个抽象和高层的编程接口和框架。程序员仅需要关心其应用层的具体计算问题，仅需编写少量的处理应用本身计算问题的程序代码。如何具体完成这个并行计算任务所相关的诸多系统层细节被隐藏起来,交给计算框架去处理： Map和Reduce为程序员提供了一个清晰的操作接口抽象描述。MapReduce中定义了如下的Map和Reduce两个抽象的编程接口，由用户去编程实现.Map和Reduce,MapReduce处理的数据类型是&lt;key,value&gt;键值对。 Map: (k1; v1) → [(k2; v2)] Reduce: (k2; [v2]) → [(k3; v3)] 一个完整的mapreduce程序在分布式运行时有三类实例进程： MRAppMaster 负责整个程序的过程调度及状态协调 MapTask 负责map阶段的整个数据处理流程 ReduceTask 负责reduce阶段的整个数据处理流程 2. MapReduce 编程规范MapReduce 的开发一共有八个步骤, 其中 Map 阶段分为 2 个步骤，Shuffle 阶段 4 个步骤，Reduce 阶段分为 2 个步骤 Map 阶段 2 个步骤 设置 InputFormat 类, 将数据切分为 Key-Value(K1和V1) 对, 输入到第二步 k1为源数据的每行的偏移量,v1为行数据 自定义 Map 逻辑, 将第一步的结果转换成另外的 Key-Value（K2和V2） 对, 输出结果 Shuffle 阶段 4 个步骤 对输出的 Key-Value 对进行分区 自定义类继承Partitioner类 方法 getPartition 类的参数与k2v2同 reduce的默认分区只有一个,k2的哈希值与上int的最大值,模上ReduceTask的个数 指定分区就是相同类型的,共性的数据发送到同一个reduce中处理. 对不同分区的数据按照相同的 Key 排序 自定义类实现WritableComparable 成员变量为不要排序的列 (可选) 对分组过的数据初步规约, 降低数据的网络拷贝 每个map都会产生大量本地输出 作用: 对map端的输出先做一次合并,减少在map和reduce节点间的数据传输量,提高网络io性能 应用前提: 不能影响最终业务逻辑 自定义类 继承 Reduce类 对数据进行分组, 相同 Key 的 Value 放入一个集合中 Reduce 阶段 2 个步骤 对多个 Map 任务的结果进行排序以及合并, 编写 Reduce 函数实现自己的逻辑, 对输入的 Key-Value 进行处理, 转为新的 Key-Value（K3和V3）输出 设置 OutputFormat 处理并保存 Reduce 输出的 Key-Value 数据 3. WordCount 需求: 在一堆给定的文本文件中统计输出每一个单词出现的总次数 Step 1. 数据格式准备 创建一个新的文件 12cd /export/serversvim wordcount.txt 向其中放入以下内容并保存 1234hello,world,hadoophive,sqoop,flume,hellokitty,tom,jerry,worldhadoop 上传到 HDFS 12hdfs dfs -mkdir /wordcount/hdfs dfs -put wordcount.txt /wordcount/ Step 2. Mapper1234567891011public class WordCountMapper extends Mapper&lt;LongWritable,Text,Text,LongWritable&gt; &#123; @Override public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String line = value.toString(); String[] split = line.split(","); for (String word : split) &#123; context.write(new Text(word),new LongWritable(1)); &#125; &#125;&#125; Step 3. Reducer12345678910111213141516171819public class WordCountReducer extends Reducer&lt;Text,LongWritable,Text,LongWritable&gt; &#123; /** * 自定义我们的reduce逻辑 * 所有的key都是我们的单词，所有的values都是我们单词出现的次数 * @param key * @param values * @param context * @throws IOException * @throws InterruptedException */ @Override protected void reduce(Text key, Iterable&lt;LongWritable&gt; values, Context context) throws IOException, InterruptedException &#123; long count = 0; for (LongWritable value : values) &#123; count += value.get(); &#125; context.write(key,new LongWritable(count)); &#125;&#125; Step 4. 定义主类, 描述 Job 并提交 Job12345678910111213141516171819202122232425262728293031323334353637383940public class JobMain extends Configured implements Tool &#123; @Override public int run(String[] args) throws Exception &#123; Job job = Job.getInstance(super.getConf(), JobMain.class.getSimpleName()); //打包到集群上面运行时候，必须要添加以下配置，指定程序的main函数 job.setJarByClass(JobMain.class); //第一步：读取输入文件解析成key，value对 job.setInputFormatClass(TextInputFormat.class); TextInputFormat.addInputPath(job,new Path("hdfs://192.168.52.250:8020/wordcount")); //第二步：设置我们的mapper类 job.setMapperClass(WordCountMapper.class); //设置我们map阶段完成之后的输出类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(LongWritable.class); //第三步，第四步，第五步，第六步，省略 //第七步：设置我们的reduce类 job.setReducerClass(WordCountReducer.class); //设置我们reduce阶段完成之后的输出类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(LongWritable.class); //第八步：设置输出类以及输出路径 job.setOutputFormatClass(TextOutputFormat.class); TextOutputFormat.setOutputPath(job,new Path("hdfs://192.168.52.250:8020/wordcount_out")); boolean b = job.waitForCompletion(true); return b?0:1; &#125; /** * 程序main函数的入口类 * @param args * @throws Exception */ public static void main(String[] args) throws Exception &#123; Configuration configuration = new Configuration(); Tool tool = new JobMain(); int run = ToolRunner.run(configuration, tool, args); System.exit(run); &#125;&#125; 常见错误如果遇到如下错误 1Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied: user=admin, access=WRITE, inode=&quot;/&quot;:root:supergroup:drwxr-xr-x 直接将hdfs-site.xml当中的权限关闭即可 1234&lt;property&gt; &lt;name&gt;dfs.permissions&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt; 最后重启一下 HDFS 集群 小细节本地运行完成之后，就可以打成jar包放到服务器上面去运行了，实际工作当中，都是将代码打成jar包，开发main方法作为程序的入口，然后放到集群上面去运行 4. MapReduce 运行模式本地运行模式 MapReduce 程序是被提交给 LocalJobRunner 在本地以单进程的形式运行 处理的数据及输出结果可以在本地文件系统, 也可以在hdfs上 怎样实现本地运行? 写一个程序, 不要带集群的配置文件, 本质是程序的 conf 中是否有 mapreduce.framework.name=local 以及 yarn.resourcemanager.hostname=local 参数 本地模式非常便于进行业务逻辑的 Debug, 只要在 idea 中打断点即可 1234configuration.set("mapreduce.framework.name","local");configuration.set(" yarn.resourcemanager.hostname","local");TextInputFormat.addInputPath(job,new Path("file:///F:\\wordcount\\input"));TextOutputFormat.setOutputPath(job,new Path("file:///F:\\wordcount\\output")); 集群运行模式 将 MapReduce 程序提交给 Yarn 集群, 分发到很多的节点上并发执行 处理的数据和输出结果应该位于 HDFS 文件系统 提交集群的实现步骤: 将程序打成JAR包，然后在集群的任意一个节点上用hadoop命令启动 1hadoop jar hadoop_hdfs_operate-1.0-SNAPSHOT.jar cn.itcast.hdfs.demo1.JobMain 5. MapReduce 分区在 MapReduce 中, 通过我们指定分区, 会将同一个分区的数据发送到同一个 Reduce 当中进行处理 例如: 为了数据的统计, 可以把一批类似的数据发送到同一个 Reduce 当中, 在同一个 Reduce 当中统计相同类型的数据, 就可以实现类似的数据分区和统计等 其实就是相同类型的数据, 有共性的数据, 送到一起去处理 Reduce 当中默认的分区只有一个 Step 1. 定义 Mapper 这个 Mapper 程序不做任何逻辑, 也不对 Key-Value 做任何改变, 只是接收数据, 然后往下发送 123456public class MyMapper extends Mapper&lt;LongWritable,Text,Text,NullWritable&gt;&#123; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; context.write(value,NullWritable.get()); &#125;&#125; Step 2. 定义 Reducer 逻辑这个 Reducer 也不做任何处理, 将数据原封不动的输出即可 123456public class MyReducer extends Reducer&lt;Text,NullWritable,Text,NullWritable&gt; &#123; @Override protected void reduce(Text key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException, InterruptedException &#123; context.write(key,NullWritable.get()); &#125;&#125; Step 3. 自定义 Partitioner主要的逻辑就在这里, 这也是这个案例的意义, 通过 Partitioner 将数据分发给不同的 Reducer 12345678910public class PartitonerOwn extends Partitioner&lt;Text,LongWritable&gt; &#123; @Override public int getPartition(Text text, LongWritable longWritable, int i) &#123; if(text.toString().length() &gt;=5 )&#123; return 0; &#125;else&#123; return 1; &#125; &#125;&#125; Step 4. Main 入口123456789101112131415161718192021222324252627282930public class PartitionMain extends Configured implements Tool &#123; public static void main(String[] args) throws Exception&#123; int run = ToolRunner.run(new Configuration(), new PartitionMain(), args); System.exit(run); &#125; @Override public int run(String[] args) throws Exception &#123; Job job = Job.getInstance(super.getConf(), PartitionMain.class.getSimpleName()); job.setJarByClass(PartitionMain.class); //第一步 job.setInputFormatClass(TextInputFormat.class); job.setOutputFormatClass(TextOutputFormat.class); TextInputFormat.addInputPath(job,new Path("hdfs://192.168.52.250:8020/partitioner")); TextOutputFormat.setOutputPath(job,new Path("hdfs://192.168.52.250:8020/outpartition")); job.setMapperClass(MyMapper.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(NullWritable.class); job.setOutputKeyClass(Text.class); job.setMapOutputValueClass(NullWritable.class); job.setReducerClass(MyReducer.class); /** * 设置我们的分区类，以及我们的reducetask的个数，注意reduceTask的个数一定要与我们的 * 分区数保持一致 */ job.setPartitionerClass(MyPartitioner.class); job.setNumReduceTasks(2); boolean b = job.waitForCompletion(true); return b?0:1; &#125;&#125; 6. MapReduce 排序和序列化 序列化 (Serialization) 是指把结构化对象转化为字节流 反序列化 (Deserialization) 是序列化的逆过程. 把字节流转为结构化对象. 当要在进程间传递对象或持久化对象的时候, 就需要序列化对象成字节流, 反之当要将接收到或从磁盘读取的字节流转换为对象, 就要进行反序列化 Java 的序列化 (Serializable) 是一个重量级序列化框架, 一个对象被序列化后, 会附带很多额外的信息 (各种校验信息, header, 继承体系等）, 不便于在网络中高效传输. 所以, Hadoop 自己开发了一套序列化机制(Writable), 精简高效. 不用像 Java 对象类一样传输多层的父子关系, 需要哪个属性就传输哪个属性值, 大大的减少网络传输的开销 Writable 是 Hadoop 的序列化格式, Hadoop 定义了这样一个 Writable 接口. 一个类要支持可序列化只需实现这个接口即可 另外 Writable 有一个子接口是 WritableComparable, WritableComparable 是既可实现序列化, 也可以对key进行比较, 我们这里可以通过自定义 Key 实现 WritableComparable 来实现我们的排序功能 数据格式如下 1234567a 1a 9b 3a 7b 8b 10a 5 要求: 第一列按照字典顺序进行排列 第一列相同的时候, 第二列按照升序进行排列 解决思路: 将 Map 端输出的 &lt;key,value&gt; 中的 key 和 value 组合成一个新的 key (newKey), value值不变 这里就变成 &lt;(key,value),value&gt;, 在针对 newKey 排序的时候, 如果 key 相同, 就再对value进行排序 Step 1. 自定义类型和比较器12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970public class PairWritable implements WritableComparable&lt;PairWritable&gt; &#123; // 组合key,第一部分是我们第一列，第二部分是我们第二列 private String first; private int second; public PairWritable() &#123; &#125; public PairWritable(String first, int second) &#123; this.set(first, second); &#125; /** * 方便设置字段 */ public void set(String first, int second) &#123; this.first = first; this.second = second; &#125; /** * 反序列化 */ @Override public void readFields(DataInput input) throws IOException &#123; this.first = input.readUTF(); this.second = input.readInt(); &#125; /** * 序列化 */ @Override public void write(DataOutput output) throws IOException &#123; output.writeUTF(first); output.writeInt(second); &#125; /* * 重写比较器 */ public int compareTo(PairWritable o) &#123; //每次比较都是调用该方法的对象与传递的参数进行比较，说白了就是第一行与第二行比较完了之后的结果与第三行比较， //得出来的结果再去与第四行比较，依次类推 System.out.println(o.toString()); System.out.println(this.toString()); int comp = this.first.compareTo(o.first); if (comp != 0) &#123; return comp; &#125; else &#123; // 若第一个字段相等，则比较第二个字段 return Integer.valueOf(this.second).compareTo( Integer.valueOf(o.getSecond())); &#125; &#125; public int getSecond() &#123; return second; &#125; public void setSecond(int second) &#123; this.second = second; &#125; public String getFirst() &#123; return first; &#125; public void setFirst(String first) &#123; this.first = first; &#125; @Override public String toString() &#123; return "PairWritable&#123;" + "first='" + first + '\'' + ", second=" + second + '&#125;'; &#125;&#125; Step 2. Mapper123456789101112131415public class SortMapper extends Mapper&lt;LongWritable,Text,PairWritable,IntWritable&gt; &#123; private PairWritable mapOutKey = new PairWritable(); private IntWritable mapOutValue = new IntWritable(); @Override public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String lineValue = value.toString(); String[] strs = lineValue.split("\t"); //设置组合key和value ==&gt; &lt;(key,value),value&gt; mapOutKey.set(strs[0], Integer.valueOf(strs[1])); mapOutValue.set(Integer.valueOf(strs[1])); context.write(mapOutKey, mapOutValue); &#125;&#125; Step 3. Reducer123456789101112public class SortReducer extends Reducer&lt;PairWritable,IntWritable,Text,IntWritable&gt; &#123; private Text outPutKey = new Text(); @Override public void reduce(PairWritable key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123;//迭代输出 for(IntWritable value : values) &#123; outPutKey.set(key.getFirst()); context.write(outPutKey, value); &#125; &#125;&#125; Step 4. Main 入口12345678910111213141516171819202122232425262728293031323334353637383940414243444546public class JobMain extends Configured implements Tool &#123; @Override public int run(String[] args) throws Exception &#123; //1:创建job对象 Job job = Job.getInstance(super.getConf(), "mapreduce_sort"); //2:配置job任务(八个步骤) //第一步:设置输入类和输入的路径 job.setInputFormatClass(TextInputFormat.class); ///TextInputFormat.addInputPath(job, new Path("hdfs://node01:8020/input/sort_input")); TextInputFormat.addInputPath(job, new Path("file:///D:\\input\\sort_input")); //第二步: 设置Mapper类和数据类型 job.setMapperClass(SortMapper.class); job.setMapOutputKeyClass(SortBean.class); job.setMapOutputValueClass(NullWritable.class); //第三，四，五，六 //第七步：设置Reducer类和类型 job.setReducerClass(SortReducer.class); job.setOutputKeyClass(SortBean.class); job.setOutputValueClass(NullWritable.class); //第八步: 设置输出类和输出的路径 job.setOutputFormatClass(TextOutputFormat.class); //TextOutputFormat.setOutputPath(job, new Path("hdfs://node01:8020/out/sort_out")); TextOutputFormat.setOutputPath(job, new Path("file:///D:\\out\\sort_out")); //3:等待任务结束 boolean bl = job.waitForCompletion(true); return bl?0:1; &#125; public static void main(String[] args) throws Exception &#123; Configuration configuration = new Configuration(); //启动job任务 int run = ToolRunner.run(configuration, new JobMain(), args); System.exit(run); &#125;&#125; MapReduce 中的计数器计数器是收集作业统计信息的有效手段之一，用于质量控制或应用级统计。计数器还可辅助诊断系统故障。如果需要将日志信息传输到 map 或 reduce 任务， 更好的方法通常是看能否用一个计数器值来记录某一特定事件的发生。对于大型分布式作业而言，使用计数器更为方便。除了因为获取计数器值比输出日志更方便，还有根据计数器值统计特定事件的发生次数要比分析一堆日志文件容易得多。 hadoop内置计数器列表 MapReduce任务计数器 org.apache.hadoop.mapreduce.TaskCounter 文件系统计数器 org.apache.hadoop.mapreduce.FileSystemCounter FileInputFormat计数器 org.apache.hadoop.mapreduce.lib.input.FileInputFormatCounter FileOutputFormat计数器 org.apache.hadoop.mapreduce.lib.output.FileOutputFormatCounter 作业计数器 org.apache.hadoop.mapreduce.JobCounter 每次mapreduce执行完成之后，我们都会看到一些日志记录出来，其中最重要的一些日志记录如下截图 所有的这些都是MapReduce的计数器的功能，既然MapReduce当中有计数器的功能，我们如何实现自己的计数器？？？ 需求：以以上分区代码为案例，统计map接收到的数据记录条数 第一种方式第一种方式定义计数器，通过context上下文对象可以获取我们的计数器，进行记录通过context上下文对象，在map端使用计数器进行统计 123456789public class PartitionMapper extends Mapper&lt;LongWritable,Text,Text,NullWritable&gt;&#123; //map方法将K1和V1转为K2和V2 @Override protected void map(LongWritable key, Text value, Context context) throws Exception&#123; Counter counter = context.getCounter("MR_COUNT", "MyRecordCounter"); counter.increment(1L); context.write(value,NullWritable.get()); &#125;&#125; 运行程序之后就可以看到我们自定义的计数器在map阶段读取了七条数据 第二种方式通过enum枚举类型来定义计数器统计reduce端数据的输入的key有多少个 12345678910public class PartitionerReducer extends Reducer&lt;Text,NullWritable,Text,NullWritable&gt; &#123; public static enum Counter&#123; MY_REDUCE_INPUT_RECORDS,MY_REDUCE_INPUT_BYTES &#125; @Override protected void reduce(Text key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException, InterruptedException &#123; context.getCounter(Counter.MY_REDUCE_INPUT_RECORDS).increment(1L); context.write(key, NullWritable.get()); &#125;&#125; 规约Combiner概念每一个 map 都可能会产生大量的本地输出，Combiner 的作用就是对 map 端的输出先做一次合并，以减少在 map 和 reduce 节点之间的数据传输量，以提高网络IO 性能，是 MapReduce 的一种优化手段之一 combiner 是 MR 程序中 Mapper 和 Reducer 之外的一种组件 combiner 组件的父类就是 Reducer combiner 和 reducer 的区别在于运行的位置 Combiner 是在每一个 maptask 所在的节点运行 Reducer 是接收全局所有 Mapper 的输出结果 combiner 的意义就是对每一个 maptask 的输出进行局部汇总，以减小网络传输量 实现步骤 自定义一个 combiner 继承 Reducer，重写 reduce 方法 在 job 中设置 job.setCombinerClass(CustomCombiner.class) combiner 能够应用的前提是不能影响最终的业务逻辑，而且，combiner 的输出 kv 应该跟 reducer 的输入 kv 类型要对应起来 MapReduce案例-流量统计需求一: 统计求和统计每个手机号的上行数据包总和，下行数据包总和，上行总流量之和，下行总流量之和分析：以手机号码作为key值，上行流量，下行流量，上行总流量，下行总流量四个字段作为value值，然后以这个key，和value作为map阶段的输出，reduce阶段的输入 Step 1: 自定义map的输出value对象FlowBean12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061public class FlowBean implements Writable &#123; private Integer upFlow; private Integer downFlow; private Integer upCountFlow; private Integer downCountFlow; @Override public void write(DataOutput out) throws IOException &#123; out.writeInt(upFlow); out.writeInt(downFlow); out.writeInt(upCountFlow); out.writeInt(downCountFlow); &#125; @Override public void readFields(DataInput in) throws IOException &#123; this.upFlow = in.readInt(); this.downFlow = in.readInt(); this.upCountFlow = in.readInt(); this.downCountFlow = in.readInt(); &#125; public FlowBean() &#123; &#125; public FlowBean(Integer upFlow, Integer downFlow, Integer upCountFlow, Integer downCountFlow) &#123; this.upFlow = upFlow; this.downFlow = downFlow; this.upCountFlow = upCountFlow; this.downCountFlow = downCountFlow; &#125; public Integer getUpFlow() &#123; return upFlow; &#125; public void setUpFlow(Integer upFlow) &#123; this.upFlow = upFlow; &#125; public Integer getDownFlow() &#123; return downFlow; &#125; public void setDownFlow(Integer downFlow) &#123; this.downFlow = downFlow; &#125; public Integer getUpCountFlow() &#123; return upCountFlow; &#125; public void setUpCountFlow(Integer upCountFlow) &#123; this.upCountFlow = upCountFlow; &#125; public Integer getDownCountFlow() &#123; return downCountFlow; &#125; public void setDownCountFlow(Integer downCountFlow) &#123; this.downCountFlow = downCountFlow; &#125; @Override public String toString() &#123; return "FlowBean&#123;" + "upFlow=" + upFlow + ", downFlow=" + downFlow + ", upCountFlow=" + upCountFlow + ", downCountFlow=" + downCountFlow + '&#125;'; &#125;&#125; Step 2: 定义FlowMapper类1234567891011121314151617public class FlowCountMapper extends Mapper&lt;LongWritable,Text,Text,FlowBean&gt; &#123; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; //1:拆分手机号 String[] split = value.toString().split("\t"); String phoneNum = split[1]; //2:获取四个流量字段 FlowBean flowBean = new FlowBean(); flowBean.setUpFlow(Integer.parseInt(split[6])); flowBean.setDownFlow(Integer.parseInt(split[7])); flowBean.setUpCountFlow(Integer.parseInt(split[8])); flowBean.setDownCountFlow(Integer.parseInt(split[9])); //3:将k2和v2写入上下文中 context.write(new Text(phoneNum), flowBean); &#125;&#125; Step 3: 定义FlowReducer类1234567891011121314151617181920212223public class FlowCountReducer extends Reducer&lt;Text,FlowBean,Text,FlowBean&gt; &#123; @Override protected void reduce(Text key, Iterable&lt;FlowBean&gt; values, Context context) throws IOException, InterruptedException &#123; //封装新的FlowBean FlowBean flowBean = new FlowBean(); Integer upFlow = 0; Integer downFlow = 0; Integer upCountFlow = 0; Integer downCountFlow = 0; for (FlowBean value : values) &#123; upFlow += value.getUpFlow(); downFlow += value.getDownFlow(); upCountFlow += value.getUpCountFlow(); downCountFlow += value.getDownCountFlow(); &#125; flowBean.setUpFlow(upFlow); flowBean.setDownFlow(downFlow); flowBean.setUpCountFlow(upCountFlow); flowBean.setDownCountFlow(downCountFlow); //将K3和V3写入上下文中 context.write(key, flowBean); &#125;&#125; Step 4: 程序main函数入口FlowMain123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960public class JobMain extends Configured implements Tool &#123; //该方法用于指定一个job任务 @Override public int run(String[] args) throws Exception &#123; //1:创建一个job任务对象 Job job = Job.getInstance(super.getConf(), "mapreduce_flowcount"); //如果打包运行出错，则需要加该配置 job.setJarByClass(JobMain.class); //2:配置job任务对象(八个步骤) //第一步:指定文件的读取方式和读取路径 job.setInputFormatClass(TextInputFormat.class); //TextInputFormat.addInputPath(job, new Path("hdfs://node01:8020/wordcount")); TextInputFormat.addInputPath(job, new Path("file:///D:\\input\\flowcount_input")); //第二步:指定Map阶段的处理方式和数据类型 job.setMapperClass(FlowCountMapper.class); //设置Map阶段K2的类型 job.setMapOutputKeyClass(Text.class); //设置Map阶段V2的类型 job.setMapOutputValueClass(FlowBean.class); //第三（分区），四 （排序） //第五步: 规约(Combiner) //第六步 分组 //第七步：指定Reduce阶段的处理方式和数据类型 job.setReducerClass(FlowCountReducer.class); //设置K3的类型 job.setOutputKeyClass(Text.class); //设置V3的类型 job.setOutputValueClass(FlowBean.class); //第八步: 设置输出类型 job.setOutputFormatClass(TextOutputFormat.class); //设置输出的路径 TextOutputFormat.setOutputPath(job, new Path("file:///D:\\out\\flowcount_out")); //等待任务结束 boolean bl = job.waitForCompletion(true); return bl ? 0:1; &#125; public static void main(String[] args) throws Exception &#123; Configuration configuration = new Configuration(); //启动job任务 int run = ToolRunner.run(configuration, new JobMain(), args); System.exit(run); &#125;&#125; 需求二: 上行流量倒序排序（递减排序）分析，以需求一的输出数据作为排序的输入数据，自定义FlowBean,以FlowBean为map输出的key，以手机号作为Map输出的value，因为MapReduce程序会对Map阶段输出的key进行排序 Step 1: 定义FlowBean实现WritableComparable实现比较排序Java 的 compareTo 方法说明: compareTo 方法用于将当前对象与方法的参数进行比较。 如果指定的数与参数相等返回 0。 如果指定的数小于参数返回 -1。 如果指定的数大于参数返回 1。 例如：o1.compareTo(o2); 返回正数的话，当前对象（调用 compareTo 方法的对象 o1）要排在比较对象（compareTo 传参对象 o2）后面，返回负数的话，放在前面 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768public class FlowBean implements WritableComparable&lt;FlowBean&gt; &#123; private Integer upFlow; private Integer downFlow; private Integer upCountFlow; private Integer downCountFlow; public FlowBean() &#123; &#125; public FlowBean(Integer upFlow, Integer downFlow, Integer upCountFlow, Integer downCountFlow) &#123; this.upFlow = upFlow; this.downFlow = downFlow; this.upCountFlow = upCountFlow; this.downCountFlow = downCountFlow; &#125; @Override public void write(DataOutput out) throws IOException &#123; out.writeInt(upFlow); out.writeInt(downFlow); out.writeInt(upCountFlow); out.writeInt(downCountFlow); &#125; @Override public void readFields(DataInput in) throws IOException &#123; upFlow = in.readInt(); downFlow = in.readInt(); upCountFlow = in.readInt(); downCountFlow = in.readInt(); &#125; public Integer getUpFlow() &#123; return upFlow; &#125; public void setUpFlow(Integer upFlow) &#123; this.upFlow = upFlow; &#125; public Integer getDownFlow() &#123; return downFlow; &#125; public void setDownFlow(Integer downFlow) &#123; this.downFlow = downFlow; &#125; public Integer getUpCountFlow() &#123; return upCountFlow; &#125; public void setUpCountFlow(Integer upCountFlow) &#123; this.upCountFlow = upCountFlow; &#125; public Integer getDownCountFlow() &#123; return downCountFlow; &#125; public void setDownCountFlow(Integer downCountFlow) &#123; this.downCountFlow = downCountFlow; &#125; @Override public String toString() &#123; return upFlow+"\t"+downFlow+"\t"+upCountFlow+"\t"+downCountFlow; &#125; @Override public int compareTo(FlowBean o) &#123; return this.upCountFlow &gt; o.upCountFlow ?-1:1; &#125;&#125; Step 2: 定义FlowMapper12345678910111213141516171819public class FlowCountSortMapper extends Mapper&lt;LongWritable,Text,FlowBean,Text&gt; &#123; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; FlowBean flowBean = new FlowBean(); String[] split = value.toString().split("\t"); //获取手机号，作为V2 String phoneNum = split[0]; //获取其他流量字段,封装flowBean，作为K2 flowBean.setUpFlow(Integer.parseInt(split[1])); flowBean.setDownFlow(Integer.parseInt(split[2])); flowBean.setUpCountFlow(Integer.parseInt(split[3])); flowBean.setDownCountFlow(Integer.parseInt(split[4])); //将K2和V2写入上下文中 context.write(flowBean, new Text(phoneNum)); &#125;&#125; Step 3: 定义FlowReducer12345678public class FlowCountSortReducer extends Reducer&lt;FlowBean,Text,Text,FlowBean&gt; &#123; @Override protected void reduce(FlowBean key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException &#123; for (Text value : values) &#123; context.write(value, key); &#125; &#125;&#125; Step 4: 程序main函数入口123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public class JobMain extends Configured implements Tool &#123; @Override public int run(String[] strings) throws Exception &#123; //创建一个任务对象 Job job = Job.getInstance(super.getConf(), "mapreduce_flowcountsort"); //打包放在集群运行时，需要做一个配置 job.setJarByClass(JobMain.class); //第一步:设置读取文件的类: K1 和V1 job.setInputFormatClass(TextInputFormat.class); TextInputFormat.addInputPath(job, new Path("hdfs://node01:8020/out/flowcount_out")); //第二步：设置Mapper类 job.setMapperClass(FlowCountSortMapper.class); //设置Map阶段的输出类型: k2 和V2的类型 job.setMapOutputKeyClass(FlowBean.class); job.setMapOutputValueClass(Text.class); //第三,四，五，六步采用默认方式(分区，排序，规约，分组) //第七步 ：设置文的Reducer类 job.setReducerClass(FlowCountSortReducer.class); //设置Reduce阶段的输出类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(FlowBean.class); //设置Reduce的个数 //第八步:设置输出类 job.setOutputFormatClass(TextOutputFormat.class); //设置输出的路径 TextOutputFormat.setOutputPath(job, new Path("hdfs://node01:8020/out/flowcountsort_out")); boolean b = job.waitForCompletion(true); return b?0:1; &#125; public static void main(String[] args) throws Exception &#123; Configuration configuration = new Configuration(); //启动一个任务 int run = ToolRunner.run(configuration, new JobMain(), args); System.exit(run); &#125;&#125; 需求三: 手机号码分区在需求一的基础上，继续完善，将不同的手机号分到不同的数据文件的当中去，需要自定义分区来实现，这里我们自定义来模拟分区，将以下数字开头的手机号进行分开 1234135 开头数据到一个分区文件136 开头数据到一个分区文件137 开头数据到一个分区文件其他分区 自定义分区123456789101112131415public class FlowPartition extends Partitioner&lt;Text,FlowBean&gt; &#123; @Override public int getPartition(Text text, FlowBean flowBean, int i) &#123; String line = text.toString(); if (line.startsWith("135"))&#123; return 0; &#125;else if(line.startsWith("136"))&#123; return 1; &#125;else if(line.startsWith("137"))&#123; return 2; &#125;else&#123; return 3; &#125; &#125;&#125; 作业运行设置12job.setPartitionerClass(FlowPartition.class); job.setNumReduceTasks(4); 修改输入输出路径, 并放入集群运行12TextInputFormat.addInputPath(job,new Path("hdfs://node01:8020/partition_flow/"));TextOutputFormat.setOutputPath(job,new Path("hdfs://node01:8020/partition_out"));]]></content>
      <tags>
        <tag>BigData</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop]]></title>
    <url>%2F2017%2F07%2F05%2FHadoop.html</url>
    <content type="text"><![CDATA[Hadoop一概述狭义上来说Hadoop就是: HDFS ：分布式文件系统 MapReduce : 分布式计算系统 Yarn：分布式样集群资源管理 广义上来说: Hadoop指代大数据的一个生态圈,包括很多其他软件: 历史版本与发行公司 2.1 Hadoop历史版本 1.x版本系列：hadoop版本当中的第二代开源版本，主要修复0.x版本的一些bug等 2.x版本系列：架构产生重大变化，引入了yarn平台等许多新特性 3.x版本系列: 加入多namenoode新特性 2.2 Hadoop三大发行版公司 免费开源版本apache: http://hadoop.apache.org/ 优点：拥有全世界的开源贡献者，代码更新迭代版本比较快， 缺点：版本的升级，版本的维护，版本的兼容性，版本的补丁都可能考虑不太周到， apache所有软件的下载地址（包括各种历史版本）： http://archive.apache.org/dist/ 免费开源版本hortonWorks： https://hortonworks.com/ hortonworks主要是雅虎主导Hadoop开发的副总裁，带领二十几个核心成员成立Hortonworks，核心产品软件HDP（ambari），HDF免费开源，并且提供一整套的web管理界面，供我们可以通过web界面管理我们的集群状态，web管理界面软件HDF网址（http://ambari.apache.org/） 软件收费版本ClouderaManager: https://www.cloudera.com/ cloudera主要是美国一家大数据公司在apache开源hadoop的版本上，通过自己公司内部的各种补丁，实现版本之间的稳定运行，大数据生态圈的各个版本的软件都提供了对应的版本，解决了版本的升级困难，版本兼容性等各种问题 二架构1.x的版本架构模型介绍 文件系统核心模块： NameNode：集群当中的主节点，管理元数据(文件的大小，文件的位置，文件的权限)，主要用于管理集群当中的各种数据 secondaryNameNode：主要能用于hadoop当中元数据信息的辅助管理 DataNode：集群当中的从节点，主要用于存储集群当中的各种数据 数据计算核心模块： JobTracker：接收用户的计算请求任务，并分配任务给从节点 TaskTracker：负责执行主节点JobTracker分配的任务 2.x的版本架构模型介绍引入了yarn,其中MapReduce运行在yarn中 第一种：NameNode与ResourceManager单节点架构模型 文件系统核心模块： NameNode：集群当中的主节点，主要用于管理集群当中的各种数据 secondaryNameNode：主要能用于hadoop当中元数据信息的辅助管理 DataNode：集群当中的从节点，主要用于存储集群当中的各种数据 数据计算核心模块： ResourceManager：接收用户的计算请求任务，并负责集群的资源分配 NodeManager：负责执行主节点APPmaster分配的任务 第二种：NameNode单节点与ResourceManager高可用架构模型 文件系统核心模块： NameNode：集群当中的主节点，主要用于管理集群当中的各种数据 secondaryNameNode：主要能用于hadoop当中元数据信息的辅助管理 DataNode：集群当中的从节点，主要用于存储集群当中的各种数据 数据计算核心模块： ResourceManager：接收用户的计算请求任务，并负责集群的资源分配，以及计算任务的划分，通过zookeeper实现ResourceManager的高可用 NodeManager：负责执行主节点ResourceManager分配的任务 第三种：NameNode高可用与ResourceManager单节点架构模型 文件系统核心模块： NameNode：集群当中的主节点，主要用于管理集群当中的各种数据，其中nameNode可以有两个，形成高可用状态 DataNode：集群当中的从节点，主要用于存储集群当中的各种数据 JournalNode：文件系统元数据信息管理 数据计算核心模块： ResourceManager：接收用户的计算请求任务，并负责集群的资源分配，以及计算任务的划分 NodeManager：负责执行主节点ResourceManager分配的任务 第四种：NameNode高可用与ResourceManager高可用架构模型 文件系统核心模块： NameNode：集群当中的主节点，主要用于管理集群当中的各种数据，一般都是使用两个，实现HA高可用 JournalNode：元数据信息管理进程，一般都是奇数个 DataNode：从节点，用于数据的存储 数据计算核心模块： ResourceManager：Yarn平台的主节点，主要用于接收各种任务，通过两个，构建成高可用 NodeManager：Yarn平台的从节点，主要用于处理ResourceManager分配的任务 三Apache hadoop 编译四 安装Apache Hadoop例如 以三台服务为例: 节点规划: 服务器IP 192.168.174.*** 192.168.174.*** 192.168.174.*** 主机名 node01 node02 node03 NameNode 是 否 否 SecondaryNameNode 是 否 否 dataNode 是 是 是 ResourceManager 是 否 否 NodeManager 是 是 是 解压: 1tar -zxvf hadoop-2.7.5.tar.gz -C ../servers/ 2 修改配置文件配置文件位置: 12cd /export/servers/hadoop-2.7.5/etc/hadoopvim core-site.xml 修改core-site.xml文件 1234567891011121314151617181920212223242526272829303132333435363738394041&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;&lt;!-- Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. See accompanying LICENSE file.--&gt;&lt;!-- Put site-specific property overrides in this file. --&gt;&lt;configuration&gt; &lt;!-- 指定集群的文件系统类型:分布式文件系统 --&gt; &lt;property&gt; &lt;name&gt;fs.default.name&lt;/name&gt; &lt;value&gt;hdfs://node01:8020&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定临时文件存储目录 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/export/servers/hadoop-2.7.5/hadoopDatas/tempDatas&lt;/value&gt; &lt;/property&gt; &lt;!-- 缓冲区大小，实际工作中根据服务器性能动态调整 --&gt; &lt;property&gt; &lt;name&gt;io.file.buffer.size&lt;/name&gt; &lt;value&gt;4096&lt;/value&gt; &lt;/property&gt; &lt;!-- 开启hdfs的垃圾桶机制，删除掉的数据可以从垃圾桶中回收，单位分钟 --&gt; &lt;property&gt; &lt;name&gt;fs.trash.interval&lt;/name&gt; &lt;value&gt;10080&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改hdfs-site.xml 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;&lt;!-- Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. See accompanying LICENSE file.--&gt;&lt;!-- Put site-specific property overrides in this file. --&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;node01:50090&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定namenode的访问地址和端口 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address&lt;/name&gt; &lt;value&gt;node01:50070&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定namenode元数据的存放位置 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:///export/servers/hadoop-2.7.5/hadoopDatas/namenodeDatas,file:///export/servers/hadoop-2.7.5/hadoopDatas/namenodeDatas2&lt;/value&gt; &lt;/property&gt; &lt;!-- 定义dataNode数据存储的节点位置，实际工作中，一般先确定磁盘的挂载目录，然后多个目录用，进行分割 --&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:///export/servers/hadoop-2.7.5/hadoopDatas/datanodeDatas,file:///export/servers/hadoop-2.7.5/hadoopDatas/datanodeDatas2&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定namenode日志文件的存放目录 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.edits.dir&lt;/name&gt; &lt;value&gt;file:///export/servers/hadoop-2.7.5/hadoopDatas/nn/edits&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.dir&lt;/name&gt; &lt;value&gt;file:///export/servers/hadoop-2.7.5/hadoopDatas/snn/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.edits.dir&lt;/name&gt; &lt;value&gt;file:///export/servers/hadoop-2.7.5/hadoopDatas/dfs/snn/edits&lt;/value&gt; &lt;/property&gt; &lt;!-- 文件切片的副本个数--&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt; &lt;!-- 设置HDFS的文件权限--&gt; &lt;property&gt; &lt;name&gt;dfs.permissions&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 设置一个文件切片的大小：128M--&gt; &lt;property&gt; &lt;name&gt;dfs.blocksize&lt;/name&gt; &lt;value&gt;134217728&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改hadoop-env.sh 1export JAVA_HOME=/export/servers/jdk1.8.0_141 修改mapred-site.xml 12345678910111213141516171819202122232425262728293031&lt;configuration&gt; &lt;!-- 如果启用了该功能，则会将一个“小的application”的所有子task在同一个JVM里面执行，达到JVM重用的目的。这个JVM便是负责该application的ApplicationMaster所用的JVM --&gt; &lt;!--mapreduce.job.ubertask.maxmaps | 9 | map任务数的阀值，如果一个application包含的map数小于该值的定义，那么该application就会被认为是一个小的application--&gt; &lt;!-- mapreduce.job.ubertask.maxreduces | 1 | reduce任务数的阀值，如果一个application包含的reduce数小于该值的定义，那么该application就会被认为是一个小的application。不过目前Yarn不支持该值大于1的情况 --&gt; &lt;!-- mapreduce.job.ubertask.maxbytes | | application的输入大小的阀值。默认为dfs.block.size的值。当实际的输入大小部超过该值的设定，便会认为该application为一个小的application。 --&gt; &lt;!-- 开启MapReduce小任务模式 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.job.ubertask.enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 设置历史任务的主机和端口 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;node01:10020&lt;/value&gt; &lt;/property&gt; &lt;!-- 设置网页访问历史任务的主机和端口 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;node01:19888&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改yarn-site.xml 12345678910111213141516171819202122232425262728293031323334353637&lt;configuration&gt; &lt;!-- 配置yarn主节点的位置 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;node01&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;!-- 开启日志聚合功能 --&gt; &lt;property&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 设置聚合日志在hdfs上的保存时间 --&gt; &lt;property&gt; &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt; &lt;value&gt;604800&lt;/value&gt; &lt;/property&gt; &lt;!-- 设置yarn集群的内存分配方案 --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt; &lt;value&gt;20480&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt; &lt;value&gt;2048&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.vmem-pmem-ratio&lt;/name&gt; &lt;value&gt;2.1&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改mapred-env.sh 1export JAVA_HOME=/export/servers/jdk1.8.0_141 修改slaves 123node01node02node03 创建目录: 12345678mkdir -p /export/servers/hadoop-2.7.5/hadoopDatas/tempDatasmkdir -p /export/servers/hadoop-2.7.5/hadoopDatas/namenodeDatasmkdir -p /export/servers/hadoop-2.7.5/hadoopDatas/namenodeDatas2mkdir -p /export/servers/hadoop-2.7.5/hadoopDatas/datanodeDatasmkdir -p /export/servers/hadoop-2.7.5/hadoopDatas/datanodeDatas2mkdir -p /export/servers/hadoop-2.7.5/hadoopDatas/nn/editsmkdir -p /export/servers/hadoop-2.7.5/hadoopDatas/snn/namemkdir -p /export/servers/hadoop-2.7.5/hadoopDatas/dfs/snn/edits 安装包分发: 12scp -r hadoop-2.7.5 node02:$PWDscp -r hadoop-2.7.5 node03:$PWD 配置Hadoop环境变量 1234vim /etc/profileexport HADOOP_HOME=/export/servers/hadoop-2.7.5export PATH=:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATHsource /etc/profile 第四步：启动集群要启动 Hadoop 集群，需要启动 HDFS 和 YARN 两个模块。注意： 首次启动 HDFS 时，必须对其进行格式化操作。 本质上是一些清理和准备工作，因为此时的 HDFS 在物理上还是不存在的。 hdfs namenode -format 或者 hadoop namenode –format 准备启动 第一台机器执行以下命令 12345cd /export/servers/hadoop-2.7.5/bin/hdfs namenode -formatsbin/start-dfs.shsbin/start-yarn.shsbin/mr-jobhistory-daemon.sh start historyserver 三个端口查看界面 http://node01:50070/explorer.html#/ 查看hdfs http://node01:8088/cluster 查看yarn集群 http://node01:19888/jobhistory 查看历史完成的任务]]></content>
      <tags>
        <tag>BigData</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hdfs]]></title>
    <url>%2F2017%2F07%2F05%2FHdfs.html</url>
    <content type="text"><![CDATA[Hadoop的核心 Hdfs1. HDFS概述1.1 介绍 在现代的企业环境中，单机容量往往无法存储大量数据，需要跨机器存储。统一管理分布在集群上的文件系统称为分布式文件系统 。 ​ HDFS（Hadoop Distributed File System）是 Apache Hadoop 项目的一个子项目. Hadoop 非常适于存储大型数据 (比如 TB 和 PB), 其就是使用 HDFS 作为存储系统. HDFS 使用多台计算机存储文件, 并且提供统一的访问接口, 像是访问一个普通文件系统一样使用分布式文件系统. 1.2 历史 Doug Cutting 在做 Lucene 的时候, 需要编写一个爬虫服务, 这个爬虫写的并不顺利, 遇到了一些问题, 诸如: 如何存储大规模的数据, 如何保证集群的可伸缩性, 如何动态容错等 2013年的时候, Google 发布了三篇论文, 被称作为三驾马车, 其中有一篇叫做 GFS, 是描述了 Google 内部的一个叫做 GFS 的分布式大规模文件系统, 具有强大的可伸缩性和容错性 Doug Cutting 后来根据 GFS 的论文, 创造了一个新的文件系统, 叫做 HDFS 2. HDFS应用场景2.1 适合的应用场景 存储非常大的文件：这里非常大指的是几百M、G、或者TB级别，需要高吞吐量，对延时没有要求。 采用流式的数据访问方式: 即一次写入、多次读取，数据集经常从数据源生成或者拷贝一次，然后在其上做很多分析工作 。 运行于商业硬件上: Hadoop不需要特别贵的机器，可运行于普通廉价机器，可以处节约成本 需要高容错性 为数据存储提供所需的扩展能力 2.2 不适合的应用场景 1） 低延时的数据访问 对延时要求在毫秒级别的应用，不适合采用HDFS。HDFS是为高吞吐数据传输设计的,因此可能牺牲延时 2）大量小文件文件的元数据保存在NameNode的内存中， 整个文件系统的文件数量会受限于NameNode的内存大小。经验而言，一个文件/目录/文件块一般占有150字节的元数据内存空间。如果有100万个文件，每个文件占用1个文件块，则需要大约300M的内存。因此十亿级别的文件数量在现有商用机器上难以支持。 3）多方读写，需要任意的文件修改HDFS采用追加（append-only）的方式写入数据。不支持文件任意offset的修改。不支持多个写入器（writer） 3. HDFS 的架构 HDFS是一个主/从（Mater/Slave）体系结构， HDFS由四部分组成，HDFS Client、NameNode、DataNode和Secondary NameNode。 **1、Client：就是客户端。 文件切分。文件上传 HDFS 的时候，Client 将文件切分成 一个一个的Block，然后进行存储。 与 NameNode 交互，获取文件的位置信息。 与 DataNode 交互，读取或者写入数据。 Client 提供一些命令来管理 和访问HDFS，比如启动或者关闭HDFS。 2、NameNode：就是 master，它是一个主管、管理者。 管理 HDFS 的名称空间 管理数据块（Block）映射信息 配置副本策略 处理客户端读写请求。 3、DataNode：就是Slave。NameNode 下达命令，DataNode 执行实际的操作。 存储实际的数据块。 执行数据块的读/写操作。 4、Secondary NameNode：并非 NameNode 的热备。当NameNode 挂掉的时候，它并不能马上替换 NameNode 并提供服务。 辅助 NameNode，分担其工作量。 定期合并 fsimage和fsedits，并推送给NameNode。 在紧急情况下，可辅助恢复 NameNode。 4:NameNode和DataNode###4.1 NameNode作用 NameNode在内存中保存着整个文件系统的名称空间和文件数据块的地址映射 整个HDFS可存储的文件数受限于NameNode的内存大小 1、NameNode元数据信息文件名，文件目录结构，文件属性(生成时间，副本数，权限)每个文件的块列表。以及列表中的块与块所在的DataNode之间的地址映射关系在内存中加载文件系统中每个文件和每个数据块的引用关系(文件、block、datanode之间的映射信息)数据会定期保存到本地磁盘（fsImage文件和edits文件） 2、NameNode文件操作NameNode负责文件元数据的操作DataNode负责处理文件内容的读写请求，数据流不经过NameNode，会询问它跟那个DataNode联系 3、NameNode副本文件数据块到底存放到哪些DataNode上，是由NameNode决定的，NN根据全局情况做出放置副本的决定 4、NameNode心跳机制全权管理数据块的复制，周期性的接受心跳和块的状态报告信息（包含该DataNode上所有数据块的列表）若接受到心跳信息，NameNode认为DataNode工作正常，如果在10分钟后还接受到不到DN的心跳，那么NameNode认为DataNode已经宕机 ,这时候NN准备要把DN上的数据块进行重新的复制。 块的状态报告包含了一个DN上所有数据块的列表，blocks report 每个1小时发送一次. 4.2 DataNode作用提供真实文件数据的存储服务。 1、Data Node以数据块的形式存储HDFS文件 2、Data Node 响应HDFS 客户端读写请求 3、Data Node 周期性向NameNode汇报心跳信息 4、Data Node 周期性向NameNode汇报数据块信息 5、Data Node 周期性向NameNode汇报缓存数据块信息 5:HDFS的副本机制和机架感知5.1 HDFS 文件副本机制所有的文件都是以 block 块的方式存放在 HDFS 文件系统当中,作用如下 一个文件有可能大于集群中任意一个磁盘，引入块机制,可以很好的解决这个问题 使用块作为文件存储的逻辑单位可以简化存储子系统 块非常适合用于数据备份进而提供数据容错能力 在 Hadoop1 当中, 文件的 block 块默认大小是 64M, hadoop2 当中, 文件的 block 块大小默认是 128M, block 块的大小可以通过 hdfs-site.xml 当中的配置文件进行指定 1234&lt;property&gt; &lt;name&gt;dfs.block.size&lt;/name&gt; &lt;value&gt;块大小 以字节为单位&lt;/value&gt;&lt;/property&gt; 5.2 机架感知HDFS分布式文件系统的内部有一个副本存放策略：以默认的副本数=3为例： 1、第一个副本块存本机 2、第二个副本块存跟本机同机架内的其他服务器节点 3、第三个副本块存不同机架的一个服务器节点上 6、hdfs的命令行使用ls 12格式： hdfs dfs -ls URI作用：类似于Linux的ls命令，显示文件列表 1hdfs dfs -ls / lsr 12格式 : hdfs dfs -lsr URI作用 : 在整个目录下递归执行ls, 与UNIX中的ls-R类似 1hdfs dfs -lsr / mkdir 12格式 ： hdfs dfs [-p] -mkdir &lt;paths&gt;作用 : 以&lt;paths&gt;中的URI作为参数，创建目录。使用-p参数可以递归创建目录 put 12格式 ： hdfs dfs -put &lt;localsrc &gt; ... &lt;dst&gt;作用 ： 将单个的源文件src或者多个源文件srcs从本地文件系统拷贝到目标文件系统中（&lt;dst&gt;对应的路径）。也可以从标准输入中读取输入，写入目标文件系统中 1hdfs dfs -put /rooot/a.txt /dir1 moveFromLocal 12格式： hdfs dfs -moveFromLocal &lt;localsrc&gt; &lt;dst&gt;作用: 和put命令类似，但是源文件localsrc拷贝之后自身被删除 1hdfs dfs -moveFromLocal /root/install.log / moveToLocal 1未实现 get 123格式 hdfs dfs -get [-ignorecrc ] [-crc] &lt;src&gt; &lt;localdst&gt;作用：将文件拷贝到本地文件系统。 CRC 校验失败的文件通过-ignorecrc选项拷贝。 文件和CRC校验和可以通过-CRC选项拷贝 1hdfs dfs -get /install.log /export/servers mv 12格式 ： hdfs dfs -mv URI &lt;dest&gt;作用： 将hdfs上的文件从原路径移动到目标路径（移动之后文件删除），该命令不能夸文件系统 1hdfs dfs -mv /dir1/a.txt /dir2 rm 1234格式： hdfs dfs -rm [-r] 【-skipTrash】 URI 【URI 。。。】作用： 删除参数指定的文件，参数可以有多个。 此命令只删除文件和非空目录。如果指定-skipTrash选项，那么在回收站可用的情况下，该选项将跳过回收站而直接删除文件；否则，在回收站可用时，在HDFS Shell 中执行此命令，会将文件暂时放到回收站中。 1hdfs dfs -rm -r /dir1 cp 123456格式: hdfs dfs -cp URI [URI ...] &lt;dest&gt;作用： 将文件拷贝到目标路径中。如果&lt;dest&gt; 为目录的话，可以将多个文件拷贝到该目录下。-f选项将覆盖目标，如果它已经存在。-p选项将保留文件属性（时间戳、所有权、许可、ACL、XAttr）。 1hdfs dfs -cp /dir1/a.txt /dir2/b.txt cat 12hdfs dfs -cat URI [uri ...]作用：将参数所指示的文件内容输出到stdout 1hdfs dfs -cat /install.log chmod 12格式: hdfs dfs -chmod [-R] URI[URI ...]作用： 改变文件权限。如果使用 -R 选项，则对整个目录有效递归执行。使用这一命令的用户必须是文件的所属用户，或者超级用户。 1hdfs dfs -chmod -R 777 /install.log chown 12格式: hdfs dfs -chmod [-R] URI[URI ...]作用： 改变文件的所属用户和用户组。如果使用 -R 选项，则对整个目录有效递归执行。使用这一命令的用户必须是文件的所属用户，或者超级用户。 1hdfs dfs -chown -R hadoop:hadoop /install.log appendToFile 12格式: hdfs dfs -appendToFile &lt;localsrc&gt; ... &lt;dst&gt;作用: 追加一个或者多个文件到hdfs指定文件中.也可以从命令行读取输入. 1 hdfs dfs -appendToFile a.xml b.xml /big.xml 7、hdfs的高级使用命令7. 1、HDFS文件限额配置​ 在多人共用HDFS的环境下，配置设置非常重要。特别是在Hadoop处理大量资料的环境，如果没有配额管理，很容易把所有的空间用完造成别人无法存取。Hdfs的配额设定是针对目录而不是针对账号，可以 让每个账号仅操作某一个目录，然后对目录设置配置。 ​ hdfs文件的限额配置允许我们以文件个数，或者文件大小来限制我们在某个目录下上传的文件数量或者文件内容总量，以便达到我们类似百度网盘网盘等限制每个用户允许上传的最大的文件的量。 1hdfs dfs -count -q -h /user/root/dir1 #查看配额信息 所谓的空间限额 7.1.1、数量限额12hdfs dfs -mkdir -p /user/root/dir #创建hdfs文件夹hdfs dfsadmin -setQuota 2 dir # 给该文件夹下面设置最多上传两个文件，发现只能上传一个文件 1hdfs dfsadmin -clrQuota /user/root/dir # 清除文件数量限制 7.1.2、空间大小限额 在设置空间配额时，设置的空间至少是block_size * 3大小 12hdfs dfsadmin -setSpaceQuota 4k /user/root/dir # 限制空间大小4KBhdfs dfs -put /root/a.txt /user/root/dir 生成任意大小文件的命令: 1dd if=/dev/zero of=1.txt bs=1M count=2 #生成2M的文件 清除空间配额限制 1hdfs dfsadmin -clrSpaceQuota /user/root/dir 7.2、hdfs的安全模式安全模式是hadoop的一种保护机制，用于保证集群中的数据块的安全性。当集群启动的时候，会首先进入安全模式。当系统处于安全模式时会检查数据块的完整性。 假设我们设置的副本数（即参数dfs.replication）是3，那么在datanode上就应该有3个副本存在，假设只存在2个副本，那么比例就是2/3=0.666。hdfs默认的副本率0.999。我们的副本率0.666明显小于0.999，因此系统会自动的复制副本到其他dataNode，使得副本率不小于0.999。如果系统中有5个副本，超过我们设定的3个副本，那么系统也会删除多于的2个副本。 在安全模式状态下，文件系统只接受读数据请求，而不接受删除、修改等变更请求。在，当整个系统达到安全标准时，HDFS自动离开安全模式。 安全模式操作命令 123hdfs dfsadmin -safemode get #查看安全模式状态hdfs dfsadmin -safemode enter #进入安全模式hdfs dfsadmin -safemode leave #离开安全模式 8. HDFS基准测试实际生产环境当中，hadoop的环境搭建完成之后，第一件事情就是进行压力测试，测试我们的集群的读取和写入速度，测试我们的网络带宽是否足够等一些基准测试 8.1 测试写入速度向HDFS文件系统中写入数据,10个文件,每个文件10MB,文件存放到/benchmarks/TestDFSIO中 1hadoop jar /export/servers/hadoop-2.7.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.5.jar TestDFSIO -write -nrFiles 10 -fileSize 10MB 完成之后查看写入速度结果 1hdfs dfs -text /benchmarks/TestDFSIO/io_write/part-00000 8.2 测试读取速度测试hdfs的读取文件性能 在HDFS文件系统中读入10个文件,每个文件10M 1hadoop jar /export/servers/hadoop-2.7.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.5.jar TestDFSIO -read -nrFiles 10 -fileSize 10MB 查看读取果 1hdfs dfs -text /benchmarks/TestDFSIO/io_read/part-00000 8.3 清除测试数据1hadoop jar /export/servers/hadoop-2.7.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.5.jar TestDFSIO -clean 9.HDFS 文件写入过程 Client 发起文件上传请求, 通过 RPC 与 NameNode 建立通讯, NameNode 检查目标文件是否已存在, 父目录是否存在, 返回是否可以上传 Client 请求第一个 block 该传输到哪些 DataNode 服务器上 NameNode 根据配置文件中指定的备份数量及机架感知原理进行文件分配, 返回可用的 DataNode 的地址如: A, B, C Hadoop 在设计时考虑到数据的安全与高效, 数据文件默认在 HDFS 上存放三份, 存储策略为本地一份, 同机架内其它某一节点上一份, 不同机架的某一节点上一份。 Client 请求 3 台 DataNode 中的一台 A 上传数据（本质上是一个 RPC 调用，建立 pipeline ）, A 收到请求会继续调用 B, 然后 B 调用 C, 将整个 pipeline 建立完成, 后逐级返回 client Client 开始往 A 上传第一个 block（先从磁盘读取数据放到一个本地内存缓存）, 以 packet 为单位（默认64K）, A 收到一个 packet 就会传给 B, B 传给 C. A 每传一个 packet 会放入一个应答队列等待应答 数据被分割成一个个 packet 数据包在 pipeline 上依次传输, 在 pipeline 反方向上, 逐个发送 ack（命令正确应答）, 最终由 pipeline 中第一个 DataNode 节点 A 将 pipelineack 发送给 Client 当一个 block 传输完成之后, Client 再次请求 NameNode 上传第二个 block 到服务 1 10.HDFS 文件读取过程 Client向NameNode发起RPC请求，来确定请求文件block所在的位置； NameNode会视情况返回文件的部分或者全部block列表，对于每个block，NameNode 都会返回含有该 block 副本的 DataNode 地址； 这些返回的 DN 地址，会按照集群拓扑结构得出 DataNode 与客户端的距离，然后进行排序，排序两个规则：网络拓扑结构中距离 Client 近的排靠前；心跳机制中超时汇报的 DN 状态为 STALE，这样的排靠后； Client 选取排序靠前的 DataNode 来读取 block，如果客户端本身就是DataNode,那么将从本地直接获取数据(短路读取特性)； 底层上本质是建立 Socket Stream（FSDataInputStream），重复的调用父类 DataInputStream 的 read 方法，直到这个块上的数据读取完毕； 当读完列表的 block 后，若文件读取还没有结束，客户端会继续向NameNode 获取下一批的 block 列表； 读取完一个 block 都会进行 checksum 验证，如果读取 DataNode 时出现错误，客户端会通知 NameNode，然后再从下一个拥有该 block 副本的DataNode 继续读。 read 方法是并行的读取 block 信息，不是一块一块的读取；NameNode 只是返回Client请求包含块的DataNode地址，并不是返回请求块的数据； 最终读取来所有的 block 会合并成一个完整的最终文件。 11.HDFS 的元数据辅助管理当 Hadoop 的集群当中, NameNode的所有元数据信息都保存在了 FsImage 与 Eidts 文件当中, 这两个文件就记录了所有的数据的元数据信息, 元数据信息的保存目录配置在了 hdfs-site.xml 当中 12345678910&lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt; file:///export/servers/hadoop2.7.5/hadoopDatas/namenodeDatas, file:///export/servers/hadoop-2.7.5/hadoopDatas/namenodeDatas2 &lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.edits.dir&lt;/name&gt; &lt;value&gt;file:///export/servers/hadoop-2.7.5/hadoopDatas/nn/edits&lt;/value&lt;/property&gt;&gt; 11.1 FsImage 和 Edits 详解 edits edits 存放了客户端最近一段时间的操作日志 客户端对 HDFS 进行写文件时会首先被记录在 edits 文件中 edits 修改时元数据也会更新 fsimage NameNode 中关于元数据的镜像, 一般称为检查点, fsimage 存放了一份比较完整的元数据信息 因为 fsimage 是 NameNode 的完整的镜像, 如果每次都加载到内存生成树状拓扑结构，这是非常耗内存和CPU, 所以一般开始时对 NameNode 的操作都放在 edits 中 fsimage 内容包含了 NameNode 管理下的所有 DataNode 文件及文件 block 及 block 所在的 DataNode 的元数据信息. 随着 edits 内容增大, 就需要在一定时间点和 fsimage 合并 11.2 fsimage 中的文件信息查看使用命令 hdfs oiv 12cd /export/servers/hadoop2.7.5/hadoopDatas/namenodeDatashdfs oiv -i fsimage_0000000000000000864 -p XML -o hello.xml 11.3. edits 中的文件信息查看使用命令 hdfs oev 12cd /export/servers/hadoop2.7.5/hadoopDatas/namenodeDatashdfs oev -i edits_0000000000000000865-0000000000000000866 -p XML -o myedit.xml 11.4 SecondaryNameNode 如何辅助管理 fsimage 与 edits 文件? SecondaryNameNode 定期合并 fsimage 和 edits, 把 edits 控制在一个范围内 配置 SecondaryNameNode SecondaryNameNode 在 conf/masters 中指定 在 masters 指定的机器上, 修改 hdfs-site.xml 1234&lt;property&gt; &lt;name&gt;dfs.http.address&lt;/name&gt; &lt;value&gt;host:50070&lt;/value&gt;&lt;/property&gt; 修改 core-site.xml, 这一步不做配置保持默认也可以 12345678910&lt;!-- 多久记录一次 HDFS 镜像, 默认 1小时 --&gt;&lt;property&gt; &lt;name&gt;fs.checkpoint.period&lt;/name&gt; &lt;value&gt;3600&lt;/value&gt;&lt;/property&gt;&lt;!-- 一次记录多大, 默认 64M --&gt;&lt;property&gt; &lt;name&gt;fs.checkpoint.size&lt;/name&gt; &lt;value&gt;67108864&lt;/value&gt;&lt;/property&gt; SecondaryNameNode 通知 NameNode 切换 editlog SecondaryNameNode 从 NameNode 中获得 fsimage 和 editlog(通过http方式) SecondaryNameNode 将 fsimage 载入内存, 然后开始合并 editlog, 合并之后成为新的 fsimage SecondaryNameNode 将新的 fsimage 发回给 NameNode NameNode 用新的 fsimage 替换旧的 fsimage 特点 完成合并的是 SecondaryNameNode, 会请求 NameNode 停止使用 edits, 暂时将新写操作放入一个新的文件中 edits.new SecondaryNameNode 从 NameNode 中通过 Http GET 获得 edits, 因为要和 fsimage 合并, 所以也是通过 Http Get 的方式把 fsimage 加载到内存, 然后逐一执行具体对文件系统的操作, 与 fsimage 合并, 生成新的 fsimage, 然后通过 Http POST 的方式把 fsimage 发送给 NameNode. NameNode 从 SecondaryNameNode 获得了 fsimage 后会把原有的 fsimage 替换为新的 fsimage, 把 edits.new 变成 edits. 同时会更新 fstime Hadoop 进入安全模式时需要管理员使用 dfsadmin 的 save namespace 来创建新的检查点 SecondaryNameNode 在合并 edits 和 fsimage 时需要消耗的内存和 NameNode 差不多, 所以一般把 NameNode 和 SecondaryNameNode 放在不同的机器上 1:HDFS 的 API 操作1.1 配置Windows下Hadoop环境在windows系统需要配置hadoop运行环境，否则直接运行代码会出现以下问题: 缺少winutils.exe 1Could not locate executable null \bin\winutils.exe in the hadoop binaries 缺少hadoop.dll 1Unable to load native-hadoop library for your platform… using builtin-Java classes where applicable 步骤: 第一步：将hadoop2.7.5文件夹拷贝到一个没有中文没有空格的路径下面 第二步：在windows上面配置hadoop的环境变量： HADOOP_HOME，并将%HADOOP_HOME%\bin添加到path中 第三步：把hadoop2.7.5文件夹中bin目录下的hadoop.dll文件放到系统盘: C:\Windows\System32 目录 第四步：关闭windows重启 1.2 导入 Maven 依赖1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859 &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;2.7.5&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;2.7.5&lt;/version&gt;&lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt; &lt;version&gt;2.7.5&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-mapreduce-client-core&lt;/artifactId&gt; &lt;version&gt;2.7.5&lt;/version&gt;&lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.1&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;!-- &lt;verbal&gt;true&lt;/verbal&gt;--&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt; &lt;version&gt;2.4.3&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;shade&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;minimizeJar&gt;true&lt;/minimizeJar&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; 1.3 使用url方式访问数据（了解）1234567891011121314151617@Testpublic void demo1()throws Exception&#123; //第一步：注册hdfs 的url URL.setURLStreamHandlerFactory(new FsUrlStreamHandlerFactory()); //获取文件输入流 InputStream inputStream = new URL("hdfs://node01:8020/a.txt").openStream(); //获取文件输出流 FileOutputStream outputStream = new FileOutputStream(new File("D:\\hello.txt")); //实现文件的拷贝 IOUtils.copy(inputStream, outputStream); //关闭流 IOUtils.closeQuietly(inputStream); IOUtils.closeQuietly(outputStream);&#125; 1.4 使用文件系统方式访问数据（掌握）1.4.1 涉及的主要类在 Java 中操作 HDFS, 主要涉及以下 Class: Configuration 该类的对象封转了客户端或者服务器的配置 FileSystem 该类的对象是一个文件系统对象, 可以用该对象的一些方法来对文件进行操作, 通过 FileSystem 的静态方法 get 获得该对象 1FileSystem fs = FileSystem.get(conf) get 方法从 conf 中的一个参数 fs.defaultFS 的配置值判断具体是什么类型的文件系统 如果我们的代码中没有指定 fs.defaultFS, 并且工程 ClassPath 下也没有给定相应的配置, conf 中的默认值就来自于 Hadoop 的 Jar 包中的 core-default.xml 默认值为 file:///, 则获取的不是一个 DistributedFileSystem 的实例, 而是一个本地文件系统的客户端对象 1.4.2 获取 FileSystem 的几种方式 第一种方式 1234567891011@Testpublic void getFileSystem1() throws IOException &#123; Configuration configuration = new Configuration(); //指定我们使用的文件系统类型: configuration.set("fs.defaultFS", "hdfs://node01:8020/"); //获取指定的文件系统 FileSystem fileSystem = FileSystem.get(configuration); System.out.println(fileSystem.toString());&#125; 第二种方式 12345@Testpublic void getFileSystem2() throws Exception&#123; FileSystem fileSystem = FileSystem.get(new URI("hdfs://node01:8020"), new Configuration()); System.out.println("fileSystem:"+fileSystem);&#125; 第三种方式 1234567@Testpublic void getFileSystem3() throws Exception&#123; Configuration configuration = new Configuration(); configuration.set("fs.defaultFS", "hdfs://node01:8020"); FileSystem fileSystem = FileSystem.newInstance(configuration); System.out.println(fileSystem.toString());&#125; 第四种方式 12345//@Testpublic void getFileSystem4() throws Exception&#123; FileSystem fileSystem = FileSystem.newInstance(new URI("hdfs://node01:8020") ,new Configuration()); System.out.println(fileSystem.toString());&#125; 1.4.3 遍历 HDFS 中所有文件 使用 API 遍历 123456789101112@Testpublic void listMyFiles()throws Exception&#123; //获取fileSystem类 FileSystem fileSystem = FileSystem.get(new URI("hdfs://node01:8020"), new Configuration()); //获取RemoteIterator 得到所有的文件或者文件夹，第一个参数指定遍历的路径，第二个参数表示是否要递归遍历 RemoteIterator&lt;LocatedFileStatus&gt; locatedFileStatusRemoteIterator = fileSystem.listFiles(new Path("/"), true); while (locatedFileStatusRemoteIterator.hasNext())&#123; LocatedFileStatus next = locatedFileStatusRemoteIterator.next(); System.out.println(next.getPath().toString()); &#125; fileSystem.close();&#125; 1.4.4 HDFS 上创建文件夹123456@Testpublic void mkdirs() throws Exception&#123; FileSystem fileSystem = FileSystem.get(new URI("hdfs://node01:8020"), new Configuration()); boolean mkdirs = fileSystem.mkdirs(new Path("/hello/mydir/test")); fileSystem.close();&#125; 1.4.4 下载文件12345678910@Testpublic void getFileToLocal()throws Exception&#123; FileSystem fileSystem = FileSystem.get(new URI("hdfs://node01:8020"), new Configuration()); FSDataInputStream inputStream = fileSystem.open(new Path("/timer.txt")); FileOutputStream outputStream = new FileOutputStream(new File("e:\\timer.txt")); IOUtils.copy(inputStream,outputStream ); IOUtils.closeQuietly(inputStream); IOUtils.closeQuietly(outputStream); fileSystem.close();&#125; 1.4.5 HDFS 文件上传123456@Testpublic void putData() throws Exception&#123; FileSystem fileSystem = FileSystem.get(new URI("hdfs://node01:8020"), new Configuration()); fileSystem.copyFromLocalFile(new Path("file:///c:\\install.log"),new Path("/hello/mydir/test")); fileSystem.close();&#125; 1.4.6 hdfs访问权限控制 停止hdfs集群，在node01机器上执行以下命令 12cd /export/servers/hadoop-2.7.5sbin/stop-dfs.sh 修改node01机器上的hdfs-site.xml当中的配置文件 12cd /export/servers/hadoop-2.7.5/etc/hadoopvim hdfs-site.xml 1234&lt;property&gt; &lt;name&gt;dfs.permissions.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt; 修改完成之后配置文件发送到其他机器上面去 12scp hdfs-site.xml node02:$PWDscp hdfs-site.xml node03:$PWD 重启hdfs集群 12cd /export/servers/hadoop-2.7.5sbin/start-dfs.sh 随意上传一些文件到我们hadoop集群当中准备测试使用 1234cd /export/servers/hadoop-2.7.5/etc/hadoophdfs dfs -mkdir /confighdfs dfs -put *.xml /confighdfs dfs -chmod 600 /config/core-site.xml 使用代码准备下载文件 123456@Testpublic void getConfig()throws Exception&#123; FileSystem fileSystem = FileSystem.get(new URI("hdfs://node01:8020"), new Configuration(),"hadoop"); fileSystem.copyToLocalFile(new Path("/config/core-site.xml"),new Path("file:///c:/core-site.xml")); fileSystem.close();&#125; 1.4.7 小文件合并由于 Hadoop 擅长存储大文件，因为大文件的元数据信息比较少，如果 Hadoop 集群当中有大量的小文件，那么每个小文件都需要维护一份元数据信息，会大大的增加集群管理元数据的内存压力，所以在实际工作当中，如果有必要一定要将小文件合并成大文件进行一起处理 在我们的 HDFS 的 Shell 命令模式下，可以通过命令行将很多的 hdfs 文件合并成一个大文件下载到本地 12cd /export/servershdfs dfs -getmerge /config/*.xml ./hello.xml 既然可以在下载的时候将这些小文件合并成一个大文件一起下载，那么肯定就可以在上传的时候将小文件合并到一个大文件里面去 123456789101112131415161718@Testpublic void mergeFile() throws Exception&#123; //获取分布式文件系统 FileSystem fileSystem = FileSystem.get(new URI("hdfs://192.168.52.250:8020"), new Configuration(),"root"); FSDataOutputStream outputStream = fileSystem.create(new Path("/bigfile.txt")); //获取本地文件系统 LocalFileSystem local = FileSystem.getLocal(new Configuration()); //通过本地文件系统获取文件列表，为一个集合 FileStatus[] fileStatuses = local.listStatus(new Path("file:///E:\\input")); for (FileStatus fileStatus : fileStatuses) &#123; FSDataInputStream inputStream = local.open(fileStatus.getPath()); IOUtils.copy(inputStream,outputStream); IOUtils.closeQuietly(inputStream); &#125; IOUtils.closeQuietly(outputStream); local.close(); fileSystem.close();&#125; 2：HDFS的高可用机制2.1 HDFS高可用介绍在Hadoop 中，NameNode 所处的位置是非常重要的，整个HDFS文件系统的元数据信息都由NameNode 来管理，NameNode的可用性直接决定了Hadoop 的可用性，一旦NameNode进程不能工作了，就会影响整个集群的正常使用。 在典型的HA集群中，两台独立的机器被配置为NameNode。在工作集群中，NameNode机器中的一个处于Active状态，另一个处于Standby状态。Active NameNode负责群集中的所有客户端操作，而Standby充当从服务器。Standby机器保持足够的状态以提供快速故障切换（如果需要）。 2.2 组件介绍 ZKFailoverController 是基于Zookeeper的故障转移控制器，它负责控制NameNode的主备切换，ZKFailoverController会监测NameNode的健康状态，当发现Active NameNode出现异常时会通过Zookeeper进行一次新的选举，完成Active和Standby状态的切换 HealthMonitor 周期性调用NameNode的HAServiceProtocol RPC接口（monitorHealth 和 getServiceStatus），监控NameNode的健康状态并向ZKFailoverController反馈 ActiveStandbyElector 接收ZKFC的选举请求，通过Zookeeper自动完成主备选举，选举完成后回调ZKFailoverController的主备切换方法对NameNode进行Active和Standby状态的切换. DataNode NameNode包含了HDFS的元数据信息和数据块信息（blockmap），其中数据块信息通过DataNode主动向Active NameNode和Standby NameNode上报 共享存储系统 共享存储系统负责存储HDFS的元数据（EditsLog），Active NameNode（写入）和 Standby NameNode（读取）通过共享存储系统实现元数据同步，在主备切换过程中，新的Active NameNode必须确保元数据同步完成才能对外提供服务 3: Hadoop的联邦机制(Federation) 3.1背景概述单NameNode的架构使得HDFS在集群扩展性和性能上都有潜在的问题，当集群大到一定程度后，NameNode进程使用的内存可能会达到上百G，NameNode成为了性能的瓶颈。因而提出了namenode水平扩展方案– Federation。 Federation中文意思为联邦,联盟，是NameNode的Federation,也就是会有多个NameNode。多个NameNode的情况意味着有多个namespace(命名空间)，区别于HA模式下的多NameNode，它们是拥有着同一个namespace。既然说到了NameNode的命名空间的概念, 我们可以很明显地看出现有的HDFS数据管理,数据存储2层分层的结构.也就是说,所有关于存储数据的信息和管理是放在NameNode这边,而真实数据的存储则是在各个DataNode下.而这些隶属于同一个NameNode所管理的数据都是在同一个命名空间下的.而一个namespace对应一个block pool。Block Pool是同一个namespace下的block的集合.当然这是我们最常见的单个namespace的情况,也就是一个NameNode管理集群中所有元数据信息的时候.如果我们遇到了之前提到的NameNode内存使用过高的问题,这时候怎么办?元数据空间依然还是在不断增大,一味调高NameNode的jvm大小绝对不是一个持久的办法.这时候就诞生了HDFS Federation的机制. 3.2 Federation架构设计HDFS Federation是解决namenode内存瓶颈问题的水平横向扩展方案。 Federation意味着在集群中将会有多个namenode/namespace。这些namenode之间是联合的，也就是说，他们之间相互独立且不需要互相协调，各自分工，管理自己的区域。分布式的datanode被用作通用的数据块存储存储设备。每个datanode要向集群中所有的namenode注册，且周期性地向所有namenode发送心跳和块报告，并执行来自所有namenode的命令。 Federation一个典型的例子就是上面提到的NameNode内存过高问题,我们完全可以将上面部分大的文件目录移到另外一个NameNode上做管理.更重要的一点在于,这些NameNode是共享集群中所有的DataNode的,它们还是在同一个集群内的**。** 这时候在DataNode上就不仅仅存储一个Block Pool下的数据了,而是多个(在DataNode的datadir所在目录里面查看BP-xx.xx.xx.xx打头的目录)。 概括起来： 多个NN共用一个集群里的存储资源，每个NN都可以单独对外提供服务。 每个NN都会定义一个存储池，有单独的id，每个DN都为所有存储池提供存储。 DN会按照存储池id向其对应的NN汇报块信息，同时，DN会向所有NN汇报本地存储可用资源情况。 HDFS Federation不足 HDFS Federation并没有完全解决单点故障问题。虽然namenode/namespace存在多个，但是从单个namenode/namespace看，仍然存在单点故障：如果某个namenode挂掉了，其管理的相应的文件便不可以访问。Federation中每个namenode仍然像之前HDFS上实现一样，配有一个secondary namenode，以便主namenode挂掉一下，用于还原元数据信息。 所以一般集群规模真的很大的时候，会采用HA+Federation的部署方案。也就是每个联合的namenodes都是ha的。 总结: 主备:解决了单点故障,形成高可用. 联邦: 解决了namenode的内存问题. namenode的可用性决定了Hadoop的可用性]]></content>
      <tags>
        <tag>BigData</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zookeeper]]></title>
    <url>%2F2017%2F07%2F04%2Fzookeeper.html</url>
    <content type="text"><![CDATA[Zookeeper学习笔记一概述zookeeper是一个开源的分布式协调服务框架,主要解决分布式集群中应用系统的一致性问题和数据管理问题. 二特点zoo本质上为一个分布式文件系统适合存放小文件. Zookeeper 中存储的其实是一个又一个 Znode, Znode 是 Zookeeper 中的节点 Znode 是有路径的, 例如 /data/host1, /data/host2, 这个路径也可以理解为是 Znode 的 Name Znode 也可以携带数据, 例如说某个 Znode 的路径是 /data/host1, 其值是一个字符串 “192.168.0.1” 正因为 Znode 的特性, 所以 Zookeeper 可以对外提供出一个类似于文件系统的视图, 可以通过操作文件系统的方式操作 Zookeeper 使用路径获取 Znode 获取 Znode 携带的数据 修改 Znode 携带的数据 删除 Znode 添加 Znode 三应用场景3.1 数据发布/订阅 数据发布/订阅系统,需要发布者将数据发布到Zookeeper的节点上，供订阅者进行数据订阅，进而达到动态获取数据的目的，实现配置信息的集中式管理和数据的动态更新。 ​ 发布/订阅一般有两种设计模式：推模式和拉模式，服务端主动将数据更新发送给所有订阅的客户端称为推模式；客户端主动请求获取最新数据称为拉模式. Zookeeper采用了推拉相结合的模式，客户端向服务端注册自己需要关注的节点，一旦该节点数据发生变更，那么服务端就会向相应的客户端推送Watcher事件通知，客户端接收到此通知后，主动到服务端获取最新的数据。 3.2 命名服务 命名服务是分步实现系统中较为常见的一类场景，分布式系统中，被命名的实体通常可以是集群中的机器、提供的服务地址或远程对象等，通过命名服务，客户端可以根据指定名字来获取资源的实体，在分布式环境中，上层应用仅仅需要一个全局唯一的名字。Zookeeper可以实现一套分布式全局唯一ID的分配机制。 通过调用Zookeeper节点创建的API接口就可以创建一个顺序节点，并且在API返回值中会返回这个节点的完整名字，利用此特性，可以生成全局ID，其步骤如下 客户端根据任务类型，在指定类型的任务下通过调用接口创建一个顺序节点，如”job-“。 创建完成后，会返回一个完整的节点名，如”job-00000001”。 客户端拼接type类型和返回值后，就可以作为全局唯一ID了，如”type2-job-00000001”。 3.3 分布式协调/通知 Zookeeper中特有的Watcher注册于异步通知机制，能够很好地实现分布式环境下不同机器，甚至不同系统之间的协调与通知，从而实现对数据变更的实时处理。通常的做法是不同的客户端都对Zookeeper上的同一个数据节点进行Watcher注册，监听数据节点的变化（包括节点本身和子节点），若数据节点发生变化，那么所有订阅的客户端都能够接收到相应的Watcher通知，并作出相应处理。 在绝大多数分布式系统中，系统机器间的通信无外乎心跳检测、工作进度汇报和系统调度。 ① 心跳检测，不同机器间需要检测到彼此是否在正常运行，可以使用Zookeeper实现机器间的心跳检测，基于其临时节点特性（临时节点的生存周期是客户端会话，客户端若当即后，其临时节点自然不再存在），可以让不同机器都在Zookeeper的一个指定节点下创建临时子节点，不同的机器之间可以根据这个临时子节点来判断对应的客户端机器是否存活。通过Zookeeper可以大大减少系统耦合。 ② 工作进度汇报，通常任务被分发到不同机器后，需要实时地将自己的任务执行进度汇报给分发系统，可以在Zookeeper上选择一个节点，每个任务客户端都在这个节点下面创建临时子节点，这样不仅可以判断机器是否存活，同时各个机器可以将自己的任务执行进度写到该临时节点中去，以便中心系统能够实时获取任务的执行进度。 ③ 系统调度，Zookeeper能够实现如下系统调度模式：分布式系统由控制台和一些客户端系统两部分构成，控制台的职责就是需要将一些指令信息发送给所有的客户端，以控制他们进行相应的业务逻辑，后台管理人员在控制台上做一些操作，实际上就是修改Zookeeper上某些节点的数据，Zookeeper可以把数据变更以时间通知的形式发送给订阅客户端。 3.4分布式锁 分布式锁用于控制分布式系统之间同步访问共享资源的一种方式，可以保证不同系统访问一个或一组资源时的一致性，主要分为排它锁和共享锁。 排它锁又称为写锁或独占锁，若事务T1对数据对象O1加上了排它锁，那么在整个加锁期间，只允许事务T1对O1进行读取和更新操作，其他任何事务都不能再对这个数据对象进行任何类型的操作，直到T1释放了排它锁。 ① 获取锁，在需要获取排它锁时，所有客户端通过调用接口，在/exclusive_lock节点下创建临时子节点/exclusive_lock/lock。Zookeeper可以保证只有一个客户端能够创建成功，没有成功的客户端需要注册/exclusive_lock节点监听。 ② 释放锁，当获取锁的客户端宕机或者正常完成业务逻辑都会导致临时节点的删除，此时，所有在/exclusive_lock节点上注册监听的客户端都会收到通知，可以重新发起分布式锁获取。 共享锁又称为读锁，若事务T1对数据对象O1加上共享锁，那么当前事务只能对O1进行读取操作，其他事务也只能对这个数据对象加共享锁，直到该数据对象上的所有共享锁都被释放。在需要获取共享锁时，所有客户端都会到/shared_lock下面创建一个临时顺序节点 3.5 分布式队列 有一些时候，多个团队需要共同完成一个任务，比如，A团队将Hadoop集群计算的结果交给B团队继续计算，B完成了自己任务再交给C团队继续做。这就有点像业务系统的工作流一样，一环一环地传下 去. 分布式环境下，我们同样需要一个类似单进程队列的组件，用来实现跨进程、跨主机、跨网络的数据共享和数据传递，这就是我们的分布式队列。 四架构Zookeeper集群是一个基于主从架构的高可用集群 ​ 每个服务器承担如下三种角色中的一种 Leader 一个Zookeeper集群同一时间只会有一个实际工作的Leader，它会发起并维护与各Follwer及Observer间的心跳。所有的写操作必须要通过Leader完成再由Leader将写操作广播给其它服务器。 Follower 一个Zookeeper集群可能同时存在多个Follower，它会响应Leader的心跳。Follower可直接处理并返回客户端的读请求，同时会将写请求转发给Leader处理，并且负责在Leader处理写请求时对请求进行投票。 Observer 角色与Follower类似，但是无投票权。 1234若想使用observer模式可在对应节点的配置文件添加如下配置peerType=observer其次,必须在配置文件指定那些节点被指定为observer如:server.1:localhost:2181:3181:observer 5:Zookeeper的选举机制 Leader选举是保证分布式数据一致性的关键所在。当Zookeeper集群中的一台服务器出现以下两种情况之一时，需要进入Leader选举。 1##### 5.1. 服务器启动时期的Leader选举 若进行Leader选举，则至少需要两台机器，这里选取3台机器组成的服务器集群为例。在集群初始化阶段，当有一台服务器Server1启动时，其单独无法进行和完成Leader选举，当第二台服务器Server2启动时，此时两台机器可以相互通信，每台机器都试图找到Leader，于是进入Leader选举过程。选举过程如下 (1) 每个Server发出一个投票。由于是初始情况，Server1和Server2都会将自己作为Leader服务器来进行投票，每次投票会包含所推举的服务器的myid和ZXID，使用(myid, ZXID)来表示，此时Server1的投票为(1, 0)，Server2的投票为(2, 0)，然后各自将这个投票发给集群中其他机器。 (2) 接受来自各个服务器的投票。集群的每个服务器收到投票后，首先判断该投票的有效性，如检查是否是本轮投票、是否来自LOOKING状态的服务器。 (3) 处理投票。针对每一个投票，服务器都需要将别人的投票和自己的投票进行PK，PK规则如下 · 优先检查ZXID。ZXID比较大的服务器优先作为Leader。 · 如果ZXID相同，那么就比较myid。myid较大的服务器作为Leader服务器。 对于Server1而言，它的投票是(1, 0)，接收Server2的投票为(2, 0)，首先会比较两者的ZXID，均为0，再比较myid，此时Server2的myid最大，于是更新自己的投票为(2, 0)，然后重新投票，对于Server2而言，其无须更新自己的投票，只是再次向集群中所有机器发出上一次投票信息即可。 (4) 统计投票。每次投票后，服务器都会统计投票信息，判断是否已经有过半机器接受到相同的投票信息，对于Server1、Server2而言，都统计出集群中已经有两台机器接受了(2, 0)的投票信息，此时便认为已经选出了Leader。 (5) 改变服务器状态。一旦确定了Leader，每个服务器就会更新自己的状态，如果是Follower，那么就变更为FOLLOWING，如果是Leader，就变更为LEADING。 5.2.服务器运行时期的Leader选举 在Zookeeper运行期间，Leader与非Leader服务器各司其职，即便当有非Leader服务器宕机或新加入，此时也不会影响Leader，但是一旦Leader服务器挂了，那么整个集群将暂停对外服务，进入新一轮Leader选举，其过程和启动时期的Leader选举过程基本一致过程相同。 五 安装集群规划 服务器IP 主机名 myid的值 192.168.174.100 node01 1 192.168.174.110 node02 2 192.168.174.120 node03 3 第一步：下载zookeeeper的压缩包，下载网址如下 http://archive.apache.org/dist/zookeeper/ 我们在这个网址下载我们使用的zk版本为3.4.9 下载完成之后，上传到我们的linux的/export/softwares路径下准备进行安装 第二步：解压 解压zookeeper的压缩包到/export/servers路径下去，然后准备进行安装 123cd /export/softwaretar -zxvf zookeeper-3.4.9.tar.gz -C ../servers/ 第三步：修改配置文件 第一台机器修改配置文件 12345cd /export/servers/zookeeper-3.4.9/conf/cp zoo_sample.cfg zoo.cfgmkdir -p /export/servers/zookeeper-3.4.9/zkdatas/ vim zoo.cfg 123456789dataDir=/export/servers/zookeeper-3.4.9/zkdatas# 保留多少个快照autopurge.snapRetainCount=3# 日志多少小时清理一次autopurge.purgeInterval=1# 集群中服务器地址server.1=node01:2888:3888server.2=node02:2888:3888server.3=node03:2888:3888 第四步：添加myid配置 在第一台机器的 /export/servers/zookeeper-3.4.9/zkdatas /这个路径下创建一个文件，文件名为myid ,文件内容为1 echo 1 &gt; /export/servers/zookeeper-3.4.9/zkdatas/myid 第五步：安装包分发并修改myid的值 安装包分发到其他机器 第一台机器上面执行以下两个命令 scp -r /export/servers/zookeeper-3.4.9/ node02:/export/servers/ scp -r /export/servers/zookeeper-3.4.9/ node03:/export/servers/ 第二台机器上修改myid的值为2 echo 2 &gt; /export/servers/zookeeper-3.4.9/zkdatas/myid 第三台机器上修改myid的值为3 echo 3 &gt; /export/servers/zookeeper-3.4.9/zkdatas/myid 第六步：三台机器启动zookeeper服务 三台机器启动zookeeper服务 这个命令三台机器都要执行 /export/servers/zookeeper-3.4.9/bin/zkServer.sh start 查看启动状态 /export/servers/zookeeper-3.4.9/bin/zkServer.sh status 六 zookeeper的shell操作1登录shell客户端: bin/zkCli.sh -server node:2181 命令 说明 参数 create [-s] [-e] path data acl 创建Znode -s 指定是顺序节点-e 指定是临时节点 ls path [watch] 列出Path下所有子Znode get path [watch] 获取Path对应的Znode的数据和属性 ls2 path [watch] 查看Path下所有子Znode以及子Znode的属性 set path data [version] 更新节点 version 数据版本 delete path [version] 删除节点, 如果要删除的节点有子Znode则无法删除 version 数据版本 rmr path 删除节点, 如果有子Znode则递归删除 `setquota -n -b val path` 修改Znode配额 -n 设置子节点最大个数-b 设置节点数据最大长度 history 列出历史记录 只有ls,ls2,get可以添加watch. 1：创建普通节点 create /app1 hello 2: 创建顺序节点 create -s /app3 world 3:创建临时节点 create -e /tempnode world 4:创建顺序的临时节点 create -s -e /tempnode2 aaa 5:获取节点数据 get /app1 6:修改节点数据 set /app1 xxx 7:删除节点 delete /app1 删除的节点不能有子节点 ​ rmr /app1 递归删除 Znode 的特点 文件系统的核心是 Znode 如果想要选取一个 Znode, 需要使用路径的形式, 例如 /test1/test11 Znode 本身并不是文件, 也不是文件夹, Znode 因为具有一个类似于 Name 的路径, 所以可以从逻辑上实现一个树状文件系统 ZK 保证 Znode 访问的原子性, 不会出现部分 ZK 节点更新成功, 部分 ZK 节点更新失败的问题 Znode 中数据是有大小限制的, 最大只能为1M Znode是由三个部分构成 stat: 状态, Znode的权限信息, 版本等 data: 数据, 每个Znode都是可以携带数据的, 无论是否有子节点 children: 子节点列表 Znode 的类型 每个Znode有两大特性, 可以构成四种不同类型的Znode 持久性 持久 客户端断开时, 不会删除持有的Znode 临时 客户端断开时, 删除所有持有的Znode, 临时Znode不允许有子Znode 顺序性 有序 创建的Znode有先后顺序, 顺序就是在后面追加一个序列号, 序列号是由父节点管理的自增,它的格式为“%10d”(10 位数字，没有数值的数位用 0 补充，例如“0000000001”)。 无序 创建的Znode没有先后顺序 Znode的属性 dataVersion 数据版本, 每次当Znode中的数据发生变化的时候, dataVersion都会自增一下 cversion 节点版本, 每次当Znode的节点发生变化的时候, cversion都会自增 aclVersion ACL(Access Control List)的版本号, 当Znode的权限信息发生变化的时候aclVersion会自增 zxid 事务ID ctime 创建时间 mtime 最近一次更新的时间 ephemeralOwner 如果Znode为临时节点, ephemeralOwner表示与该节点关联的SessionId watcher机制 通知类似于数据库中的触发器, 对某个Znode设置 Watcher, 当Znode发生变化的时候, WatchManager会调用对应的Watcher 当Znode发生删除, 修改, 创建, 子节点修改的时候, 对应的Watcher会得到通知 Watcher的特点 一次性触发 一个 Watcher 只会被触发一次, 如果需要继续监听, 则需要再次添加 Watcher 事件封装: Watcher 得到的事件是被封装过的, 包括三个内容 keeperState, eventType, path KeeperState EventType 触发条件 说明 None 连接成功 SyncConnected NodeCreated Znode被创建 此时处于连接状态 SyncConnected NodeDeleted Znode被删除 此时处于连接状态 SyncConnected NodeDataChanged Znode数据被改变 此时处于连接状态 SyncConnected NodeChildChanged Znode的子Znode数据被改变 此时处于连接状态 Disconnected None 客户端和服务端断开连接 此时客户端和服务器处于断开连接状态 Expired None 会话超时 会收到一个SessionExpiredException AuthFailed None 权限验证失败 会收到一个AuthFailedException 会话 在ZK中所有的客户端和服务器的交互都是在某一个Session中的, 客户端和服务器创建一个连接的时候同时也会创建一个Session Session会在不同的状态之间进行切换: CONNECTING, CONNECTED, RECONNECTING, RECONNECTED, CLOSED ZK中的会话两端也需要进行心跳检测, 服务端会检测如果超过超时时间没收到客户端的心跳, 则会关闭连接, 释放资源, 关闭会话 七 zookeeper的JavaAPI操作​ 这里操作Zookeeper的JavaAPI使用的是一套zookeeper客户端框架 Curator ，解决了很多Zookeeper客户端非常底层的细节开发工作 。 Curator包含了几个包： curator-framework：对zookeeper的底层api的一些封装 curator-recipes：封装了一些高级特性，如：Cache事件监听、选举、分布式锁、分布式计数器等 PERSISTENT：永久节点 EPHEMERAL：临时节点 PERSISTENT_SEQUENTIAL：永久节点、序列化 EPHEMERAL_SEQUENTIAL：临时节点、序列化 Maven依赖(使用curator的版本：2.12.0，对应Zookeeper的版本为：3.4.x，如果跨版本会有兼容性问题，很有可能导致节点操作失败)： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849maven依赖: &lt;!-- &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;cloudera&lt;/id&gt; &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos/&lt;/url &lt;/repository&gt; &lt;/repositories&gt; --&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.curator&lt;/groupId&gt; &lt;artifactId&gt;curator-framework&lt;/artifactId&gt; &lt;version&gt;2.12.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.curator&lt;/groupId&gt; &lt;artifactId&gt;curator-recipes&lt;/artifactId&gt; &lt;version&gt;2.12.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.google.collections&lt;/groupId&gt; &lt;artifactId&gt;google-collections&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-simple&lt;/artifactId&gt; &lt;version&gt;1.7.25&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;!-- java编译插件 --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.2&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; 创建永久节点: 123456789101112131415@Testpublic void createNode() throws Exception &#123;//参数一 重试连接间隔 参数二 重试次数 RetryPolicy retryPolicy = new ExponentialBackoffRetry(1000, 1); //获取客户端对象 第一个1000 session超时时间,第二个1000连接超时时间 CuratorFramework client = CuratorFrameworkFactory.newClient("192.168.174.100:2181,192.168.174.110:2181,192.168.174.120:2181", 1000, 1000, retryPolicy); //调用start开启客户端操作 client.start(); //通过create来进行创建节点，并且需要指定节点类型 client.create().creatingParentsIfNeeded().withMode(CreateMode.PERSISTENT).forPath("/hello3/world");//关闭资源 client.close();&#125; 创建临时节点: 123456789@Testpublic void createNode2() throws Exception&#123; RetryPolicy retryPolicy = new ExponentialBackoffRetry(3000, 1); CuratorFramework client = CuratorFrameworkFactory.newClient("node01:2181,node02:2181,node03:2181", 3000, 3000, retryPolicy);client.start();client.create().creatingParentsIfNeeded().withMode(CreateMode.EPHEMERAL).forPath("/hello5/world");Thread.sleep(5000);client.close();&#125; 修改节点数据 123456789101112131415161718192021/** * 节点下面添加数据与修改是类似的，一个节点下面会有一个数据，新的数据会覆盖旧的数据 * @throws Exception */ @Test public void nodeData() throws Exception &#123; RetryPolicy retryPolicy = new ExponentialBackoffRetry(3000, 1); CuratorFramework client = CuratorFrameworkFactory.newClient("node01:2181,node02:2181,node03:2181", 3000, 3000, retryPolicy); client.start(); client.setData().forPath("/hello5", "hello7".getBytes()); client.close();&#125; 节点数据查询 123456789@Test public void demo03() throws Exception &#123; RetryPolicy retryPolicy = new ExponentialBackoffRetry(3000, 1); CuratorFramework client = CuratorFrameworkFactory.newClient("192.168.80.100:2181,192.168.80.110:2181,192.168.80.120:2181", 3000, 3000, retryPolicy); client.start(); byte[] forPath = client.getData().forPath("/aaa3"); System.out.println(new String(forPath)+"12"); client.close(); &#125; 删除节点: 12345678@Test public void demo04() throws Exception&#123; RetryPolicy retryPolicy = new ExponentialBackoffRetry(3000, 1); CuratorFramework client = CuratorFrameworkFactory.newClient("192.168.80.100:2181,192.168.80.110:2181,192.168.80.120:2181", 3000, 3000, retryPolicy); client.start(); client.delete().deletingChildrenIfNeeded().forPath("/aaa3"); client.close(); &#125; 节点的监听: 123456789101112131415161718192021222324252627282930313233343536@Test public void demo05() throws Exception&#123; RetryPolicy policy = new ExponentialBackoffRetry(3000, 3); CuratorFramework client = CuratorFrameworkFactory.newClient("192.168.80.100:2181,192.168.80.110:2181,192.168.80.120:2181", policy); client.start(); // ExecutorService pool = Executors.newCachedThreadPool(); //设置节点的cache TreeCache treeCache = new TreeCache(client, "/hello5"); //设置监听器和处理过程 treeCache.getListenable().addListener(new TreeCacheListener() &#123; @Override public void childEvent(CuratorFramework client, TreeCacheEvent event) throws Exception &#123; ChildData data = event.getData(); if(data !=null)&#123; switch (event.getType()) &#123; case NODE_ADDED: System.out.println("NODE_ADDED : "+ data.getPath() +" 数据:"+ new String(data.getData())); break; case NODE_REMOVED: System.out.println("NODE_REMOVED : "+ data.getPath() +" 数据:"+ new String(data.getData())); break; case NODE_UPDATED: System.out.println("NODE_UPDATED : "+ data.getPath() +" 数据:"+ new String(data.getData())); break; default: break; &#125; &#125;else&#123; System.out.println( "data is null : "+ event.getType()); &#125; &#125; &#125;); //开始监听 treeCache.start(); Thread.sleep(50000000); &#125;]]></content>
      <tags>
        <tag>BigData</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Shell]]></title>
    <url>%2F2017%2F07%2F04%2FShell.html</url>
    <content type="text"><![CDATA[Shell一简介​ Shell 是一个用 C 语言编写的程序， 通过 Shell 用户可以访问操作系统内核服务。它类似于 DOS 下的 command 和后来的 cmd.exe。Shell 既是一种命令语言，又是一种程序设计语言。 ​ Shell script 是一种为 shell 编写的脚本程序。 Shell 编程一般指 shell脚本编程，不是指开发 shell 自身。​ Shell 编程跟 java、 php 编程一样，只要有一个能编写代码的文本编辑器和一个能解释执行的脚本解释器就可以了。​ Linux 的 Shell 种类众多，一个系统可以存在多个 shell，可以通过 cat/etc/shells 命令查看系统中安装的 shell。Bash 由于易用和免费，在日常工作中被广泛使用。同时， Bash 也是大多数Linux 系统默认的 Shell. 二 基本格式​ 使用 vi 编辑器新建一个文件 hello.sh。 扩展名并不影响脚本执行，见名知意。 比如用 php 写 shell 脚本，扩展名就用 .php。 例如 #!是一个约定的标记，它告诉系统这个脚本需要什么解释器来执行，即使用哪一种 Shell。 echo 用于向窗口输出文本. 12#! /bin/bashecho "hello shell" 执行: chmod +x ./hello.sh #使脚本具有执行权限 ./hello.sh #执行脚本 直接写 hello.sh， linux系统会去PATH里寻找有没有叫 hello.sh的。 用 ./hello.sh 告诉系统说，就在当前目录找。 还可以作为解释器参数运行。 直接运行解释器，其参数就是 shell 脚本的文件名，如： /bin/sh /root/hello.sh /bin/php test.php 这种方式运行脚本，不需要在第一行指定解释器信息，写了也不生效 三shell变量注意: 除了等号不空格,其他处处都空格 变量=值 you=”buca” 注意: 变量名和等号之间不能有空格，同时，变量名的命名须遵循如下规则：l 首个字符必须为字母（ a-z， A-Z）l 中间不能有空格，可以使用下划线l 不能使用标点符号l 不能使用 bash里的关键字(可用 help 命令查看保留关键字 123name="nicai"echo $name ##使用变量echo $&#123;name&#125; ##花括号是可选的，加不加都行，加花括号是为了帮助解释器识别变量的边界。已定义的变量，可以被重新定义。 使用 readonly 命令可以将变量定义为只读变量，只读变量的值不能被改变。使用 unset 命令可以删除变量。 不能删除只读变量。 12readonly variable nameunset variable name 变量类型: 局部变量: ​ 局部变量在脚本或命令中定义，仅在当前 shell 实例中有效，其他 shell 启动的程序不能访问局部变量。 环境变量: ​ 所有的程序，包括 shell 启动的程序，都能访问环境变量，有些程序需要环境变量来保证其正常运行。 可以用过 set 命令查看当前环境变量。 shell变量: ​ shell 变量是由 shell 程序设置的特殊变量。 shell 变量中有一部分是环境变量，有一部分是局部变量，这些变量保证了 shell 的正常运行 参数传递: 在执行 Shell 脚本时， 可以向脚本传递参数。脚本内获取参数的格式为： 1$n n 代表一个数字， 1 为执行脚本的第一个参数， 2 为执行脚本的第二个参数，以此类推……$0 表示当前脚本名称。 特殊字符: $# 传递到脚本的参数个数 $* 以一个单字符串显示所有向脚本传递的参数。 $$ 脚本运行的当前进程 ID 号 $! 后台运行的最后一个进程的 ID 号 $@ 与$*相同，但是使用时加引号，并在引号中返回每个参数。 $? 显示最后命令的退出状态。 0 表示没有错误，其他任何值表明有错误。 例子: 12345#!/bin/bashecho "第一个参数为： $1";echo "参数个数为： $#";echo "传递的参数作为一个字符串显示： $*";执行脚本： ./test.sh 1 2 3 1234567$*和$@区别相同点： 都表示传递给脚本的所有参数。不同点：不被" "包含时， $*和$@都以$1 $2… $n 的形式组成参数列表。被" "包含时， "$*" 会将所有的参数作为一个整体，以"$1 $2 … $n"的形式组成一个整串； "$@" 会将各个参数分开，以"$1" "$2" … "$n" 的形式组成一个参数列表。 四 shell运算符Shell 和其他编程语音一样，支持包括：算术、关系、 布尔、字符串等运 算符。 原生 bash 不支持简单的数学运算，但是可以通过其他命令来实现，例如 expr。 expr 是一款表达式计算工具，使用它能完成表达式的求值操作。例如加，减，乘，除等操作 注意：表达式和运算符之间要有，例如 2+2 是不对的，必须写成 2 + 2。完整的表达式要被 包含，注意不是单引号，在 Esc 键下边。 123456789101112#!/bin/bashecho "hello world"a=4b=20#加法运算echo `expr $a + $b`#减法运算echo `expr $b - $a`#乘法运算，注意*号前面需要反斜杠echo `expr $a \* $b`#除法运算echo `expr $b / $a` 123456此外，还可以通过(())、 $[]进行算术运算。count=1;((count++));echo $count;a=$((1+2));a=$[1+2]; 五 流程控制1 if else格式 123456789if condition1thencommand1elif condition2thencommand2elsecommandNfi 条件表达式: 123456EQ 就是 EQUAL等于NQ 就是 NOT EQUAL不等于 GT 就是 GREATER THAN大于 LT 就是 LESS THAN小于 GE 就是 GREATER THAN OR EQUAL 大于等于 LE 就是 LESS THAN OR EQUAL 小于等于 例子: 123456789101112131415161718192021222324252627282930313233343536373839#!/bin/basha=10b=20if [ $a -eq $b ]then echo "$a -eq $b : a 等于 b"else echo "$a -eq $b: a 不等于 b"fiif [ $a -ne $b ]then echo "$a -ne $b: a 不等于 b"else echo "$a -ne $b : a 等于 b"fiif [ $a -gt $b ]then echo "$a -gt $b: a 大于 b"else echo "$a -gt $b: a 不大于 b"fiif [ $a -lt $b ]then echo "$a -lt $b: a 小于 b"else echo "$a -lt $b: a 不小于 b"fiif [ $a -ge $b ]then echo "$a -ge $b: a 大于或等于 b"else echo "$a -ge $b: a 小于 b"fiif [ $a -le $b ]then echo "$a -le $b: a 小于或等于 b"else echo "$a -le $b: a 大于 b"fi 2for循环方式一 12345678for N in 1 2 3doecho $Ndone或for N in 1 2 3; do echo $N; done或for N in &#123;1..3&#125;; do echo $N; done 方式二 123456for ((i = 0; i &lt;= 5; i++))doecho "welcome $i times"done或for ((i = 0; i &lt;= 5; i++)); do echo "welcome $i times"; done 例子: 1234567891011121314151617#! /bin/bashfor n in 1 2 3doecho $ndonea=1b=2c=3for N in $a $b $cdo echo $Ndone#打印当前系统所有进程for N in `ps -ef`do echo $Ndone 3 while语法方式一 12345while expressiondocommand…done 方式二 1234567#!/bin/bashi=1while (( i &lt;= 3))do let i++ echo $idone let 命令是 BASH 中用于计算的工具，用于执行一个或多个表达式，变量计算中不需要加上 $ 来表示变量。 自加操作： letno++ 自减操作： let no– 无限循环: 1234while truedocommanddone 3 case语句1234567891011121314case 值 in模式 1)command1command2...commandN;;模式 2）command1command2...commandN;;esac 例子: read aNum 等待键盘输入 12345678910111213141516#!/bin/bashecho '输入 1 到 4 之间的数字:'echo '你输入的数字为:'read aNumcase $aNum in 1) echo '你选择了 1' ;; 2) echo '你选择了 2' ;; 3) echo '你选择了 3' ;; 4) echo '你选择了 4' ;; *) echo '你没有输入 1 到 4 之间的数字' ;;esac 六函数的使用12345678#! /bin/bashhello()&#123;echo "hello"echo "第一个参数为 $1"echo "第二个参数为 $2"&#125;hello adv 123]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux常用命令]]></title>
    <url>%2F2017%2F07%2F03%2FLinux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4.html</url>
    <content type="text"><![CDATA[一 目录结构 Linux系统它是文件系统。 它的根目录 是”/”,是以树型结构来管理。 root用户登录后，显示一个~ ，表示root目录 root目录 管理员的 其他用户的在home目录中 二常用命令1． 切换目录命令cd：12345使用cd app 切换到app目录cd .. 切换到上一层目录cd / 切换到系统根目录cd~ 切换到root用户主目录cd - 切换到上一个所在目录 2． 列出文件列表：ls ll dir(*)ls(list)是一个非常有用的命令，用来显示当前目录下的内容。配合参数的使用，能以不同的方式显示目录内容。 格式：ls[参数] [路径或文件名] 常用： 1234在linux中以 . 开头的文件都是隐藏的文件* ls* ls -a 显示所有文件或目录（包含隐藏的文件）* ls -l 缩写成ll 3． 创建目录和移除目录：mkdir rmdir123456mkdir(make directory)命令可用来创建子目录。mkdir app 在当前目录下创建app目录mkdir –p app2/test 级联创建aap2以及test目 递归创建rmdir(remove directory)命令可用来删除“空”的子目录：rmdir app 删除app目录 4． 浏览文件【cat、more、less】 12cat操作：展示所有内容—适合查看小文件cat a.txt 1234567more与less用法类似 ： 适合查看大文件—翻页查看more a.logless b.log空格 ：查看下一页Enter ：查看下一行Q ：退出Less 可以使用上下箭头查看行。 123456tail命令是在实际使用过程中使用非常多的一个命令，它的功能是：用于显示文件后几行的内容。用法:tail -10 /etc/passwd 查看后10行数据tail -f catalina.log 动态查看日志(*****)ctrl+c 12345Ctrl+c 与ctrl+z 的区别两者都为中断进程Ctrl+c 是强制中断程序的执行ctrl+z 是将任务中断,但任务并未结束,他任然在进程中只是挂起了,用户可以使用fg/bg操作继续前台或后台的任务,fg命令重新启动前台被中断的任务,bg命令把被中断的任务放在后台执行. 5． 文件操作：cp是copy操作 mv它是move相当于剪切 123456789cp(copy)命令可以将文件从一处复制到另一处。一般在使用cp命令时将一个文件复制成另一个文件或复制到某目录时，需要指定源文件名与目标文件名或目录。cp a.txt b.txt  将a.txt复制为b.txt文件cp a.txt ../  将a.txt文件复制到上一层目录中mv 移动或者重命名mv a.txt ../  将a.txt文件移动到上一层目录中mv a.txt b.txt  将a.txt文件重命名为b.txtmv a.txt /b/b.txt 将a.txt移动到根目录下的b目录下的b.txt tar 命令 12345678910tar命令位于/bin目录下，它能够将用户所指定的文件或目录打包成一个文件，但不做压缩。一般Linux上常用的压缩方式是选用tar将许多文件打包成一个文件，再以gzip压缩命令压缩成xxx.tar.gz(或称为xxx.tgz)的文件。常用参数：-c：创建一个新tar文件-v：显示运行过程的信息-f：指定文件名-z：调用gzip压缩命令进行压缩-t：查看压缩文件的内容-x：解开tar文件打包：tar –cvf xxx.tar ./*打包并且压缩：tar –zcvf xxx.tar.gz ./* 解压 tar –xvf xxx.tartar -xvf xxx.tar.gz -C /usr/aaa find 命令 1234567find指令用于查找符合条件的文件示例：find / -name “ins*” 查找文件名称是以ins开头的文件find / -name “ins*” –ls find / –user itcast –ls 查找用户itcast的文件find / –user itcast –type d –ls 查找用户itcast的目录find /-perm -777 –type d-ls 查找权限是777的文件 grep 命令 常与|命令一起使用 1234查找文件里符合条件的字符串。用法: grep [选项]... PATTERN [FILE]...示例：grep lang anaconda-ks.cfg 在文件中查找langgrep lang anaconda-ks.cfg –color 高亮显示 6． 其他常用命令【pwd】 显示当前所在目录 【touch】 创建一个空文件 * touch a.txt 【ll -h】 友好显示文件大小 【wget】 下载资料 * wget http://nginx.org/download/nginx-1.9.12.tar.gz 7vi 和vim 编辑器123456789101112131415161718192021222324252627282930在Linux下一般使用vi编辑器来编辑文件。vi既可以查看文件也可以编辑文件。三种模式：命令行、插入、底行模式。切换到命令行模式：按Esc键；切换到插入模式：按 i 、o、a键； i 在当前位置前插入 I 在当前行首插入 a 在当前位置后插入 A 在当前行尾插入 o 在当前行之后插入一行 O 在当前行之前插入一行切换到底行模式：按 :（冒号）；更多详细用法，查询文档《Vim命令合集.docx》和《vi使用方法详细介绍.docx》打开文件：vim file退出：esc  :q修改文件：输入i进入插入模式保存并退出：esc:wq不保存退出：esc:q!三种进入插入模式：i:在当前的光标所在处插入o:在当前光标所在的行的下一行插入a:在光标所在的下一个字符插入快捷键：dd – 快速删除一行yy - 复制当前行nyy - 从当前行向后复制几行p - 粘贴R – 替换 8 重定向输出&gt; &gt;&gt;12345&gt; 重定向输出，覆盖原有内容；&gt;&gt; 重定向输出，又追加功能；示例：cat /etc/passwd &gt; a.txt 将输出定向到a.txt中cat /etc/passwd &gt;&gt; a.txt 输出并且追加ifconfig &gt; ifconfig.txt 9 管道1234567管道是Linux命令中重要的一个概念，其作用是将一个命令的输出用作另一个命令的输入。示例ls --help | more 分页查询帮助信息ps –ef | grep java 查询名称中包含java的进程ifconfig | morecat index.html | moreps –ef | grep aio 10 &amp;&amp;命令执行控制12345命令之间使用 &amp;&amp; 连接，实现逻辑与的功能。 只有在 &amp;&amp; 左边的命令返回真（命令返回值 $? == 0），&amp;&amp; 右边的命令才会被执行。 只要有一个命令返回假（命令返回值 $? == 1），后面的命令就不会被执行。mkdir test &amp;&amp; cd test 11 系统管理命令12345678910111213141516171819date 显示或设置系统时间date 显示当前系统时间date -s “2014-01-01 10:10:10“ 设置系统时间df 显示磁盘信息df –h 友好显示大小free 显示内存状态free –m 以mb单位显示内存组昂头top 显示，管理执行中的程序clear 清屏幕ps 正在运行的某个进程的状态ps –ef 查看所有进程ps –ef | grep ssh 查找某一进程kill 杀掉某一进程kill 2868 杀掉2868编号的进程kill -9 2868 强制杀死进程du 显示目录或文件的大小。du –h 显示当前目录的大小who 显示目前登入系统的用户信息。uname 显示系统信息。uname -a 显示本机详细信息。依次为：内核名称(类别)，主机名，内核版本号，内核版本，内核编译日期，硬件名，处理器类型，硬件平台类型，操作系统名称 12 用户和组1用户 12345678910111213useradd 添加一个用户useradd test 添加test用户useradd test -d /home/t1 指定用户home目录passwd 设置、修改密码passwd test 为test用户设置密码切换登录：ssh -l test -p 22 192.168.19.128su – 用户名userdel 删除一个用户userdel test 删除test用户(不会删除home目录)userdel –r test 删除用户以及home目录 2 组 123456当在创建一个新用户user时，若没有指定他所属于的组，就建立一个和该用户同名的私有组创建用户时也可以指定所在组groupadd 创建组groupadd public 创建一个名为public的组useradd u1 –g public 创建用户指定组groupdel 删除组，如果该组有用户成员，必须先删除用户才能删除组。groupdel public 3 ID su 命令 123功能：查看一个用户的UID和GID用法：id [选项]... [用户名]直接使用id直接使用id 用户名 123功能：切换用户。用法：su [选项]... [-] [用户 [参数]... ]示例：su u1 切换到u1用户su - u1 切换到u1用户，并且将环境也切换到u1用户的环境（推荐使用） 12345678910111213/etc/passwd 用户文件/etc/shadow 密码文件/etc/group 组信息文件用户文件root:x:0:0:root:/root:/bin/bash账号名称： 在系统中是唯一的用户密码： 此字段存放加密口令用户标识码(User ID)： 系统内部用它来标示用户组标识码(Group ID)： 系统内部用它来标识用户属性用户相关信息： 例如用户全名等用户目录： 用户登录系统后所进入的目录用户环境: 用户工作的环境密码文件shadow文件中每条记录用冒号间隔的9个字段组成.用户名：用户登录到系统时使用的名字，而且是惟一的口令： 存放加密的口令最后一次修改时间: 标识从某一时刻起到用户最后一次修改时间最大时间间隔: 口令保持有效的最大天数，即多少天后必须修改口令最小时间间隔： 再次修改口令之间的最小天数警告时间：从系统开始警告到口令正式失效的天数不活动时间： 口令过期少天后，该账号被禁用失效时间：指示口令失效的绝对天数(从1970年1月1日开始计算)标志：未使用组文件root:x:0:组名：用户所属组组口令：一般不用GID：组ID用户列表：属于该组的所有用户 13 权限命令123456789drwxr-xr-x.d 文件类型 rwx r:对文件是指可读取内容 对目录是可以lsw:对文件是指可修改文件内容，对目录 是指可以在其中创建或删除子节点(目录或文件)x:对文件是指是否可以运行这个文件，对目录是指是否可以cd进入这个目录rwx 属主r-x 属组r-x 其他用户 1234普通文件： 包括文本文件、数据文件、可执行的二进制程序文件等。 目录文件： Linux系统把目录看成是一种特殊的文件，利用它构成文件系统的树型结构。 设备文件： Linux系统把每一个设备都看成是一个文件普通文件（-）目录（d）符号链接（l） 1234567权限管理chmod 变更文件或目录的权限。chmod 755 a.txt chmod u=rwx,g=rx,o=rx a.txtchmod 000 a.txt / chmod 777 a.txtchown 变更文件或目录改文件所属用户和组chown u1:public a.txt ：变更当前的目录或文件的所属用户和组chown -R u1:public dir ：变更目录中的所有的子目录及文件的所属用户和组 14 网络操作1234主机名hostname 查看主机名hostname xxx 修改主机名 重启后无效如果想要永久生效，可以修改/etc/sysconfig/network文件 1234567ip地址setup设置ip地址ifconfig 查看(修改)ip地址(重启后无效)ifconfig eth0 192.168.12.22 修改ip地址如果想要永久生效修改 /etc/sysconfig/network-scripts/ifcfg-eth0文件 12域名映射/etc/hosts文件用于在通过主机名进行访问时做ip地址解析之用 12345678910111213141516网络服务管理service network status 查看网络的状态service network stop 停止网络服务service network start 启动网络服务service network restart 重启网络服务service --status–all 查看系统中所有后台服务netstat –nltp 查看系统中网络进程的端口监听情况防火墙设置防火墙根据配置文件/etc/sysconfig/iptables来控制本机的”出”、”入”网络访问行为。service iptables status 查看防火墙状态service iptables stop 关闭防火墙service iptables start 启动防火墙chkconfig iptables off 禁止防火墙自启 123mysql服务打开、关闭、查看状态service mysqld start、stop、status 15 其他yum 安装 yum install -y telnet 测试机器之间能否通信 ping 192.122… 测试能否与某个应用（比如mysql）通信 telnet 192.123.. 3306 查看进程 ps -ef 过滤相关信息 grep netstat -nltp | grep 3306 查看端口 jps | grep NameNode cat | grep -v “#” 查看文件 cat filename more filename tail -f/-F/-300f filename 查看文件后300行 head [-number]filename查看文件头 节点传送文件 scp -r /export/servers/hadoop node02:/export/servers scp -r /export/servers/hadoop node02:$PWD (发送到当前同级目录) scp -r /export/servers/hadoop user@node02:/export/servers 查看日期 date date +”%Y-%m-%d %H:%M:%S” date -d “-1 day” +”%Y-%m-%d %H:%M:%S” 创建文本 while true; do echo 1 &gt;&gt; /root/a.txt ; sleep 1;done 2、用户管理 添加用户 useradd username 更改用户密码 password username 删除用户 userdel username 删除用户（不删除用户数据 userdel -r username 删除用户和用户数据 3、压缩包管理 gz压缩包 tar czf file.tar.gz file 制作file的压缩包 tar zxvf file.tar.gz -C /directory 解压缩包 zip压缩包 zip file.zip file 将file制成名为file.zip unzip file.zip 解压缩 4、查看属性 查看磁盘大小 df -h 查看内存大小 free -h 查看文件大小 du -h 清理缓存 echo 1 &gt; /proc/sys/vm/drop_caches 12345Ctrl+c 与ctrl+z 的区别两者都为中断进程Ctrl+c 是强制中断程序的执行ctrl+z 是将任务中断,但任务并未结束,他任然在进程中只是挂起了,用户可以使用fg/bg操作继续前台或后台的任务,fg命令重新启动前台被中断的任务,bg命令把被中断的任务放在后台执行. 123456/etc/hosts 修改主机名与IP映射/etc/sysconfig/network 修改主机名visudo 设置其他用户权限/etc/profile 配置环境变量/etc/udev/rules.d/70-persistent-net.rules 更改mac地址/etc/sysconfig/network-scripts/ifcfg-eth0 更改IP地址 12345678三台虚拟机免密登录1,三台机器生成公钥与私钥ssh -keygen -t rsa2.三台机器都把 公钥拷到同一台机器ssh-copy-id node01.hadoop.com3.复制第一台机器(有三个公钥的那个)的认证到其他两台机器scp /root/.ssh/authorized_keys node02.hadoop.com:$PWDscp /root/.ssh/authorized_keys node03.hadoop.com:/root/.ssh 123456789虚拟机的时钟同步 通过网络连接外网进行时钟同步,必须保证虚拟机连上外网 ntpdate us.pool.ntp.org 阿里云时钟同步服务器 ntpdate ntp4.aliyun.com定时任务 定时同步时钟crontab -e */1 * * * * /usr/sbin/ntpdate ntp4.aliyun.com;*/1 * * * * /usr/sbin/ntpdate us.pool.ntp.org; 二选一]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux开发环境搭建]]></title>
    <url>%2F2017%2F07%2F03%2Flinux%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA.html</url>
    <content type="text"><![CDATA[linux常用软件安装如有需要,自行参考 一 mysql的安装第一步：在线安装mysql相关的软件包 1yum install mysql mysql-server mysql-devel 第二步：启动mysql的服务 /etc/init.d/mysqld start 第三步：通过mysql安装自带脚本进行设置 /usr/bin/mysql_secure_installation 第四步：进入mysql的客户端然后进行授权 12 grant all privileges on *.* to 'root'@'%' identified by '123456' with grant option;flush privileges; yum 安装的卸载 一、使用以下命令查看当前安装mysql情况，查找以前是否装有mysql 1`rpm -qa|``grep` `-i mysql` 显示之前安装了： ​ MySQL-client-5.5.25a-1.rhel5 ​ MySQL-server-5.5.25a-1.rhel5 2、停止mysql服务、删除之前安装的mysql service mysqld stop 删除命令：rpm -e –nodeps 包名 1`rpm -ev MySQL-client-5.5.25a-1.rhel5 ``rpm -ev MySQL-server-5.5.25a-1.rhel5` 如果提示依赖包错误，则使用以下命令尝试 1`rpm -ev MySQL-client-5.5.25a-1.rhel5 --nodeps` 如果提示错误：error: %preun(xxxxxx) scriptlet failed, exit status 1 则用以下命令尝试： 1`rpm -e --noscripts MySQL-client-5.5.25a-1.rhel5` 3、查找之前老版本mysql的目录、并且删除老版本mysql的文件和库 1`find` `/ -name mysql` 查找结果如下： 1`find` `/ -name mysql ` `/var/lib/mysql``/var/lib/mysql/mysql``/usr/lib64/mysql` 删除对应的mysql目录 1`rm` `-rf ``/var/lib/mysql``rm` `-rf ``/var/lib/mysql``rm` `-rf ``/usr/lib64/mysql` 注意：卸载后/etc/my.cnf不会删除，需要进行手工删除 1`rm` `-rf ``/etc/my``.cnf` 4、再次查找机器是否安装mysql 1`rpm -qa|``grep` `-i mysql` 二 jdk的安装1.1 查看自带的openjdk并卸载 查询 rpm -qa | grep java 卸载 rpm -e java-1.6.0-openjdk-1.6.0.41-1.13.13.1.el6_8.x86_64 tzdata-java-2016j-1.el6.noarch java-1.7.0-openjdk-1.7.0.131-2.6.9.0.el6_8.x86_64 --nodeps –nodeps 不管依赖直接删 2上传解压 1tar -zxvf jdk-8u141-linux-x64.tar.gz 3配置环境变量 12345vim /etc/profile添加export JAVA_HOME=/export/servers/jdk1.8.0_141export PATH=:$JAVA_HOME/bin:$PATH 4 生效 1source /etc/profile zookeeper的安装Hadoop的安装hive的安装1上传压塑包并解压 1tar -zxvf apache-hive-2.1.1-bin.tar.gz 2 解压后可重命名 1mv apache-hive-2.1.1-bin hive 3 安装mysql省略 4修改配置文件 12345cp hive-env.sh.template hive-env.sh配置HADOOP_HOME=/export/servers/hadoop-2.7.5export HIVE_CONF_DIR=/export/servers/hive/conf 创建文件: 12345678910111213141516171819202122232425262728293031323334vim hive-site.xml&lt;?xml version="1.0" encoding="UTF-8" standalone="no"?&gt;&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;&lt;configuration&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;123456&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://node03:3306/hive?createDatabaseIfNotExist=true&amp;amp;useSSL=false&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.metastore.schema.verification&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;datanucleus.schema.autoCreateAll&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.server2.thrift.bind.host&lt;/name&gt; &lt;value&gt;node03&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 5将数据库驱动加入hive下的lib目录下 6配置hive环境变量 1234567sudo vim /etc/profileexport HIVE_HOME=/export/servers/hiveexport PATH=:$HIVE_HOME/bin:$PATH配后:source /etc/profile flume的安装sqoop的安装安装sqoop的前提是已经具备java和hadoop的环境 上传解压后: 配置文件修改: conf下的: 12345mv sqoop-env-template.sh sqoop-env.shvi sqoop-env.shexport HADOOP_COMMON_HOME= /export/servers/hadoop-2.7.5 export HADOOP_MAPRED_HOME= /export/servers/hadoop-2.7.5export HIVE_HOME= /export/servers/hive 把数据库驱动加入lib目录下 测试: 1234bin/sqoop list-databases \--connect jdbc:mysql://node01:3306/ \--username root \--password 123456 列出数据库中所有的数据库 azkaban的安装telnet 安装123yum list telnet* 列出telnet相关的安装包yum install telnet-server 安装telnet服务yum install telnet.* 安装telnet客户端]]></content>
      <tags>
        <tag>BigData</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux根目录权限修复方法]]></title>
    <url>%2F2017%2F07%2F03%2Flinux%E6%A0%B9%E7%9B%AE%E5%BD%95%E6%9D%83%E9%99%90%E4%BF%AE%E5%A4%8D%E6%96%B9%E6%B3%95.html</url>
    <content type="text"><![CDATA[今天由于权限问题一般把/这个目录用chmod -R 777执行了一遍，结果各种问题出现，su root就报su:鉴定故障的错误。然后上网找教程很多都要求在root权限下操作来修复，真是悔不当初，哭都哭不出来，只想剁手。幸好最好予以解决了，不然就真得重装系统了，在此把解决方案记录下来，希望能给踩到坑的朋友抢救一下。 step1新建一个.c文件，在这里我命名为chmodfix.c，把如下内容写到这个.c文件中 1234567891011121314151617181920#include &lt;stdio.h&gt; #include &lt;stdlib.h&gt;#include &lt;string.h&gt;#include &lt;sys/stat.h&gt;#include &lt;ftw.h&gt;int list(const char *name, const struct stat *status, int type)&#123; if(type == FTW_NS) return 0; printf("%s 0%3o\n", name, status-&gt;st_mode &amp; 07777); return 0;&#125;int main(int argc, char *argv[])&#123; if(argc == 1) ftw(".", list, 1); else ftw(argv[1], list, 2); exit(0);&#125; 然后在终端命令行下使用gcc编译得到可执行文件chmodfix.com 1gcc chmodfix.c -o chmodfix.com step2新建一个.sh文件，在这里我命名为chmodfix.sh，把如下内容写到这个.sh文件中 123456789101112131415#!/bin/sh if [ $# != 1 ] then echo Usage : $0 \&lt;filename\&gt; exitfiPERMFILE=$1cat $PERMFILE | while read LINEdo FILE=`echo $LINE | awk '&#123;print $1&#125;'` PERM=`echo $LINE | awk '&#123;print $2&#125;'` chmod $PERM $FILE #echo "chmod $PERM $FILE"doneecho "change perm finished! " step3找到另一台装有centos7并且系统权限正常的电脑，利用step1中得到的chmodfix.com从这台电脑上获取被你损坏的目录下所有文件的正常权限 123# 假设原电脑上权限损坏的目录为/usr/bin./chmodfix.com /usr/bin &gt;&gt; chmodfix.txt 输出文件chmodfix.txt的内容形式如下 123456/usr/bin 0755 /usr/bin/cp 0755/usr/bin/lua 0755/usr/bin/captoinfo 0755/usr/bin/csplit 0755/usr/bin/clear 0755/usr/bin/cut 0755 将得到的权限文件chmodfix.txt复制到权限受损的电脑上 step4权限受损电脑进入单用户模式: CentOS6.x版本单用户模式，就是你现在站在这台机器面前能干的活，再通俗点就是你能够接触到这个物理设备。 一般干这个活的话，基本上是系统出现严重故障或者其他的root密码忘记等等，单用户模式就非常有用了； 1、在开机启动的时候能看到引导目录，用上下方向键选择你忘记密码的那个系统，然后按“e”； 2、接下来你可以看到如下图所示的画面，然后你再用上下键选择最新的内核，然后在按“e”； 3、执行完上步操作后 在rhgb quiet最后加“空格”，然后键入“single”，或者直接输入数字的“1”并回车确定； 4、按“b”键，重新引导系统； 5、然后就进入了单用户模式下，你就可以使用root功能的东西了，改完你要改的文件后reboot即可。 centos7版本采用的是grub2，和centos6.x进入单用户的方法不同。 init方法 1、centos7的grub2界面会有两个入口，正常系统入口和救援模式； 2、修改grub2引导 在正常系统入口上按下”e”，会进入edit模式，搜寻ro那一行，以linux16开头的； 把只读更改成可写把ro更改成rw 指定shell环境增加init=/sysroot/bin/sh或init=/sysroot/bin/bash 按下ctrl+x来启动系统。 3、进入系统以后将/sysroot/设置为根 1chroot /sysroot/ 4、做相应的系统维护工作 如：修改密码 1passwd 5、系统启用了selinux，必须运行以下命令，否则将无法正常启动系统： 1touch /.autorelabel 6、退出并重启系统退出之前设置的根 1exit 重启系统 1reboot 另外还有一种rd.break方法 1、启动的时候，在启动界面，相应启动项，内核名称上按“e”； 2、进入后，找到linux16开头的地方，按“end”键到最后，输入rd.break，按ctrl+x进入； 3、进去后输入命令mount，发现根为/sysroot/，并且不能写，只有ro=readonly权限； 4、重新挂载，之后mount，发现有了r,w权限； 1mount -o remount,rw /sysroot/ 5、改变根 1chroot /sysroot/ 在/tmp/下创建一个aaa的目录 1mkdir /tmp/aaa 6、系统启用了selinux，必须运行以下命令，否则将无法正常启动系统： 1touch /.autorelabel 7、退出之前设置的根 1exit 8、重启系统 1reboot 以上结束后然后进入到chmodfix.sh和chmodfix.txt所存放的文件夹下，执行chmodfix.sh以根据chmodfix.txt恢复受损文件的正确权限 1bash chmodfix.sh chmodfix.txt]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA-JVM-初探]]></title>
    <url>%2F2017%2F07%2F02%2FJAVA-JVM-%E5%88%9D%E6%8E%A2.html</url>
    <content type="text"><![CDATA[JVM 一 Java -version 1 hotspot 热点探测 jvm核心 jdk1.5 之后有 为执行引擎 优化class文件 因为CPU不会直接执行class文件 要转为二进制才可以 2 jdk 有client 和server默认启动为client 可在jre/lib/i386 里吧两者顺序调整 前面的先起 当大型项目时 可改为server 在jdk1.8之后都默认为server 且client 为ignore 二 Jvm结构 ​ 三 堆结构图及分代 Jvm 根据对象的存活周期不同 把堆内存分为 新生代,老年代,和永久代 目的 提高对象内存分配和垃圾回收效率 经过多次回收荏苒存活的对象放在老年代,静态属性和类信息放在永久代, 新生代中对象存活时间短 在新生代中做垃圾回收 老年代垃圾回收频率少 永久代一般不进行垃圾回收,可根据不同的代用不同的;垃圾回收算法. 堆有五个区(细) 新生代(3个) Eden from to eden 回收最频繁 =幸存的放在from和to的幸存区在此幸存放在 老年去 Old区 永久区 1新生代 新创建的对象放在新生代,这里边存活率很低,一次回收可回收百分之八十到九十五, Eden from to 比例8:1:1 在jdk1.8和之后没有老年代 老年代与方法区作用类似; 三 垃圾回收 1常见垃圾回收算法 1.1 1.2复制算法 1.3标记清除]]></content>
      <tags>
        <tag>Jvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql优化]]></title>
    <url>%2F2017%2F07%2F02%2FMysql%E4%BC%98%E5%8C%96.html</url>
    <content type="text"><![CDATA[1 选取最适用的字段属性]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库]]></title>
    <url>%2F2017%2F07%2F01%2F%E6%95%B0%E6%8D%AE%E5%BA%93.html</url>
    <content type="text"><![CDATA[1、数据库介绍篇重置密码: 1234567891011121314151617181920212223[root@node1 db]# mysqlERROR 1045 (28000): Unknown error 1045[root@node1 db]# vim /etc/my.cnf #使用完后去掉[mysqld]skip-grant-tables=1 重启mysql,再修改service mysqld restart mysql&gt; set password = PASSWORD('123456');ERROR 1290 (HY000): Unknown error 1290mysql&gt; ALTER USER 'root'@'localhost' IDENTIFIED BY '123456';ERROR 1290 (HY000): Unknown error 1290mysql&gt; flush privileges;Query OK, 0 rows affected (0.00 sec) mysql&gt; set password = PASSWORD('123456');ERROR 1133 (42000):mysql&gt; ALTER USER 'root'@'localhost' IDENTIFIED BY '123456';Query OK, 0 rows affected (0.01 sec) mysql&gt; set password for root@localhost = password('123456');Query OK, 0 rows affected, 1 warning (0.00 sec) 1.1什么是数据库​ 数据库：保存数据的仓库。它体现我们电脑中，就是一个文件系统。然后把数据都保存这些特殊的文件中，并且需要使用固定的语言（SQL语言）去操作文件中的数据。 技术定义： ​ 数据库(Database)是按照数据结构来组织、[存储和管理数据的建立在计算机存储设备上的仓库。 1.2数据库介绍​ 我们开发应用程序的时候，程序中的所有数据，最后都需要保存到专业软件中。这些专业的保存数据的软件我们称为数据库。 我们学习数据库，并不是学习如何去开发一个数据库软件，我们学习的是如何使用数据库以及数据库中的数据记录的操作。而数据库软件是由第三方公司研发。 1.3数据库的分类​ 关系型、非关系型的数据库 常见的数据库软件： Oracle：它是Oracle公司的大型关系型数据库，它是收费的。 DB2：IBM公司的数据库，它是收费的。 SqlServer：微软数据库。收费 Sybase：Sybase公司的。 工具PowerDesign 数据库建模工具。 MySql：早期瑞典一个公司发明，后期被sun公司收购，后期被Oracle。 Java开发应用程序主要使用的数据库： MySQL（5.5）、Oracle、DB2。 1.4什么是关系型数据库​ 在开发软件的时候，软件中的数据之间必然会有一定的关系存在，需要把这些数据保存在数据库中，同时也要维护数据之间的关系，这时就可以直接使用上述的那些数据库。而上述的所有数据库都属于关系型数据库。 ​ 描述数据之间的关系，并保存在数据库中，同时学习如果根据这些关系查询数据库中的数据， 关系型数据：设计数据库的时候，需要使用E-R图来描述。实体关系 E-R：实体关系图。 ​ 实体：可以理解成我们Java程序中的一个对象。在E-R图中使用 矩形(长方形) 表示。 针对一个实体中的属性，我们称为这个实体的数据，在E-R图中使用 椭圆表示。 实体和实体之间的关系：在E-R图中使用菱形表示。 2、mysql在linux-安装篇2.1、vmware中安装linux注意事项2.1.1、记得关闭防火墙12service iptables stopchkconfig iptables off（关闭开机自启：所谓的永久关闭防火墙） 2.1.2、创建统一的管理目录123mkdir -p /exprot/softwaremkdir -p /export/servers 2.1.3软件环境1VMware、crt、centos6.9 2.1.4安装环境12345678910111、VMware软件安装 2、构建虚拟机 3、需要配置Linux（ip,mac地址，hostname，防火墙），就可以通过crt这个客户端连接进行操作 4、在linux操作系统进行安装msyql-5.6 说明：因为在linux操作系统中，安装软件的方式主要有3种：1、源码安装（redis）2、rpm安装 3、yum在线安装（安装MySQL为例）---linux联网（） 2.2、centos6.9安装mysql2.2.1、检查是否有自带的mysql12[root@hadoop-01 servers]# rpm -qa |grep mysql mysql-libs-5.1.73-8.el6_8.x86_64 2.2.2、卸载自带的mysql12[root@hadoop-01 servers]# rpm -e --nodeps mysql-libs-5.1.73-8.el6_8.x86_64[root@hadoop-01 servers]# 2.2.3、下载mysql安装包2.2.4、上传安装包到linux服务器12rz 上传文件到指定的目录（yum install lrzsz）/export/software/mysql 2.2.5、安装1rpm -ivh *.rpm 2.2.6、查看初始化密码12A RANDOM PASSWORD HAS BEEN SET FOR THE MySQL root USER !You will find that password in '/root/.mysql_secret'. 1234[root@mysql ~]# cat /root/.mysql_secret# The random password set for the root user at Wed Aug 8 22:19:00 2018 (local time): xQkcU3kbyuZby1_V[root@mysql ~]# 2.2.7、启动mysql并登录12#启动mysqlservice mysql start 123#登录mysqlmysql -uroot -p(粘贴密码：xQkcU3kbyuZby1_V) 2.2.8、修改密码1set PASSWORD=PASSWORD('123456'); 2.2.9、退出mysql客户端1mysql&gt;quit 2.2.10、用新密码进行登录12mysql -uroot -p123456（新密码） 2.2.11、远程授权12grant all privileges on *.* to 'root' @'%' identified by '123456'; flush privileges; 2.2.12、验证远程授权是否成功1通过windows的mysql客户端工具连接，是否能连接上，能连接上就授权成功，没有连接上，说明没有授权成功！ 3、mysql-基础操作篇3.1、登录mysql12mysql -uroot -p123456 3.2、退出mysql1mysql&gt;quit 3.3、输入查询 查看当前mysql的版本号及当前时间 1SELECT VERSION(), CURRENT_DATE; 1234567mysql&gt; SELECT VERSION(), CURRENT_DATE;+-----------+--------------+| VERSION() | CURRENT_DATE |+-----------+--------------+| 5.6.25 | 2018-08-08 |+-----------+--------------+1 row in set (0.32 sec) mysql中sql语句不区分大小写 123mysql&gt; SELECT VERSION(), CURRENT_DATE;mysql&gt; select version(), current_date;mysql&gt; SeLeCt vErSiOn(), current_DATE; 12345678910111213141516171819202122232425mysql&gt; SELECT VERSION(), CURRENT_DATE;+-----------+--------------+| VERSION() | CURRENT_DATE |+-----------+--------------+| 5.6.25 | 2018-08-08 |+-----------+--------------+1 row in set (0.00 sec)mysql&gt; select version(), current_date;+-----------+--------------+| version() | current_date |+-----------+--------------+| 5.6.25 | 2018-08-08 |+-----------+--------------+1 row in set (0.00 sec)mysql&gt; SeLeCt vErSiOn(), current_DATE;+-----------+--------------+| vErSiOn() | current_DATE |+-----------+--------------+| 5.6.25 | 2018-08-08 |+-----------+--------------+1 row in set (0.00 sec)mysql&gt; 可以进行简单的计算（如下所示） 123456789mysql&gt;SELECT SIN(PI()/4), (4+1)*5;mysql&gt; SELECT SIN(PI()/4), (4+1)*5;+--------------------+---------+| SIN(PI()/4) | (4+1)*5 |+--------------------+---------+| 0.7071067811865475 | 25 |+--------------------+---------+1 row in set (0.34 sec) 多条语句比较短，可以写在一行 12345678910111213141516mysql&gt;SELECT VERSION(); SELECT NOW();mysql&gt; SELECT VERSION(); SELECT NOW();+-----------+| VERSION() |+-----------+| 5.6.25 |+-----------+1 row in set (0.00 sec)+---------------------+| NOW() |+---------------------+| 2018-08-08 23:11:11 |+---------------------+1 row in set (0.00 sec) 多个字段之间可以用逗号分隔，多行组成一条语句结束以分号结束 123456789mysql&gt; SELECT-&gt; USER()-&gt; ,-&gt; CURRENT_DATE;+---------------+--------------+| USER() | CURRENT_DATE |+---------------+--------------+| jon@localhost | 2010-08-06 |+---------------+--------------+ sql语句写了一半，又不想执行可以在语句末尾加上’\c’ 1234mysql&gt; select -&gt; user() -&gt; \cmysql&gt; 3.4、创建和使用数据库 查看当前有哪些数据库 123456789101112mysql&gt;show databases;mysql&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || mysql || performance_schema || test |+--------------------+4 rows in set (0.07 sec) 创建数据库 1mysql&gt; CREATE DATABASE menagerie; 使用及切换数据库 12mysql&gt; USE menagerieDatabase changed 3.5、创建表及使用 查看当前数据库有哪些表 1mysql&gt;show tables; 创建一个表 12mysql&gt; CREATE TABLE pet (name VARCHAR(20), owner VARCHAR(20), -&gt; species VARCHAR(20), sex CHAR(1), birth DATE, death DATE); 校验创建表语句是否和执行的一致 1234567891011121314mysql&gt;show create table pet;+-------+--------------------------------| Table | Create Table +-------+--------------------------------| pet | CREATE TABLE `pet` ( `name` varchar(20) DEFAULT NULL, `owner` varchar(20) DEFAULT NULL, `species` varchar(20) DEFAULT NULL, `sex` char(1) DEFAULT NULL, `birth` date DEFAULT NULL, `death` date DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=latin1 |+-------+-------------------------------- 查看表详情 1234567891011mysql&gt; desc pet;+---------+-------------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+---------+-------------+------+-----+---------+-------+| name | varchar(20) | YES | | NULL | || owner | varchar(20) | YES | | NULL | || species | varchar(20) | YES | | NULL | || sex | char(1) | YES | | NULL | || birth | date | YES | | NULL | || death | date | YES | | NULL | |+---------+-------------+------+-----+---------+-------+ 准备数据 12345678Fluffy Harold cat f 1993-02-04Claws Gwen cat m 1994-03-17Buffy Harold dog f 1989-05-13Fang Benny dog m 1990-08-27Bowser Diane dog m 1979-08-31 1995-07-29Chirpy Gwen bird f 1998-09-11Whistler Gwen bird 1997-12-09Slim Benny snake m 1996-04-29 3.6、表中导入数据​ 在表中导入数据的方式有两种 第一种：将以上数据整理成SQL语句，insert into pet…. 第二种：通过加载文件的方式将数据导入到表中 1、创建一个pet.txt的文件（注：每个字段中用tab键隔开，字段没有值得记录用\N代替） 12345678Fluffy Harold cat f 1993-02-04Claws Gwen cat m 1994-03-17Buffy Harold dog f 1989-05-13Fang Benny dog m 1990-08-27Bowser Diane dog m 1979-08-31 1995-07-29Chirpy Gwen bird f 1998-09-11Whistler Gwen bird \N 1997-12-09 \NSlim Benny snake m 1996-04-29 2、加载数据 123mysql&gt; load data local infile &apos;/root/data/pet.txt&apos; into table pet;Query OK, 8 rows affected, 6 warnings (0.06 sec)Records: 8 Deleted: 0 Skipped: 0 Warnings: 6 3、校验是否加载进去 1234567891011121314mysql&gt; select *from pet;+----------+--------+---------+------+------------+------------+| name | owner | species | sex | birth | death |+----------+--------+---------+------+------------+------------+| Fluffy | Harold | cat | f | 1993-02-04 | NULL || Claws | Gwen | cat | m | 1994-03-17 | NULL || Buffy | Harold | dog | f | 1989-05-13 | NULL || Fang | Benny | dog | m | 1990-08-27 | NULL || Bowser | Diane | dog | m | 1979-08-31 | 1995-07-29 || Chirpy | Gwen | bird | f | 1998-09-11 | NULL || Whistler | Gwen | bird | NULL | 1997-12-09 | NULL || Slim | Benny | snake | m | 1996-04-29 | NULL |+----------+--------+---------+------+------------+------------+8 rows in set (0.01 sec) 3.7、数据检索部分3.7.1、检索全部数据1234567891011121314mysql&gt; select *from pet;+----------+--------+---------+------+------------+------------+| name | owner | species | sex | birth | death |+----------+--------+---------+------+------------+------------+| Fluffy | Harold | cat | f | 1993-02-04 | NULL || Claws | Gwen | cat | m | 1994-03-17 | NULL || Buffy | Harold | dog | f | 1989-05-13 | NULL || Fang | Benny | dog | m | 1990-08-27 | NULL || Bowser | Diane | dog | m | 1979-08-31 | 1995-07-29 || Chirpy | Gwen | bird | f | 1998-09-11 | NULL || Whistler | Gwen | bird | NULL | 1997-12-09 | NULL || Slim | Benny | snake | m | 1996-04-29 | NULL |+----------+--------+---------+------+------------+------------+8 rows in set (0.01 sec) 3.7.2、删除表中全部数据12mysql&gt; DELETE FROM pet;mysql&gt; LOAD DATA LOCAL INFILE &apos;/path/pet.txt&apos; INTO TABLE pet; 3.7.3、更新表中特定记录的数据 更新表中名字为Bowser的生日 1mysql&gt; UPDATE pet SET birth = &apos;1989-08-31&apos; WHERE name = &apos;Bowser&apos;; 3.7.4、查询特定的行 查询名字为Bowser的记录 123456mysql&gt; SELECT * FROM pet WHERE name = &apos;Bowser&apos;;+--------+-------+---------+------+------------+------------+| name | owner | species | sex | birth | death |+--------+-------+---------+------+------------+------------+| Bowser | Diane | dog | m | 1989-08-31 | 1995-07-29 |+--------+-------+---------+------+------------+------------+ 说明：字符串比较不区分大小写！如下所示： 1234567891011121314151617181920212223mysql&gt; SELECT * FROM pet WHERE name = &apos;Bowser&apos;;+--------+-------+---------+------+------------+------------+| name | owner | species | sex | birth | death |+--------+-------+---------+------+------------+------------+| Bowser | Diane | dog | m | 1979-08-31 | 1995-07-29 |+--------+-------+---------+------+------------+------------+1 row in set (0.00 sec)mysql&gt; SELECT * FROM pet WHERE name = &apos;BowsEr&apos;;+--------+-------+---------+------+------------+------------+| name | owner | species | sex | birth | death |+--------+-------+---------+------+------------+------------+| Bowser | Diane | dog | m | 1979-08-31 | 1995-07-29 |+--------+-------+---------+------+------------+------------+1 row in set (0.00 sec)mysql&gt; SELECT * FROM pet WHERE name = &apos;BOWSER&apos;;+--------+-------+---------+------+------------+------------+| name | owner | species | sex | birth | death |+--------+-------+---------+------+------------+------------+| Bowser | Diane | dog | m | 1979-08-31 | 1995-07-29 |+--------+-------+---------+------+------------+------------+1 row in set (0.00 sec) 3.7.4.1、查找生日在1998年以后的特定查询1234567mysql&gt; SELECT * FROM pet WHERE birth &gt;= &apos;1998-1-1&apos;;+----------+-------+---------+------+------------+-------+| name | owner | species | sex | birth | death |+----------+-------+---------+------+------------+-------+| Chirpy | Gwen | bird | f | 1998-09-11 | NULL || Puffball | Diane | hamster | f | 1999-03-30 | NULL |+----------+-------+---------+------+------------+-------+ 3.7.4.2、多条件查询（and | or）123456mysql&gt; SELECT * FROM pet WHERE species = &apos;dog&apos; AND sex = &apos;f&apos;;+-------+--------+---------+------+------------+-------+| name | owner | species | sex | birth | death |+-------+--------+---------+------+------------+-------+| Buffy | Harold | dog | f | 1989-05-13 | NULL |+-------+--------+---------+------+------------+-------+ 12345678mysql&gt; SELECT * FROM pet WHERE species = &apos;snake&apos; OR species = &apos;bird&apos;;+----------+-------+---------+------+------------+-------+| name | owner | species | sex | birth | death |+----------+-------+---------+------+------------+-------+| Chirpy | Gwen | bird | f | 1998-09-11 | NULL || Whistler | Gwen | bird | NULL | 1997-12-09 | NULL || Slim | Benny | snake | m | 1996-04-29 | NULL |+----------+-------+---------+------+------------+-------+ 优先执行括号中的逻辑 12345678mysql&gt; SELECT * FROM pet WHERE (species = &apos;cat&apos; AND sex = &apos;m&apos;)-&gt; OR (species = &apos;dog&apos; AND sex = &apos;f&apos;);+-------+--------+---------+------+------------+-------+| name | owner | species | sex | birth | death |+-------+--------+---------+------+------------+-------+| Claws | Gwen | cat | m | 1994-03-17 | NULL || Buffy | Harold | dog | f | 1989-05-13 | NULL |+-------+--------+---------+------+------------+-------+ 3.7.5、检索特定的列1234567891011121314mysql&gt; SELECT name, birth FROM pet;+----------+------------+| name | birth |+----------+------------+| Fluffy | 1993-02-04 || Claws | 1994-03-17 || Buffy | 1989-05-13 || Fang | 1990-08-27 || Bowser | 1989-08-31 || Chirpy | 1998-09-11 || Whistler | 1997-12-09 || Slim | 1996-04-29 || Puffball | 1999-03-30 |+----------+------------+ 查询不重复的字段要使用关键词DISTINCT 123456789mysql&gt; SELECT DISTINCT owner FROM pet;+--------+| owner |+--------+| Benny || Diane || Gwen || Harold |+--------+ 可以使用组合条件查询特定的列 1234567891011mysql&gt; SELECT name, species, birth FROM pet-&gt; WHERE species = &apos;dog&apos; OR species = &apos;cat&apos;;+--------+---------+------------+| name | species | birth |+--------+---------+------------+| Fluffy | cat | 1993-02-04 || Claws | cat | 1994-03-17 || Buffy | dog | 1989-05-13 || Fang | dog | 1990-08-27 || Bowser | dog | 1989-08-31 |+--------+---------+------------+ 3.7.6、排序 根据某个字段进行排序（关键词：ORDER BY ） 1234567891011121314mysql&gt; SELECT name, birth FROM pet ORDER BY birth;+----------+------------+| name | birth |+----------+------------+| Buffy | 1989-05-13 || Bowser | 1989-08-31 || Fang | 1990-08-27 || Fluffy | 1993-02-04 || Claws | 1994-03-17 || Slim | 1996-04-29 || Whistler | 1997-12-09 || Chirpy | 1998-09-11 || Puffball | 1999-03-30 |+----------+------------+ 升降序排列（desc：降序；asc：升序） 12mysql&gt; SELECT name, birth FROM pet ORDER BY birth desc;//降序排列mysql&gt; SELECT name, birth FROM pet ORDER BY birth asc ;//升序排列 多列排序 根据species字段升序排列，根据birth字段降序排列 注： ORDER BY species 中无asc，desc，默认为升序排列 123456789101112131415mysql&gt; SELECT name, species, birth FROM pet-&gt; ORDER BY species, birth DESC;+----------+---------+------------+| name | species | birth |+----------+---------+------------+| Chirpy | bird | 1998-09-11 || Whistler | bird | 1997-12-09 || Claws | cat | 1994-03-17 || Fluffy | cat | 1993-02-04 || Fang | dog | 1990-08-27 || Bowser | dog | 1989-08-31 || Buffy | dog | 1989-05-13 || Puffball | hamster | 1999-03-30 || Slim | snake | 1996-04-29 |+----------+---------+------------+ 3.7.7、日期计算查看宠物多少岁，就可以使用计算日期的函数TIMESTAMPDIFF() 12345678910111213141516171819202122232425262728293031#查询当前的日期mysql&gt; select curdate() from pet;+------------+| curdate() |+------------+| 2018-08-09 |+------------+#获取当年的年mysql&gt; select YEAR(&apos;2018-02-05&apos;) AS YEARS from pet;+-------+| YEARS |+-------+| 2018 |+-------+#获取当年的月mysql&gt; select month(&apos;2018-02-05&apos;) AS YEARS from pet; +-------+| YEARS |+-------+| 2 |+-------+#获取当年的日mysql&gt; select day(&apos;2018-02-05&apos;) AS YEARS from pet; +-------+| YEARS |+-------+| 5 |+-------+ 123mysql&gt; SELECT name, birth, CURDATE(),-&gt; TIMESTAMPDIFF(YEAR,birth,CURDATE()) AS age-&gt; FROM pet; 3.7.8、null和not null值对一些字段类型要进行检查，判断某些字段是否为NULL，或者 non-NULL 12345678mysql&gt; SELECT name, birth, death,-&gt; TIMESTAMPDIFF(YEAR,birth,death) AS age-&gt; FROM pet WHERE death IS NOT NULL ORDER BY age;+--------+------------+------------+------+| name | birth | death | age |+--------+------------+------------+------+| Bowser | 1989-08-31 | 1995-07-29 | 5 |+--------+------------+------------+------+ 4、实例以下是如何解决MySQL的一些常见问题的示例。 4.1、首先创建一个表，并且导入数据12345678910CREATE TABLE shop (article INT(4) UNSIGNED ZEROFILL DEFAULT '0000' NOT NULL,dealer CHAR(20) DEFAULT '' NOT NULL,price DOUBLE(16,2) DEFAULT '0.00' NOT NULL,PRIMARY KEY(article, dealer));INSERT INTO shop VALUES(1,'A',3.45),(1,'B',3.99),(2,'A',10.99),(3,'B',1.45),(3,'C',1.69),(3,'D',1.25),(4,'D',19.95); 4.2、检索表中的全部数据12345678910111213select * from shop;+---------+--------+-------+| article | dealer | price |+---------+--------+-------+| 0001 | A | 3.45 || 0001 | B | 3.99 || 0002 | A | 10.99 || 0003 | B | 1.45 || 0003 | C | 1.69 || 0003 | D | 1.25 || 0004 | D | 19.95 |+---------+--------+-------+ 4.3、求某一列的最大值或者 最小值12345678910111213141516SELECT MAX(article) AS article FROM shop;+---------+| article |+---------+| 4 |+---------+//求某一列的最小值 select min(price) as article from shop; +---------+| article |+---------+| 1.25 |+---------+ 4.4、过滤出某个字段值最大的整条记录数据-涉及到子查询123456789SELECT article, dealer, priceFROM shopWHERE price=(SELECT MAX(price) FROM shop);+---------+--------+-------+| article | dealer | price |+---------+--------+-------+| 0004 | D | 19.95 |+---------+--------+-------+ 4.4、也可以通过关联查询来进行检索123456789SELECT s1.article, s1.dealer, s1.priceFROM shop s1LEFT JOIN shop s2 ON s1.price &lt; s2.priceWHERE s2.article IS NULL;SELECT article, dealer, priceFROM shopORDER BY price DESCLIMIT 1; 4.5、求出每一列的最大值，并且根据某一个字段进行分组–分组topn求法123456789101112SELECT article, MAX(price) AS priceFROM shopGROUP BY article;+---------+-------+| article | price |+---------+-------+| 0001 | 3.99 || 0002 | 10.99 || 0003 | 1.69 || 0004 | 19.95 |+---------+-------+ 4.5的另一种写法 1234567891011121314SELECT article, dealer, priceFROM shop s1WHERE price=(SELECT MAX(s2.price)FROM shop s2WHERE s1.article = s2.article);+---------+--------+-------+| article | dealer | price |+---------+--------+-------+| 0001 | B | 3.99 || 0002 | A | 10.99 || 0003 | C | 1.69 || 0004 | D | 19.95 |+---------+--------+-------+ 5、SQL中的聚合函数​ SQL语言中定义了部分的函数，可以帮助我们完成对查询结果的计算操作： 1.count 统计个数（行数） 2.sum函数：求和 3.avg函数：求平均值 4.max、min 求最大值和最小值 5.1、count函数语法：select count(*)|count(列名) from表名 注意： count在根据指定的列统计的时候，如果这一列中有null 不会被统计在其中。 12345678910111213141516171819202122232425262728293031323334353637mysql&gt; select * from pet;+----------+--------+---------+------+------------+------------+| name | owner | species | sex | birth | death |+----------+--------+---------+------+------------+------------+| Fluffy | Harold | cat | f | 1993-02-04 | NULL || Claws | Gwen | cat | m | 1994-03-17 | NULL || Buffy | Harold | dog | f | 1989-05-13 | NULL || Fang | Benny | dog | m | 1990-08-27 | NULL || Bowser | Diane | dog | m | 1989-08-31 | 1995-07-29 || Chirpy | Gwen | bird | f | 1998-09-11 | NULL || Whistler | Gwen | bird | NULL | 1997-12-09 | NULL || Slim | Benny | snake | m | 1996-04-29 | NULL |+----------+--------+---------+------+------------+------------+8 rows in set (0.00 sec)mysql&gt; select count(sex) from pet; +------------+| count(sex) |+------------+| 7 |+------------+1 row in set (0.00 sec)mysql&gt; select count(owner) from pet; +--------------+| count(owner) |+--------------+| 8 |+--------------+1 row in set (0.00 sec)mysql&gt; select count(death) from pet; +--------------+| count(death) |+--------------+| 1 |+--------------+ 5.2、sum函数语法：select sum(列名) from 表名; 注意事项： 1、如果使用sum 多列进行求和的时候，如果某一列中的有null，这一列所在的行中的其他数据不会被加到总和。 2、可以使用mysql 数据库提供的函数 ifnull(列名,值) 3、在数据库中定义double类型数据，是一个近似值，需要确定准确的位数，这时可以把这一列设计成numeric类型。numeric(数据的总列数,小数位数) numericdouble float 123456mysql&gt; select sum(price) from shop; +------------+| sum(price) |+------------+| 42.77 |+------------+ 5.3、avg函数语法：select avg(列名) from 表名; 123456mysql&gt; select avg(price) from shop; +------------+| avg(price) |+------------+| 6.110000 |+------------+ 5.4、max函数语法：select max(列名) from 表名; 123456mysql&gt; select max(price) from shop; +------------+| max(price) |+------------+| 19.95 |+------------+ 5.5、min函数语法：select min(列名) from 表名; 123456mysql&gt; select min(price) from shop; +------------+| min(price) |+------------+| 1.25 |+------------+ 6、SQL分类6.1、DDL （数据定义问题）数据定义语言 - Data Definition Language 用来定义数据库的对象，如数据表、视图、索引等 1234创建数据库：create database test;创建视图：create view test;创建索引：create index test;创建表：create table test1; 6.2、DML （数据操纵问题）数据处理语言 - Data Manipulation Language 在数据库表中更新，增加和删除记录 如 update， insert， delete 12345update tableName set age='18' where name='lisi'insert into tableName value('1','2','3');drop table tableName //删除表操作 6.3、DCL （数据控制问题）数据控制语言 – Data Control Language 指用于设置用户权限和控制事务语句 如grant，revoke，if…else，while，begintransaction 6.4、DQL （数据查询问题）数据查询语言 – Data Query Language select 6.5、小结12345671、创建数据库：create database itcast;2、使用数据库：use itcast;3、查看当前数据库中的所有表：show tables ;4、查看所有的数据库：show databases;5、删除数据库：drop database itcast;6、删除数据库中的表：drop table t1; 7、数据库的备份与恢复7.1、备份命令​ 在mysql的安装目录的bin目录下有mysqldump命令，可以完成对数据库的备份。 语法：mysqldump -u 用户名 -p 数据库名&gt; 磁盘SQL文件路径 由于mysqldump命令不是sql命令，需要在dos窗口下使用。 ​ 注意：在备份数据的时候，数据库不会被删除。可以手动删除数据库。同时在恢复数据的时候，不会 自动的给我们创建数据库，仅仅只会恢复数据库中的表和表中的数据。 1234mysqldump -uroot -p123456 menagerie &gt;/root/data/menagerie.sql//备份的文件-rw-r--r--. 1 root root 3118 Oct 20 04:04 menagerie.sql 7.2、恢复命令恢复数据库，需要手动的先创建数据库： create database heima2; 语法：mysql -u 用户名-p 导入库名&lt; 硬盘SQL文件绝对路径 需求： ​ 1、创建heima8数据库。 ​ 2、重新开启一个新的dos窗口。 ​ 3、将mydb2备份的数据表和表数据 恢复到mydb6中。 1234//恢复命令mysql -uroot -p123456 itcast&lt;/root/data/menagerie.sql//恢复校验 8、多表查询8.1、笛卡尔积介绍​ 笛卡尔乘积是指在数学中，两个集合X和Y的笛卡尓积（Cartesian product），又称直积，表示为X × Y，第一个对象是X的成员而第二个对象是Y的所有可能有序对的其中一个成员 准备数据： 123456789101112131415create table A( A_ID int primary key auto_increment, A_NAME varchar(20) not null);insert into A values(1,'apple');insert into A values(2,'orange');insert into A values(3,'banana');create table B( A_ID int primary key auto_increment, B_PRICE double);insert into B values(1,2.30);insert into B values(2,3.50);insert into B values(4,null); 展示效果： 1234567891011121314mysql&gt; select * from A,B;+------+--------+------+---------+| A_ID | A_NAME | A_ID | B_PRICE |+------+--------+------+---------+| 1 | apple | 1 | 2.3 || 2 | orange | 1 | 2.3 || 3 | banana | 1 | 2.3 || 1 | apple | 2 | 3.5 || 2 | orange | 2 | 3.5 || 3 | banana | 2 | 3.5 || 1 | apple | 4 | NULL || 2 | orange | 4 | NULL || 3 | banana | 4 | NULL |+------+--------+------+---------+ 作用：笛卡尔积的数据，对程序是没有意义的，我们需要对笛卡尔积中的数据再次进行过滤。 对于多表查询操作，需要过滤出满足条件的数据，需要把多个表进行连接，连接之后需要加上过滤的条件。 12345678910111213141516mysql&gt; select * from A,B where B.A_ID=1;+------+--------+------+---------+| A_ID | A_NAME | A_ID | B_PRICE |+------+--------+------+---------+| 1 | apple | 1 | 2.3 || 2 | orange | 1 | 2.3 || 3 | banana | 1 | 2.3 |+------+--------+------+---------+3 rows in set (0.00 sec)mysql&gt; select * from A,B where B.A_ID=1 and A.A_ID=1;+------+--------+------+---------+| A_ID | A_NAME | A_ID | B_PRICE |+------+--------+------+---------+| 1 | apple | 1 | 2.3 |+------+--------+------+---------+ 8.2、内连接内连接：语法一：select 列名 , 列名 …. from 表名1,表名2 where 表名1.列名 = 表名2.列名; 语法二： select * from 表名1 inner join 表名2 on 条件 1234567mysql&gt; select * from A inner join B on A.A_ID=B.A_ID;+------+--------+------+---------+| A_ID | A_NAME | A_ID | B_PRICE |+------+--------+------+---------+| 1 | apple | 1 | 2.3 || 2 | orange | 2 | 3.5 |+------+--------+------+---------+ 8.3、左外连接外链接：左外连接、右外连接、全连接、自连接。 左外连接：用左边表去右边表中查询对应记录，不管是否找到，都将显示左边表中全部记录。 即：虽然右表没有香蕉对应的价格，也要把他查询出来。 语法：select * from 表1 left outer join 表2 on 条件; 12345678mysql&gt; select * from A left join B on A.A_ID=B.A_ID;+------+--------+------+---------+| A_ID | A_NAME | A_ID | B_PRICE |+------+--------+------+---------+| 1 | apple | 1 | 2.3 || 2 | orange | 2 | 3.5 || 3 | banana | NULL | NULL |+------+--------+------+---------+ 8.4、右外连接用右边表去左边表查询对应记录，不管是否找到，右边表全部记录都将显示。 即：不管左方能够找到右方价格对应的水果，都要把左方的价格显示出来。 语法：select * from 表1 right outer join 表2 on 条件; 12345678mysql&gt; select * from A right join B on A.A_ID=B.A_ID; +------+--------+------+---------+| A_ID | A_NAME | A_ID | B_PRICE |+------+--------+------+---------+| 1 | apple | 1 | 2.3 || 2 | orange | 2 | 3.5 || NULL | NULL | 4 | NULL |+------+--------+------+---------+ 8.5、全外连接全外连接：左外连接和右外连接的结果合并，单会去掉重复的记录。select from 表1 full outer join 表2 on 条件select from a full outer join b on a.A_ID = b.A_ID; 但是mysql数据库不支持此语法。 8.6、关联子查询子查询：把一个sql的查询结果作为另外一个查询的参数存在。 8.6.1、in和exist关键词的用法关联子查询其他的关键字使用： 回忆：age=23 or age=24 等价于 age in (23,24) in 表示条件应该是在多个列值中。 in：使用在where后面，经常表示是一个列表中的数据，只要被查询的数据在这个列表中存在即可。 123456789101112131415161718192021222324252627282930mysql&gt; select * from A where A_ID in(1,2,3);+------+--------+| A_ID | A_NAME |+------+--------+| 1 | apple || 2 | orange || 3 | banana |+------+--------+3 rows in set (0.00 sec)mysql&gt; select * from A where A_ID =1 or A_ID =2 or A_ID =3;+------+--------+| A_ID | A_NAME |+------+--------+| 1 | apple || 2 | orange || 3 | banana |+------+--------+//not inmysql&gt; select * from A where A_ID not in (1,2,3,4);Empty set (0.00 sec)mysql&gt; select * from A where A_ID not in (3,4); +------+--------+| A_ID | A_NAME |+------+--------+| 1 | apple || 2 | orange |+------+--------+2 rows in set (0.00 sec) exists： exists：表示存在，当子查询的结果存在，就会显示主查询中的所有数据。 使用exists完成： 1234567891011mysql&gt; select * from A where exists(select A_ID from B);+------+--------+| A_ID | A_NAME |+------+--------+| 1 | apple || 2 | orange || 3 | banana |+------+--------+mysql&gt; select * from A where not exists(select A_ID from B);Empty set (0.00 sec) 8.6.2、union 和union all使用法UNION 语句：用于将不同表中相同列中查询的数据展示出来；（不包括重复数据） UNION ALL 语句：用于将不同表中相同列中查询的数据展示出来；（包括重复数据） 123456789101112131415161718192021222324mysql&gt; select * from A union select * from B; +------+--------+| A_ID | A_NAME |+------+--------+| 1 | apple || 2 | orange || 3 | banana || 1 | 2.3 || 2 | 3.5 || 4 | NULL |+------+--------+6 rows in set (0.00 sec)mysql&gt; select * from A union all select * from B;+------+--------+| A_ID | A_NAME |+------+--------+| 1 | apple || 2 | orange || 3 | banana || 1 | 2.3 || 2 | 3.5 || 4 | NULL |+------+--------+ 8.6.3、case when 语句case when 语句语法结构： 1234CASE sex WHEN '1' THEN '男' WHEN '2' THEN '女' ELSE '其他' END 准备数据 1234567891011121314151617181920//创建表create table employee(empid int , deptid int , sex varchar(20) , salary double );//加载数据1 10 female 5500.02 10 male 4500.03 20 female 1900.04 20 male 4800.05 40 female 6500.06 40 female 14500.07 40 male 44500.08 50 male 6500.09 50 male 7500.0load data local infile '/root/data/emp.txt' into table employee ; 1234567891011select *,case when salary &lt; 5000 then "低等收入" when salary&gt;= 5000 and salary &lt; 10000 then "中等收入"when salary &gt; 10000 then "高等收入" end as level,case sexwhen "female" then 1 when "male" then 0end as flag from employee; 9、MySQL 数据类型MySQL中定义数据字段的类型对你数据库的优化是非常重要的。 MySQL支持多种类型，大致可以分为三类：数值、日期/时间和字符串(字符)类型。 9.1、数值类型MySQL支持所有标准SQL数值数据类型。 这些类型包括严格数值数据类型(INTEGER、SMALLINT、DECIMAL和NUMERIC)，以及近似数值数据类型(FLOAT、REAL和DOUBLE PRECISION)。 关键字INT是INTEGER的同义词，关键字DEC是DECIMAL的同义词。 BIT数据类型保存位字段值，并且支持MyISAM、MEMORY、InnoDB和BDB表。 作为SQL标准的扩展，MySQL也支持整数类型TINYINT、MEDIUMINT和BIGINT。下面的表显示了需要的每个整数类型的存储和范围。 类型 大小 范围（有符号） 范围（无符号） 用途 TINYINT 1 字节 (-128，127) (0，255) 小整数值 SMALLINT 2 字节 (-32 768，32 767) (0，65 535) 大整数值 MEDIUMINT 3 字节 (-8 388 608，8 388 607) (0，16 777 215) 大整数值 INT或INTEGER 4 字节 (-2 147 483 648，2 147 483 647) (0，4 294 967 295) 大整数值 BIGINT 8 字节 (-9 233 372 036 854 775 808，9 223 372 036 854 775 807) (0，18 446 744 073 709 551 615) 极大整数值 FLOAT 4 字节 (-3.402 823 466 E+38，-1.175 494 351 E-38)，0，(1.175 494 351 E-38，3.402 823 466 351 E+38) 0，(1.175 494 351 E-38，3.402 823 466 E+38) 单精度浮点数值 DOUBLE 8 字节 (-1.797 693 134 862 315 7 E+308，-2.225 073 858 507 201 4 E-308)，0，(2.225 073 858 507 201 4 E-308，1.797 693 134 862 315 7 E+308) 0，(2.225 073 858 507 201 4 E-308，1.797 693 134 862 315 7 E+308) 双精度浮点数值 DECIMAL 对DECIMAL(M,D) ，如果M&gt;D，为M+2否则为D+2 依赖于M和D的值 依赖于M和D的值 小数值 9.2、日期和时间类型表示时间值的日期和时间类型为DATETIME、DATE、TIMESTAMP、TIME和YEAR。 每个时间类型有一个有效值范围和一个”零”值，当指定不合法的MySQL不能表示的值时使用”零”值。 TIMESTAMP类型有专有的自动更新特性，将在后面描述。 类型 大小(字节) 范围 格式 用途 DATE 3 1000-01-01/9999-12-31 YYYY-MM-DD 日期值 TIME 3 ‘-838:59:59’/‘838:59:59’ HH:MM:SS 时间值或持续时间 YEAR 1 1901/2155 YYYY 年份值 DATETIME 8 1000-01-01 00:00:00/9999-12-31 23:59:59 YYYY-MM-DD HH:MM:SS 混合日期和时间值 TIMESTAMP 4 1970-01-01 00:00:00/2038结束时间是第 2147483647 秒，北京时间 2038-1-19 11:14:07，格林尼治时间 2038年1月19日 凌晨 03:14:07 YYYYMMDD HHMMSS 混合日期和时间值，时间戳 9.3、字符串类型字符串类型指CHAR、VARCHAR、BINARY、VARBINARY、BLOB、TEXT、ENUM和SET。该节描述了这些类型如何工作以及如何在查询中使用这些类型。 类型 大小 用途 CHAR 0-255字节 定长字符串 VARCHAR 0-65535 字节 变长字符串 TINYBLOB 0-255字节 不超过 255 个字符的二进制字符串 TINYTEXT 0-255字节 短文本字符串 BLOB 0-65 535字节 二进制形式的长文本数据 TEXT 0-65 535字节 长文本数据 MEDIUMBLOB 0-16 777 215字节 二进制形式的中等长度文本数据 MEDIUMTEXT 0-16 777 215字节 中等长度文本数据 LONGBLOB 0-4 294 967 295字节 二进制形式的极大文本数据 LONGTEXT 0-4 294 967 295字节 极大文本数据 CHAR 和 VARCHAR 类型类似，但它们保存和检索的方式不同。它们的最大长度和是否尾部空格被保留等方面也不同。在存储或检索过程中不进行大小写转换。 BINARY 和 VARBINARY 类似于 CHAR 和 VARCHAR，不同的是它们包含二进制字符串而不要非二进制字符串。也就是说，它们包含字节字符串而不是字符字符串。这说明它们没有字符集，并且排序和比较基于列值字节的数值值。 BLOB 是一个二进制大对象，可以容纳可变数量的数据。有 4 种 BLOB 类型：TINYBLOB、BLOB、MEDIUMBLOB 和 LONGBLOB。它们区别在于可容纳存储范围不同。 有 4 种 TEXT 类型：TINYTEXT、TEXT、MEDIUMTEXT 和 LONGTEXT。对应的这 4 种 BLOB 类型，可存储的最大长度不同，可根据实际情况选择。 MySQL 5.0 以上的版本： 1、一个汉字占多少长度与编码有关： UTF－8：一个汉字＝3个字节 GBK：一个汉字＝2个字节 2、varchar(n) 表示 n 个字符，无论汉字和英文，Mysql 都能存入 n 个字符，仅是实际字节长度有所区别 3、MySQL 检查长度，可用 SQL 语言来查看： 1select LENGTH(fieldname) from tablename 1、整型 MySQL数据类型 含义(有符号) tinyint(m) 1个字节 范围(-128~127) smallint(m) 2个字节 范围(-32768~32767) mediumint(m) 3个字节 范围(-8388608~8388607) int(m) 4个字节 范围(-2147483648~2147483647) bigint(m) 8个字节 范围(+-9.22*10的18次方) 取值范围加了unsigned，则最大值翻倍，如tinyint unsigned 的取值范围为（0-256） int(m) 里的 m 是表示 SELECT 查询结果集中的显示宽度，并不影响实际的取值范围，没有影响到显示的宽度，不知道这个 m 有什么用。 2、浮点型(float 和 double) MySQL数据类型 含义 float(m,d) 单精度浮点型 8位精度(4字节) m总个数，d小数位 double(m,d) 双精度浮点型 16位精度(8字节) m总个数，d小数位 设一个字段定义为 float(5,3)，如果插入一个数 123.45678,实际数据库里存的是 123.457，但总个数还以实际为准，即 6 位。 3、定点数 浮点型在数据库中存放的是近似值，而定点类型在数据库中存放的是精确值。 decimal(m,d) 参数 m&lt;65 是总个数，d&lt;30 且 d&lt;m 是小数位。 4、字符串(char,varchar,_text) MySQL数据类型 含义 char(n) 固定长度，最多255个字符 varchar(n) 固定长度，最多65535个字符 tinytext 可变长度，最多255个字符 text 可变长度，最多65535个字符 mediumtext 可变长度，最多2的24次方-1个字符 longtext 可变长度，最多2的32次方-1个字符 char 和 varchar： ** 1.char(n) 若存入字符数小于n，则以空格补于其后，查询之时再将空格去掉。所以 char 类型存储的字符串末尾不能有空格，varchar 不限于此。 ** 2.char(n) 固定长度，char(4) 不管是存入几个字符，都将占用 4 个字节，varchar 是存入的实际字符数 +1 个字节（n小于等于255）或2个字节(n&gt;255)，所以 varchar(4),存入 3 个字符将占用 4 个字节。 ** 3.char 类型的字符串检索速度要比 varchar 类型的快。 varchar 和 text： ** 1.varchar 可指定 n，text 不能指定，内部存储 varchar 是存入的实际字符数 +1 个字节（n&lt;=255）或 2 个字节(n&gt;255)，text 是实际字符数 +2 个字节。 ** 2.text 类型不能有默认值。 ** 3.varchar 可直接创建索引，text 创建索引要指定前多少个字符。varchar 查询速度快于 text, 在都创建索引的情况下，text 的索引似乎不起作用。 5.二进制数据(_Blob) 1._BLOB和_text存储方式不同，_TEXT以文本方式存储，英文存储区分大小写，而_Blob是以二进制方式存储，不分大小写。 2._BLOB存储的数据只能整体读出。 3TEXT可以指定字符集，_BLOB不用指定字符集。 6.日期时间类型 MySQL数据类型 含义 date 日期 ‘2008-12-2’ time 时间 ‘12:25:36’ datetime 日期时间 ‘2008-12-2 22:06:44’ timestamp 自动存储记录修改时间 若定义一个字段为timestamp，这个字段里的时间数据会随其他字段修改的时候自动刷新，所以这个数据类型的字段可以存放这条记录最后被修改的时间。 数据类型的属性 MySQL关键字 含义 NULL 数据列可包含NULL值 NOT NULL 数据列不允许包含NULL值 DEFAULT 默认值 PRIMARY KEY 主键 AUTO_INCREMENT 自动递增，适用于整数类型 UNSIGNED 无符号 CHARACTER SET name 指定一个字符集 10、MySQL GROUP BY 语句GROUP BY 语句根据一个或多个列对结果集进行分组。 在分组的列上我们可以使用 COUNT, SUM, AVG,等函数。 语法结构： 1234SELECT column_name, function(column_name)FROM table_nameWHERE column_name operator valueGROUP BY column_name; 准备数据： 1234567891011CREATE TABLE `employee_tbl` ( `id` int(11) NOT NULL, `name` char(10) NOT NULL DEFAULT '', `date` datetime NOT NULL, `singin` tinyint(4) NOT NULL DEFAULT '0' COMMENT '登录次数', PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8;-- ----------------------------INSERT INTO `employee_tbl` VALUES ('1', '小明', '2016-04-22 15:25:33', '1'), ('2', '小王', '2016-04-20 15:25:47', '3'), ('3', '小丽', '2016-04-19 15:26:02', '2'), ('4', '小王', '2016-04-07 15:26:14', '4'), ('5', '小明', '2016-04-11 15:26:40', '4'), ('6', '小明', '2016-04-04 15:26:54', '2'); 123456789101112131415161718192021222324252627282930mysql&gt; SELECT name, COUNT(*) FROM employee_tbl GROUP BY name;+--------+----------+| name | COUNT(*) |+--------+----------+| 小丽 | 1 || 小明 | 3 || 小王 | 2 |+--------+----------+mysql&gt; select * from employee_tbl;+----+------+---------------------+--------+| id | name | date | singin |+----+------+---------------------+--------+| 1 | ?? | 2016-04-22 15:25:33 | 1 || 2 | ?? | 2016-04-20 15:25:47 | 3 || 3 | ?? | 2016-04-19 15:26:02 | 2 || 4 | ?? | 2016-04-07 15:26:14 | 4 || 5 | ?? | 2016-04-11 15:26:40 | 4 || 6 | ?? | 2016-04-04 15:26:54 | 2 |+----+------+---------------------+--------+mysql&gt; select * from employee_tbl group by singin;+----+------+---------------------+--------+| id | name | date | singin |+----+------+---------------------+--------+| 1 | ?? | 2016-04-22 15:25:33 | 1 || 3 | ?? | 2016-04-19 15:26:02 | 2 || 2 | ?? | 2016-04-20 15:25:47 | 3 || 4 | ?? | 2016-04-07 15:26:14 | 4 |+----+------+---------------------+--------+ 注意： 1、group by 可以实现一个最简单的去重查询，假设想看下有哪些员工，除了用 distinct,还可以用： 1SELECT name FROM employee_tbl GROUP BY name; 返回的结果集就是所有员工的名字。 2、分组后的条件使用 HAVING 来限定，WHERE 是对原始数据进行条件限制。几个关键字的使用顺序为 where 、group by 、having、order by ，例如： 1SELECT name ,sum(*) FROM employee_tbl WHERE id&lt;&gt;1 GROUP BY name HAVING sum(*)&gt;5 ORDER BY sum(*) DESC; 11、MySQL LIKE 子句我们知道在 MySQL 中使用 SQL SELECT 命令来读取数据， 同时我们可以在 SELECT 语句中使用 WHERE 子句来获取指定的记录。 WHERE 子句中可以使用等号 = 来设定获取数据的条件，如 “company = ‘itcast”。 但是有时候我们需要获取 company 字段含有 “it” 字符的所有记录，这时我们就需要在 WHERE 子句中使用 SQL LIKE 子句。 SQL LIKE 子句中使用百分号 %字符来表示任意字符，类似于UNIX或正则表达式中的星号 。 如果没有使用百分号 %, LIKE 子句与等号 = 的效果是一样的。 语法： 以下是 SQL SELECT 语句使用 LIKE 子句从数据表中读取数据的通用语法： 123SELECT field1, field2,...fieldN FROM table_nameWHERE field1 LIKE condition1 [AND [OR]] filed2 = 'somevalue' 你可以在 WHERE 子句中指定任何条件。 你可以在 WHERE 子句中使用LIKE子句。 你可以使用LIKE子句代替等号 =。 LIKE 通常与 % 一同使用，类似于一个元字符的搜索。 你可以使用 AND 或者 OR 指定一个或多个条件。 你可以在 DELETE 或 UPDATE 命令中使用 WHERE…LIKE 子句来指定条件。 12345678910mysql&gt; select * from pet where species like '%d%';+----------+--------+---------+------+------------+------------+| name | owner | species | sex | birth | death |+----------+--------+---------+------+------------+------------+| Buffy | Harold | dog | f | 1989-05-13 | NULL || Fang | Benny | dog | m | 1990-08-27 | NULL || Bowser | Diane | dog | m | 1989-08-31 | 1995-07-29 || Chirpy | Gwen | bird | f | 1998-09-11 | NULL || Whistler | Gwen | bird | NULL | 1997-12-09 | NULL |+----------+--------+---------+------+------------+------------+ 12、MySQL NULL 值处理我们已经知道 MySQL 使用 SQL SELECT 命令及 WHERE 子句来读取数据表中的数据,但是当提供的查询条件字段为 NULL 时，该命令可能就无法正常工作。 为了处理这种情况，MySQL提供了三大运算符: IS NULL: 当列的值是 NULL,此运算符返回 true。 IS NOT NULL: 当列的值不为 NULL, 运算符返回 true。 &lt;=&gt;: 比较操作符（不同于=运算符），当比较的的两个值为 NULL 时返回 true。 关于 NULL 的条件比较运算是比较特殊的。你不能使用 = NULL 或 != NULL 在列中查找 NULL 值 。 在 MySQL 中，NULL 值与任何其它值的比较（即使是 NULL）永远返回 false，即 NULL = NULL 返回false 。 MySQL 中处理 NULL 使用 IS NULL 和 IS NOT NULL 运算符。 13、MySQL 元数据你可能想知道MySQL以下三种信息： 查询结果信息： SELECT, UPDATE 或 DELETE语句影响的记录数。 数据库和数据表的信息： 包含了数据库及数据表的结构信息。 MySQL服务器信息： 包含了数据库服务器的当前状态，版本号等。 在MySQL的命令提示符中，我们可以很容易的获取以上服务器信息。 获取服务器元数据以下命令语句可以在 MySQL 的命令提示符使用，也可以在脚本中 使用，如PHP脚本。 命令 描述 SELECT VERSION( ) 服务器版本信息 SELECT DATABASE( ) 当前数据库名 (或者返回空) SELECT USER( ) 当前用户名 SHOW STATUS 服务器状态 SHOW VARIABLES 服务器配置变量 14、MySQL ALTER命令当我们需要修改数据表名或者修改数据表字段时，就需要使用到MySQL ALTER命令。 14.1、删除、添加或修改表字段如下命令使用了 ALTER 命令及 DROP 子句来删除以上创建表的 i 字段： 1mysql&gt; ALTER TABLE testalter_tbl DROP i; 如果数据表中只剩余一个字段则无法使用DROP来删除字段。 MySQL 中使用 ADD 子句来向数据表中添加列，如下实例在表 testalter_tbl 中添加 i 字段，并定义数据类型: 1mysql&gt; ALTER TABLE testalter_tbl ADD i INT; 执行以上命令后，i 字段会自动添加到数据表字段的末尾 14.2、修改字段类型及名称如果需要修改字段类型及名称, 你可以在ALTER命令中使用 MODIFY 或 CHANGE 子句 。 例如，把字段 c 的类型从 CHAR(1) 改为 CHAR(10)，可以执行以下命令: 1mysql&gt; ALTER TABLE testalter_tbl MODIFY c CHAR(10); 14.3、修改表名如果需要修改数据表的名称，可以在 ALTER TABLE 语句中使用 RENAME 子句来实现。 尝试以下实例将数据表 testalter_tbl 重命名为 alter_tbl： 1mysql&gt; ALTER TABLE testalter_tbl RENAME TO alter_tbl; 15、MySQL 函数MySQL 有很多内置的函数，以下列出了这些函数的说明。 15.1、MySQL 字符串函数 函数 描述 实例 ASCII(s) 返回字符串 s 的第一个字符的 ASCII 码。 返回 CustomerName 字段第一个字母的 ASCII 码：SELECT ASCII(CustomerName) AS NumCodeOfFirstCharFROM Customers; CHAR_LENGTH(s) 返回字符串 s 的字符数 返回字符串 itcast 的字符数SELECT CHAR_LENGTH(&quot;itcast&quot;) AS LengthOfString; CHARACTER_LENGTH(s) 返回字符串 s 的字符数 返回字符串 itcast 的字符数SELECT CHARACTER_LENGTH(&quot;itcast&quot;) AS LengthOfString; CONCAT(s1,s2…sn) 字符串 s1,s2 等多个字符串合并为一个字符串 合并多个字符串SELECT CONCAT(&quot;SQL &quot;, &quot;itcast &quot;, &quot;Gooogle &quot;, &quot;Facebook&quot;) AS ConcatenatedString; CONCAT_WS(x, s1,s2…sn) 同 CONCAT(s1,s2,…) 函数，但是每个字符串直接要加上 x，x 可以是分隔符 合并多个字符串，并添加分隔符：SELECT CONCAT_WS(&quot;-&quot;, &quot;SQL&quot;, &quot;Tutorial&quot;, &quot;is&quot;, &quot;fun!&quot;)AS ConcatenatedString; FIELD(s,s1,s2…) 返回第一个字符串 s 在字符串列表(s1,s2…)中的位置 返回字符串 c 在列表值中的位置：SELECT FIELD(&quot;c&quot;, &quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;); FIND_IN_SET(s1,s2) 返回在字符串s2中与s1匹配的字符串的位置 返回字符串 c 在指定字符串中的位置：SELECT FIND_IN_SET(&quot;c&quot;, &quot;a,b,c,d,e&quot;); FORMAT(x,n) 函数可以将数字 x 进行格式化 “#,###.##”, 将 x 保留到小数点后 n 位，最后一位四舍五入。 格式化数字 “#,###.##” 形式：SELECT FORMAT(250500.5634, 2); -- 输出 250,500.56 INSERT(s1,x,len,s2) 字符串 s2 替换 s1 的 x 位置开始长度为 len 的字符串 从字符串第一个位置开始的 6 个字符替换为 itcast：SELECT INSERT(&quot;google.com&quot;, 1, 6, &quot;runnob&quot;); -- 输出：itcast.com LOCATE(s1,s) 从字符串 s 中获取 s1 的开始位置 获取 b 在字符串 abc 中的位置：SELECT INSTR(&#39;abc&#39;,&#39;b&#39;) -- 2 LCASE(s) 将字符串 s 的所有字母变成小写字母 字符串 itcast 转换为小写：SELECT LOWER(&#39;itcast&#39;) -- itcast LEFT(s,n) 返回字符串 s 的前 n 个字符 返回字符串 itcast 中的前两个字符：SELECT LEFT(&#39;itcast&#39;,2) -- it LEFT(s,n) 返回字符串 s 的前 n 个字符 返回字符串 abcde 的前两个字符：SELECT LEFT(&#39;abcde&#39;,2) -- ab LOCATE(s1,s) 从字符串 s 中获取 s1 的开始位置 返回字符串 abc 中 b 的位置：SELECT LOCATE(&#39;b&#39;, &#39;abc&#39;) -- 2 LOWER(s) 将字符串 s 的所有字母变成小写字母 字符串 itcast 转换为小写：SELECT LOWER(&#39;itcast&#39;) -- itcast LPAD(s1,len,s2) 在字符串 s1 的开始处填充字符串 s2，使字符串长度达到 len 将字符串 xx 填充到 abc 字符串的开始处：SELECT LPAD(&#39;abc&#39;,5,&#39;xx&#39;) -- xxabc LTRIM(s) 去掉字符串 s 开始处的空格 去掉字符串 itcast开始处的空格：SELECT LTRIM(&quot; itcast&quot;) AS LeftTrimmedString;-- itcast MID(s,n,len) 从字符串 s 的 start 位置截取长度为 length 的子字符串，同 SUBSTRING(s,n,len) 从字符串 itcast 中的第 2 个位置截取 3个 字符：SELECT MID(&quot;itcast&quot;, 2, 3) AS ExtractString; -- UNO POSITION(s1 IN s) 从字符串 s 中获取 s1 的开始位置 返回字符串 abc 中 b 的位置：SELECT POSITION(&#39;b&#39; in &#39;abc&#39;) -- 2 REPEAT(s,n) 将字符串 s 重复 n 次 将字符串 itcast 重复三次：SELECT REPEAT(&#39;itcast&#39;,3) -- itcastitcastitcast REPLACE(s,s1,s2) 将字符串 s2 替代字符串 s 中的字符串 s1 将字符串 abc 中的字符 a 替换为字符 x：SELECT REPLACE(&#39;abc&#39;,&#39;a&#39;,&#39;x&#39;) --xbc REVERSE(s) 将字符串s的顺序反过来 将字符串 abc 的顺序反过来：SELECT REVERSE(&#39;abc&#39;) -- cba RIGHT(s,n) 返回字符串 s 的后 n 个字符 返回字符串 itcast 的后两个字符：SELECT RIGHT(&#39;itcast&#39;,2) -- ob RPAD(s1,len,s2) 在字符串 s1 的结尾处添加字符串 s1，使字符串的长度达到 len 将字符串 xx 填充到 abc 字符串的结尾处：SELECT RPAD(&#39;abc&#39;,5,&#39;xx&#39;) -- abcxx RTRIM(s) 去掉字符串 s 结尾处的空格 去掉字符串 itcast 的末尾空格：SELECT RTRIM(&quot;itcast &quot;) AS RightTrimmedString; -- itcast SPACE(n) 返回 n 个空格 返回 10 个空格：SELECT SPACE(10); STRCMP(s1,s2) 比较字符串 s1 和 s2，如果 s1 与 s2 相等返回 0 ，如果 s1&gt;s2 返回 1，如果 s1小余s2 返回 -1 比较字符串：SELECT STRCMP(itcast, itcast); – 0 SUBSTR(s, start, length) 从字符串 s 的 start 位置截取长度为 length 的子字符串 从字符串 itcast 中的第 2 个位置截取 3个 字符：`SELECT SUBSTR(itcast, 2, 3) AS ExtractString; – UNO SUBSTRING(s, start, length) 从字符串 s 的 start 位置截取长度为 length 的子字符串 从字符串 itcast 中的第 2 个位置截取 3个 字符：SELECT SUBSTRING(&quot;itcast&quot;, 2, 3) AS ExtractString; -- UNO SUBSTRING_INDEX(s, delimiter, number) 返回从字符串 s 的第 number 个出现的分隔符 delimiter 之后的子串。如果 number 是正数，返回第 number 个字符左边的字符串。如果 number 是负数，返回第(number 的绝对值(从右边数))个字符右边的字符串。 SELECT SUBSTRING_INDEX(&#39;a*b&#39;,&#39;*&#39;,1) -- aSELECT SUBSTRING_INDEX(&#39;a*b&#39;,&#39;*&#39;,-1) -- bSELECT SUBSTRING_INDEX(SUBSTRING_INDEX(&#39;a*b*c*d*e&#39;,&#39;*&#39;,3),&#39;*&#39;,-1) -- c TRIM(s) 去掉字符串 s 开始和结尾处的空格 去掉字符串 itcast 的首尾空格：SELECT TRIM(&#39; itcast &#39;) AS TrimmedString; UCASE(s) 将字符串转换为大写 将字符串 itcast 转换为大写：SELECT UCASE(&quot;itcast&quot;); -- itcast UPPER(s) 将字符串转换为大写 将字符串 itcast 转换为大写：SELECT UPPER(&quot;itcast&quot;); -- itcast 15.2、MySQL 数字函数 函数名 描述 实例 ABS(x) 返回 x 的绝对值 返回 -1 的绝对值：SELECT ABS(-1) -- 返回1 ACOS(x) 求 x 的反余弦值(参数是弧度) SELECT ACOS(0.25); ASIN(x) 求反正弦值(参数是弧度) SELECT ASIN(0.25); ATAN(x) 求反正切值(参数是弧度) SELECT ATAN(2.5); ATAN2(n, m) 求反正切值(参数是弧度) SELECT ATAN2(-0.8, 2); AVG(expression) 返回一个表达式的平均值，expression 是一个字段 返回 Products 表中Price 字段的平均值：SELECT AVG(Price) AS AveragePrice FROM Products; CEIL(x) 返回大于或等于 x 的最小整数 SELECT CEIL(1.5) -- 返回2 CEILING(x) 返回大于或等于 x 的最小整数 SELECT CEIL(1.5) -- 返回2 COS(x) 求余弦值(参数是弧度) SELECT COS(2); COT(x) 求余切值(参数是弧度) SELECT COT(6); COUNT(expression) 返回查询的记录总数，expression 参数是一个字段或者 * 号 返回 Products 表中 products 字段总共有多少条记录：SELECT COUNT(ProductID) AS NumberOfProducts FROM Products; DEGREES(x) 将弧度转换为角度 SELECT DEGREES(3.1415926535898) -- 180 n DIV m 整除，n 为被除数，m 为除数 计算 10 除于 5：SELECT 10 DIV 5; -- 2 EXP(x) 返回 e 的 x 次方 计算 e 的三次方：SELECT EXP(3) -- 20.085536923188 FLOOR(x) 返回小于或等于 x 的最大整数 小于或等于 1.5 的整数：SELECT FLOOR(1.5) -- 返回1 GREATEST(expr1, expr2, expr3, …) 返回列表中的最大值 返回以下数字列表中的最大值：SELECT GREATEST(3, 12, 34, 8, 25); -- 34返回以下字符串列表中的最大值：SELECT GREATEST(&quot;Google&quot;, &quot;itcast&quot;, &quot;Apple&quot;); -- itcast LEAST(expr1, expr2, expr3, …) 返回列表中的最小值 返回以下数字列表中的最小值：SELECT LEAST(3, 12, 34, 8, 25); -- 3返回以下字符串列表中的最小值：SELECT LEAST(&quot;Google&quot;, &quot;itcast&quot;, &quot;Apple&quot;); -- Apple LN 返回数字的自然对数 返回 2 的自然对数：SELECT LN(2); -- 0.6931471805599453 LOG(x) 返回自然对数(以 e 为底的对数) SELECT LOG(20.085536923188) -- 3 LOG10(x) 返回以 10 为底的对数 SELECT LOG10(100) -- 2 LOG2(x) 返回以 2 为底的对数 返回以 2 为底 6 的对数：SELECT LOG2(6); -- 2.584962500721156 MAX(expression) 返回字段 expression 中的最大值 返回数据表 Products 中字段 Price 的最大值：SELECT MAX(Price) AS LargestPrice FROM Products; MIN(expression) 返回字段 expression 中的最小值 返回数据表 Products 中字段 Price 的最小值：SELECT MIN(Price) AS LargestPrice FROM Products; MOD(x,y) 返回 x 除以 y 以后的余数 5 除于 2 的余数：SELECT MOD(5,2) -- 1 PI() 返回圆周率(3.141593） SELECT PI() --3.141593 POW(x,y) 返回 x 的 y 次方 2 的 3 次方：SELECT POW(2,3) -- 8 POWER(x,y) 返回 x 的 y 次方 2 的 3 次方：SELECT POWER(2,3) -- 8 RADIANS(x) 将角度转换为弧度 180 度转换为弧度：SELECT RADIANS(180) -- 3.1415926535898 RAND() 返回 0 到 1 的随机数 SELECT RAND() --0.93099315644334 ROUND(x) 返回离 x 最近的整数 SELECT ROUND(1.23456) --1 SIGN(x) 返回 x 的符号，x 是负数、0、正数分别返回 -1、0 和 1 SELECT SIGN(-10) -- (-1) SIN(x) 求正弦值(参数是弧度) SELECT SIN(RADIANS(30)) -- 0.5 SQRT(x) 返回x的平方根 25 的平方根：SELECT SQRT(25) -- 5 SUM(expression) 返回指定字段的总和 计算 OrderDetails 表中字段 Quantity 的总和：SELECT SUM(Quantity) AS TotalItemsOrdered FROM OrderDetails; TAN(x) 求正切值(参数是弧度) SELECT TAN(1.75); -- -5.52037992250933 TRUNCATE(x,y) 返回数值 x 保留到小数点后 y 位的值（与 ROUND 最大的区别是不会进行四舍五入） SELECT TRUNCATE(1.23456,3) -- 1.234 15.3、MySQL 日期函数 函数名 描述 实例 ADDDATE(d,n) 计算其实日期 d 加上 n 天的日期 SELECT ADDDATE(&quot;2017-06-15&quot;, INTERVAL 10 DAY);-&gt;2017-06-25 ADDTIME(t,n) 时间 t 加上 n 秒的时间 SELECT ADDTIME(&#39;2011-11-11 11:11:11&#39;, 5)-&gt;2011-11-11 11:11:16 (秒) CURDATE() 返回当前日期 SELECT CURDATE();-&gt; 2018-09-19 CURRENT_DATE() 返回当前日期 SELECT CURRENT_DATE();-&gt; 2018-09-19 CURRENT_TIME 返回当前时间 SELECT CURRENT_TIME();-&gt; 19:59:02 CURRENT_TIMESTAMP() 返回当前日期和时间 SELECT CURRENT_TIMESTAMP()-&gt; 2018-09-19 20:57:43 CURTIME() 返回当前时间 SELECT CURTIME();-&gt; 19:59:02 DATE() 从日期或日期时间表达式中提取日期值 SELECT DATE(&quot;2017-06-15&quot;); -&gt; 2017-06-15 DATEDIFF(d1,d2) 计算日期 d1-&gt;d2 之间相隔的天数 SELECT DATEDIFF(&#39;2001-01-01&#39;,&#39;2001-02-02&#39;)-&gt; -32 DATE_ADD(d，INTERVAL expr type) 计算起始日期 d 加上一个时间段后的日期 SELECT ADDDATE(&#39;2011-11-11 11:11:11&#39;,1)-&gt; 2011-11-12 11:11:11 (默认是天)SELECT ADDDATE(&#39;2011-11-11 11:11:11&#39;, INTERVAL 5 MINUTE)-&gt; 2011-11-11 11:16:11 (TYPE的取值与上面那个列出来的函数类似) DATE_FORMAT(d,f) 按表达式 f的要求显示日期 d SELECT DATE_FORMAT(&#39;2011-11-11 11:11:11&#39;,&#39;%Y-%m-%d %r&#39;)-&gt; 2011-11-11 11:11:11 AM DATE_SUB(date,INTERVAL expr type) 函数从日期减去指定的时间间隔。 Orders 表中 OrderDate 字段减去 2 天：SELECT OrderId,DATE_SUB(OrderDate,INTERVAL 2 DAY) AS OrderPayDateFROM Orders DAY(d) 返回日期值 d 的日期部分 SELECT DAY(&quot;2017-06-15&quot;); -&gt; 15 DAYNAME(d) 返回日期 d 是星期几，如 Monday,Tuesday SELECT DAYNAME(&#39;2011-11-11 11:11:11&#39;)-&gt;Friday DAYOFMONTH(d) 计算日期 d 是本月的第几天 SELECT DAYOFMONTH(&#39;2011-11-11 11:11:11&#39;)-&gt;11 DAYOFWEEK(d) 日期 d 今天是星期几，1 星期日，2 星期一，以此类推 SELECT DAYOFWEEK(&#39;2011-11-11 11:11:11&#39;)-&gt;6 DAYOFYEAR(d) 计算日期 d 是本年的第几天 SELECT DAYOFYEAR(&#39;2011-11-11 11:11:11&#39;)-&gt;315 EXTRACT(type FROM d) 从日期 d 中获取指定的值，type 指定返回的值。 type可取值为： MICROSECONDSECONDMINUTEHOURDAYWEEKMONTHQUARTERYEARSECOND_MICROSECONDMINUTE_MICROSECONDMINUTE_SECONDHOUR_MICROSECONDHOUR_SECONDHOUR_MINUTEDAY_MICROSECONDDAY_SECONDDAY_MINUTEDAY_HOURYEAR_MONTH SELECT EXTRACT(MINUTE FROM &#39;2011-11-11 11:11:11&#39;) -&gt; 11 ROM_DAYS(n) 计算从 0000 年 1 月 1 日开始 n 天后的日期 SELECT FROM_DAYS(1111)-&gt; 0003-01-16 HOUR(t) 返回 t 中的小时值 SELECT HOUR(&#39;1:2:3&#39;)-&gt; 1 LAST_DAY(d) 返回给给定日期的那一月份的最后一天 SELECT LAST_DAY(&quot;2017-06-20&quot;);-&gt; 2017-06-30 LOCALTIME() 返回当前日期和时间 SELECT LOCALTIME()-&gt; 2018-09-19 20:57:43 LOCALTIMESTAMP() 返回当前日期和时间 SELECT LOCALTIMESTAMP()-&gt; 2018-09-19 20:57:43 MAKEDATE(year, day-of-year) 基于给定参数年份 year 和所在年中的天数序号 day-of-year 返回一个日期 SELECT MAKEDATE(2017, 3);-&gt; 2017-01-03 MAKETIME(hour, minute, second) 组合时间，参数分别为小时、分钟、秒 SELECT MAKETIME(11, 35, 4);-&gt; 11:35:04 MICROSECOND(date) 返回日期参数所对应的毫秒数 SELECT MICROSECOND(&quot;2017-06-20 09:34:00.000023&quot;);-&gt; 23 MINUTE(t) 返回 t 中的分钟值 SELECT MINUTE(&#39;1:2:3&#39;)-&gt; 2 MONTHNAME(d) 返回日期当中的月份名称，如 Janyary SELECT MONTHNAME(&#39;2011-11-11 11:11:11&#39;)-&gt; November MONTH(d) 返回日期d中的月份值，1 到 12 SELECT MONTH(&#39;2011-11-11 11:11:11&#39;)-&gt;11 NOW() 返回当前日期和时间 SELECT NOW()-&gt; 2018-09-19 20:57:43 PERIOD_ADD(period, number) 为 年-月 组合日期添加一个时段 SELECT PERIOD_ADD(201703, 5); -&gt; 201708 PERIOD_DIFF(period1, period2) 返回两个时段之间的月份差值 SELECT PERIOD_DIFF(201710, 201703);-&gt; 7 QUARTER(d) 返回日期d是第几季节，返回 1 到 4 SELECT QUARTER(&#39;2011-11-11 11:11:11&#39;)-&gt; 4 SECOND(t) 返回 t 中的秒钟值 SELECT SECOND(&#39;1:2:3&#39;)-&gt; 3 SEC_TO_TIME(s) 将以秒为单位的时间 s 转换为时分秒的格式 SELECT SEC_TO_TIME(4320)-&gt; 01:12:00 STR_TO_DATE(string, format_mask) 将字符串转变为日期 SELECT STR_TO_DATE(&quot;August 10 2017&quot;, &quot;%M %d %Y&quot;);-&gt; 2017-08-10 SUBDATE(d,n) 日期 d 减去 n 天后的日期 SELECT SUBDATE(&#39;2011-11-11 11:11:11&#39;, 1)-&gt;2011-11-10 11:11:11 (默认是天) SUBTIME(t,n) 时间 t 减去 n 秒的时间 SELECT SUBTIME(&#39;2011-11-11 11:11:11&#39;, 5)-&gt;2011-11-11 11:11:06 (秒) SYSDATE() 返回当前日期和时间 SELECT SYSDATE()-&gt; 2018-09-19 20:57:43 TIME(expression) 提取传入表达式的时间部分 SELECT TIME(&quot;19:30:10&quot;);-&gt; 19:30:10 TIME_FORMAT(t,f) 按表达式 f 的要求显示时间 t SELECT TIME_FORMAT(&#39;11:11:11&#39;,&#39;%r&#39;)11:11:11 AM TIME_TO_SEC(t) 将时间 t 转换为秒 SELECT TIME_TO_SEC(&#39;1:12:00&#39;)-&gt; 4320 TIMEDIFF(time1, time2) 计算时间差值 SELECT TIMEDIFF(&quot;13:10:11&quot;, &quot;13:10:10&quot;);-&gt; 00:00:01 TIMESTAMP(expression, interval) 单个参数时，函数返回日期或日期时间表达式；有2个参数时，将参数加和 SELECT TIMESTAMP(&quot;2017-07-23&quot;, &quot;13:10:11&quot;);-&gt; 2017-07-23 13:10:11 TO_DAYS(d) 计算日期 d 距离 0000 年 1 月 1 日的天数 SELECT TO_DAYS(&#39;0001-01-01 01:01:01&#39;)-&gt; 366 WEEK(d) 计算日期 d 是本年的第几个星期，范围是 0 到 53 SELECT WEEK(&#39;2011-11-11 11:11:11&#39;)-&gt; 45 WEEKDAY(d) 日期 d 是星期几，0 表示星期一，1 表示星期二 SELECT WEEKDAY(&quot;2017-06-15&quot;);-&gt; 3 WEEKOFYEAR(d) 计算日期 d 是本年的第几个星期，范围是 0 到 53 SELECT WEEKOFYEAR(&#39;2011-11-11 11:11:11&#39;)-&gt; 45 YEAR(d) 返回年份 SELECT YEAR(&quot;2017-06-15&quot;);-&gt; 2017 YEARWEEK(date, mode) 返回年份及第几周（0到53），mode 中 0 表示周天，1表示周一，以此类推 SELECT YEARWEEK(&quot;2017-06-15&quot;);-&gt; 201724 15.4、MySQL 高级函数 函数名 描述 实例 BIN(x) 返回 x 的二进制编码 15 的 2 进制编码:SELECT BIN(15); -- 1111 BINARY(s) 将字符串 s 转换为二进制字符串 SELECT BINARY &quot;itcast&quot;;-&gt; itcast CASE expression WHEN condition1 THEN result1 WHEN condition2 THEN result2 ... WHEN conditionN THEN resultN ELSE resultEND CASE 表示函数开始，END 表示函数结束。如果 condition1 成立，则返回 result1, 如果 condition2 成立，则返回 result2，当全部不成立则返回 result，而当有一个成立之后，后面的就不执行了。 SELECT CASE WHEN 1 &gt; 0 THEN &#39;1 &gt; 0&#39; WHEN 2 &gt; 0 THEN &#39;2 &gt; 0&#39; ELSE &#39;3 &gt; 0&#39; END-&gt;1 &gt; 0 CAST(x AS type) 转换数据类型 字符串日期转换为日期：SELECT CAST(&quot;2017-08-29&quot; AS DATE);-&gt; 2017-08-29 COALESCE(expr1, expr2, …., expr_n) 返回参数中的第一个非空表达式（从左向右） SELECT COALESCE(NULL, NULL, NULL, &#39;itcast.com&#39;, NULL, &#39;google.com&#39;);-&gt; itcast.com CONNECTION_ID() 返回服务器的连接数 SELECT CONNECTION_ID();-&gt; 4292835 CONV(x,f1,f2) 返回 f1 进制数变成 f2 进制数 SELECT CONV(15, 10, 2);-&gt; 1111 CONVERT(s USING cs) 函数将字符串 s 的字符集变成 cs SELECT CHARSET(&#39;ABC&#39;)-&gt;utf-8 SELECT CHARSET(CONVERT(&#39;ABC&#39; USING gbk))-&gt;gbk CURRENT_USER() 返回当前用户 SELECT CURRENT_USER();-&gt; guest@% DATABASE() 返回当前数据库名 SELECT DATABASE(); -&gt; itcast IF(expr,v1,v2) 如果表达式 expr 成立，返回结果 v1；否则，返回结果 v2。 SELECT IF(1 &gt; 0,&#39;正确&#39;,&#39;错误&#39;) -&gt;正确 IFNULL(v1,v2) 如果 v1 的值不为 NULL，则返回 v1，否则返回 v2。 SELECT IFNULL(null,&#39;Hello Word&#39;)-&gt;Hello Word ISNULL(expression) 判断表达式是否为空 SELECT ISNULL(NULL);-&gt;1 LAST_INSERT_ID() 返回最近生成的 AUTO_INCREMENT 值 SELECT LAST_INSERT_ID();-&gt;6 NULLIF(expr1, expr2) 比较两个字符串，如果字符串 expr1 与 expr2 相等 返回 NULL，否则返回 expr1 SELECT NULLIF(25, 25);-&gt; SESSION_USER() 返回当前用户 SELECT SESSION_USER();-&gt; guest@% SYSTEM_USER() 返回当前用户 SELECT SYSTEM_USER();-&gt; guest@% USER() 返回当前用户 SELECT USER();-&gt; guest@% VERSION() 返回数据库的版本号 SELECT VERSION()-&gt; 5.6.34 16、MySQL 索引MySQL索引的建立对于MySQL的高效运行是很重要的，索引可以大大提高MySQL的检索速度。 索引分单列索引和组合索引。单列索引，即一个索引只包含单个列，一个表可以有多个单列索引，但这不是组合索引。组合索引，即一个索引包含多个列。 创建索引时，你需要确保该索引是应用在 SQL 查询语句的条件(一般作为 WHERE 子句的条件)。 实际上，索引也是一张表，该表保存了主键与索引字段，并指向实体表的记录。 上面都在说使用索引的好处，但过多的使用索引将会造成滥用。因此索引也会有它的缺点：虽然索引大大提高了查询速度，同时却会降低更新表的速度，如对表进行INSERT、UPDATE和DELETE。因为更新表时，MySQL不仅要保存数据，还要保存一下索引文件。 建立索引会占用磁盘空间的索引文件。 16.1、普通索引16.1.1、创建索引这是最基本的索引，它没有任何限制。它有以下几种创建方式： 1234CREATE INDEX indexName ON mytable(username(length)); //创建索引 create index id on B(A_ID); 如果是CHAR，VARCHAR类型，length可以小于字段实际长度；如果是BLOB和TEXT类型，必须指定 length。 16.1.2、修改表结构(添加索引)1ALTER table tableName ADD INDEX indexName(columnName) 16.1.3、创建表的时候直接指定123456789CREATE TABLE mytable( ID INT NOT NULL, username VARCHAR(16) NOT NULL, INDEX [indexName] (username(length)) ); 16.1.4、删除索引的语法1DROP INDEX [indexName] ON mytable; 16.2、唯一索引它与前面的普通索引类似，不同的就是：索引列的值必须唯一，但允许有空值。如果是组合索引，则列值的组合必须唯一。它有以下几种创建方式： 16.2.1、创建索引1CREATE UNIQUE INDEX indexName ON mytable(username(length)) 16.2.2、修改表结构1ALTER table mytable ADD UNIQUE [indexName] (username(length)) 16.2.3、创建表的时候直接指定123456789CREATE TABLE mytable( ID INT NOT NULL, username VARCHAR(16) NOT NULL, UNIQUE [indexName] (username(length)) ); 16.3、使用ALTER 命令添加和删除索引有四种方式来添加数据表的索引： ALTER TABLE tbl_name ADD PRIMARY KEY (column_list): 该语句添加一个主键，这意味着索引值必须是唯一的，且不能为NULL。 ALTER TABLE tbl_name ADD UNIQUE index_name (column_list): 这条语句创建索引的值必须是唯一的（除了NULL外，NULL可能会出现多次）。 ALTER TABLE tbl_name ADD INDEX index_name (column_list): 添加普通索引，索引值可出现多次。 ALTER TABLE tbl_name ADD FULLTEXT index_name (column_list):该语句指定了索引为 FULLTEXT ，用于全文索引。 以下实例为在表中添加索引。 1mysql&gt; ALTER TABLE testalter_tbl ADD INDEX (c); 你还可以在 ALTER 命令中使用 DROP 子句来删除索引。尝试以下实例删除索引: 1mysql&gt; ALTER TABLE testalter_tbl DROP INDEX c; 16.4、使用 ALTER 命令添加和删除主键主键只能作用于一个列上，添加主键索引时，你需要确保该主键默认不为空（NOT NULL）。实例如下： 12mysql&gt; ALTER TABLE testalter_tbl MODIFY itcast INT NOT NULL;mysql&gt; ALTER TABLE testalter_tbl ADD PRIMARY KEY (itcast); 你也可以使用 ALTER 命令删除主键： 1mysql&gt; ALTER TABLE testalter_tbl DROP PRIMARY KEY; 删除主键时只需指定PRIMARY KEY，但在删除索引时，你必须知道索引名。 16.5、显示索引信息你可以使用 SHOW INDEX 命令来列出表中的相关的索引信息。可以通过添加 \G 来格式化输出信息。 尝试以下实例: 1mysql&gt; SHOW INDEX FROM table_name; \G mysql&gt; show index from B;+——-+————+———-+————–+————-+———–+————-+———-+——–+——+————+———+—————+| Table | Non_unique | Key_name | Seq_in_index | Column_name | Collation | Cardinality | Sub_part | Packed | Null | Index_type | Comment | Index_comment |+——-+————+———-+————–+————-+———–+————-+———-+——–+——+————+———+—————+| B | 0 | PRIMARY | 1 | A_ID | A | 3 | NULL | NULL | | BTREE | | |+——-+————+———-+————–+————-+———–+————-+———-+——–+——+————+———+—————+ 17、MySQL 事务​ MySQL 事务主要用于处理操作量大，复杂度高的数据。比如说，在人员管理系统中，你删除一个人员，你即需要删除人员的基本资料，也要删除和该人员相关的信息，如信箱，文章等等，这样，这些数据库操作语句就构成一个事务！ 在 MySQL 中只有使用了 Innodb 数据库引擎的数据库或表才支持事务。 事务处理可以用来维护数据库的完整性，保证成批的 SQL 语句要么全部执行，要么全部不执行。 事务用来管理 insert,update,delete 语句 一般来说，事务是必须满足4个条件（ACID）：：原子性（Atomicity，或称不可分割性）、一致性（Consistency）、隔离性（Isolation，又称独立性）、持久性（Durability）。 原子性：一个事务（transaction）中的所有操作，要么全部完成，要么全部不完成，在中间某个环节不会结束。事务在执行过程中发生错误，会被回滚（Rollback）到事务开始前的状态，就像这个事务从来没有执行过一样。 一致性：在事务开始之前和事务结束以后，数据库的完整性没有被破坏。这表示写入的资料必须完全符合所有的预设规则，这包含资料的精确度、串联性以及后续数据库可以自发性地完成预定的工作。 隔离性：数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。事务隔离分为不同级别，包括读未提交（Read uncommitted）、读提交（read committed）、可重复读（repeatable read）和串行化（Serializable）。 持久性：事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。 17.1、事务控制语句： BEGIN或START TRANSACTION；显式地开启一个事务； COMMIT；也可以使用COMMIT WORK，不过二者是等价的。COMMIT会提交事务，并使已对数据库进行的所有修改成为永久性的； ROLLBACK；有可以使用ROLLBACK WORK，不过二者是等价的。回滚会结束用户的事务，并撤销正在进行的所有未提交的修改； SAVEPOINT identifier；SAVEPOINT允许在事务中创建一个保存点，一个事务中可以有多个SAVEPOINT； RELEASE SAVEPOINT identifier；删除一个事务的保存点，当没有指定的保存点时，执行该语句会抛出一个异常； ROLLBACK TO identifier；把事务回滚到标记点； SET TRANSACTION；用来设置事务的隔离级别。InnoDB存储引擎提供事务的隔离级别有READ UNCOMMITTED、READ COMMITTED、REPEATABLE READ和SERIALIZABLE。 17.2、MYSQL 事务处理主要有两种方法：1、用 BEGIN, ROLLBACK, COMMIT来实现 BEGIN 开始一个事务 ROLLBACK 事务回滚 COMMIT 事务确认 2、直接用 SET 来改变 MySQL 的自动提交模式: SET AUTOCOMMIT=0 禁止自动提交 SET AUTOCOMMIT=1 开启自动提交]]></content>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Stream与Lambda]]></title>
    <url>%2F2017%2F06%2F23%2FStream%E4%B8%8ELambda.html</url>
    <content type="text"><![CDATA[stream 流 jdk1.8 操作集合和数组 集合list.stream()数组Stream.of(数组)特点只能使用一次 后流关闭 方法有两种 延迟方法 终结方法(count() forEach())除了终结 都是延迟 方法(全支持 lambda表达式)forEach()遍历filter() 过滤map() 映射 如string数组 映射为integer数组 也可映射对象count() 获取个数limit() 获取前几个 传入 longskip() 跳过 几个参数 传入 longconcat() 合并流 传入 两个流 常用的函数式接口SupplierConsumerPredicateFunction Lambda 表达式一函数式编程思想面向对象的思想:做一件事情,找一个能解决这个事情的对象,调用对象的方法,完成事情.函数式编程思想:只要能获取到结果,谁去做的,怎么做的都不重要,重视的是结果,不重视过程 二 Lambda 转换线程写法java原始写法匿名内部类 123456789101112public class Demo01Runnable &#123; public static void main(String[] args) &#123; // 匿名内部类 Runnable task = new Runnable() &#123; @Override public void run() &#123; // 覆盖重写抽象方法 System.out.println("多线程任务执行！"); &#125; &#125;; new Thread(task).start(); // 启动线程 &#125;&#125; 代码分析: 对于Runnable 的匿名内部类用法，可以分析出几点内容： Thread 类需要Runnable 接口作为参数，其中的抽象run 方法是用来指定线程任务内容的核心； ​ 为了指定run 的方法体，不得不需要Runnable 接口的实现类； ​ 为了省去定义一个RunnableImpl 实现类的麻烦，不得不使用匿名内部类；​ 必须覆盖重写抽象run 方法，所以方法名称、方法参数、方法返回值不得不再写一遍，且不能写错；​ 而实际上，似乎只有方法体才是关键所在。 思想转换用lambda 12345public class Demo02LambdaRunnable &#123; public static void main(String[] args) &#123; new Thread(() ‐&gt; System.out.println("多线程任务执行！")).start(); // 启动线程 &#125;&#125; 匿名内部类分析匿名内部类的好处与弊端 一方面，匿名内部类可以帮我们省去实现类的定义；另一方面，匿名内部类的语法——确实太复杂了！语义分析仔细分析该代码中的语义， Runnable 接口只有一个run 方法的定义： public abstract void run();即制定了一种做事情的方案（其实就是一个函数）： 无参数：不需要任何条件即可执行该方案。 无返回值：该方案不产生任何结果。 代码块（方法体）：该方案的具体执行步骤。同样的语义体现在Lambda 语法中，要更加简单： 1() ‐&gt; System.out.println("多线程任务执行！") ​ 前面的一对小括号即run 方法的参数（无），代表不需要任何条件；​ 中间的一个箭头代表将前面的参数传递给后面的代码；​ 后面的输出语句即业务逻辑代码。 lambda 格式Lambda省去面向对象的条条框框，格式由3个部分组成： 一些参数 一个箭头 一段代码Lambda表达式的标准格式为： 1(参数类型 参数名称) ‐&gt; &#123; 代码语句 &#125; 格式说明： 小括号内的语法与传统方法参数列表一致：无参数则留空；多个参数则用逗号分隔。 -&gt; 是新引入的语法格式，代表指向动作。 大括号内的语法与传统方法体要求基本一致。]]></content>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[反爬虫技术]]></title>
    <url>%2F2017%2F03%2F02%2F%E5%8F%8D%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF.html</url>
    <content type="text"><![CDATA[一 反爬概述1 反爬解决的问题:1 恶意占座影响系统公平,平衡 2 影响系统资源,网络带宽 /计算资源 3 商业恶意竞争,爬取机密数据]]></content>
      <categories>
        <category>反爬</category>
      </categories>
      <tags>
        <tag>反爬</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[旅游]]></title>
    <url>%2F2017%2F03%2F01%2F%E6%97%85%E6%B8%B8.html</url>
    <content type="text"><![CDATA[大数据常见面试题一 zookeeper1 ZooKeeper的角色是什么？123456789101112 Leader 角色 Leader 服务器是整个zookeeper 集群的核心，主要的工作任务有两项： ①事物请求的唯一调度和处理者，保证集群事物处理的顺序性。 ②集群内部各服务器的调度者。 Follower 角色 Follower 角色的主要职责是： ①处理客户端非事物请求、转发事物请求给leader服务器。 ②参与事物请求Proposal的投票（Leader发起的提案，要求 Follower投票，需要半数以上follower节点通过， leader才会commit数据）。 ③参与Leader选举的投票。Observer 角色 ①Observer 是 zookeeper3.3 开始引入的一个全新的服务器角色，从字面来理解，该角色充当了观察者的角色。观察 zookeeper 集群中的最新状态变化并将这些状态变化同步到 observer 服务器上。Observer 的工作原理与 follower 角色基本一致，而它和 follower 角色唯一的不同在于 observer 不参与任何形式的投票，包括事务请求Proposal的投票和leader选举的投票。简单来说，observer服务器只提供非事物请求服务，通常在于不影响集群事务处理能力的前提下提升集群非事物处理的能力 2 zk的用途和选举原理用途 1234561分布式锁2服务注册和发现利用Znode和Watcher，可以实现分布式服务的注册和发现。最著名的应用就是阿里的分布式RPC框架Dubbo。3共享配置和状态信息Redis的分布式解决方案Codis（豌豆荚），就利用了Zookeeper来存放数据路由表和 codis-proxy 节点的元信息。同时 codis-config 发起的命令都会通过 ZooKeeper 同步到各个存活的 codis-proxy。4软负载均衡 选举 12341每个 server 发出一个投票： 投票的最基本元素是（SID-服务器id,ZXID-事物id）2接受来自各个服务器的投票3处理投票：优先检查 ZXID(数据越新ZXID越大),ZXID比较大的作为leader，ZXID一样的情况下比较SID4统计投票：这里有个过半的概念，大于集群机器数量的一半，即大于或等于（n/2+1）,我们这里的由三台，大于等于2即为达到“过半”的要求。这里也有引申到为什么 Zookeeper 集群推荐是单数。 3 zk的watch机制4 分布式锁5 zookeeper 搭建首先我们要在每台pc上配置zookeeper环境变量，在cd到zookeeper下的conf文件夹下在zoo_simjle.cfg文件中添加datadir路径，再到zookeeper下新建data文件夹，创建myid，在文件里添加上server的ip地址。在启动zkserver.sh start便ok了。 二 Hadoop常见面试题 hdfs mr yarn1 hdfs的写文件2 hdfs读文件3 Hadoop的shuffle过程1.Map端的shuffleMap端会处理输入数据并产生中间结果，这个中间结果会写到本地磁盘，而不是HDFS。每个Map的输出会先写到内存缓冲区中，当写入的数据达到设定的阈值时，系统将会启动一个线程将缓冲区的数据写到磁盘，这个过程叫做spill。 在spill写入之前，会先进行二次排序，首先根据数据所属的partition进行排序，然后每个partition中的数据再按key来排序。partition的目是将记录划分到不同的Reducer上去，以期望能够达到负载均衡，以后的Reducer就会根据partition来读取自己对应的数据。接着运行combiner(如果设置了的话)，combiner的本质也是一个Reducer，其目的是对将要写入到磁盘上的文件先进行一次处理，这样，写入到磁盘的数据量就会减少。最后将数据写到本地磁盘产生spill文件(spill文件保存在{mapred.local.dir}指定的目录中，Map任务结束后就会被删除)。 最后，每个Map任务可能产生多个spill文件，在每个Map任务完成前，会通过多路归并算法将这些spill文件归并成一个文件。至此，Map的shuffle过程就结束了。 2.Reduce端的shuffle Reduce端的shuffle主要包括三个阶段，copy、sort(merge)和reduce。 首先要将Map端产生的输出文件拷贝到Reduce端，但每个Reducer如何知道自己应该处理哪些数据呢？因为Map端进行partition的时候，实际上就相当于指定了每个Reducer要处理的数据(partition就对应了Reducer)，所以Reducer在拷贝数据的时候只需拷贝与自己对应的partition中的数据即可。每个Reducer会处理一个或者多个partition，但需要先将自己对应的partition中的数据从每个Map的输出结果中拷贝过来。 接下来就是sort阶段，也成为merge阶段，因为这个阶段的主要工作是执行了归并排序。从Map端拷贝到Reduce端的数据都是有序的，所以很适合归并排序。最终在Reduce端生成一个较大的文件作为Reduce的输入。 最后就是Reduce过程了，在这个过程中产生了最终的输出结果，并将其写到HDFS上。 4 fsimage和edit的区别当NN,SN要进行数据同步时叫做checkpoint时就用到了fsimage与edit，fsimage是保存最新的元数据的信息，当fsimage数据到一定的大小事会去生成一个新的文件来保存元数据的信息，这个新的文件就是edit，edit会回滚最新的数据。 5 简述Hadoop的MapReduce模型首先map task会从本地文件系统读取数据，转换成key-value形式的键值对集合，使用的是hadoop内置的数据类型，如Text，Longwritable等。 将键值对集合输入mapper进行业务处理过程，将其转化成需要的key-value再输出。 之后会进行一个partition分区操作，默认使用的是hashpartitioner，可以通过重写hashpartitioner的getPartition方法来自定义分区规则。 之后会对key进行sort排序，grouping分组操作将相同key的value合并分组输出，在这里可以使用自定义的数据类型，重写WritableComparator的Comparator方法来自定义排序规则，重写RawComparator的compara方法来自定义分组规则。 之后进行一个combiner归约操作，就是一个本地的reduce预处理，以减小shuffle，reducer的工作量。 Reduce task会用过网络将各个数据收集进行reduce处理，最后将数据保存或者显示，结束整个job。 6 Hadoop需要哪些守护进程7 Hadoop的textinputformat作用 如何自定义InputFormat会在map操作之前对数据进行两方面的预处理。 1.是getSplits，返回的是InputSplit数组，对数据进行Split分片，每片交给map操作一次。 2.是getRecordReader，返回的是RecordReader对象，对每个Split分片进行转换为key-value键值对格式传递给map常用的InputFormat是TextInputFormat，使用的是LineRecordReader对每个分片进行键值对的转换，以行偏移量作为键，行内容作为值。 自定义类继承InputFormat接口，重写createRecordReader和isSplitable方法在createRecordReader中可以自定义分隔符。 8 Hadoop与spark都是并行计算,他们的相同和区别同: 1两者都使用mr模型来进行并行计算 不同: 1 hadoop的一个作业称为job，job里面分为map task和reduce task，每个task都是在自己的进程中运行的，当task结束时，进程也会结束。 Spark用户提交的任务称为application，一个application对应一个SparkContext，app中存在多个job，没触发一个action操作就会产生一个job。 2这些job可以并行或者串行执行，每个job有多个stage，stage是shuffle过程中DAGSchaduler通过RDD之间的依赖关系划分job而来的，每个stage里面有多个task，组成taskset有TaskSchaduler分发到各个executor中执行，executor的生命周期是和application一样的，即使没有job运行也是存在的，所以task可以快速启动读取内存进行计算的。 Hadoop的job只有map和reduce操作，表达能力比较欠缺而且在mr过程中会重复的读写hdfs，造成大量的io操作，多个job需要自己管理关系。 Spark的迭代计算都是在内存中进行的，API中提供了大量的RDD操作join，groupby等，而且通过DAG图可以实现良好的容错。 9 为什么使用flume导入hdfs , hdfs的架构是怎样的Flume可以实时的导入数据到hdfs中，当hdfs上的文件达到一个指定大小的时候会形成一个文件，或者超时所指定时间的话也形成一个文件。 文件都是存储在datanode上的，namenode存储着datanode的元数据信息，而namenode的元数据信息是存在内存中的，所以当文件切片很小或者很多的时候会卡死。 10MR程序运行的时候会有什么比较常见的问题？比如说作业中大部分都完成了，但是总有几个reduce一直在运行。 这是因为这几个reduce中的处理的数据要远远大于其他的reduce，可能是对键值对任务划分的不均匀造成的数据倾斜。 解决的方法可以在分区的时候重新定义分区规则对于value数据很多的key可以进行拆分、均匀打散等处理，或者是在map端的combiner中进行数据预处理的操作。 11 简述Hadoop和spark的shuffle过程Hadoop：map端保存分片数据，通过网络收集到reduce端。 Spark：spark的shuffle实在DAGSchedular划分Stage的时候产生的，TaskSchedular要分发Stage到各个worker的executor。减少shuffle可以提高性能。 12 yarn的理解：YARN是Hadoop2.0版本引进的资源管理系统，直接从MR1演化而来。核心思想：将MR1中的JobTracker的资源管理和作业调度两个功能分开，分别由ResourceManager和ApplicationMaster进程实现。 ResourceManager：负责整个集群的资源管理和调度 ApplicationMaster：负责应用程序相关事务，比如任务调度、任务监控和容错等。 YARN的出现，使得多个计算框架可以运行在同一个集群之中。 1. 每一个应用程序对应一个ApplicationMaster。 2. 目前可以支持多种计算框架运行在YARN上面，比如MapReduce、storm、Spark、Flink。 13 HDFS 的默认block块的大小原为64 m后改为128m有何影响block块为一个逻辑概念 设计更大 是为了 1 减少寻道时间 寻道也是一个逻辑概念,这里的寻道时间是定位到块的时间,hdfs是设计存储大数据的,如果块设计很小,一个文件就会有很多块组成,hdfs上的最小单位是块,这样,寻找块的时间就会大大增加,降低读写效率 2 减少任务数 一个map和reduce 都是以块为单位处理,如果块很小,MapReduce的任务数会非常多,任务之间的切换开销变大,效率变低 ,同样 如果块很大,又会单个任务就会很慢 3 减少元数据 ，在hdfs中，文件块的信息都是元数据，集群运行过程中元数据会都加载进namenode的内存中，如果块很小的情况下，元数据信息就会很多，namenode压力就会很大。实际上当集群规模超过4000台的时候，namenode内存再大都已经没有什么用，因为此时文件数已经太多了。 4 减少网络开销 如果数据块很小，一个文件要分成很多块，而每个文件都有副本，当文件删除或者拷贝时，就会导致大量块移动，寻道开销和网络开销都会很大。 5优化 当集群规模增大的时候，可以适当增加块的大小，比如集群规模大到一定程度可以将块增大到256M，降低相应开销。 可以修改block的大小 14 namenode 与Secondarynamenode 的区别15 hdfs得block默认保存几分3份 16 hdfs的启动过程HDFS的启动过程分为四个阶段：第一阶段：NameNode 读取包含元数据信息的fsimage文件，并加载到内存；第二阶段：NameNode读取体现HDFS最新状态的edits日志文件，并加载到内存中第三阶段：生成检查点，SecondaryNameNode将edits日志中的信息合并到fsimage文件中第四阶段：进入安全模式，检查数据块的完整性 17 hdfs的 client端，复制到第三个副本时宕机，hdfs怎么恢复下次写第三副本？block块信息是先写dataNode还是先写nameNode？ Datanode会定时上报block块的信息给namenode ，namenode就会得知副本缺失，然后namenode就会启动副本复制流程以保证数据块的备份！Client向NameNode发起文件写入的请求。NameNode根据文件大小和文件块配置情况，返回给Client它所管理部分DataNode的信息。Client将文件划分为多个Block，根据DataNode的地址信息，按顺序写入到每一个DataNode块中。 18hdfs，replica如何定位1st replica.如果写请求方所在机器是其中一个datanode,则直接存放在本地,否则随机在集群中选择一个datanode. 2nd replica.第二个副本存放于不同第一个副本的所在的机架. 3rd replica.第三个副本存放于第二个副本所在的机架,但是属于不同的节点. 19 mr处理数据倾斜原因: map /reduce程序执行时，reduce节点大部分执行完毕，但是有一个或者几个reduce节点运行很慢，导致整个程序的处理时间很长，这是因为某一个key的条数比其他key多很多（有时是百倍或者千倍之多），这条key所在的reduce节点所处理的数据量比其他节点就大很多，从而导致某几个节点迟迟运行不完，此称之为数据倾斜。 解决: (1)设置一个hash份数N，用来对条数众多的key进行打散。 (2)对有多条重复key的那份数据进行处理：从1到N将数字加在key后面作为新key，如果需要和另一份数据关联的话，则要重写比较类和分发类（方法如上篇《hadoop job解决大数据量关联的一种方法》）。如此实现多条key的平均分发。 int iNum = iNum % iHashNum; String strKey = key + CTRLC + String.valueOf(iNum) + CTRLB + “B”; （3）上一步之后，key被平均分散到很多不同的reduce节点。如果需要和其他数据关联，为了保证每个reduce节点上都有关联的key，对另一份单一key的数据进行处理：循环的从1到N将数字加在key后面作为新key for(int i = 0; i &lt; iHashNum; ++i){ String strKey =key + CTRLC + String.valueOf(i) ; output.collect(new Text(strKey), new Text(strValues));} 以此解决数据倾斜的问题，经试验大大减少了程序的运行时间。但此方法会成倍的增加其中一份数据的数据量，以增加shuffle数据量为代价，所以使用此方法时，要多次试验，取一个最佳的hash份数值。 ====================================== 用上述的方法虽然可以解决数据倾斜，但是当关联的数据量巨大时，如果成倍的增长某份数据，会导致reduce shuffle的数据量变的巨大，得不偿失，从而无法解决运行时间慢的问题。 有一个新的办法可以解决 成倍增长数据 的缺陷： 在两份数据中找共同点，比如两份数据里除了关联的字段以外，还有另外相同含义的字段，如果这个字段在所有log中的重复率比较小，则可以用这个字段作为计算hash的值，如果是数字，可以用来模hash的份数，如果是字符可以用hashcode来模hash的份数（当然数字为了避免落到同一个reduce上的数据过多，也可以用hashcode），这样如果这个字段的值分布足够平均的话，就可以解决上述的问题。 20hdfs的数据压缩算法1 在HDFS之上将数据压缩好后，再存储到HDFS2、在HDFS内部支持数据压缩，这里又可以分为几种方法： 2.1、压缩工作在DataNode上完成，这里又分两种方法： 2.1.1、数据接收完后，再压缩 这个方法对HDFS的改动最小，但效果最低，只需要在block文件close后，调用压缩工具，将block文件压缩一下，然后再打开block文件时解压一下即可，几行代码就可以搞定 2.1.2、边接收数据边压缩，使用第三方提供的压缩库 效率和复杂度折中方法，Hook住系统的write和read操作，在数据写入磁盘之前，先压缩一下，但write和read对外的接口行为不变，比如：原始大小为100KB的数据，压缩后大小为10KB，当写入100KB后，仍对调用者返回100KB，而不是10KB 2.2、压缩工作交给DFSClient做，DataNode只接收和存储 这个方法效果最高，压缩分散地推给了HDFS客户端，但DataNode需要知道什么时候一个block块接收完成了。推荐最终实现采用2.2这个方法，该方法需要修改的HDFS代码量也不大，但效果最高。 21mr的调度MapReduce是hadoop提供一个可进行分布式计算的框架或者平台，显然这个平台是多用户的，每个合法的用户可以向这个平台提交作业，那么这就带来一个问题，就是作业调度。 任何调度策略都考虑自己平台调度需要权衡的几个维度，例如操作系统中的进程调度，他需要考虑的维度就是资源（CPU）的最大利用率（吞吐）和实时性，操作系统对实时性的要求很高，所以操作系统往往采用基于优先级的、可抢占式的调度策略，并且赋予IO密集型（相对于计算密集型）的进程较高的优先级，扯的有点远。 回到hadoop平台，其实MapReduce的作业调度并没有很高的实时性的要求，本着最大吞吐的原则去设计的，所以MapReduce默认采用的调度策略是FIFO（基于优先级队列实现的FIFO，不是纯粹的FIFO，这样每次h），这种策略显然不是可抢占式的调度，所以带来的问题就是高优先级的任务会被先前已经在运行并且还要运行很久的低优先级的作业给堵塞住。 22.mapreduce 作业，不使用 reduce 来输出，用什么能代替 reduce 的功能三 flume常见面试题1 Flume的工作及时是什么？核心概念是agent，里面包括source，channel和sink三个组件。 Source运行在日志收集节点进行日志采集，之后临时存储在channel中，sink负责将channel中的数据发送到目的地。 只有发送成功channel中的数据才会被删除。 首先书写flume配置文件，定义agent、source、channel和sink然后将其组装，执行flume-ng命令。 四 hive1 hive中存放的是什么？表。 存的是和hdfs的映射关系，hive是逻辑上的数据仓库，实际操作的都是hdfs上的文件，HQL就是用SQL语法来写的MR程序。 2Hive与关系型数据库的关系？没有关系，hive是数据仓库，不能和数据库一样进行实时的CRUD操作。 是一次写入多次读取的操作，可以看成是ETL的工具。 3 请说明hive中Sort By、Order By、Cluster By，Distribute By各代表什么意思？order by：会对输入做全局排序，因此只有一个reducer（多个reducer无法保证全局有序）。只有一个reducer，会导致当输入规模较大时，需要较长的计算时间。 sort by：不是全局排序，其在数据进入reducer前完成排序。 distribute by：按照指定的字段对数据进行划分输出到不同的reduce中。 cluster by：除了具有 distribute by 的功能外还兼具 sort by 的功能。 4 habse与hive,kudu的区别Hbase： Hadoop database 的简称，也就是基于Hadoop数据库，是一种NoSQL数据库，主要适用于海量明细数据（十亿、百亿）的随机实时查询，如日志明细、交易清单、轨迹行为等。 Hive：Hive是Hadoop数据仓库，严格来说，不是数据库，主要是让开发人员能够通过SQL来计算和处理HDFS上的结构化数据，适用于离线的批量数据计算。 通过元数据来描述Hdfs上的结构化文本数据，通俗点来说，就是定义一张表来描述HDFS上的结构化文本，包括各列数据名称，数据类型是什么等，方便我们处理数据，当前很多SQL ON Hadoop的计算引擎均用的是hive的元数据，如Spark SQL、Impala等； 基于第一点，通过SQL来处理和计算HDFS的数据，Hive会将SQL翻译为Mapreduce来处理数据； 5 hive中的压缩格式RCFile.TextFile.SequenceFile各有什么区别，以上三种格式一样大的文件哪个占用空间大小textfile(默认) 存储空间消耗比较大，并且压缩的text 无法分割和合并 查询的效率最低,可以直接存储，加载数据的速度最高sequencefile 存储空间消耗最大,压缩的文件可以分割和合并 查询效率高，需要通过text文件转化来加载rcfile 存储空间最小，查询的效率最高 ，需要通过text文件转化来加载，加载的速度最低 6 hive底层与数据库交互原理Hive和Hbase有各自不同的特征：hive是高延迟、结构化和面向分析的，hbase是低延迟、非结构化和面向编程的。Hive数据仓库在hadoop上是高延迟的。Hive集成Hbase就是为了使用hbase的一些特性。如下是hive和hbase的集成架构： Hive集成HBase可以有效利用HBase数据库的存储特性，如行更新和列索引等。在集成的过程中注意维持HBase jar包的一致性。Hive集成HBase需要在Hive表和HBase表之间建立映射关系，也就是Hive表的列(columns)和列类型(column types)与HBase表的列族(column families)及列限定词(column qualifiers)建立关联。每一个在Hive表中的域都存在于HBase中，而在Hive表中不需要包含所有HBase中的列。HBase中的RowKey对应到Hive中为选择一个域使用:key来对应，列族(cf:)映射到Hive中的其它所有域，列为(cf:cq)。 7 hive的内外部表的区别1、在导入数据到外部表，数据并没有移动到自己的数据仓库目录下，也就是说外部表中的数据并不是由它自己来管理的！而 表则不一样；2、在删除表的时候，Hive将会把属于表的元数据和数据全部删掉；而删除外部表的时候，Hive仅仅删除外部表的元数据，数 据 是不会删除的！那么，应该如何选择使用哪种表呢？在大多数情况没有太多的区别，因此选择只是个人喜好的问题。但是作为一个经验，如 果所有处理都需要由Hive完成，那么你应该创建表，否则使用外部表！ 五 hbase1Hbase行键列族的概念，物理模型，表的设计原则？行键：是hbase表自带的，每个行键对应一条数据。 列族：是创建表时指定的，为列的集合，每个列族作为一个文件单独存储，存储的数据都是字节数组，其中数据可以有很多，通过时间戳来区分。 物理模型：整个hbase表会拆分成多个region，每个region记录着行键的起始点保存在不同的节点上，查询时就是对各个节点的并行查询，当region很大时使用.META表存储各个region的起始点，-ROOT又可以存储.META的起始点。 Rowkey的设计原则：各个列族数据平衡，长度原则、相邻原则，创建表的时候设置表放入regionserver缓存中，避免自动增长和时间，使用字节数组代替string，最大长度64kb，最好16字节以内，按天分表，两个字节散列，四个字节存储时分毫秒。 列族的设计原则：尽可能少(按照列族进行存储，按照region进行读取，不必要的io操作)，经常和不经常使用的两类数据放入不同列族中，列族名字尽可能短。 2HBase简单读写流程？读： 找到要读数据的region所在的RegionServer，然后按照以下顺序进行读取：先去BlockCache读取，若BlockCache没有，则到Memstore读取，若Memstore中没有，则到HFile中去读。 写： 找到要写数据的region所在的RegionServer，然后先将数据写到WAL(Write-Ahead Logging，预写日志系统)中，然后再将数据写到Memstore等待刷新，回复客户端写入完成。 3HBase的特点是什么？(1)hbase是一个分布式的基于列式存储的数据库，基于hadoop的HDFS存储，zookeeper进行管理。 (2)hbase适合存储半结构化或非结构化数据，对于数据结构字段不够确定或者杂乱无章很难按一个概念去抽取的数据。 (3)hbase为null的记录不会被存储。 (4)基于的表包括rowkey，时间戳和列族。新写入数据时，时间戳更新，同时可以查询到以前的版本。 (5)hbase是主从结构。Hmaster作为主节点，hregionserver作为从节点。 4 请描述如何解决Hbase中region太小和region太大带来的结果。Region**过大会发生多次compaction，将数据读一遍并重写一遍到hdfs 上，占用io，region过小会造成多次split，region 会下线，影响访问服务，调整hbase.hregion.max.filesize 为256m** 5 描述Hbase中scan和get的功能以及实现的异同1.**按指定RowKey 获取唯一一条记录， get 是以一个 row 来标记的.一个 row 中可以有很多 family 和 column.** 2.scan 方法实现条件查询功能使用的就是 scan 方式. 1)scan 批处理提高速度(以空间换时间)； 2)scan 可以通过 setStartRow 与 setEndRow 来限定范围,scan 可以通过 setFilter 方法添加过滤器，这也是分页、多条件查询的基础。 6 简述 HBASE中compact用途是什么，什么时候触发，分为哪两种,有什么区别，有哪些相关配置参数？在 hbase 中每当有 memstore 数据 flush 到磁盘之后，就形成一个 storefile，当 storeFile 的数量达到一定程度后，就需要将 storefile 文件来进行 compaction 操作。 Compact 的作用： 1&gt;.合并文件 2&gt;.清除过期，多余版本的数据 3&gt;.提高读写数据的效率 HBase 中实现了两种 compaction 的方式： minor and major. 这两种 compaction 方式的区别是： 1、 Minor 操作只用来做部分文件的合并操作以及包括 minVersion=0 并且设置 ttl 的过期版本清理，不做任何删除数据、多版本数据的清理工作。 2、 Major 操作是对 Region 下的 HStore 下的所有 StoreFile 执行合并操作，最终的结果是整理合并出一个文件。简述 Hbase filter 的实现原理是什么？结合实际项目经验，写出几个使用 filter 的场景HBase 为筛选数据提供了一组过滤器，通过这个过滤器可以在 HBase 中的数据的多个维度（行，列，数据版本）上进行对数据的筛选操作，也就是说过滤器最终能够筛选的数据能够细化到具体的一个存储单元格上（由行键，列名，时间戳定位）。 RowFilter、 PrefixFilter。。。hbase的filter是通过scan设置的，所以是基于scan的查询结果进行过滤.过滤器的类型很多，但是可以分为两大类——比较过滤器，专用过滤器过滤器的作用是在服务端判断数据是否满足条件，然后只将满足条件的数据返回给客户端；如在进行订单开发的时候，我们使用rowkeyfilter过滤出某个用户的所有订单 7 描述Hbase的rowKey的设计原则1：长度原则，最短越好，最大不能超过64K。太长的影响有两点，一是极大影响了HFile的存储效率。二是缓存memstore不能得到有效利用，缓存不能存放太多的信息，造成检索效率的降低。 2：唯一原则 保证rowkey的唯一性，这条没有什么要讲的。 3：自己一条原则 尽量保证经常一起用的rowkey存储在同一个region上，有助于提升检索效率。但要避免热点问题。 4：对于常用的检索的rowkey，尽量使用高表（行多列少），二部选择宽表（列多行少）。 8Hbase内部是什么机制在HBase 中无论是增加新行还是修改已有的行，其内部流程都是相同的。HBase 接到命令后存下变化信息，或者写入失败抛出异常。默认情况下，执行写入时会写到两个地方：预写式日志（write-ahead log，也称HLog）和MemStore（见图2-1）。HBase 的默认方式是把写入动作记录在这两个地方，以保证数据持久化。只有当这两个地方的变化信息都写入并确认后，才认为写动作完成。MemStore 是内存里的写入缓冲区，HBase 中数据在永久写入硬盘之前在这里累积。当MemStore 填满后，其中的数据会刷写到硬盘，生成一个HFile。HFile 是HBase 使用的底层存储格式。HFile 对应于列族，一个列族可以有多个HFile，但一个HFile 不能存储多个列族的数据。在集群的每个节点上，每个列族有一个MemStore。大型分布式系统中硬件故障很常见，HBase 也不例外。设想一下，如果MemStore还没有刷写，服务器就崩溃了，内存中没有写入硬盘的数据就会丢失。HBase 的应对办法是在写动作完成之前先写入WAL。HBase 集群中每台服务器维护一个WAL 来记录发生的变化。WAL 是底层文件系统上的一个文件。直到WAL 新记录成功写入后，写动作才被认为成功完成。这可以保证HBase 和支撑它的文件系统满足持久性。大多数情况下，HBase 使用Hadoop 分布式文件系统（HDFS）来作为底层文件系统。如果HBase 服务器宕机，没有从MemStore 里刷写到HFile 的数据将可以通过回放WAL 来恢复。你不需要手工执行。Hbase 的内部机制中有恢复流程部分来处理。每台HBase 服务器有一个WAL，这台服务器上的所有表（和它们的列族）共享这个WAL。你可能想到，写入时跳过WAL 应该会提升写性能。但我们不建议禁用WAL，除非你愿意在出问题时丢失数据。如果你想测试一下，如下代码可以禁用WAL： 注意：不写入WAL 会在RegionServer 故障时增加丢失数据的风险。关闭WAL，出现故障时HBase 可能无法恢复数据，没有刷写到硬盘的所有写入数据都会丢失。 9 介绍一下HBase过滤器HBase为筛选数据提供了一组过滤器，通过这个过滤器可以在HBase中的数据的多个维度（行，列，数据版本）上进行对数据的筛选操作，也就是说过滤器最终能够筛选的数据能够细化到具体的一个存储单元格上（由行键，列明，时间戳定位）。通常来说，通过行键，值来筛选数据的应用场景较多。 RowFilter：筛选出匹配的所有的行，对于这个过滤器的应用场景，是非常直观的：使用BinaryComparator可以筛选出具有某个行键的行，或者通过改变比较运算符（下面的例子中是CompareFilter.CompareOp.EQUAL）来筛选出符合某一条件的多条数据，以下就是筛选出行键为row1的一行数据： PrefixFilter：筛选出具有特定前缀的行键的数据。这个过滤器所实现的功能其实也可以由RowFilter结合RegexComparator来实现，不过这里提供了一种简便的使用方法，以下过滤器就是筛选出行键以row为前缀的所有的行： KeyOnlyFilter：这个过滤器唯一的功能就是只返回每行的行键，值全部为空，这对于只关注于行键的应用场景来说非常合适，这样忽略掉其值就可以减少传递到客户端的数据量，能起到一定的优化作用： RandomRowFilter：从名字上就可以看出其大概的用法，本过滤器的作用就是按照一定的几率（&lt;=0会过滤掉所有的行，&gt;=1会包含所有的行）来返回随机的结果集，对于同样的数据集，多次使用同一个RandomRowFilter会返回不通的结果集，对于需要随机抽取一部分数据的应用场景，可以使用此过滤器 10hbase 宕机如何处理问题分析: Hbase的Regionserver进程随机挂掉（该异常几乎每次都发生，只是挂掉的Regionserver节点不同） HMaster进程随机挂掉 主备Namenode节点随机挂掉 Zookeeper节点随机挂掉 Zookeeper连接超时 JVM GC睡眠时间过长 datanode写入超时 解决问题: 问题解决需从以下几个方面着手： 1、Hbase的ZK连接超时相关参数调优：默认的ZK超时设置太短，一旦发生FULL GC，极其容易导致ZK连接超时； 2、Hbase的JVM GC相关参数调优：可以通过GC调优获得更好的GC性能，减少单次GC的时间和FULL GC频率； 3、ZK Server调优：这里指的是ZK的服务端调优，ZK客户端（比如Hbase的客户端）的ZK超时参数必须在服务端超时参数的范围内，否则ZK客户端设置的超时参数起不到 效果； 4、HDFS读写数据相关参数需调优； 5、YARN针对各个节点分配资源参数调整：YARN需根据真实节点配置分配资源，之前的YARN配置为每个节点分配的资源都远大于真实虚拟机的硬件资源； 6、集群规划需优化：NameNode、NodeManager、DataNode，RegionServer会混用同一个节点，这样会导致这些关键的枢纽节点通信和内存压力过大，从而在计算压力 较大时容易发生异常。正确的做法是将枢纽节点（NameNode，ResourceManager，HMaster）和数据+计算节点分开 六 kafka1.Kafka 与传统消息系统之间有三个关键区别(1).Kafka 持久化日志，这些日志可以被重复读取和无限期保留 (2).Kafka 是一个分布式系统：它以集群的方式运行，可以灵活伸缩，在内部通过复制数据提升容错能力和高可用性 (3).Kafka 支持实时的流式处理 2.Kafka的消费者如何消费数据创建 KafkaConsumer 对象订阅指定主题topic的数据，groupid指定消费群组，使用轮询，指定key,指定offset等方式进行消费 3. kafka生产数据时数据的分组策略第一种：给定了分区号，直接将数据发送到指定的分区里面去 第二种：没有给定分区号，给定数据的key值，通过key取上hashCode进行分区 第三种：既没有给定分区号，也没有给定key值，直接轮循进行分区 第四种：自定义分区 4 在数据制作过程中，你如何能从Kafka得到准确的信息?在数据中，为了精确地获得Kafka的消息，你必须遵循两件事: 在数据消耗期间避免重复，在数据生产过程中避免重复。 这里有两种方法，可以在数据生成时准确地获得一个语义: 每个分区使用一个单独的写入器，每当你发现一个网络错误，检查该分区中的最后一条消息，以查看您的最后一次写入是否成功 在消息中包含一个主键(UUID或其他)，并在用户中进行反复制 5 kafka集群的规模，消费速度是多少。一般中小型公司是10个节点，每秒20M左右。 七 Spark1 spark streming在实时处理时会发生什么故障，如何停止，解决和Kafka整合时消息无序： 修改Kafka的ack参数，当ack=1时，master确认收到消息就算投递成功。ack=0时，不需要收到消息便算成功，高效不准确。sck=all，master和server都要受到消息才算成功，准确不高效。 StreamingContext.stop会把关联的SparkContext对象也停止，如果不想把SparkContext对象也停止的话可以把StremingContext.stop的可选参数stopSparkContext设为flase。一个SparkContext对象可以和多个streamingcontext对象关联。只要对前一个stremingcontext.stop(stopsparkcontext=false),然后再创建新的stremingcontext对象就可以了。 八 sqoop1 Sgoop导入数据到mysql中 如何让数据不重复导入？ 存在数据问题sgoop会怎么样额外:1请列出正常的hadoop集群中hadoop都分别需要启动 哪些进程，他们的作用分别都是什么，请尽量列的详细一些。namenode：负责管理hdfs中文件块的元数据，响应客户端请求，管理datanode上文件block的均衡，维持副本数量 Secondname:主要负责做checkpoint操作；也可以做冷备，对一定范围内数据做快照性备份。 Datanode:存储数据块，负责客户端对数据块的io请求 Jobtracker :管理任务，并将任务分配给 tasktracker。 Tasktracker: 执行JobTracker分配的任务。 Resourcemanager、Nodemanager、Journalnode、Zookeeper、Zkfc 2mysql，mongodb，rides的端口。面试数据库介绍的再好，不知到默认端口，也证明你没有经验。mysql：3306，mongdb：27017，rides：6379。 3 数据来源1.webServer ：用户访问我们的网站，对日志进行收集，记录在反向的日志文件里 tomcat下logs 2, js代码嵌入前端页面（埋点）：js的sdk会获取用户行为，document会得到元素调用function，通过ngix集群进行日志收集。 4 聊项目搞清楚，数据的来源，数据的收集，数据的分析，数据的储存，数据的展示。 主要解决了啥业务。遇到了啥问题，数据的格式，有哪些优化，等等等 5 .列出你所知道的调度器 说明其工作方法a) Fifo schedular 默认的调度器 先进先出b) Capacity schedular 计算能力调度器 选择占用内存小 优先级高的c) Fair schedular 调肚脐 公平调度器 所有job 占用相同资源 6 现有 1 亿个整数均匀分布，如果要得到前 1K 个最大的数，求最优的算法。自己的思路: :分块，比如分 1W 块，每块 1W 个，然后分别找出每块最大值，从这最大的 1W 个值中找最大 1K 个，那么其他的 9K 个最大值所在的块即可扔掉，从剩下的最大的 1K 个值所在的块中找前 1K个即可。那么原问题的规模就缩小到了 1/10。问题：（1）这种分块方法的最优时间复杂度。（2）如何分块达到最优。比如也可分 10W 块，每块 1000 个数。则问题规模可降到原来1/100。但事实上复杂度并没降低。 7 毒酒问题－－－1000桶酒，其中1桶有毒，而一旦吃了，毒性会在一周后发作。问最少需要多少只老鼠可在一周内找出毒酒? 1 2 4 8 16 32 64 128 256 512 10只 8 4亿个数字，找出哪些是重复的，要用最小的比较次数，写程序实现9 算法题：有 2 个桶，容量分别为 3 升和 5 升，如何得到 4 升的水，假设水无限使用，写出步骤。 先把五升的水桶倒满, 然后倒进三升的水桶 ,再把三升的水桶的水倒掉 ,把五升的水桶里剩余的2升导入三升的水桶,再把五升的水桶倒满 在倒进有两升的三升桶种 则 剩余4升 海量数据处理面试题:1海量日志数据，提取出某日访问百度次数最多的那个 IP。首先是这一天，并且是访问百度的日志中的 IP 取出来，逐个写入到一个大文件中。注意到IP 是 32 位的，最多有个 2^32 个 IP。同样可以采用映射的方法， 比如模 1000，把整个大文件映射为 1000 个小文件，再找出每个小文中出现频率最大的 IP（可以采用 hash_map进行频率统计，然后再找出频率最大 的几个）及相应的频率。然后再在这 1000 个最大的IP 中，找出那个频率最大的 IP，即为所求。或者如下阐述（雪域之鹰）：算法思想：分而治之+Hash（1）.IP 地址最多有 2^32=4G 种取值情况，所以不能完全加载到内存中处理；（2）.可以考虑采用“分而治之”的思想，按照 IP 地址的 Hash(IP)%1024 值，把海量 IP日志分别存储到 1024 个小文件中。这样，每个小文件最多包含 4MB 个 IP 地址；（3）.对于每一个小文件，可以构建一个 IP 为 key，出现次数为 value 的 Hash map，同时记录当前出现次数最多的那个 IP 地址；（4）.可以得到 1024 个小文件中的出现次数最多的 IP，再依据常规的排序算法得到总体上出现次数最多的 IP； 2 搜索引擎会通过日志文件把用户每次检索使用的所有检索串都记录下来，每个查询串的长度为 1-255 字节。 假设目前有一千万个记录（这些查询串的重复度比较高，虽然总数是 1 千万，但如果除去重复后，不超过 3 百万个。一个查询串的重复度越高，说明查询它的用户越多，也就是越热门。），请你统计最热门的 10 个查询串，要求使用的内存不能超过 1G。 典型的 Top K 算法，还是在这篇文章里头有所阐述，详情请参见：十一、从头到尾彻底解析 Hash 表算法。文中，给出的最终算法是：第一步、先对这批海量数据预处理，在 O（N）的时间内用 Hash 表完成统计（之前写成了排序，特此订正。July、2011.04.27）； 第二步、借助堆这个数据结构，找出 Top K，时间复杂度为 N‘logK。即，借助堆结构，我们可以在 log 量级的时间内查找和调整/移动。因此，维护一个 K(该题目中是 10)大小的小根堆，然后遍历 300 万的 Query，分别 和根元素进行对比所以，我们最终的时间复杂度是：O（N） + N’乘以O（logK），（N 为 1000 万，N’为 300 万）。ok，更多，详情，请参考原文。或者：采用 trie 树，关键字域存该查询串出现的次数，没有出现为 0。最后用 10 个元素的最小推来对出现频率进行排序。 3有一个 1G 大小的一个文件，里面每一行是一个词，词的大小不超过 16 字节，内存限制大小是 1M。返回频数最高的 100 个词。 方案：顺序读文件中，对于每个词 x，取 hash(x)%5000，然后按照该值存到 5000 个小文件（记为 x0,x1,…x4999）中。这样每个文件大概是 200k 左右。如果其中的有的文件超过了 1M 大小，还可以按照类似的方法继续往下分，直到分解得到的小文件的大小都不超过 1M。对每个小文件，统计每个文件中出现的词以及相应的频率（可以采用 trie 树/hash_map 等），并取出出现频率最大的 100 个词（可以用含 100 个结 点的最小堆），并把 100 个词及相应的频率存入文件，这样又得到了 5000 个文件。下一步就是把这 5000 个文件进行归并（类似与归并排序）的过程了。 4 有 10 个文件，每个文件 1G，每个文件的每一行存放的都是用户的 query，每个文件的query 都可能重复。要求你按照 query 的频度排序。 还是典型的 TOP K 算法，解决方案如下： 方案 1：顺序读取 10 个文件，按照 hash(query)%10 的结果将 query 写入到另外 10 个文件（记为）中。这样新生成的文件每个的大小大约也 1G（假设 hash 函数是随机的）。找一台内存在 2G 左右的机器，依次对用 hash_map(query, query_count)来统计每个query 出现的次数。利用快速/堆/归并排序按照出现次数进行排序。将排序好的 query 和对应的 query_cout 输出到文件中。这样得到了 10 个排好序的文件（记为）。对这 10 个文件进行归并排序（内排序与外排序相结合）。方案 2：一般 query 的总量是有限的，只是重复的次数比较多而已，可能对于所有的 query，一次性就可以加入到内存了。这样，我们就可以采用 trie 树/hash_map等直接来统计每个 query出现的次数，然后按出现次数做快速/堆/归并排序就可以了。方案 3：与方案 1 类似，但在做完 hash，分成多个文件后，可以交给多个文件来处理，采用分布式的架构来处理（比如 MapReduce），最后再进行合并。 5 给定 a、b 两个文件，各存放 50 亿个 url，每个 url 各占 64 字节，内存限制是 4G，让你找出 a、b 文件共同的 url？ 方案 1：可以估计每个文件安的大小为 5G×64=320G，远远大于内存限制的 4G。所以不可能将其完全加载到内存中处理。考虑采取分而治之的方法。遍历文件 a，对每个 url 求取 hash(url)%1000，然后根据所取得的值将 url 分别存储到 1000个小文件（记为 a0,a1,…,a999）中。这样每个小文件的大约为 300M。遍历文件 b，采取和 a 相同的方式将 url 分别存储到 1000 小文件（记为 b0,b1,…,b999）。这样处理后，所有可能相同的 url 都在对应的小 文件（a0vsb0,a1vsb1,…,a999vsb999）中，不对应的小文件不可能有相同的 url。然后我们只要求出 1000 对小文件中相同的 url即可。求每对小文件中相同的 url 时，可以把其中一个小文件的 url 存储到 hash_set 中。然后遍历另一个小文件的每个 url，看其是否在刚才构建的 hash_set 中，如果是，那么就是共同的 url，存到文件里面就可以了。方案 2：如果允许有一定的错误率，可以使用 Bloom filter，4G 内存大概可以表示 340 亿bit。将其中一个文件中的 url 使用 Bloom filter 映射为这 340 亿 bit，然后挨个读取另外一个文件的 url，检查是否与 Bloom filter，如果是，那么该 url 应该是共同的 url（注意会有一定的错误率）。 6 在 2.5 亿个整数中找出不重复的整数，注，内存不足以容纳这 2.5 亿个整数。方案 1：采用 2-Bitmap（每个数分配 2bit，00 表示不存在，01 表示出现一次，10 表示多次，11 无意义）进行，共需内存 2^32 * 2 bit=1 GB 内存，还可以接受。然后扫描这 2.5亿个整数，查看 Bitmap 中相对应位，如果是 00 变 01，01 变 10，10 保持不变。所描完事后，查看 bitmap，把对应位是 01 的整数输出即可。方案 2：也可采用与第 1 题类似的方法，进行划分小文件的方法。然后在小文件中找出不重复的整数，并排序。然后再进行归并，注意去除重复的元素。 7 腾讯面试题：给 40 亿个不重复的 unsigned int 的整数，没排过序的，然后再给一个数，如何快速判断这个数是否在那 40 亿个数当中？ 与上第 6 题类似，我的第一反应时快速排序+二分查找。以下是其它更好的方法：方案 1：oo，申请 512M 的内存，一个 bit 位代表一个 unsigned int 值。读入 40 亿个数，设置相应的 bit 位，读入要查询的数，查看相应 bit 位是否为 1，为 1 表示存在，为 0 表示不存在。dizengrong：方案 2：这个问题在《编程珠玑》里有很好的描述，大家可以参考下面的思路，探讨一下：又因为 2^32 为 40 亿多，所以给定一个数可能在，也可能不在其中；这里我们把 40 亿个数中的每一个用 32 位的二进制来表示假设这 40 亿个数开始放在一个文件中。然后将这 40 亿个数分成两类:1.最高位为 02.最高位为 1并将这两类分别写入到两个文件中，其中一个文件中数的个数&lt;=20 亿，而另一个&gt;=20 亿（这相当于折半了）；与要查找的数的最高位比较并接着进入相应的文件再查找再然后把这个文件为又分成两类:1.次最高位为 02.次最高位为 1并将这两类分别写入到两个文件中，其中一个文件中数的个数&lt;=10 亿，而另一个&gt;=10 亿（这相当于折半了）；与要查找的数的次最高位比较并接着进入相应的文件再查找。……. 以此类推，就可以找到了,而且时间复杂度为 O(logn)，方案 2 完。附：这里，再简单介绍下，位图方法：使用位图法判断整形数组是否存在重复判断集合中存在重复是常见编程任务之一，当集合中数据量比较大时我们通常希望少进行几次扫描，这时双重循环法就不可取了。位图法比较适合于这种情况，它的做法是按照集合中最大元素 max 创建一个长度为 max+1的新数组，然后再次扫描原数组，遇到几就给新数组的第几位置上 1，如遇到 5 就给新数组的第六个元素置 1，这样下次再遇到 5 想置位时发现新数组的第六个元素已经是 1 了，这说明这次的数据肯定和以前的数据存在着重复。这 种给新数组初始化时置零其后置一的做法类似于位图的处理方法故称位图法。它的运算次数最坏的情况为 2N。如果已知数组的最大值即能事先给新数组定长的话效 率还能提高一倍。欢迎，有更好的思路，或方法，共同交流。 8 怎么在海量数据中找出重复次数最多的一个？方案 1：先做 hash，然后求模映射为小文件，求出每个小文件中重复次数最多的一个，并记录重复次数。然后找出上一步求出的数据中重复次数最多的一个就是所求（具体参考前面的题）。 9 上千万或上亿数据（有重复），统计其中出现次数最多的钱 N 个数据。方案 1：上千万或上亿的数据，现在的机器的内存应该能存下。所以考虑采用 hash_map/搜索二叉树/红黑树等来进行统计次数。然后就是取出前 N 个出现次数最多的数据了，可以用第 2 题提到的堆机制完成。 10一个文本文件，大约有一万行，每行一个词，要求统计出其中最频繁出现的前 10 个词，请给出思想，给出时间复杂度分析 方案 1：这题是考虑时间效率。用 trie 树统计每个词出现的次数，时间复杂度是 O(nle)（le表示单词的平准长度）。然后是找出出现最频繁的前 10 个词，可以用堆来实现，前面的题中已经讲到了，时间复杂度是 O(nlg10)。所以总的时间复杂度，是 O(nle)与 O(nlg10)中较大的哪一 个。 附、100w 个数中找出最大的 100 个数。方案 1：在前面的题中，我们已经提到了，用一个含 100 个元素的最小堆完成。复杂度为O(100wlg100)。方案 2：采用快速排序的思想，每次分割之后只考虑比轴大的一部分，知道比轴大的一部分在比 100 多的时候，采用传统排序算法排序，取前 100 个。复杂度为 O(100w100)。方案 3：采用局部淘汰法。选取前 100 个元素，并排序，记为序列 L。然后一次扫描剩余的元素 x，与排好序的 100 个元素中最小的元素比，如果比这个最小的 要大，那么把这个最小的元素删除，并把 x 利用插入排序的思想，插入到序列 L 中。依次循环，知道扫描了所有的元素。复杂度为 O(100w乘以100)。 11 海量数据处理方法总结]]></content>
  </entry>
</search>
