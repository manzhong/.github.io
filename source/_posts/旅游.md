---
title: Base
encrypt: true
enc_pwd: 789456
abbrlink: 21209
date: 2017-03-01 22:19:14
tags:
categories:
summary_img:
---

# 一  zookeeper

## 1  ZooKeeper的角色是什么？

```
 Leader 角色 
        Leader 服务器是整个zookeeper 集群的核心，主要的工作任务有两项： 
            ①事物请求的唯一调度和处理者，保证集群事物处理的顺序性。 
            ②集群内部各服务器的调度者。 
 Follower 角色 
    Follower 角色的主要职责是： 
        ①处理客户端非事物请求、转发事物请求给leader服务器。 
        ②参与事物请求Proposal的投票（Leader发起的提案，要求 Follower投票，需要半数以上follower节点通过，      leader才会commit数据）。
        ③参与Leader选举的投票。

Observer 角色 
        ①Observer 是 zookeeper3.3 开始引入的一个全新的服务器角色，从字面来理解，该角色充当了观察者的角色。观察 zookeeper 集群中的最新状态变化并将这些状态变化同步到 observer 服务器上。Observer 的工作原理与 follower 角色基本一致，而它和 follower 角色唯一的不同在于 observer 不参与任何形式的投票，包括事务请求Proposal的投票和leader选举的投票。简单来说，observer服务器只提供非事物请求服务，通常在于不影响集群事务处理能力的前提下提升集群非事物处理的能力
```

## 2 zk的用途和选举原理

**用途**

```
1分布式锁
2服务注册和发现
利用Znode和Watcher，可以实现分布式服务的注册和发现。最著名的应用就是阿里的分布式RPC框架Dubbo。
3共享配置和状态信息
Redis的分布式解决方案Codis（豌豆荚），就利用了Zookeeper来存放数据路由表和 codis-proxy 节点的元信息。同时 codis-config 发起的命令都会通过 ZooKeeper 同步到各个存活的 codis-proxy。
4软负载均衡
```

**选举**

```
1每个 server 发出一个投票： 投票的最基本元素是（SID-服务器id,ZXID-事物id）
2接受来自各个服务器的投票
3处理投票：优先检查 ZXID(数据越新ZXID越大),ZXID比较大的作为leader，ZXID一样的情况下比较SID
4统计投票：这里有个过半的概念，大于集群机器数量的一半，即大于或等于（n/2+1）,我们这里的由三台，大于等于2即为达到“过半”的要求。这里也有引申到为什么 Zookeeper 集群推荐是单数。
```

## 3  zk的watch机制



## 4 分布式锁

## 5 zookeeper 搭建

首先我们要在每台pc上配置zookeeper环境变量，在cd到zookeeper下的conf文件夹下在zoo_simjle.cfg文件中添加datadir路径，再到zookeeper下新建data文件夹，创建myid，在文件里添加上server的ip地址。在启动zkserver.sh start便ok了。



# 二 Hadoop常见面试题   hdfs  mr   yarn

## 1 hdfs的写文件

## 2 hdfs读文件

## 3 Hadoop的shuffle过程

1.Map端的shuffle
Map端会处理输入数据并产生中间结果，这个中间结果会写到本地磁盘，而不是HDFS。每个Map的输出会先写到内存缓冲区中，当写入的数据达到设定的阈值时，系统将会启动一个线程将缓冲区的数据写到磁盘，这个过程叫做spill。
　　在spill写入之前，会先进行二次排序，首先根据数据所属的partition进行排序，然后每个partition中的数据再按key来排序。partition的目是将记录划分到不同的Reducer上去，以期望能够达到负载均衡，以后的Reducer就会根据partition来读取自己对应的数据。接着运行combiner(如果设置了的话)，combiner的本质也是一个Reducer，其目的是对将要写入到磁盘上的文件先进行一次处理，这样，写入到磁盘的数据量就会减少。最后将数据写到本地磁盘产生spill文件(spill文件保存在{mapred.local.dir}指定的目录中，Map任务结束后就会被删除)。

　　最后，每个Map任务可能产生多个spill文件，在每个Map任务完成前，会通过多路归并算法将这些spill文件归并成一个文件。至此，Map的shuffle过程就结束了。

2.Reduce端的shuffle

Reduce端的shuffle主要包括三个阶段，copy、sort(merge)和reduce。
　　首先要将Map端产生的输出文件拷贝到Reduce端，但每个Reducer如何知道自己应该处理哪些数据呢？因为Map端进行partition的时候，实际上就相当于指定了每个Reducer要处理的数据(partition就对应了Reducer)，所以Reducer在拷贝数据的时候只需拷贝与自己对应的partition中的数据即可。每个Reducer会处理一个或者多个partition，但需要先将自己对应的partition中的数据从每个Map的输出结果中拷贝过来。
　　接下来就是sort阶段，也成为merge阶段，因为这个阶段的主要工作是执行了归并排序。从Map端拷贝到Reduce端的数据都是有序的，所以很适合归并排序。最终在Reduce端生成一个较大的文件作为Reduce的输入。

　　最后就是Reduce过程了，在这个过程中产生了最终的输出结果，并将其写到HDFS上。

## 4 fsimage和edit的区别

当NN,SN要进行数据同步时叫做checkpoint时就用到了fsimage与edit，fsimage是保存最新的元数据的信息，当fsimage数据到一定的大小事会去生成一个新的文件来保存元数据的信息，这个新的文件就是edit，edit会回滚最新的数据。

## 5 简述Hadoop的MapReduce模型

首先map task会从本地文件系统读取数据，转换成key-value形式的键值对集合，使用的是hadoop内置的数据类型，如Text，Longwritable等。

将键值对集合输入mapper进行业务处理过程，将其转化成需要的key-value再输出。

之后会进行一个partition分区操作，默认使用的是hashpartitioner，可以通过重写hashpartitioner的getPartition方法来自定义分区规则。

之后会对key进行sort排序，grouping分组操作将相同key的value合并分组输出，在这里可以使用自定义的数据类型，重写WritableComparator的Comparator方法来自定义排序规则，重写RawComparator的compara方法来自定义分组规则。

之后进行一个combiner归约操作，就是一个本地的reduce预处理，以减小shuffle，reducer的工作量。

Reduce task会用过网络将各个数据收集进行reduce处理，最后将数据保存或者显示，结束整个job。

## 6 Hadoop需要哪些守护进程

## 7 Hadoop的textinputformat作用   如何自定义

InputFormat会在map操作之前对数据进行两方面的预处理。

1.是getSplits，返回的是InputSplit数组，对数据进行Split分片，每片交给map操作一次。

2.是getRecordReader，返回的是RecordReader对象，对每个Split分片进行转换为key-value键值对格式传递给map常用的InputFormat是TextInputFormat，使用的是LineRecordReader对每个分片进行键值对的转换，以行偏移量作为键，行内容作为值。

自定义类继承InputFormat接口，重写createRecordReader和isSplitable方法在createRecordReader中可以自定义分隔符。

## 8 Hadoop与spark都是并行计算,他们的相同和区别

同:

1两者都使用mr模型来进行并行计算

不同:

1  hadoop的一个作业称为job，job里面分为map task和reduce task，每个task都是在自己的进程中运行的，当task结束时，进程也会结束。

Spark用户提交的任务称为application，一个application对应一个SparkContext，app中存在多个job，没触发一个action操作就会产生一个job。

2这些job可以并行或者串行执行，每个job有多个stage，stage是shuffle过程中DAGSchaduler通过RDD之间的依赖关系划分job而来的，每个stage里面有多个task，组成taskset有TaskSchaduler分发到各个executor中执行，executor的生命周期是和application一样的，即使没有job运行也是存在的，所以task可以快速启动读取内存进行计算的。

Hadoop的job只有map和reduce操作，表达能力比较欠缺而且在mr过程中会重复的读写hdfs，造成大量的io操作，多个job需要自己管理关系。

Spark的迭代计算都是在内存中进行的，API中提供了大量的RDD操作join，groupby等，而且通过DAG图可以实现良好的容错。

## 9 为什么使用flume导入hdfs , hdfs的架构是怎样的

Flume可以实时的导入数据到hdfs中，当hdfs上的文件达到一个指定大小的时候会形成一个文件，或者超时所指定时间的话也形成一个文件。

文件都是存储在datanode上的，namenode存储着datanode的元数据信息，而namenode的元数据信息是存在内存中的，所以当文件切片很小或者很多的时候会卡死。

## 10MR程序运行的时候会有什么比较常见的问题？

比如说作业中大部分都完成了，但是总有几个reduce一直在运行。

这是因为这几个reduce中的处理的数据要远远大于其他的reduce，可能是对键值对任务划分的不均匀造成的数据倾斜。

解决的方法可以在分区的时候重新定义分区规则对于value数据很多的key可以进行拆分、均匀打散等处理，或者是在map端的combiner中进行数据预处理的操作。

## 11 简述Hadoop和spark的shuffle过程

Hadoop：map端保存分片数据，通过网络收集到reduce端。

Spark：spark的shuffle实在DAGSchedular划分Stage的时候产生的，TaskSchedular要分发Stage到各个worker的executor。减少shuffle可以提高性能。

## 12 yarn的理解：

YARN是Hadoop2.0版本引进的资源管理系统，直接从MR1演化而来。 
核心思想：将MR1中的JobTracker的资源管理和作业调度两个功能分开，分别由ResourceManager和ApplicationMaster进程实现。

ResourceManager：负责整个集群的资源管理和调度 ApplicationMaster：负责应用程序相关事务，比如任务调度、任务监控和容错等。 YARN的出现，使得多个计算框架可以运行在同一个集群之中。 1. 每一个应用程序对应一个ApplicationMaster。 2. 目前可以支持多种计算框架运行在YARN上面，比如MapReduce、storm、Spark、Flink。

## 13 HDFS 的默认block块的大小原为64 m后改为128m有何影响

block块为一个逻辑概念   设计更大 是为了

1  减少寻道时间  寻道也是一个逻辑概念,这里的寻道时间是定位到块的时间,hdfs是设计存储大数据的,如果块设计很小,一个文件就会有很多块组成,hdfs上的最小单位是块,这样,寻找块的时间就会大大增加,降低读写效率

2 减少任务数  一个map和reduce 都是以块为单位处理,如果块很小,MapReduce的任务数会非常多,任务之间的切换开销变大,效率变低 ,同样 如果块很大,又会单个任务就会很慢

3 减少元数据  ，在hdfs中，文件块的信息都是元数据，集群运行过程中元数据会都加载进namenode的内存中，如果块很小的情况下，元数据信息就会很多，namenode压力就会很大。实际上当集群规模超过4000台的时候，namenode内存再大都已经没有什么用，因为此时文件数已经太多了。

4 减少网络开销  如果数据块很小，一个文件要分成很多块，而每个文件都有副本，当文件删除或者拷贝时，就会导致大量块移动，寻道开销和网络开销都会很大。

5优化  当集群规模增大的时候，可以适当增加块的大小，比如集群规模大到一定程度可以将块增大到256M，降低相应开销。

可以修改block的大小

## 14 namenode 与Secondarynamenode 的区别

## 15 hdfs得block默认保存几分  

3份

## 16 hdfs的启动过程

HDFS的启动过程分为四个阶段：
第一阶段：NameNode 读取包含元数据信息的fsimage文件，并加载到内存；
第二阶段：NameNode读取体现HDFS最新状态的edits日志文件，并加载到内存中
第三阶段：生成检查点，SecondaryNameNode将edits日志中的信息合并到fsimage文件中
第四阶段：进入安全模式，检查数据块的完整性

## 17 hdfs的　client端，复制到第三个副本时宕机，hdfs怎么恢复下次写第三副本？block块信息是先写
dataNode还是先写nameNode？

Datanode会定时上报block块的信息给namenode ，namenode就会得知副本缺失，然后namenode就会启动副本复制流程以保证数据块的备份！Client向NameNode发起文件写入的请求。NameNode根据文件大小和文件块配置情况，返回给Client它所管理部分DataNode的信息。Client将文件划分为多个Block，根据DataNode的地址信息，按顺序写入到每一个DataNode块中。

## 18hdfs，replica如何定位

1st replica.如果写请求方所在机器是其中一个datanode,则直接存放在本地,否则随机在集群中选择一个datanode. 

2nd replica.第二个副本存放于不同第一个副本的所在的机架. 

3rd replica.第三个副本存放于第二个副本所在的机架,但是属于不同的节点.

## 19 mr处理数据倾斜

原因:

map /reduce程序执行时，reduce节点大部分执行完毕，但是有一个或者几个reduce节点运行很慢，导致整个程序的处理时间很长，这是因为某一个key的条数比其他key多很多（有时是百倍或者千倍之多），这条key所在的reduce节点所处理的数据量比其他节点就大很多，从而导致某几个节点迟迟运行不完，此称之为数据倾斜。

解决:

(1)设置一个hash份数N，用来对条数众多的key进行打散。

(2)对有多条重复key的那份数据进行处理：从1到N将数字加在key后面作为新key，如果需要和另一份数据关联的话，则要重写比较类和分发类（方法如上篇《hadoop job解决大数据量关联的一种方法》）。如此实现多条key的平均分发。

int iNum = iNum % iHashNum;

String strKey = key + CTRLC + String.valueOf(iNum) + CTRLB + “B”;



（3）上一步之后，key被平均分散到很多不同的reduce节点。如果需要和其他数据关联，为了保证每个reduce节点上都有关联的key，对另一份单一key的数据进行处理：循环的从1到N将数字加在key后面作为新key

for(int i = 0; i < iHashNum; ++i){

String strKey =key + CTRLC + String.valueOf(i) ;

output.collect(new Text(strKey), new Text(strValues));}

以此解决数据倾斜的问题，经试验大大减少了程序的运行时间。但此方法会成倍的增加其中一份数据的数据量，以增加shuffle数据量为代价，所以使用此方法时，要多次试验，取一个最佳的hash份数值。

======================================

用上述的方法虽然可以解决数据倾斜，但是当关联的数据量巨大时，如果成倍的增长某份数据，会导致reduce shuffle的数据量变的巨大，得不偿失，从而无法解决运行时间慢的问题。

有一个新的办法可以解决 成倍增长数据 的缺陷：

在两份数据中找共同点，比如两份数据里除了关联的字段以外，还有另外相同含义的字段，如果这个字段在所有log中的重复率比较小，则可以用这个字段作为计算hash的值，如果是数字，可以用来模hash的份数，如果是字符可以用hashcode来模hash的份数（当然数字为了避免落到同一个reduce上的数据过多，也可以用hashcode），这样如果这个字段的值分布足够平均的话，就可以解决上述的问题。

## 20hdfs的数据压缩算法

1 在HDFS之上将数据压缩好后，再存储到HDFS
2、在HDFS内部支持数据压缩，这里又可以分为几种方法：
    2.1、压缩工作在DataNode上完成，这里又分两种方法：
           2.1.1、数据接收完后，再压缩
                     这个方法对HDFS的改动最小，但效果最低，只需要在block文件close后，调用压缩工具，将block文件压缩一下，然后再打开block文件时解压一下即可，几行代码就可以搞定
           2.1.2、边接收数据边压缩，使用第三方提供的压缩库
                     效率和复杂度折中方法，Hook住系统的write和read操作，在数据写入磁盘之前，先压缩一下，但write和read对外的接口行为不变，比如：原始大小为100KB的数据，压缩后大小为10KB，当写入100KB后，仍对调用者返回100KB，而不是10KB
    2.2、压缩工作交给DFSClient做，DataNode只接收和存储
           这个方法效果最高，压缩分散地推给了HDFS客户端，但DataNode需要知道什么时候一个block块接收完成了。
推荐最终实现采用2.2这个方法，该方法需要修改的HDFS代码量也不大，但效果最高。

## 21mr的调度

MapReduce是hadoop提供一个可进行分布式计算的框架或者平台，显然这个平台是多用户的，每个合法的用户可以向这个平台提交作业，那么这就带来一个问题，就是作业调度。
      任何调度策略都考虑自己平台调度需要权衡的几个维度，例如操作系统中的进程调度，他需要考虑的维度就是资源（CPU）的最大利用率（吞吐）和实时性，操作系统对实时性的要求很高，所以操作系统往往采用基于优先级的、可抢占式的调度策略，并且赋予IO密集型（相对于计算密集型）的进程较高的优先级，扯的有点远。
      回到hadoop平台，其实MapReduce的作业调度并没有很高的实时性的要求，本着最大吞吐的原则去设计的，所以MapReduce默认采用的调度策略是FIFO（基于优先级队列实现的FIFO，不是纯粹的FIFO，这样每次h），这种策略显然不是可抢占式的调度，所以带来的问题就是高优先级的任务会被先前已经在运行并且还要运行很久的低优先级的作业给堵塞住。

## 22.mapreduce 作业，不使用 reduce 来输出，用什么能代替 reduce 的功能 



# 三  flume常见面试题

## 1 **Flume的工作及时是什么？**

核心概念是agent，里面包括source，channel和sink三个组件。

Source运行在日志收集节点进行日志采集，之后临时存储在channel中，sink负责将channel中的数据发送到目的地。

只有发送成功channel中的数据才会被删除。

首先书写flume配置文件，定义agent、source、channel和sink然后将其组装，执行flume-ng命令。

# 四 hive

## 1 **hive中存放的是什么？**

表。

存的是和hdfs的映射关系，hive是逻辑上的数据仓库，实际操作的都是hdfs上的文件，HQL就是用SQL语法来写的MR程序。

## 2Hive与关系型数据库的关系？

没有关系，hive是数据仓库，不能和数据库一样进行实时的CRUD操作。

是一次写入多次读取的操作，可以看成是ETL的工具。

## 3 **请说明hive中Sort By、Order By、Cluster By，Distribute By各代表什么意思？**

order by：会对输入做全局排序，因此只有一个reducer（多个reducer无法保证全局有序）。只有一个reducer，会导致当输入规模较大时，需要较长的计算时间。

sort by：不是全局排序，其在数据进入reducer前完成排序。

distribute by：按照指定的字段对数据进行划分输出到不同的reduce中。

cluster by：除了具有 distribute by 的功能外还兼具 sort by 的功能。

## 4 habse与hive,kudu的区别

**Hbase**： Hadoop database 的简称，也就是基于Hadoop数据库，是一种NoSQL数据库，主要适用于海量明细数据（十亿、百亿）的随机实时查询，如日志明细、交易清单、轨迹行为等。

**Hive**：Hive是Hadoop数据仓库，严格来说，不是数据库，主要是让开发人员能够通过SQL来计算和处理HDFS上的结构化数据，适用于离线的批量数据计算。

通过元数据来描述Hdfs上的结构化文本数据，通俗点来说，就是定义一张表来描述HDFS上的结构化文本，包括各列数据名称，数据类型是什么等，方便我们处理数据，当前很多SQL ON Hadoop的计算引擎均用的是hive的元数据，如Spark SQL、Impala等；

基于第一点，通过SQL来处理和计算HDFS的数据，Hive会将SQL翻译为Mapreduce来处理数据；

## 5 hive中的压缩格式RCFile.TextFile.SequenceFile各有什么区别，以上三种格式一样大的文件哪个占用空间大小

textfile(默认) 存储空间消耗比较大，并且压缩的text 无法分割和合并 查询的效率最低,可以直接存储，加载数据的速度最高
sequencefile 存储空间消耗最大,压缩的文件可以分割和合并 查询效率高，需要通过text文件转化来加载
rcfile 存储空间最小，查询的效率最高 ，需要通过text文件转化来加载，加载的速度最低

## 6 hive底层与数据库交互原理

Hive和Hbase有各自不同的特征：hive是高延迟、结构化和面向分析的，hbase是低延迟、非结构化和面向编程的。Hive数据仓库在hadoop上是高延迟的。Hive集成Hbase就是为了使用hbase的一些特性。如下是hive和hbase的集成架构：

Hive集成HBase可以有效利用HBase数据库的存储特性，如行更新和列索引等。在集成的过程中注意维持HBase jar包的一致性。Hive集成HBase需要在Hive表和HBase表之间建立映射关系，也就是Hive表的列(columns)和列类型(column types)与HBase表的列族(column families)及列限定词(column qualifiers)建立关联。每一个在Hive表中的域都存在于HBase中，而在Hive表中不需要包含所有HBase中的列。HBase中的RowKey对应到Hive中为选择一个域使用:key来对应，列族(cf:)映射到Hive中的其它所有域，列为(cf:cq)。

## 7 hive的内外部表的区别

1、在导入数据到外部表，数据并没有移动到自己的数据仓库目录下，也就是说外部表中的数据并不是由它自己来管理的！而 表则不一
样；
2、在删除表的时候，Hive将会把属于表的元数据和数据全部删掉；而删除外部表的时候，Hive仅仅删除外部表的元数据，数 据 是不会
删除的！
那么，应该如何选择使用哪种表呢？在大多数情况没有太多的区别，因此选择只是个人喜好的问题。但是作为一个经验，如 果所有处理
都需要由Hive完成，那么你应该创建表，否则使用外部表！

# 五 hbase

## 1Hbase行键列族的概念，物理模型，表的设计原则？

行键：是hbase表自带的，每个行键对应一条数据。

列族：是创建表时指定的，为列的集合，每个列族作为一个文件单独存储，存储的数据都是字节数组，其中数据可以有很多，通过时间戳来区分。

物理模型：整个hbase表会拆分成多个region，每个region记录着行键的起始点保存在不同的节点上，查询时就是对各个节点的并行查询，当region很大时使用.META表存储各个region的起始点，-ROOT又可以存储.META的起始点。

Rowkey的设计原则：各个列族数据平衡，长度原则、相邻原则，创建表的时候设置表放入regionserver缓存中，避免自动增长和时间，使用字节数组代替string，最大长度64kb，最好16字节以内，按天分表，两个字节散列，四个字节存储时分毫秒。

列族的设计原则：尽可能少(按照列族进行存储，按照region进行读取，不必要的io操作)，经常和不经常使用的两类数据放入不同列族中，列族名字尽可能短。

## 2**HBase简单读写流程？**

读：

找到要读数据的region所在的RegionServer，然后按照以下顺序进行读取：先去BlockCache读取，若BlockCache没有，则到Memstore读取，若Memstore中没有，则到HFile中去读。

写：

找到要写数据的region所在的RegionServer，然后先将数据写到WAL(Write-Ahead Logging，预写日志系统)中，然后再将数据写到Memstore等待刷新，回复客户端写入完成。

## 3**HBase的特点是什么？**

(1)hbase是一个分布式的基于列式存储的数据库，基于hadoop的HDFS存储，zookeeper进行管理。

(2)hbase适合存储半结构化或非结构化数据，对于数据结构字段不够确定或者杂乱无章很难按一个概念去抽取的数据。

(3)hbase为null的记录不会被存储。

(4)基于的表包括rowkey，时间戳和列族。新写入数据时，时间戳更新，同时可以查询到以前的版本。

(5)hbase是主从结构。Hmaster作为主节点，hregionserver作为从节点。

## 4 **请描述如何解决Hbase中region太小和region太大带来的结果。**

**Region****过大会发生多次compaction，将数据读一遍并重写一遍到hdfs 上，占用io，region过小会造成多次split，region 会下线，影响访问服务，调整hbase.hregion.max.filesize 为256m**

## 5  **描述Hbase中scan和get的功能以及实现的异同**

**1.****按指定RowKey 获取唯一一条记录，  get 是以一个 row 来标记的.一个 row 中可以有很多 family 和 column.**

2.scan 方法实现条件查询功能使用的就是 scan 方式.

1)scan 批处理提高速度(以空间换时间)；

 2)scan 可以通过 setStartRow 与 setEndRow 来限定范围,scan 可以通过 setFilter 方法添加过滤器，这也是分页、多条件查询的基础。

## 6  **简述 HBASE中compact用途是什么，什么时候触发，分为哪两种,有什么区别，有哪些相关配置参数？**

**在 hbase 中每当有 memstore 数据 flush 到磁盘之后，就形成一个 storefile，当 storeFile 的数量达到一定程度后，就需要将 storefile 文件来进行 compaction 操作。**

　　**Compact 的作用：**

　　　　　　　　　　**1>.合并文件**

　　　　　　　　　　**2>.清除过期，多余版本的数据**

　　　　　　　　　　**3>.提高读写数据的效率**

　　**HBase 中实现了两种 compaction 的方式：**

　　**minor and major. 这两种 compaction 方式的区别是：**

　　　　**1、 Minor 操作只用来做部分文件的合并操作以及包括 minVersion=0 并且设置 ttl 的过期版本清理，不做任何删除数据、多版本数据的清理工作。**

　　　　**2、 Major 操作是对 Region 下的 HStore 下的所有 StoreFile 执行合并操作，最终的结果是整理合并出一个文件。简述 Hbase filter 的实现原理是什么？结合实际项目经验，写出几个使用 filter 的场景HBase 为筛选数据提供了一组过滤器，通过这个过滤器可以在 HBase 中的数据的多个维度（行，列，数据版本）上进行对数据的筛选操作，也就是说过滤器最终能够筛选的数据能够细化到具体的一个存储单元格上（由行键，列名，时间戳定位）。 RowFilter、 PrefixFilter。。。hbase的filter是通过scan设置的，所以是基于scan的查询结果进行过滤.过滤器的类型很多，但是可以分为两大类——比较过滤器，专用过滤器过滤器的作用是在服务端判断数据是否满足条件，然后只将满足条件的数据返回给客户端；如在进行订单开发的时候，我们使用rowkeyfilter过滤出某个用户的所有订单**

## 7 **描述Hbase的rowKey的设计原则**

**1**：长度原则，最短越好，最大不能超过64K。太长的影响有两点，一是极大影响了HFile的存储效率。二是缓存memstore不能得到有效利用，缓存不能存放太多的信息，造成检索效率的降低。

**2**：唯一原则

**保证rowkey的唯一性，这条没有什么要讲的。**

**3**：自己一条原则

**尽量保证经常一起用的rowkey存储在同一个region上，有助于提升检索效率。但要避免热点问题。**

**4**：对于常用的检索的rowkey，尽量使用高表（行多列少），二部选择宽表（列多行少）。

## 8Hbase内部是什么机制

在HBase 中无论是增加新行还是修改已有的行，其内部流程都是相同的。HBase 接到命令后存下变化信息，或者写入失败抛出异常。默认情况下，执行写入时会写到两个地方：预写式日志（write-ahead log，也称HLog）和MemStore（见图2-1）。HBase 的默认方式是把写入动作记录在这两个地方，以保证数据持久化。只有当这两个地方的变化信息都写入并确认后，才认为写动作完成。
MemStore 是内存里的写入缓冲区，HBase 中数据在永久写入硬盘之前在这里累积。当MemStore 填满后，其中的数据会刷写到硬盘，生成一个HFile。HFile 是HBase 使用的底层存储格式。HFile 对应于列族，一个列族可以有多个HFile，但一个HFile 不能存储多个列族的数据。在集群的每个节点上，每个列族有一个MemStore。
大型分布式系统中硬件故障很常见，HBase 也不例外。设想一下，如果MemStore还没有刷写，服务器就崩溃了，内存中没有写入硬盘的数据就会丢失。HBase 的应对办法是在写动作完成之前先写入WAL。HBase 集群中每台服务器维护一个WAL 来记录发生的变化。WAL 是底层文件系统上的一个文件。直到WAL 新记录成功写入后，写动作才被认为成功完成。这可以保证HBase 和支撑它的文件系统满足持久性。大多数情况下，HBase 使用Hadoop 分布式文件系统（HDFS）来作为底层文件系统。
如果HBase 服务器宕机，没有从MemStore 里刷写到HFile 的数据将可以通过回放WAL 来恢复。你不需要手工执行。Hbase 的内部机制中有恢复流程部分来处理。每台HBase 服务器有一个WAL，这台服务器上的所有表（和它们的列族）共享这个WAL。
你可能想到，写入时跳过WAL 应该会提升写性能。但我们不建议禁用WAL，除非你愿意在出问题时丢失数据。如果你想测试一下，如下代码可以禁用WAL：


注意：不写入WAL 会在RegionServer 故障时增加丢失数据的风险。关闭WAL，出现故障时HBase 可能无法恢复数据，没有刷写到硬盘的所有写入数据都会丢失。

## 9 介绍一下HBase过滤器

HBase为筛选数据提供了一组过滤器，通过这个过滤器可以在HBase中的数据的多个维度（行，列，数据版本）上进行对数据的筛选操
作，也就是说过滤器最终能够筛选的数据能够细化到具体的一个存储单元格上（由行键，列明，时间戳定位）。通常来说，通过行键，值
来筛选数据的应用场景较多。

1. RowFilter：筛选出匹配的所有的行，对于这个过滤器的应用场景，是非常直观的：使用BinaryComparator可以筛选出具有某个行键的行，
   或者通过改变比较运算符（下面的例子中是CompareFilter.CompareOp.EQUAL）来筛选出符合某一条件的多条数据，以下就是筛选出行键为
   row1的一行数据：
2. PrefixFilter：筛选出具有特定前缀的行键的数据。这个过滤器所实现的功能其实也可以由RowFilter结合RegexComparator来实现，不过这
   里提供了一种简便的使用方法，以下过滤器就是筛选出行键以row为前缀的所有的行：
3. KeyOnlyFilter：这个过滤器唯一的功能就是只返回每行的行键，值全部为空，这对于只关注于行键的应用场景来说非常合适，这样忽略
   掉其值就可以减少传递到客户端的数据量，能起到一定的优化作用：
4. RandomRowFilter：从名字上就可以看出其大概的用法，本过滤器的作用就是按照一定的几率（<=0会过滤掉所有的行，>=1会包含所有的
   行）来返回随机的结果集，对于同样的数据集，多次使用同一个RandomRowFilter会返回不通的结果集，对于需要随机抽取一部分数据的应用场景，可以使用此过滤器

## 10hbase 宕机如何处理

问题分析:

- Hbase的Regionserver进程随机挂掉（该异常几乎每次都发生，只是挂掉的Regionserver节点不同）
- HMaster进程随机挂掉
- 主备Namenode节点随机挂掉
- Zookeeper节点随机挂掉
- Zookeeper连接超时
- JVM GC睡眠时间过长
- datanode写入超时

解决问题:

问题解决需从以下几个方面着手：

1、Hbase的ZK连接超时相关参数调优：默认的ZK超时设置太短，一旦发生FULL GC，极其容易导致ZK连接超时；

2、Hbase的JVM GC相关参数调优：可以通过GC调优获得更好的GC性能，减少单次GC的时间和FULL GC频率；

3、ZK Server调优：这里指的是ZK的服务端调优，ZK客户端（比如Hbase的客户端）的ZK超时参数必须在服务端超时参数的范围内，否则ZK客户端设置的超时参数起不到 效果；

4、HDFS读写数据相关参数需调优；

5、YARN针对各个节点分配资源参数调整：YARN需根据真实节点配置分配资源，之前的YARN配置为每个节点分配的资源都远大于真实虚拟机的硬件资源；

6、集群规划需优化：NameNode、NodeManager、DataNode，RegionServer会混用同一个节点，这样会导致这些关键的枢纽节点通信和内存压力过大，从而在计算压力 较大时容易发生异常。正确的做法是将枢纽节点（NameNode，ResourceManager，HMaster）和数据+计算节点分开

# 六 kafka

## 1.**Kafka** 与传统消息系统之间有三个关键区别

(1).Kafka 持久化日志，这些日志可以被重复读取和无限期保留

(2).Kafka 是一个分布式系统：它以集群的方式运行，可以灵活伸缩，在内部通过复制数据提升容错能力和高可用性

(3).Kafka 支持实时的流式处理

## 2.**Kafka的消费者如何消费数据**

**创建 KafkaConsumer 对象订阅指定主题topic的数据，groupid指定消费群组，使用轮询，指定key,指定offset等方式进行消费**

## 3. **kafka生产数据时数据的分组策略**

**第一种：给定了分区号，直接将数据发送到指定的分区里面去**

**第二种：没有给定分区号，给定数据的key值，通过key取上hashCode进行分区**

**第三种：既没有给定分区号，也没有给定key值，直接轮循进行分区**

**第四种：自定义分区**

## 4   **在数据制作过程中，你如何能从Kafka得到准确的信息?**

在数据中，为了精确地获得Kafka的消息，你必须遵循两件事: 在数据消耗期间避免重复，在数据生产过程中避免重复。 

这里有两种方法，可以在数据生成时准确地获得一个语义: 

每个分区使用一个单独的写入器，每当你发现一个网络错误，检查该分区中的最后一条消息，以查看您的最后一次写入是否成功 

在消息中包含一个主键(UUID或其他)，并在用户中进行反复制

## 5 kafka集群的规模，消费速度是多少。

一般中小型公司是10个节点，每秒20M左右。

# 七   Spark 

## 1 spark streming在实时处理时会发生什么故障，如何停止，解决

和Kafka整合时消息无序：

修改Kafka的ack参数，当ack=1时，master确认收到消息就算投递成功。ack=0时，不需要收到消息便算成功，高效不准确。sck=all，master和server都要受到消息才算成功，准确不高效。

StreamingContext.stop会把关联的SparkContext对象也停止，如果不想把SparkContext对象也停止的话可以把StremingContext.stop的可选参数stopSparkContext设为flase。一个SparkContext对象可以和多个streamingcontext对象关联。只要对前一个stremingcontext.stop(stopsparkcontext=false),然后再创建新的stremingcontext对象就可以了。



# 八 sqoop

## 1 Sgoop导入数据到mysql中 如何让数据不重复导入？ 存在数据问题sgoop会怎么样





# 额外:

## 1请列出正常的hadoop集群中hadoop都分别需要启动 哪些进程，他们的作用分别都是什么，请尽量列的详细一些。

namenode：负责管理hdfs中文件块的元数据，响应客户端请求，管理datanode上文件block的均衡，维持副本数量

Secondname:主要负责做checkpoint操作；也可以做冷备，对一定范围内数据做快照性备份。

Datanode:存储数据块，负责客户端对数据块的io请求

Jobtracker :管理任务，并将任务分配给 tasktracker。

Tasktracker: 执行JobTracker分配的任务。

Resourcemanager、Nodemanager、Journalnode、Zookeeper、Zkfc

## 2mysql，mongodb，rides的端口。

面试数据库介绍的再好，不知到默认端口，也证明你没有经验。mysql：3306，mongdb：27017，rides：6379。

## 3 数据来源

1.webServer ：用户访问我们的网站，对日志进行收集，记录在反向的日志文件里 tomcat下logs

2, js代码嵌入前端页面（埋点）：js的sdk会获取用户行为，document会得到元素调用function，通过ngix集群进行日志收集。

## 4 聊项目

搞清楚，数据的来源，数据的收集，数据的分析，数据的储存，数据的展示。

主要解决了啥业务。遇到了啥问题，数据的格式，有哪些优化，等等等

## 5 .列出你所知道的调度器  说明其工作方法

a) Fifo schedular 默认的调度器  先进先出
b) Capacity schedular  计算能力调度器  选择占用内存小  优先级高的
c) Fair schedular 调肚脐  公平调度器  所有job 占用相同资源

## 6 现有 1 亿个整数均匀分布，如果要得到前 1K 个最大的数，求最优的算法。  

自己的思路:

:分块，比如分 1W 块，每块 1W 个，然后分别找出每块最大值，从这最
大的 1W 个值中找最大 1K 个， 
那么其他的 9K 个最大值所在的块即可扔掉，从剩下的最大的 1K 个值所在的块中找前 1K
个即可。那么原问题的规模就缩小到了 1/10。 
问题： 
（1）这种分块方法的最优时间复杂度。 
（2）如何分块达到最优。比如也可分 10W 块，每块 1000 个数。则问题规模可降到原来
1/100。但事实上复杂度并没降低。 

## 7 毒酒问题－－－1000桶酒，其中1桶有毒，而一旦吃了，毒性会在一周后发作。问最少需要多少
只老鼠可在一周内找出毒酒?

1 2 4 8 16 32 64 128 256 512 10只

## 8 4亿个数字，找出哪些是重复的，要用最小的比较次数，写程序实现



## 9 算法题：有 2 个桶，容量分别为 3 升和 5 升，如何得到 4 升的水，假设水无限使用，写
出步骤。 

先把五升的水桶倒满,  然后倒进三升的水桶  ,再把三升的水桶的水倒掉  ,把五升的水桶里剩余的2升导入三升的水桶,再把五升的水桶倒满  在倒进有两升的三升桶种   则 剩余4升

# 海量数据处理面试题:

## 1海量日志数据，提取出某日访问百度次数最多的那个 IP。 

首先是这一天，并且是访问百度的日志中的 IP 取出来，逐个写入到一个大文件中。注意到
IP 是 32 位的，最多有个 2^32 个 IP。同样可以采用映射的方法， 比如模 1000，把整个
大文件映射为 1000 个小文件，再找出每个小文中出现频率最大的 IP（可以采用 hash_map
进行频率统计，然后再找出频率最大 的几个）及相应的频率。然后再在这 1000 个最大的
IP 中，找出那个频率最大的 IP，即为所求。 
或者如下阐述（雪域之鹰）： 
算法思想：分而治之+Hash 
（1）.IP 地址最多有 2^32=4G 种取值情况，所以不能完全加载到内存中处理； 
（2）.可以考虑采用“分而治之”的思想，按照 IP 地址的 Hash(IP)%1024 值，把海量 IP
日志分别存储到 1024 个小文件中。这样，每个小文件最多包含 4MB 个 IP 地址； 
（3）.对于每一个小文件，可以构建一个 IP 为 key，出现次数为 value 的 Hash map，同
时记录当前出现次数最多的那个 IP 地址； 
（4）.可以得到 1024 个小文件中的出现次数最多的 IP，再依据常规的排序算法得到总体上
出现次数最多的 IP； 

## 2 搜索引擎会通过日志文件把用户每次检索使用的所有检索串都记录下来，每个查询串的
长度为 1-255 字节。 
    假设目前有一千万个记录（这些查询串的重复度比较高，虽然总数是 1 千万，但如果
除去重复后，不超过 3 百万个。一个查询串的重复度越高，说明查询它的用户越多，也就
是越热门。），请你统计最热门的 10 个查询串，要求使用的内存不能超过 1G。 典型的 Top K 算法，还是在这篇文章里头有所阐述，详情请参见：十一、从头到尾彻底解
析 Hash 表算法。 
文中，给出的最终算法是： 
第一步、先对这批海量数据预处理，在 O（N）的时间内用 Hash 表完成统计（之前写成了
排序，特此订正。July、2011.04.27）； 

第二步、借助堆这个数据结构，找出 Top K，时间复杂度为 N‘logK。 
即，借助堆结构，我们可以在 log 量级的时间内查找和调整/移动。因此，维护一个 K(该题
目中是 10)大小的小根堆，然后遍历 300 万的 Query，分别 和根元素进行对比所以，我们
最终的时间复杂度是：O（N） + N’乘以O（logK），（N 为 1000 万，N’为 300 万）。ok，
更多，详情，请参考原文。 
或者：采用 trie 树，关键字域存该查询串出现的次数，没有出现为 0。最后用 10 个元素的
最小推来对出现频率进行排序。 

## 3有一个 1G 大小的一个文件，里面每一行是一个词，词的大小不超过 16 字节，内存限
制大小是 1M。返回频数最高的 100 个词。 

方案：顺序读文件中，对于每个词 x，取 hash(x)%5000，然后按照该值存到 5000 个小文
件（记为 x0,x1,…x4999）中。这样每个文件大概是 200k 左右。 
如果其中的有的文件超过了 1M 大小，还可以按照类似的方法继续往下分，直到分解得到
的小文件的大小都不超过 1M。 
对每个小文件，统计每个文件中出现的词以及相应的频率（可以采用 trie 树/hash_map 等），
并取出出现频率最大的 100 个词（可以用含 100 个结 点的最小堆），并把 100 个词及相应
的频率存入文件，这样又得到了 5000 个文件。下一步就是把这 5000 个文件进行归并（类
似与归并排序）的过程了。 

## 4  有 10 个文件，每个文件 1G，每个文件的每一行存放的都是用户的 query，每个文件的
query 都可能重复。要求你按照 query 的频度排序。

	还是典型的 TOP K 算法，解决方案如下： 

方案 1： 
顺序读取 10 个文件，按照 hash(query)%10 的结果将 query 写入到另外 10 个文件（记为）
中。这样新生成的文件每个的大小大约也 1G（假设 hash 函数是随机的）。 
找一台内存在 2G 左右的机器，依次对用 hash_map(query, query_count)来统计每个
query 出现的次数。利用快速/堆/归并排序按照出现次数进行排序。将排序好的 query 和
对应的 query_cout 输出到文件中。这样得到了 10 个排好序的文件（记为）。 
对这 10 个文件进行归并排序（内排序与外排序相结合）。 
方案 2： 
一般 query 的总量是有限的，只是重复的次数比较多而已，可能对于所有的 query，一次
性就可以加入到内存了。这样，我们就可以采用 trie 树/hash_map等直接来统计每个 query
出现的次数，然后按出现次数做快速/堆/归并排序就可以了。 
方案 3： 
与方案 1 类似，但在做完 hash，分成多个文件后，可以交给多个文件来处理，采用分布式
的架构来处理（比如 MapReduce），最后再进行合并。 

## 5 给定 a、b 两个文件，各存放 50 亿个 url，每个 url 各占 64 字节，内存限制是 4G，让
你找出 a、b 文件共同的 url？

方案 1：可以估计每个文件安的大小为 5G×64=320G，远远大于内存限制的 4G。所以不
可能将其完全加载到内存中处理。考虑采取分而治之的方法。 
遍历文件 a，对每个 url 求取 hash(url)%1000，然后根据所取得的值将 url 分别存储到 1000
个小文件（记为 a0,a1,…,a999）中。这样每个小文件的大约为 300M。 
遍历文件 b，采取和 a 相同的方式将 url 分别存储到 1000 小文件（记为 b0,b1,…,b999）。
这样处理后，所有可能相同的 url 都在对应的小 文件（a0vsb0,a1vsb1,…,a999vsb999）
中，不对应的小文件不可能有相同的 url。然后我们只要求出 1000 对小文件中相同的 url
即可。 
求每对小文件中相同的 url 时，可以把其中一个小文件的 url 存储到 hash_set 中。然后遍
历另一个小文件的每个 url，看其是否在刚才构建的 hash_set 中，如果是，那么就是共同
的 url，存到文件里面就可以了。 
方案 2：如果允许有一定的错误率，可以使用 Bloom filter，4G 内存大概可以表示 340 亿
bit。将其中一个文件中的 url 使用 Bloom filter 映射为这 340 亿 bit，然后挨个读取另外一
个文件的 url，检查是否与 Bloom filter，如果是，那么该 url 应该是共同的 url（注意会有
一定的错误率）。 

## 6 在 2.5 亿个整数中找出不重复的整数，注，内存不足以容纳这 2.5 亿个整数。 

方案 1：采用 2-Bitmap（每个数分配 2bit，00 表示不存在，01 表示出现一次，10 表示多
次，11 无意义）进行，共需内存 2^32 * 2 bit=1 GB 内存，还可以接受。然后扫描这 2.5
亿个整数，查看 Bitmap 中相对应位，如果是 00 变 01，01 变 10，10 保持不变。所描完
事后，查看 bitmap，把对应位是 01 的整数输出即可。 
方案 2：也可采用与第 1 题类似的方法，进行划分小文件的方法。然后在小文件中找出不重
复的整数，并排序。然后再进行归并，注意去除重复的元素。 

## 7  腾讯面试题：给 40 亿个不重复的 unsigned int 的整数，没排过序的，然后再给一个数，
如何快速判断这个数是否在那 40 亿个数当中？ 

与上第 6 题类似，我的第一反应时快速排序+二分查找。以下是其它更好的方法： 
方案 1：oo，申请 512M 的内存，一个 bit 位代表一个 unsigned int 值。读入 40 亿个数，
设置相应的 bit 位，读入要查询的数，查看相应 bit 位是否为 1，为 1 表示存在，为 0 表示
不存在。 
dizengrong： 
方案 2：这个问题在《编程珠玑》里有很好的描述，大家可以参考下面的思路，探讨一下： 
又因为 2^32 为 40 亿多，所以给定一个数可能在，也可能不在其中； 
这里我们把 40 亿个数中的每一个用 32 位的二进制来表示 
假设这 40 亿个数开始放在一个文件中。 
然后将这 40 亿个数分成两类: 
1.最高位为 0 
2.最高位为 1 
并将这两类分别写入到两个文件中，其中一个文件中数的个数<=20 亿，而另一个>=20 亿
（这相当于折半了）； 
与要查找的数的最高位比较并接着进入相应的文件再查找 
再然后把这个文件为又分成两类: 
1.次最高位为 0 
2.次最高位为 1 
并将这两类分别写入到两个文件中，其中一个文件中数的个数<=10 亿，而另一个>=10 亿
（这相当于折半了）； 
与要查找的数的次最高位比较并接着进入相应的文件再查找。 
……. 

以此类推，就可以找到了,而且时间复杂度为 O(logn)，方案 2 完。 
附：这里，再简单介绍下，位图方法： 
使用位图法判断整形数组是否存在重复 
判断集合中存在重复是常见编程任务之一，当集合中数据量比较大时我们通常希望少进行几
次扫描，这时双重循环法就不可取了。 
位图法比较适合于这种情况，它的做法是按照集合中最大元素 max 创建一个长度为 max+1
的新数组，然后再次扫描原数组，遇到几就给新数组的第几位置上 1，如遇到 5 就给新数
组的第六个元素置 1，这样下次再遇到 5 想置位时发现新数组的第六个元素已经是 1 了，这
说明这次的数据肯定和以前的数据存在着重复。这 种给新数组初始化时置零其后置一的做
法类似于位图的处理方法故称位图法。它的运算次数最坏的情况为 2N。如果已知数组的最
大值即能事先给新数组定长的话效 率还能提高一倍。 
欢迎，有更好的思路，或方法，共同交流。 

## 8 怎么在海量数据中找出重复次数最多的一个？ 

方案 1：先做 hash，然后求模映射为小文件，求出每个小文件中重复次数最多的一个，并
记录重复次数。然后找出上一步求出的数据中重复次数最多的一个就是所求（具体参考前面
的题）。 

## 9 上千万或上亿数据（有重复），统计其中出现次数最多的钱 N 个数据。 

方案 1：上千万或上亿的数据，现在的机器的内存应该能存下。所以考虑采用 hash_map/
搜索二叉树/红黑树等来进行统计次数。然后就是取出前 N 个出现次数最多的数据了，可以
用第 2 题提到的堆机制完成。 

## 10一个文本文件，大约有一万行，每行一个词，要求统计出其中最频繁出现的前 10 个词，
请给出思想，给出时间复杂度分析

方案 1：这题是考虑时间效率。用 trie 树统计每个词出现的次数，时间复杂度是 O(n*le)（le
表示单词的平准长度）。然后是找出出现最频繁的前 10 个词，可以用堆来实现，前面的题
中已经讲到了，时间复杂度是 O(n*lg10)。所以总的时间复杂度，是 O(n*le)与 O(n*lg10)
中较大的哪一 个。 

附、100w 个数中找出最大的 100 个数。 
方案 1：在前面的题中，我们已经提到了，用一个含 100 个元素的最小堆完成。复杂度为
O(100w*lg100)。 
方案 2：采用快速排序的思想，每次分割之后只考虑比轴大的一部分，知道比轴大的一部分
在比 100 多的时候，采用传统排序算法排序，取前 100 个。复杂度为 O(100w*100)。 
方案 3：采用局部淘汰法。选取前 100 个元素，并排序，记为序列 L。然后一次扫描剩余的
元素 x，与排好序的 100 个元素中最小的元素比，如果比这个最小的 要大，那么把这个最
小的元素删除，并把 x 利用插入排序的思想，插入到序列 L 中。依次循环，知道扫描了所有
的元素。复杂度为 O(100w乘以100)。 

## 11 海量数据处理方法总结

