<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-bounce.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":false,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="一 Spark的动态资源分配 1 问题背景: 用户提交Spark应用到Yarn上时，可以通过spark-submit的num-executors参数显示地指定executor个数，随后，ApplicationMaster会为这些executor申请资源，每个executor作为一个Container在Yarn上运行。Spark调度器会把Task按照合适的策略分配到executor上执行。所有任务执">
<meta property="og:type" content="article">
<meta property="og:title" content="SparkSql调优">
<meta property="og:url" content="http://example.com/2020/05/17/SparkSql%E8%B0%83%E4%BC%98/index.html">
<meta property="og:site_name" content="春雨里洗过的太阳">
<meta property="og:description" content="一 Spark的动态资源分配 1 问题背景: 用户提交Spark应用到Yarn上时，可以通过spark-submit的num-executors参数显示地指定executor个数，随后，ApplicationMaster会为这些executor申请资源，每个executor作为一个Container在Yarn上运行。Spark调度器会把Task按照合适的策略分配到executor上执行。所有任务执">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2020-05-17T13:12:22.000Z">
<meta property="article:modified_time" content="2021-03-17T14:21:19.880Z">
<meta property="article:author" content="HF">
<meta property="article:tag" content="Spark">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/2020/05/17/SparkSql%E8%B0%83%E4%BC%98/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>
<link rel="stylesheet" type="text/css" href="/css/injector.css" />
  <title>SparkSql调优 | 春雨里洗过的太阳</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">春雨里洗过的太阳</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">世间所有的相遇，都是久别重逢</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/05/17/SparkSql%E8%B0%83%E4%BC%98/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/hexo.jpg">
      <meta itemprop="name" content="HF">
      <meta itemprop="description" content="第二名就是头号输家">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="春雨里洗过的太阳">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          SparkSql调优
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-05-17 21:12:22" itemprop="dateCreated datePublished" datetime="2020-05-17T21:12:22+08:00">2020-05-17</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-03-17 22:21:19" itemprop="dateModified" datetime="2021-03-17T22:21:19+08:00">2021-03-17</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Spark/" itemprop="url" rel="index"><span itemprop="name">Spark</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>19k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>17 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="一-Spark的动态资源分配"><a href="#一-Spark的动态资源分配" class="headerlink" title="一 Spark的动态资源分配"></a>一 Spark的动态资源分配</h1><h2 id="1-问题背景"><a href="#1-问题背景" class="headerlink" title="1 问题背景:"></a>1 问题背景:</h2><p>用户提交Spark应用到Yarn上时，可以通过spark-submit的<strong>num-executors</strong>参数显示地指定executor个数，随后，ApplicationMaster会为这些executor申请资源，每个executor作为一个Container在Yarn上运行。Spark调度器会把Task按照合适的策略分配到executor上执行。所有任务执行完后，executor被杀死，应用结束。在job运行的过程中，无论executor是否领取到任务，都会一直占有着资源不释放。很显然，这在任务量小且显示指定大量executor的情况下会很容易造成资源浪费。</p>
<p>在探究Spark如何实现之前，首先思考下如果自己来解决这个问题，需要考虑哪些因素？大致的方案很容易想到：如果executor在一段时间内一直处于空闲状态，那么就可以kill该executor，释放其占用的资源。当然，一些细节及边界条件需要考虑到：</p>
<ul>
<li>executor动态调整的范围？无限减少？无限制增加？</li>
<li>executor动态调整速率？线性增减？指数增减？</li>
<li>何时移除Executor？</li>
<li>何时新增Executor了？只要由新提交的Task就新增Executor吗？</li>
<li>Spark中的executor不仅仅提供计算能力，还可能存储持久化数据，这些数据在宿主executor被kill后，该如何访问？</li>
</ul>
<h2 id="2-原理分析"><a href="#2-原理分析" class="headerlink" title="2 原理分析"></a>2 原理分析</h2><h3 id="2-1-Executor的生命周期"><a href="#2-1-Executor的生命周期" class="headerlink" title="2.1 Executor的生命周期"></a>2.1 Executor的生命周期</h3><h1 id="五-参数"><a href="#五-参数" class="headerlink" title="五: 参数"></a>五: 参数</h1><h4 id="提交"><a href="#提交" class="headerlink" title="提交:"></a>提交:</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-submit \</span><br><span class="line">  --master spark://192.168.1.1:7077 \</span><br><span class="line">  --num-executors 100 \</span><br><span class="line">  --executor-memory 6G \</span><br><span class="line">  --executor-cores 4 \</span><br><span class="line">　--total-executor-cores 400 \ ##standalone default all cores </span><br><span class="line">  --driver-memory 1G \</span><br><span class="line">  --conf spark.default.parallelism=1000 \</span><br><span class="line">  --conf spark.storage.memoryFraction=0.5 \</span><br><span class="line">  --conf spark.shuffle.memoryFraction=0.3 \</span><br></pre></td></tr></table></figure>

<h3 id="yarn和log"><a href="#yarn和log" class="headerlink" title="yarn和log:"></a>yarn和log:</h3><table>
<thead>
<tr>
<th>参数</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>spark.yarn.jars=hdfs://Ucluster/home/hadoop/lib/lib_20190327143532/spark2/*.jar</td>
<td></td>
</tr>
<tr>
<td>spark.yarn.historyServer.address=ip:port</td>
<td></td>
</tr>
<tr>
<td>spark.eventLog.dir=hdfs://Ucluster/var/log/spark</td>
<td></td>
</tr>
<tr>
<td>spark.eventLog.enabled=true</td>
<td></td>
</tr>
<tr>
<td>spark.history.fs.logDirectory=hdfs://Ucluster/var/log/spark</td>
<td></td>
</tr>
</tbody></table>
<h3 id="队列"><a href="#队列" class="headerlink" title="队列:"></a>队列:</h3><table>
<thead>
<tr>
<th>功能</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td>队列</td>
<td>spark.yarn.queue(–queue)</td>
<td></td>
</tr>
</tbody></table>
<h3 id="动态资源分配"><a href="#动态资源分配" class="headerlink" title="动态资源分配:"></a>动态资源分配:</h3><table>
<thead>
<tr>
<th>功能</th>
<th>命令</th>
<th>解释</th>
</tr>
</thead>
<tbody><tr>
<td>动态资源分配</td>
<td>spark.dynamicAllocation.enabled=true                                        spark.shuffle.service.enabled=true</td>
<td>动态资源分配开关(以上两个参数同时设置为true可开启动态资源分配，开启后可防止资源浪费情况。Spark-2.1.0默认关闭动态资源分配，Spark-2.3.3默认打开)(spark.shuffle.service.enabled  NodeManager中一个长期运行的辅助服务，用于提升Shuffle计算性能。默认为false，表示不启用该功能)</td>
</tr>
<tr>
<td></td>
<td>spark.dynamicAllocation.minExecutors=0</td>
<td>Executor调整下限</td>
</tr>
<tr>
<td></td>
<td>spark.dynamicAllocation.maxExecutors=4</td>
<td>Executor调整上限</td>
</tr>
<tr>
<td></td>
<td>spark.dynamicAllocation.initialExecutors=1</td>
<td>Executor初始数量（默认值：minExecutors）三者的关系必须满足：minExecutors &lt;= initialExecutors &lt;= maxExecutors   如果显示指定了num-executors参数，那么initialExecutors就是num-executor指定的值。</td>
</tr>
<tr>
<td></td>
<td>spark.dynamicAllocation.executorIdleTimeout=10</td>
<td>默认值60s  Executor超时：当Executor不执行任何任务时，会被标记为Idle状态。空闲一段时间后即被认为超时，会被kill</td>
</tr>
<tr>
<td></td>
<td>spark.dynamicAllocation.schedulerBacklogTimeout=1</td>
<td>默认1s  资源不足时，何时新增Executor：当有Task处于pending状态，意味着资源不足，此时需要增加Executor,</td>
</tr>
<tr>
<td></td>
<td>spark.dynamicAllocation.cachedExecutorIdleTimeout=12</td>
<td>如果Executor中缓存了数据，那么该Executor的Idle-timeout时间就不是由executorIdleTimeout决定，而是用spark.dynamicAllocation.cachedExecutorIdleTimeout控制，默认值：Integer.MAX_VALUE。如果手动设置了该值，当这些缓存数据的Executor被kill后，我们可以通过NodeManannger的External Shuffle Server来访问这些数据。这就要求NodeManager中spark.shuffle.service.enabled必须开启。</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>
<h3 id="Executor-task-cores-yarn等参数"><a href="#Executor-task-cores-yarn等参数" class="headerlink" title="Executor,task,cores,yarn等参数:"></a>Executor,task,cores,yarn等参数:</h3><table>
<thead>
<tr>
<th>功能</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td></td>
<td>num-executors或spark.num.executors=4</td>
<td>executor的数目是由每个节点运行的executor数目和集群的节点数共同决定    如40个节点 每个节点7个executor 则总的为280 (现实比这个小,driver也会占用core和内存)    Hive性能与用于运行查询的executor数量直接相关。 但是，不通查询还是不通。 通常，性能与executor的数量成比例。 例如，查询使用四个executor大约需要使用两个executor的一半时间。 但是，性能在一定数量的executor中达到峰值，高于此值时，增加数量不会改善性能并且可能产生不利影响。    在大多数情况下，使用一半的集群容量（executor数量的一半）可以提供良好的性能。 为了获得最佳性能，最好使用所有可用的executor。 例如，设置spark.executor.instances = 280。 对于基准测试和性能测量，强烈建议这样做。</td>
</tr>
<tr>
<td></td>
<td>spark.executor.instances = 280</td>
<td><code>Executors</code>的个数。这个配置和<code>spark.dynamicAllocation.enabled</code>不兼容。当同时配置这两个配置时，动态分配关闭，<code>spark.executor.instances</code>被使用</td>
</tr>
<tr>
<td></td>
<td>driver-memory(spark.driver.memory)=</td>
<td>当运行hive on spark的时候，每个spark driver能申请的最大jvm 堆内存,该参数结合 spark.driver.memoryOverhead共同决定着driver的内存大小。</td>
</tr>
<tr>
<td></td>
<td>spark.yarn.driver.memoryOverhead(spark.driver.memoryOverhead)=executorMemory * 0.10<code>，并且不小于</code>384m</td>
<td>每个driver能从yarn申请的堆外内存的大小。driver的内存大小并不直接影响性能，但是也不要job的运行受限于driver的内存,y一般 spark.driver.memory和 spark.driver.memoryOverhead内存的总和占总内存的10%-15%。假设 yarn.nodemanager.resource.memory-mb=100*1024MB,那么driver内存设置为12GB，此时 spark.driver.memory=10.5gb和spark.driver.memoryOverhead=1.5gb</td>
</tr>
<tr>
<td></td>
<td>executor-memory(spark.executor.memory)=3g</td>
<td>能申请的最大jvm 堆内存</td>
</tr>
<tr>
<td></td>
<td>spark.executor.memoryOverhead(spark.yarn.executor.memoryOverhead)=executorMemory * 0.10<code>，并且不小于</code>384m</td>
<td>YARN 请求的每个 executor的额外堆内存大小,spark.executor.memory 和spark.executor.memoryOverhead 共同决定着 executor内存。建议 spark.executor.memoryOverhead站总内存的 15%-20%。        (或executorMemory * 0.07, with minimum of 384) 那么最终 spark.executor.memoryOverhead=2 G 和spark.executor.memory=12 G  executor总为14g</td>
</tr>
<tr>
<td></td>
<td>executor-cores(spark.executor.cores)=4</td>
<td>每个executor的核数(也为task的个数)在executor运行的task共享内存,其实，executor内部是用newCachedThreadPool运行task的。</td>
</tr>
<tr>
<td></td>
<td>spark.task.cpus = 1</td>
<td>每个task配置核数</td>
</tr>
<tr>
<td></td>
<td>spark.sql.windowExec.buffer.spill.threshold</td>
<td>当用户的SQL中包含窗口函数时，并不会把一个窗口中的所有数据全部读进内存，而是维护一个缓存池，当池中的数据条数大于该参数表示的阈值时，spark将数据写到磁盘</td>
</tr>
<tr>
<td></td>
<td>spark.driver.cores</td>
<td>driver的核数</td>
</tr>
<tr>
<td></td>
<td>spark.yarn.am.memory=512</td>
<td>在客户端模式（<code>client mode</code>）下，<code>yarn</code>应用<code>master</code>使用的内存数。在集群模式（<code>cluster mode</code>）下，使用<code>spark.driver.memory</code>代替。</td>
</tr>
<tr>
<td></td>
<td>spark.yarn.am.cores</td>
<td>在客户端模式（<code>client mode</code>）下，<code>yarn</code>应用的<code>master</code>使用的核数。在集群模式下，使用<code>spark.driver.cores</code>代替。</td>
</tr>
<tr>
<td></td>
<td>spark.cores.max =12</td>
<td>限制使用的最大核数</td>
</tr>
<tr>
<td></td>
<td>total-executor-cores(spark.total.executor.cores)</td>
<td>executor 的总核数</td>
</tr>
</tbody></table>
<h3 id="动态分区-hive"><a href="#动态分区-hive" class="headerlink" title="动态分区(hive)"></a>动态分区(hive)</h3><table>
<thead>
<tr>
<th>功能</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td></td>
<td>hive.exec.dynamic.partition.mode=nonstrict;</td>
<td></td>
</tr>
<tr>
<td></td>
<td>hive.exec.dynamic.partition=true;</td>
<td></td>
</tr>
<tr>
<td></td>
<td>hive.exec.max.dynamic.partitions=1000;</td>
<td></td>
</tr>
<tr>
<td></td>
<td>hive.exec.max.dynamic.partitions.pernode=100;</td>
<td></td>
</tr>
</tbody></table>
<h3 id="广播"><a href="#广播" class="headerlink" title="广播:"></a>广播:</h3><table>
<thead>
<tr>
<th>功能</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td></td>
<td>spark.sql.adaptive.join.enabled = true</td>
<td>在join操作时自动进行小表广播</td>
</tr>
<tr>
<td></td>
<td>spark.sql.autoBroadcastJoinThreshold = 33554432</td>
<td>-1禁止广播，广播阈值</td>
</tr>
<tr>
<td></td>
<td>spark.sql.broadcastTimeout=600</td>
<td>广播超时时间(默认5min)当广播小表时，如果广播时间超过此参数设置值，会导致任务失败</td>
</tr>
<tr>
<td></td>
<td></td>
<td>SELECT /*+ MAPJOIN(b) <em>/    或SELECT /</em>+ BROADCASTJOIN(b) <em>/    或  SELECT /</em>+ BROADCAST(b) */</td>
</tr>
</tbody></table>
<h3 id="shuffle调优"><a href="#shuffle调优" class="headerlink" title="shuffle调优"></a>shuffle调优</h3><table>
<thead>
<tr>
<th>参数</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>spark.storage.memoryFraction</td>
<td>参数说明：该参数用于设置RDD持久化数据在Executor内存中能占的比例，默认是0.6。也就是说，默认Executor 60%的内存，可以用来保存持久化的RDD数据。根据你选择的不同的持久化策略，如果内存不够时，可能数据就不会持久化，或者数据会写入磁盘。                                                                                                                                 参数调优建议：如果Spark作业中，有较多的RDD持久化操作，该参数的值可以适当提高一些，保证持久化的数据能够容纳在内存中。避免内存不够缓存所有的数据，导致数据只能写入磁盘中，降低了性能。但是如果Spark作业中的shuffle类操作比较多，而持久化操作比较少，那么这个参数的值适当降低一些比较合适。此外，如果发现作业由于频繁的gc导致运行缓慢（通过spark web ui可以观察到作业的gc耗时），意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。</td>
</tr>
<tr>
<td>spark.shuffle.memoryFraction</td>
<td>参数说明：该参数用于设置shuffle过程中一个task拉取到上个stage的task的输出后，进行聚合操作时能够使用的Executor内存的比例，默认是0.2。也就是说，Executor默认只有20%的内存用来进行该操作。shuffle操作在进行聚合时，如果发现使用的内存超出了这个20%的限制，那么多余的数据就会溢写到磁盘文件中去，此时就会极大地降低性能。                                                                                       参数调优建议：如果Spark作业中的RDD持久化操作较少，shuffle操作较多时，建议降低持久化操作的内存占比，提高shuffle操作的内存占比比例，避免shuffle过程中数据过多时内存不够用，必须溢写到磁盘上，降低了性能。此外，如果发现作业由于频繁的gc导致运行缓慢，意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。</td>
</tr>
<tr>
<td>spark.shuffle.service.enabled=true</td>
<td>NodeManager中一个长期运行的辅助服务，用于提升Shuffle计算性能。默认为false，表示不启用该功能。spark.shuffle.service.port(Shuffle服务监听数据获取请求的端口。可选配置，默认值为“7337”。)</td>
</tr>
<tr>
<td>spark.reducer.maxSizeInFlight=48m</td>
<td><strong>参数说明：</strong>   该参数用于设置shuffle read task的buffer缓冲大小，而这个buffer缓冲决定了每次能够拉取多少数据。<strong>调优建议：</strong>   如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小（比如96m），从而减少拉取数据的次数，也就可以减少网络传输的次数，进而提升性能。在实践中发现，合理调节该参数，性能会有1%~5%的提升。</td>
</tr>
<tr>
<td>spark.reducer.maxReqsInFlight=Int.MaxValue</td>
<td>限制远程机器拉取本机器文件块的请求数，随着集群增大，需要对此做出限制。否则可能会使本机负载过大而挂掉</td>
</tr>
<tr>
<td>spark.reducer.maxBlocksInFlightPerAddress=Int.MaxValue</td>
<td>限制了每个主机每次reduce可以被多少台远程主机拉取文件块，调低这个参数可以有效减轻node manager的负载</td>
</tr>
<tr>
<td>spark.maxRemoteBlockSizeFetchToMem=Int.MaxValue - 512</td>
<td>远程block大小大于该阈值多情况下，直接拉取存储</td>
</tr>
<tr>
<td>spark.shuffle.compress=true</td>
<td>map输出文件将采用 spark.io.compression.codec.格式压缩</td>
</tr>
<tr>
<td>spark.shuffle.file.buffer=32k</td>
<td>task的BufferedOutputStream的buffer缓冲大小。将数据写到磁盘文件之前，会先写入buffer缓冲中，待缓冲写满之后，才会溢写到磁盘。</td>
</tr>
<tr>
<td>spark.shuffle.io.maxRetries=3</td>
<td>拉取失败重试次数</td>
</tr>
<tr>
<td>spark.shuffle.io.numConnectionsPerPeer=1</td>
<td></td>
</tr>
<tr>
<td>spark.shuffle.io.preferDirectBufs=true</td>
<td>堆外缓存区开关，可减少shuffle和block缓存传输中垃圾的回收，当堆外内存受到限制时关闭此功能</td>
</tr>
<tr>
<td>spark.shuffle.io.retryWait=5s</td>
<td>重试拉取的时间间隔</td>
</tr>
<tr>
<td>spark.shuffle.service.enabled=false</td>
<td>动态移除空闲资源时打开，防止移除后文件丢失</td>
</tr>
<tr>
<td>spark.shuffle.service.index.cache.size=100m</td>
<td>缓存index文件大小，fetch数据的时候会读取index文件，如果这过程比较慢，就需要调大</td>
</tr>
<tr>
<td>spark.shuffle.sort.bypassMergeThreshold=200</td>
<td>当shuffle manager配置为sortShuffleManager ，而且reduce个数小于该值，map端不需要做combine，则采用BypassMergeSortShuffle</td>
</tr>
<tr>
<td>spark.shuffle.spill.compress=true</td>
<td>shuffle 阶段spill文件是否需要压缩，压缩方法spark.io.compression.codec</td>
</tr>
</tbody></table>
<h3 id="sparksql的小文件合并"><a href="#sparksql的小文件合并" class="headerlink" title="sparksql的小文件合并"></a>sparksql的小文件合并</h3><h4 id="1-sql有shuffle阶段"><a href="#1-sql有shuffle阶段" class="headerlink" title="1 sql有shuffle阶段"></a>1 sql有shuffle阶段</h4><table>
<thead>
<tr>
<th>参数</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>spark.sql.adaptive.enabled=true</td>
<td>默认关闭，如果开启请确保资源动态分配生效,参数是用于开启spark的自适应执行(用来控制是否开启adaptive execution，默认为false。一直以来，Spark只能设置固定的并行度（参考spark.sql.shuffle.partitions），在大促期间，数据量激增，每个task处理的数量增加，很容易出现oom的情况。在开启此项参数后，Spark将会按照spark.sql.ataptive.shuffle.targetPostShuffleInputSize设置的每个task的目标处理数据量自动调整stage并行度，减少task出现oom的情况。)</td>
</tr>
<tr>
<td>spark.sql.adaptive.shuffle.targetPostShuffleInputSize=128000000</td>
<td>reduce读最少数据量,用于控制之后的shuffle 阶段的平均输入数据大小，防止产生过多的task。Spark将会根据此参数值动态调整task个数，</td>
</tr>
<tr>
<td>spark.sql.adaptive.shuffle.targetPostShuffleRowCount=</td>
<td>接收最少记录条数</td>
</tr>
<tr>
<td>spark.sql.adaptive.minNumPostShufflePartitions=</td>
<td>reduce最小值</td>
</tr>
<tr>
<td>spark.sql.adaptive.maxNumPostShufflePartitions=</td>
<td>reduce最大值</td>
</tr>
<tr>
<td>spark.sql.ataptive.skewedJoin.enabled（spark-2.3.3）</td>
<td>在开启adaptive execution时，用来控制是否开启自动处理join时的数据倾斜，默认为false。</td>
</tr>
<tr>
<td>spark.sql.ataptive.skewedPartitionMaxSplits（spark-2.3.3）</td>
<td>在开启adaptive execution时，控制处理一个倾斜 Partition 的 Task 个数上限，默认值为 5。</td>
</tr>
<tr>
<td>spark.sql.ataptive.skewedPartitionRowCountThreshold（spark-2.3.3）</td>
<td>在开启adaptive execution时，设置一个 Partition 被视为倾斜 Partition 的行数下限，也即行数低于该值的 Partition 不会被当作倾斜 Partition 处理。其默认值为 10L * 1000 * 1000 即一千万。</td>
</tr>
<tr>
<td>spark.sql.ataptive.skewedPartitionSizeThreshold（spark-2.3.3）</td>
<td>在开启adaptive execution时，设置一个 Partition 被视为倾斜 Partition 的大小下限，也即大小小于该值的 Partition 不会被视作倾斜 Partition。其默认值为 64 * 1024 * 1024 也即 64MB。</td>
</tr>
<tr>
<td>spark.sql.ataptive.skewedPartitionFactor（spark-2.3.3）</td>
<td>在开启adaptive execution时，设置倾斜因子。如果一个 Partition 的大小大于 <code>spark.sql.adaptive.skewedPartitionSizeThreshold</code> 的同时大于各 Partition 大小中位数与该因子的乘积，或者行数大于 <code>spark.sql.adaptive.skewedPartitionRowCountThreshold</code> 的同时大于各 Partition 行数中位数与该因子的乘积，则它会被视为倾斜的 Partition。默认为10。</td>
</tr>
</tbody></table>
<p>当自适应关闭时,合并小文件:</p>
<table>
<thead>
<tr>
<th>参数</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>spark.sql.shuffle.partitions=2 默认值200</td>
<td>(对sql生效)shuffle默认设置，动态推测关闭有效(设置为1 则会只有一个文件)(只针对sql有效);(调整stage的并行度，也就是每个stage的task个数，默认值为40。此参数一般设置为任务申请的总core数的2-4倍，如：申请100个executor，每个executor申请2个core，那么总core数为200，此参数设置的合理范围是400-800。注意，此参数不能调整某些读外部数据stage的并行度，如：读hdfs的stage，绝大多数情况它的并行度取决于需要读取的文件数。)</td>
</tr>
<tr>
<td>spark.default.parallelism=5</td>
<td>对sql不生效 对df,ds生效(该参数用于设置每个stage的默认task数量。这个参数极为重要，如果不设置可能会直接影响你的Spark作业性能。因此Spark官网建议的设置原则是，设置该参数为num-executors * executor-cores的2~3倍较为合适，比如Executor的总CPU core数量为300个，那么设置1000个task是可以的，此时可以充分地利用Spark集群的资源。)</td>
</tr>
</tbody></table>
<h4 id="2-sql无shuffle阶段"><a href="#2-sql无shuffle阶段" class="headerlink" title="2 sql无shuffle阶段"></a>2 sql无shuffle阶段</h4><p>思路:无shuffle</p>
<h3 id="1文件为parquet-orc格式"><a href="#1文件为parquet-orc格式" class="headerlink" title="1文件为parquet,orc格式"></a>1文件为parquet,orc格式</h3><h4 id="1-1源数据小文件多"><a href="#1-1源数据小文件多" class="headerlink" title="1.1源数据小文件多:"></a>1.1源数据小文件多:</h4><table>
<thead>
<tr>
<th>参数</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>spark.sql.files.maxPartitionBytes</td>
<td>控制一个分区最大多少，默认128M</td>
</tr>
<tr>
<td>spark.sql.files.openCostInBytes</td>
<td>控制当一个文件小于该阈值时会继续扫描新的文件将其放到到一个分区，默认4M</td>
</tr>
<tr>
<td>spark.default.parallelism</td>
<td>hdfs文件rdd分区最终多少个，也与spark.default.parallelism有关，启动spark集群的时候可以指定该参数</td>
</tr>
<tr>
<td>spark.sql.files.maxRecordsPerFile</td>
<td>写入一个文件的最大记录数。如果该值为零或负，则没有限制</td>
</tr>
</tbody></table>
<h4 id="1-2-源数据大文件"><a href="#1-2-源数据大文件" class="headerlink" title="1.2 源数据大文件"></a>1.2 源数据大文件</h4><table>
<thead>
<tr>
<th>参数</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>spark.files.maxPartitionBytes=   默认128m</td>
<td></td>
</tr>
<tr>
<td>parquet.block.size   默认128m</td>
<td></td>
</tr>
<tr>
<td></td>
<td>第一个参数是针对session有效的，也就是因为这你在读的时候设置就会立刻生效。在没有第二个参数配合的情况下，就已经能够增加分区数了，缺点是，分区里的数据可能不会很均匀，因为均匀程度也会受到第二个参数的影响。</td>
</tr>
</tbody></table>
<h3 id="2-文件为其他格式"><a href="#2-文件为其他格式" class="headerlink" title="2 文件为其他格式"></a>2 文件为其他格式</h3><table>
<thead>
<tr>
<th>参数</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>spark.hadoop.mapreduce.input.fileinputformat.split.minsize</td>
<td></td>
</tr>
<tr>
<td>spark.hadoop.mapreduce.input.fileinputformat.split.maxsize</td>
<td></td>
</tr>
<tr>
<td>spark.hadoop.mapreduce.input.fileinputformat.split.minsize.per.node</td>
<td></td>
</tr>
<tr>
<td>spark.hadoop.mapreduce.input.fileinputformat.split.minsize.per.rack</td>
<td></td>
</tr>
<tr>
<td>spark.hadoop.hive.exec.max.created.files</td>
<td></td>
</tr>
</tbody></table>
<h1 id="六-sparksql参数"><a href="#六-sparksql参数" class="headerlink" title="六 sparksql参数"></a>六 sparksql参数</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//查看参数</span></span><br><span class="line">sparkSession.sql(<span class="string">&quot;set -v&quot;</span>).show(<span class="number">200</span>,<span class="literal">false</span>)</span><br></pre></td></tr></table></figure>

<table>
<thead>
<tr>
<th>配置项</th>
<th>默认值</th>
<th>概述</th>
</tr>
</thead>
<tbody><tr>
<td>spark.sql.optimizer.maxIterations</td>
<td>100</td>
<td>sql优化器最大迭代次数</td>
</tr>
<tr>
<td>spark.sql.optimizer.inSetConversionThreshold</td>
<td>10</td>
<td>插入转换的集合大小阈值</td>
</tr>
<tr>
<td>spark.sql.inMemoryColumnarStorage.compressed</td>
<td>TRUE</td>
<td>当设置为true时，SCAPK SQL将根据数据的统计自动为每个列选择压缩编解码器</td>
</tr>
<tr>
<td>spark.sql.inMemoryColumnarStorage.batchSize</td>
<td>10000</td>
<td>控制用于列缓存的批处理的大小。较大的批处理大小可以提高内存利用率和压缩率，但缓存数据时会出现OOM风险</td>
</tr>
<tr>
<td>spark.sql.inMemoryColumnarStorage.partitionPruning</td>
<td>TRUE</td>
<td>启用内存中的列表分区剪枝</td>
</tr>
<tr>
<td>spark.sql.join.preferSortMergeJoin</td>
<td>TRUE</td>
<td>When true, 使用sort merge join 代替 shuffle hash join</td>
</tr>
<tr>
<td>spark.sql.sort.enableRadixSort</td>
<td>TRUE</td>
<td>使用基数排序，基数排序性能非常快，但是会额外使用over heap.当排序比较小的Row时，overheap 需要提高50%</td>
</tr>
<tr>
<td>spark.sql.autoBroadcastJoinThreshold</td>
<td>10L * 1024 * 1024</td>
<td>当执行join时，被广播到worker节点上表最大字节。当被设置为-1，则禁用广播。当前仅仅支持 Hive Metastore  tables，表大小的统计直接基于hive表的源文件大小</td>
</tr>
<tr>
<td>spark.sql.limit.scaleUpFactor</td>
<td>4</td>
<td>在执行查询时，两次尝试之间读取partation数目的增量。较高的值会导致读取过多分区，较少的值会导致执行时间过长，因为浙江运行更多的作业</td>
</tr>
<tr>
<td>spark.sql.statistics.fallBackToHdfs</td>
<td>FALSE</td>
<td>当不能从table metadata中获取表的统计信息，返回到hdfs。这否有用取决与表是否足够小到能够使用auto broadcast  joins</td>
</tr>
<tr>
<td>spark.sql.defaultSizeInBytes</td>
<td>Long.MaxValue</td>
<td>在查询计划中表默认大小，默认被设置成Long.MaxValue  大于spark.sql.autoBroadcastJoinThreshold的值，也就意味着默认情况下不会广播一个表，除非他足够小</td>
</tr>
<tr>
<td>spark.sql.shuffle.partitions</td>
<td>200</td>
<td>当为join/aggregation shuffle数据时，默认partition的数量</td>
</tr>
<tr>
<td>spark.sql.adaptive.shuffle.targetPostShuffleInputSize</td>
<td>64 * 1024 * 1024byte</td>
<td>The target post-shuffle input size in bytes of a task.</td>
</tr>
<tr>
<td>spark.sql.adaptive.enabled</td>
<td>FALSE</td>
<td>是否开启adaptive query execution（自适应查询执行）</td>
</tr>
<tr>
<td>spark.sql.adaptive.minNumPostShufflePartitions</td>
<td>-1</td>
<td>测试用</td>
</tr>
<tr>
<td>spark.sql.subexpressionElimination.enabled</td>
<td>TRUE</td>
<td>When true, common subexpressions will be eliminated</td>
</tr>
<tr>
<td>当为真时，将删除公共子表达式</td>
<td></td>
<td></td>
</tr>
<tr>
<td>spark.sql.caseSensitive</td>
<td>FALSE</td>
<td>查询分析器是否区分大小写，默认情况下不区分。强烈建议不区分大小写</td>
</tr>
<tr>
<td>spark.sql.constraintPropagation.enabled</td>
<td></td>
<td>是否开启优化，在查询优化器期间推断和传播查询计划中的数据约束。对于某种类型的查询计划（例如有大量谓语和别名的查询），约束传播是昂贵的，会对整个运行时间产生负面影响。</td>
</tr>
<tr>
<td>spark.sql.parser.escapedStringLiterals</td>
<td>FALSE</td>
<td>2.0之前默认值为true，知否默认是否。正常文字能否包含在正则表达式中。</td>
</tr>
<tr>
<td>spark.sql.parquet.mergeSchema</td>
<td>FALSE</td>
<td>若为true,在读取parquet数据源时，schema从所有文件中合并出来。否则如果没有可用的摘要文件，则从概要文件或随机文件中选择模式</td>
</tr>
<tr>
<td>spark.sql.parquet.respectSummaryFiles</td>
<td>FALSE</td>
<td>若为ture,假设parquet的所有部分文件和概要文件一致，在合并模式时会忽略他们。否则将会合并所有的部分文件</td>
</tr>
<tr>
<td>spark.sql.parquet.binaryAsString</td>
<td>FALSE</td>
<td>是否向下兼容其他parquet生产系统（eg impala or older version spark sql  ），不区分字节数据和string数据写到parquet schema,这个配置促使spark sql将二进制数据作为string达到兼容</td>
</tr>
<tr>
<td>spark.sql.parquet.int96AsTimestamp</td>
<td>TRUE</td>
<td>是否使用Int96作为timestamp的存储格式，可以避免精度损失丢失纳秒部分，为其他parquet系统提供兼容（impala)</td>
</tr>
<tr>
<td>spark.sql.parquet.int64AsTimestampMillis</td>
<td>FALSE</td>
<td>当为true，timestamp值将以Int64作为mlibs的存储扩展类型，这种模式微秒将被丢弃</td>
</tr>
<tr>
<td>spark.sql.parquet.cacheMetadata</td>
<td>TRUE</td>
<td>是否缓存parquet的schema数据元，可以提升静态数据的查询性能</td>
</tr>
<tr>
<td>spark.sql.parquet.compression.codec</td>
<td>snappy</td>
<td>支持类型：uncompressed”, “snappy”, “gzip”,  “lzo”。 指定parquet写文件的压缩编码方式</td>
</tr>
<tr>
<td>spark.sql.parquet.filterPushdown</td>
<td>TRUE</td>
<td>是否开启parquet过滤条件下推</td>
</tr>
<tr>
<td>spark.sql.parquet.writeLegacyFormat</td>
<td>FALSE</td>
<td>spark sql在拼接schema时是否遵循parquet的schema的规范</td>
</tr>
<tr>
<td>spark.sql.parquet.output.committer.class</td>
<td>org.apache.parquet.hadoop.ParquetOutputCommitter</td>
<td>parquet输出提交器类，同城必须是org.apache.hadoop.mapreduce.OutputCommitter的子类，如果不是将不会创建数据源摘要，即使配置开启了parquet.enable.summary-metadata</td>
</tr>
<tr>
<td>spark.sql.parquet.enableVectorizedReader</td>
<td>TRUE</td>
<td>开启parquet向量解码</td>
</tr>
<tr>
<td>spark.sql.orc.filterPushdown</td>
<td>FALSE</td>
<td>是否开启条件下推到orc文件写</td>
</tr>
<tr>
<td>spark.sql.hive.verifyPartitionPath</td>
<td>FALSE</td>
<td>当为true时，在读取HDFS中存储的数据时，检查表根目录下的所有分区路径</td>
</tr>
<tr>
<td>spark.sql.hive.metastorePartitionPruning</td>
<td>TRUE</td>
<td>当为true，spark sql的谓语将被下推到hive metastore中，更早的消除不匹配的分区，会影响到违背转换成文件源关系的hive表</td>
</tr>
<tr>
<td>spark.sql.hive.manageFilesourcePartitions</td>
<td>TRUE</td>
<td>是否使用hive metastore管理spark sql的  dataSource表分区,若为true，dataSource表会在执行计划期间使用分区剪枝</td>
</tr>
<tr>
<td>spark.sql.hive.filesourcePartitionFileCacheSize</td>
<td>250 * 1024 * 1024</td>
<td>当非0时，开启将分区文件数据元缓存到内存中，所有表共享一个缓存，当开启 hive filesource partition  management（spark.sql.hive.manageFilesourcePartitions）时才会生效</td>
</tr>
<tr>
<td>spark.sql.hive.caseSensitiveInferenceMode</td>
<td>INFER_AND_SAVE</td>
<td>设置无法从hive表属性读取分区大小写模式时所采取的操作，虽然Spice  SQL本身不区分大小写，但hive兼容的文件格式如parquet。Spark  sql必须使用一个保持情况的模式，当查询由包含区分大小写字段名或查询的文件支持的任何表可能无法返回准确的结果时。有效选项包括INFER_AND_SAVE(默认模式——从基础数据文件推断出区分大小写的模式，并将其写入表属性），INFER_ONLY（推断schema但不尝试将其写入表属性）和NEVER_INFER（回退到使用区分大小写间接转移模式代替推断)</td>
</tr>
<tr>
<td>spark.sql.optimizer.metadataOnly</td>
<td>TRUE</td>
<td>当为true时，启用仅使用表的元数据的元数据查询优化来生成分区列，而不是表扫描。当扫描的所有列都是分区列，并且查询具有满足不同语义的聚合运算符时，它适用。</td>
</tr>
<tr>
<td>spark.sql.columnNameOfCorruptRecord</td>
<td>_corrupt_record</td>
<td>当json/csv数据内部列解析失败时，失败列的名称</td>
</tr>
<tr>
<td>spark.sql.broadcastTimeout”</td>
<td>5*60</td>
<td>在broadCast join时 ，广播等待的超时时间</td>
</tr>
<tr>
<td>spark.sql.thriftserver.scheduler.pool</td>
<td></td>
<td>为JDBC客户端会话设置公平调度程序池</td>
</tr>
<tr>
<td>spark.sql.thriftServer.incrementalCollect</td>
<td>FALSE</td>
<td>当TRUE时，启用增量集合以在thrift server中执行</td>
</tr>
<tr>
<td>spark.sql.thriftserver.ui.retainedStatements</td>
<td>200</td>
<td>JDBC/ODBC Web用户界面历史记录中SQL语句的数量</td>
</tr>
<tr>
<td>spark.sql.thriftserver.ui.retainedSessions</td>
<td>200</td>
<td>JDBC/ODBC Web UI历史中保存的SQL客户端会话数</td>
</tr>
<tr>
<td>spark.sql.sources.default</td>
<td>parquet</td>
<td>输入输出默认数据元</td>
</tr>
<tr>
<td>spark.sql.hive.convertCTAS</td>
<td>FALSE</td>
<td>如果时true，将使用spark.sql.sources.default.设置数据源，不指定任何存储属性到hive ctas语句</td>
</tr>
<tr>
<td>spark.sql.hive.gatherFastStats</td>
<td>TRUE</td>
<td>在修复表分区时，将快速收集STATS（文件数量和所有文件的总大小），以避免HIVE转移子中的顺序列表。</td>
</tr>
<tr>
<td>spark.sql.sources.partitionColumnTypeInference.enabled</td>
<td>TRUE</td>
<td>是否自动推断分区列的数据类型</td>
</tr>
<tr>
<td>spark.sql.sources.bucketing.enabled</td>
<td>TRUE</td>
<td>当false时，分桶表当作普通表处理</td>
</tr>
<tr>
<td>spark.sql.crossJoin.enabled</td>
<td>FALSE</td>
<td>当false时，如果查询中语法笛卡儿积 却语法中没有显示join，将会抛出异常</td>
</tr>
<tr>
<td>spark.sql.orderByOrdinal</td>
<td>TRUE</td>
<td>当为true时，排序字段放置到seleect List，否则被忽略</td>
</tr>
<tr>
<td>spark.sql.groupByOrdinal</td>
<td>TRUE</td>
<td>当为true时，按组子句的序号被视为选择列表中的位置。当为false时，序数被忽略。</td>
</tr>
<tr>
<td>spark.sql.groupByAliases</td>
<td>TRUE</td>
<td>group by后的别名是否能够被用到 select list中，若为否将抛出分析异常</td>
</tr>
<tr>
<td>spark.sql.sources.parallelPartitionDiscovery.threshold</td>
<td>32</td>
<td>允许在driver端列出文件的最大路径数。如果在分区发现期间检测到的路径的数量超过该值，则尝试用另一个SCAPLE分布式作业来列出文件。这适用于parquet、ORC、CSV、JSON和LIbSVM数据源。</td>
</tr>
<tr>
<td>spark.sql.sources.parallelPartitionDiscovery.parallelism</td>
<td>10000</td>
<td>递归地列出路径集合的并行数，设置阻止文件列表生成太多任务的序号</td>
</tr>
<tr>
<td>spark.sql.selfJoinAutoResolveAmbiguity</td>
<td>TRUE</td>
<td>自动解决子链接中的连接条件歧义，修复bug SPARK-6231</td>
</tr>
<tr>
<td>spark.sql.retainGroupColumns</td>
<td>TRUE</td>
<td>是否保留分组列</td>
</tr>
<tr>
<td>spark.sql.pivotMaxValues</td>
<td>10000</td>
<td></td>
</tr>
<tr>
<td>spark.sql.runSQLOnFiles</td>
<td>TRUE</td>
<td>当为true,在sql查询时，能够使用dataSource.path作为表(eg:”select a,b from  hdfs://xx/xx/*”)</td>
</tr>
<tr>
<td>spark.sql.codegen.wholeStage</td>
<td>TRUE</td>
<td>当为true,多个算子的整个stage将被便宜到一个java方法中</td>
</tr>
<tr>
<td>spark.sql.codegen.maxFields</td>
<td>100</td>
<td>在激活整个stage codegen之前支持的最大字段（包括嵌套字段)</td>
</tr>
<tr>
<td>spark.sql.codegen.fallback</td>
<td>TRUE</td>
<td>当为true,在整个stage的codegen,对于编译generated code 失败的query 部分，将会暂时关闭</td>
</tr>
<tr>
<td>spark.sql.codegen.maxCaseBranches</td>
<td>20</td>
<td>支持最大的codegen</td>
</tr>
<tr>
<td>spark.sql.files.maxPartitionBytes</td>
<td>128 * 1024 * 1024</td>
<td>在读取文件时，一个分区最大被读取的数量，默认值=parquet.block.size</td>
</tr>
<tr>
<td>spark.sql.files.openCostInBytes</td>
<td>4 * 1024 * 1024</td>
<td>为了测定打开一个文件的耗时，通过同时扫描配置的字节数来测定，最好是过度估计，那么小文件的分区将比具有较大文件的分区更快（首先调度</td>
</tr>
<tr>
<td>spark.sql.files.ignoreCorruptFiles</td>
<td>FALSE</td>
<td>是否自动跳过不正确的文件</td>
</tr>
<tr>
<td>spark.sql.files.maxRecordsPerFile</td>
<td>0</td>
<td>写入单个文件的最大条数，如果时0或者负数，则无限制</td>
</tr>
<tr>
<td>spark.sql.exchange.reuse</td>
<td>TRUE</td>
<td>planer是否尝试找出重复的 exchanges并复用</td>
</tr>
<tr>
<td>spark.sql.streaming.stateStore.minDeltasForSnapshot</td>
<td>10</td>
<td>在合并成快照之前需要生成的状态存储增量文件的最小数目</td>
</tr>
<tr>
<td>spark.sql.streaming.checkpointLocation</td>
<td></td>
<td>检查点数据流的查询的默认存储位置</td>
</tr>
<tr>
<td>spark.sql.streaming.minBatchesToRetain</td>
<td>100</td>
<td>流式计算最小批次长度</td>
</tr>
<tr>
<td>spark.sql.streaming.unsupportedOperationCheck</td>
<td>TRUE</td>
<td>streaming query的logical plan 检查不支持的操作</td>
</tr>
<tr>
<td>spark.sql.variable.substitute</td>
<td>TRUE</td>
<td></td>
</tr>
<tr>
<td>spark.sql.codegen.aggregate.map.twolevel.enable</td>
<td></td>
<td>启用两级聚合哈希映射。当启用时，记录将首先“插入/查找第一级、小、快的映射，然后在第一级满或无法找到键时回落到第二级、更大、较慢的映射。当禁用时，记录直接进入第二级。默认为真</td>
</tr>
<tr>
<td>spark.sql.view.maxNestedViewDepth</td>
<td>100</td>
<td>嵌套视图中视图引用的最大深度。嵌套视图可以引用其他嵌套视图，依赖关系被组织在有向无环图（DAG）中。然而，DAG深度可能变得太大，导致意外的行为。此配置限制了这一点：当分析期间视图深度超过该值时，我们终止分辨率以避免潜在错误。</td>
</tr>
<tr>
<td>spark.sql.objectHashAggregate.sortBased.fallbackThreshold</td>
<td>128</td>
<td>在ObjectHashAggregateExec的情况下，当内存中哈希映射的大小增长过大时，我们将回落到基于排序的聚合。此选项为哈希映射的大小设置行计数阈值。</td>
</tr>
<tr>
<td>spark.sql.execution.useObjectHashAggregateExec</td>
<td>TRUE</td>
<td>是否使用 ObjectHashAggregateExec</td>
</tr>
<tr>
<td>spark.sql.streaming.fileSink.log.deletion</td>
<td>TRUE</td>
<td>是否删除文件流接收器中的过期日志文件</td>
</tr>
<tr>
<td>spark.sql.streaming.fileSink.log.compactInterval</td>
<td>10</td>
<td>日志文件合并阈值，然后将所有以前的文件压缩到下一个日志文件中</td>
</tr>
<tr>
<td>spark.sql.streaming.fileSink.log.cleanupDelay</td>
<td>10min</td>
<td>保证一个日志文件被所有用户可见的时长</td>
</tr>
<tr>
<td>spark.sql.streaming.fileSource.log.deletion</td>
<td>TRUE</td>
<td>是否删除文件流源中过期的日志文件</td>
</tr>
<tr>
<td>spark.sql.streaming.fileSource.log.compactInterval</td>
<td>10</td>
<td>日志文件合并阈值，然后将所有以前的文件压缩到下一个日志文件中</td>
</tr>
<tr>
<td>spark.sql.streaming.fileSource.log.cleanupDelay</td>
<td>10min</td>
<td>保证一个日志文件被所有用户可见的时长</td>
</tr>
<tr>
<td>spark.sql.streaming.schemaInference</td>
<td>FALSE</td>
<td>基于文件的流,是否推断它的模式</td>
</tr>
<tr>
<td>spark.sql.streaming.pollingDelay</td>
<td>10L(MILLISECONDS)</td>
<td>在没有数据可用时延迟查询新数据多长时间</td>
</tr>
<tr>
<td>spark.sql.streaming.noDataProgressEventInterval</td>
<td>10000L(MILLISECONDS)</td>
<td>在没有数据的情况下，在两个进度事件之间等待时间</td>
</tr>
<tr>
<td>spark.sql.streaming.metricsEnabled</td>
<td>FALSE</td>
<td>是否为活动流查询报告DoopWalth/CODAHALE度量</td>
</tr>
<tr>
<td>spark.sql.streaming.numRecentProgressUpdates</td>
<td>100</td>
<td>streaming query 保留的进度更新数量</td>
</tr>
<tr>
<td>spark.sql.statistics.ndv.maxError</td>
<td>0.05</td>
<td>生成列级统计量时超对数G+++算法允许的最大估计误差</td>
</tr>
<tr>
<td>spark.sql.cbo.enabled</td>
<td>FALSE</td>
<td>在设定true时启用CBO来估计计划统计信息</td>
</tr>
<tr>
<td>spark.sql.cbo.joinReorder.enabled</td>
<td>FALSE</td>
<td>Enables join reorder in CBO.</td>
</tr>
<tr>
<td>spark.sql.cbo.joinReorder.dp.threshold</td>
<td>12</td>
<td>The maximum number of joined nodes allowed in the dynamic programming  algorithm</td>
</tr>
<tr>
<td>spark.sql.cbo.joinReorder.card.weight</td>
<td>0.07</td>
<td>The weight of cardinality (number of rows) for plan cost comparison in  join reorder: rows * weight + size * (1 - weight)</td>
</tr>
<tr>
<td>spark.sql.cbo.joinReorder.dp.star.filter</td>
<td>FALSE</td>
<td>Applies star-join filter heuristics to cost based join enumeration</td>
</tr>
<tr>
<td>spark.sql.cbo.starSchemaDetection</td>
<td>FALSE</td>
<td>When true, it enables join reordering based on star schema detection</td>
</tr>
<tr>
<td>spark.sql.cbo.starJoinFTRatio</td>
<td>0.9</td>
<td>Specifies the upper limit of the ratio between the largest fact  tables for a star join to be considered</td>
</tr>
<tr>
<td>spark.sql.session.timeZone</td>
<td>TimeZone.getDefault.getID</td>
<td>时间时区</td>
</tr>
<tr>
<td>spark.sql.windowExec.buffer.in.memory.threshold</td>
<td>4096</td>
<td>窗口操作符保证存储在内存中的行数的阈值</td>
</tr>
<tr>
<td>spark.sql.windowExec.buffer.spill.threshold</td>
<td>spark.sql.windowExec.buffer.in.memory.threshold</td>
<td>窗口操作符溢出的行数的阈值</td>
</tr>
<tr>
<td>spark.sql.sortMergeJoinExec.buffer.in.memory.threshold</td>
<td>Int.MaxValue</td>
<td>由sortMergeJoin运算符保证存储在内存中的行数的阈值</td>
</tr>
<tr>
<td>spark.sql.sortMergeJoinExec.buffer.spill.threshold</td>
<td>spark.sql.sortMergeJoinExec.buffer.in.memory.threshold</td>
<td>由排序合并连接运算符溢出的行数的阈值</td>
</tr>
<tr>
<td>spark.sql.cartesianProductExec.buffer.in.memory.threshold</td>
<td>4096</td>
<td>笛卡尔乘积算子保证存储在内存中的行数的阈值</td>
</tr>
<tr>
<td>spark.sql.cartesianProductExec.buffer.spill.threshold</td>
<td>spark.sql.cartesianProductExec.buffer.in.memory.threshold</td>
<td>笛卡尔乘积算子溢出的行数阈值</td>
</tr>
<tr>
<td>spark.sql.redaction.options.regex</td>
<td>(?i)url.r</td>
<td></td>
</tr>
</tbody></table>

    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>HF
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="http://example.com/2020/05/17/SparkSql%E8%B0%83%E4%BC%98/" title="SparkSql调优">http://example.com/2020/05/17/SparkSql调优/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Spark/" rel="tag"># Spark</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/05/02/%E6%95%B0%E4%BB%93%E5%B7%A5%E4%BD%9C%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E4%BC%98%E5%8C%96%E5%8F%8A%E5%87%BD%E6%95%B0/" rel="prev" title="数仓工作中常用的优化及函数">
      <i class="fa fa-chevron-left"></i> 数仓工作中常用的优化及函数
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/05/31/SparkSql%E8%AF%AD%E6%B3%95%E8%AF%8D%E6%B3%95%E8%A7%A3%E6%9E%90/" rel="next" title="SparkSql语法词法解析">
      SparkSql语法词法解析 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%B8%80-Spark%E7%9A%84%E5%8A%A8%E6%80%81%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D"><span class="nav-number">1.</span> <span class="nav-text">一 Spark的动态资源分配</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-%E9%97%AE%E9%A2%98%E8%83%8C%E6%99%AF"><span class="nav-number">1.1.</span> <span class="nav-text">1 问题背景:</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90"><span class="nav-number">1.2.</span> <span class="nav-text">2 原理分析</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-Executor%E7%9A%84%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F"><span class="nav-number">1.2.1.</span> <span class="nav-text">2.1 Executor的生命周期</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BA%94-%E5%8F%82%E6%95%B0"><span class="nav-number">2.</span> <span class="nav-text">五: 参数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8F%90%E4%BA%A4"><span class="nav-number">2.0.0.1.</span> <span class="nav-text">提交:</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#yarn%E5%92%8Clog"><span class="nav-number">2.0.1.</span> <span class="nav-text">yarn和log:</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%98%9F%E5%88%97"><span class="nav-number">2.0.2.</span> <span class="nav-text">队列:</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8A%A8%E6%80%81%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D"><span class="nav-number">2.0.3.</span> <span class="nav-text">动态资源分配:</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Executor-task-cores-yarn%E7%AD%89%E5%8F%82%E6%95%B0"><span class="nav-number">2.0.4.</span> <span class="nav-text">Executor,task,cores,yarn等参数:</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8A%A8%E6%80%81%E5%88%86%E5%8C%BA-hive"><span class="nav-number">2.0.5.</span> <span class="nav-text">动态分区(hive)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B9%BF%E6%92%AD"><span class="nav-number">2.0.6.</span> <span class="nav-text">广播:</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#shuffle%E8%B0%83%E4%BC%98"><span class="nav-number">2.0.7.</span> <span class="nav-text">shuffle调优</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#sparksql%E7%9A%84%E5%B0%8F%E6%96%87%E4%BB%B6%E5%90%88%E5%B9%B6"><span class="nav-number">2.0.8.</span> <span class="nav-text">sparksql的小文件合并</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-sql%E6%9C%89shuffle%E9%98%B6%E6%AE%B5"><span class="nav-number">2.0.8.1.</span> <span class="nav-text">1 sql有shuffle阶段</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-sql%E6%97%A0shuffle%E9%98%B6%E6%AE%B5"><span class="nav-number">2.0.8.2.</span> <span class="nav-text">2 sql无shuffle阶段</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1%E6%96%87%E4%BB%B6%E4%B8%BAparquet-orc%E6%A0%BC%E5%BC%8F"><span class="nav-number">2.0.9.</span> <span class="nav-text">1文件为parquet,orc格式</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1%E6%BA%90%E6%95%B0%E6%8D%AE%E5%B0%8F%E6%96%87%E4%BB%B6%E5%A4%9A"><span class="nav-number">2.0.9.1.</span> <span class="nav-text">1.1源数据小文件多:</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-%E6%BA%90%E6%95%B0%E6%8D%AE%E5%A4%A7%E6%96%87%E4%BB%B6"><span class="nav-number">2.0.9.2.</span> <span class="nav-text">1.2 源数据大文件</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E6%96%87%E4%BB%B6%E4%B8%BA%E5%85%B6%E4%BB%96%E6%A0%BC%E5%BC%8F"><span class="nav-number">2.0.10.</span> <span class="nav-text">2 文件为其他格式</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%85%AD-sparksql%E5%8F%82%E6%95%B0"><span class="nav-number">3.</span> <span class="nav-text">六 sparksql参数</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="HF"
      src="/images/hexo.jpg">
  <p class="site-author-name" itemprop="name">HF</p>
  <div class="site-description" itemprop="description">第二名就是头号输家</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">78</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">38</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">39</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      推荐阅读
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="http://www.54tianzhisheng.cn/tags/Flink/" title="http:&#x2F;&#x2F;www.54tianzhisheng.cn&#x2F;tags&#x2F;Flink&#x2F;" rel="noopener" target="_blank">Flink</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://nginxconfig.io/" title="https:&#x2F;&#x2F;nginxconfig.io&#x2F;" rel="noopener" target="_blank">Nginxconfig</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://linux.51yip.com/" title="http:&#x2F;&#x2F;linux.51yip.com&#x2F;" rel="noopener" target="_blank">Linux命令手册</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://echarts.baidu.com/index.html" title="https:&#x2F;&#x2F;echarts.baidu.com&#x2F;index.html" rel="noopener" target="_blank">echarts可视化库</a>
        </li>
    </ul>
  </div>
<div style="">
  <canvas id="canvas" style="width:60%;">当前浏览器不支持canvas，请更换浏览器后再试</canvas>
</div>
<script>
(function(){

   var digit=
    [
        [
            [0,0,1,1,1,0,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,0,1,1,0],
            [0,0,1,1,1,0,0]
        ],//0
        [
            [0,0,0,1,1,0,0],
            [0,1,1,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [1,1,1,1,1,1,1]
        ],//1
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,1,1],
            [1,1,1,1,1,1,1]
        ],//2
        [
            [1,1,1,1,1,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//3
        [
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,0],
            [0,0,1,1,1,1,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,1,1,0],
            [1,1,1,1,1,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,1]
        ],//4
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,1,1,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//5
        [
            [0,0,0,0,1,1,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//6
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0]
        ],//7
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//8
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,1,1,0,0,0,0]
        ],//9
        [
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0],
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0]
        ]//:
    ];

var canvas = document.getElementById('canvas');

if(canvas.getContext){
    var cxt = canvas.getContext('2d');
    //声明canvas的宽高
    var H = 100,W = 700;
    canvas.height = H;
    canvas.width = W;
    cxt.fillStyle = '#f00';
    cxt.fillRect(10,10,50,50);

    //存储时间数据
    var data = [];
    //存储运动的小球
    var balls = [];
    //设置粒子半径
    var R = canvas.height/20-1;
    (function(){
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        //存储时间数字，由十位小时、个位小时、冒号、十位分钟、个位分钟、冒号、十位秒钟、个位秒钟这7个数字组成
        data.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
    })();

    /*生成点阵数字*/
    function renderDigit(index,num){
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    cxt.beginPath();
                    cxt.arc(14*(R+2)*index + j*2*(R+1)+(R+1),i*2*(R+1)+(R+1),R,0,2*Math.PI);
                    cxt.closePath();
                    cxt.fill();
                }
            }
        }
    }

    /*更新时钟*/
    function updateDigitTime(){
        var changeNumArray = [];
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        var NewData = [];
        NewData.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
        for(var i = data.length-1; i >=0 ; i--){
            //时间发生变化
            if(NewData[i] !== data[i]){
                //将变化的数字值和在data数组中的索引存储在changeNumArray数组中
                changeNumArray.push(i+'_'+(Number(data[i])+1)%10);
            }
        }
        //增加小球
        for(var i = 0; i< changeNumArray.length; i++){
            addBalls.apply(this,changeNumArray[i].split('_'));
        }
        data = NewData.concat();
    }

    /*更新小球状态*/
    function updateBalls(){
        for(var i = 0; i < balls.length; i++){
            balls[i].stepY += balls[i].disY;
            balls[i].x += balls[i].stepX;
            balls[i].y += balls[i].stepY;
            if(balls[i].x > W + R || balls[i].y > H + R){
                balls.splice(i,1);
                i--;
            }
        }
    }

    /*增加要运动的小球*/
    function addBalls(index,num){
        var numArray = [1,2,3];
        var colorArray =  ["#3BE","#09C","#A6C","#93C","#9C0","#690","#FB3","#F80","#F44","#C00"];
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    var ball = {
                        x:14*(R+2)*index + j*2*(R+1)+(R+1),
                        y:i*2*(R+1)+(R+1),
                        stepX:Math.floor(Math.random() * 4 -2),
                        stepY:-2*numArray[Math.floor(Math.random()*numArray.length)],
                        color:colorArray[Math.floor(Math.random()*colorArray.length)],
                        disY:1
                    };
                    balls.push(ball);
                }
            }
        }
    }

    /*渲染*/
    function render(){
        //重置画布宽度，达到清空画布的效果
        canvas.height = 100;
        //渲染时钟
        for(var i = 0; i < data.length; i++){
            renderDigit(i,data[i]);
        }
        //渲染小球
        for(var i = 0; i < balls.length; i++){
            cxt.beginPath();
            cxt.arc(balls[i].x,balls[i].y,R,0,2*Math.PI);
            cxt.fillStyle = balls[i].color;
            cxt.closePath();
            cxt.fill();
        }
    }

    clearInterval(oTimer);
    var oTimer = setInterval(function(){
        //更新时钟
        updateDigitTime();
        //更新小球状态
        updateBalls();
        //渲染
        render();
    },50);
}

})();
</script>
      </div>

    
          <script type="text/javascript" charset="utf-8" src="/js/tagcloud.js"></script>
         <script type="text/javascript" charset="utf-8" src="/js/tagcanvas.js"></script>
           <div class="widget-wrap">
        <h3 class="widget-title">Tag Cloud</h3>
        <div id="myCanvasContainer" class="widget tagcloud">
            <canvas width="250" height="250" id="resCanvas" style="width=100%">
                <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Ai/" rel="tag">Ai</a><span class="tag-list-count">7</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Azkaban/" rel="tag">Azkaban</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Blog/" rel="tag">Blog</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ClouderaManager/" rel="tag">ClouderaManager</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ElSearch/" rel="tag">ElSearch</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Flink/" rel="tag">Flink</a><span class="tag-list-count">7</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Flume/" rel="tag">Flume</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Git/" rel="tag">Git</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hadoop/" rel="tag">Hadoop</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hbase/" rel="tag">Hbase</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hdfs/" rel="tag">Hdfs</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hexo/" rel="tag">Hexo</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hive/" rel="tag">Hive</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hue/" rel="tag">Hue</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Impala/" rel="tag">Impala</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Java/" rel="tag">Java</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Jvm/" rel="tag">Jvm</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Kafka/" rel="tag">Kafka</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Kettle/" rel="tag">Kettle</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Kudu/" rel="tag">Kudu</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linux/" rel="tag">Linux</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Livy/" rel="tag">Livy</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/MapReduce/" rel="tag">MapReduce</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Mysql/" rel="tag">Mysql</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Oozie/" rel="tag">Oozie</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python/" rel="tag">Python</a><span class="tag-list-count">8</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Redis/" rel="tag">Redis</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Scala/" rel="tag">Scala</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Shell/" rel="tag">Shell</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Spark/" rel="tag">Spark</a><span class="tag-list-count">14</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Sqoop/" rel="tag">Sqoop</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Web/" rel="tag">Web</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Yarn/" rel="tag">Yarn</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ZK/" rel="tag">ZK</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E5%8F%AF%E8%A7%86%E5%8C%96/" rel="tag">数据分析与可视化</a><span class="tag-list-count">5</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/" rel="tag">数据库</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E4%B8%8E%E5%88%86%E6%9E%90/" rel="tag">数据挖掘与分析</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/" rel="tag">数据结构与算法</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习与深度学习</a><span class="tag-list-count">2</span></li></ul>
            </canvas>
        </div>
    </div>
    

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright" style=" text-align:center;">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">HF</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">1.2m</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">18:46</span>
</div>

  <!-- 网站运行时间的设置 -->
<div class="run_time" style=" text-align:center;">
  <span id="timeDate">载入天数...</span><span id="times">载入时分秒...</span>
  <script>
    var now = new Date(); 
    function createtime() { 
        var grt= new Date("07/23/2017 10:00:00");//此处修改你的建站时间或者网站上线时间 
        now.setTime(now.getTime()+250); 
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days); 
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours); 
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum); 
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;} 
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum); 
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;} 
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 "; 
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒"; 
    } 
    setInterval("createtime()",250);
  </script>
</div>
        
<div class="busuanzi-count" style=" text-align:center;">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      我的第 <span class="busuanzi-value" id="busuanzi_value_site_uv"></span> 位朋友，
    </span>
  



  
    <span class="site-pv">
      历经 <span class="busuanzi-value" id="busuanzi_value_site_pv"></span> 次回眸才与你相遇
    </span>
  
</div>










      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>

<script src="/js/bookmark.js"></script>


  <script defer src="/lib/three/three.min.js"></script>
    <script defer src="/lib/three/canvas_lines.min.js"></script>


  




  
<script src="/js/local-search.js"></script>













  

  
<div class="moon-menu">
  <div class="moon-menu-items">
    
    <div class="moon-menu-item" onclick="back2bottom()">
      <i class='fas fa-chevron-down'></i>    </div>
    
    <div class="moon-menu-item" onclick="back2top()">
      <i class='fas fa-chevron-up'></i>    </div>
    
  </div>
  <div class="moon-menu-button">
    <svg class="moon-menu-bg">
      <circle class="moon-menu-cricle" cx="50%" cy="50%" r="44%"></circle>
      <circle class="moon-menu-border" cx="50%" cy="50%" r="48%"></circle>
    </svg>
    <div class="moon-menu-content">
      <div class="moon-menu-icon"><i class='fas fa-ellipsis-v'></i></div>
      <div class="moon-menu-text"></div>
    </div>
  </div>
</div><script src="/js/injector.js"></script>
</body>
</html>
<!-- 页面点击小红心 -->
<script type="text/javascript" src="/js/clicklove.js"></script>
<!-- 雪花特效 -->
<script type="text/javascript" src="/js/jquery.js"></script>
<script type="text/javascript" src="/js/jquery.min.js"></script>
<script type="text/javascript" src="/js/snow.js"></script>