<!DOCTYPE html>












  


<html class="theme-next mist use-motion" lang="zh-CN">

<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">




  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link rel="stylesheet" href="/lib/pace/pace-theme-bounce.min.css?v=1.0.2">













  <meta name="baidu-site-verification" content="true">



  
  
  <link rel="stylesheet" href="/lib/fancybox/source/jquery.fancybox.css">







<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2">

<link rel="stylesheet" href="/css/main.css?v=7.1.1">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.1.1">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.1.1">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.1.1">


  <link rel="mask-icon" href="/images/logo.svg?v=7.1.1" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '7.1.1',
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":true,"dimmer":false},
    back2top: true,
    back2top_sidebar: true,
    fancybox: true,
    fastclick: true,
    lazyload: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"shrinkIn","post_header":"slideLeftIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideDownIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="Hivehive中所有查询除了select  from table 其他都要通过MapReduce方式执行 即使只有一行一列 如不是select  from table 也要查询8,9秒. 1. 数据仓库1.1. 基本概念英文名称为Data Warehouse，可简写为DW或DWH。数据仓库的目的是构建面向分析的集成化数据环境，为企业提供决策支持（Decision Support）。 数据仓库是存">
<meta name="keywords" content="BigData">
<meta property="og:type" content="article">
<meta property="og:title" content="Hive">
<meta property="og:url" content="https://manzhong.github.io/2017/07/07/Hive.html">
<meta property="og:site_name" content="春雨里洗过的太阳">
<meta property="og:description" content="Hivehive中所有查询除了select  from table 其他都要通过MapReduce方式执行 即使只有一行一列 如不是select  from table 也要查询8,9秒. 1. 数据仓库1.1. 基本概念英文名称为Data Warehouse，可简写为DW或DWH。数据仓库的目的是构建面向分析的集成化数据环境，为企业提供决策支持（Decision Support）。 数据仓库是存">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://manzhong.github.io/images/hive/1561705874041.png">
<meta property="og:image" content="https://manzhong.github.io/images/hive/1561705895292.png">
<meta property="og:image" content="https://manzhong.github.io/images/hive/1561705918286.png">
<meta property="og:image" content="https://manzhong.github.io/images/hive/1561705940248.png">
<meta property="og:image" content="https://manzhong.github.io/images/hive/1561705959607.png">
<meta property="og:image" content="http://ppw6n93dt.bkt.clouddn.com/8dda7bfdfab0655e99a3c3b17afc422e.png">
<meta property="og:image" content="https://manzhong.github.io/images/hive/wps1.jpg">
<meta property="og:image" content="https://manzhong.github.io/images/hive/wps2.jpg">
<meta property="og:image" content="https://manzhong.github.io/images/hive/wps3.jpg">
<meta property="og:updated_time" content="2020-03-15T14:45:49.934Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Hive">
<meta name="twitter:description" content="Hivehive中所有查询除了select  from table 其他都要通过MapReduce方式执行 即使只有一行一列 如不是select  from table 也要查询8,9秒. 1. 数据仓库1.1. 基本概念英文名称为Data Warehouse，可简写为DW或DWH。数据仓库的目的是构建面向分析的集成化数据环境，为企业提供决策支持（Decision Support）。 数据仓库是存">
<meta name="twitter:image" content="https://manzhong.github.io/images/hive/1561705874041.png">



  <link rel="alternate" href="/atom.xml" title="春雨里洗过的太阳" type="application/atom+xml">



  
  
  <link rel="canonical" href="https://manzhong.github.io/2017/07/07/Hive">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>Hive | 春雨里洗过的太阳</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>
 
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">春雨里洗过的太阳</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-bookmark"></i> <br>分类</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-about">

    
    
    
      
    

    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>关于</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-互动">

    
    
    
      
    

    

    <a href="/guestbook/" rel="section"><i class="menu-item-icon fa fa-fw fa-comments"></i> <br>互动</a>

  </li>

      
      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>搜索</a>
        </li>
      
    </ul>
  

  
    

  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://manzhong.github.io/2017/07/07/Hive.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="丨HF丨">
      <meta itemprop="description" content="第二名就是头号输家!!!">
      <meta itemprop="image" content="/images/hexo.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="春雨里洗过的太阳">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Hive

              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2017-07-07 10:18:23" itemprop="dateCreated datePublished" datetime="2017-07-07T10:18:23+08:00">2017-07-07</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2020-03-15 22:45:49" itemprop="dateModified" datetime="2020-03-15T22:45:49+08:00">2020-03-15</time>
              
            
          </span>

          

          
            
            
          

          
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">本文字数：</span>
                
                <span title="本文字数">37k</span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">33 分钟</span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="Hive"><a href="#Hive" class="headerlink" title="Hive"></a>Hive</h1><p>hive中所有查询除了select <em> from table 其他都要通过MapReduce方式执行 即使只有一行一列 如不是select </em> from table 也要查询8,9秒.</p>
<h2 id="1-数据仓库"><a href="#1-数据仓库" class="headerlink" title="1. 数据仓库"></a>1. 数据仓库</h2><h3 id="1-1-基本概念"><a href="#1-1-基本概念" class="headerlink" title="1.1. 基本概念"></a>1.1. 基本概念</h3><p>英文名称为Data Warehouse，可简写为DW或DWH。数据仓库的目的是构建面向分析的集成化数据环境，为企业提供决策支持（Decision Support）。</p>
<p>数据仓库是存数据的，企业的各种数据往里面存，主要目的是为了分析有效数据，后续会基于它产出供分析挖掘的数据，或者数据应用需要的数据，如企业的分析性报告和各类报表等。 </p>
<p>可以理解为：<code>面向分析的存储系统</code>。 </p>
<h3 id="1-2-主要特征"><a href="#1-2-主要特征" class="headerlink" title="1.2. 主要特征"></a>1.2. 主要特征</h3><p>数据仓库是面向主题的（Subject-Oriented ）、集成的（Integrated）、非易失的（Non-Volatile）和时变的（Time-Variant ）数据集合，用以支持管理决策。</p>
<h4 id="1-2-1-面向主题"><a href="#1-2-1-面向主题" class="headerlink" title="1.2.1. 面向主题"></a>1.2.1. 面向主题</h4><p> 数据仓库是面向主题的,数据仓库通过一个个主题域将多个业务系统的数据加载到一起，为了各个主题（如：用户、订单、商品等）进行分析而建，操作型数据库是为了支撑各种业务而建立。 </p>
<h4 id="1-2-2-集成性"><a href="#1-2-2-集成性" class="headerlink" title="1.2.2. 集成性"></a>1.2.2. 集成性</h4><p>数据仓库会将不同源数据库中的数据汇总到一起,数据仓库中的综合数据不能从原有的数据库系统直接得到。因此在数据进入数据仓库之前，必然要经过统一与整合，这一步是数据仓库建设中最关键、最复杂的一步(ETL)，要统一源数据中所有矛盾之处，如字段的同名异义、异名同义、单位不统一、字长不一致，等等。</p>
<h4 id="1-2-3-非易失性"><a href="#1-2-3-非易失性" class="headerlink" title="1.2.3. 非易失性"></a>1.2.3. 非易失性</h4><p>操作型数据库主要服务于日常的业务操作，使得数据库需要不断地对数据实时更新，以便迅速获得当前最新数据，不至于影响正常的业务运作。</p>
<p>在数据仓库中只要保存过去的业务数据，不需要每一笔业务都实时更新数据仓库，而是根据商业需要每隔一段时间把一批较新的数据导入数据仓库。<br>数据仓库的数据反映的是一段相当长的时间内历史数据的内容，是不同时点的数据库的集合，以及基于这些快照进行统计、综合和重组的导出数据。数据仓库中的数据一般仅执行查询操作，很少会有删除和更新。但是需定期加载和刷新数据。 </p>
<h4 id="1-2-4-时变性"><a href="#1-2-4-时变性" class="headerlink" title="1.2.4. 时变性"></a>1.2.4. 时变性</h4><p>数据仓库包含各种粒度的历史数据。数据仓库中的数据可能与某个特定日期、星期、月份、季度或者年份有关。数据仓库的目的是通过分析企业过去一段时间业务的经营状况，挖掘其中隐藏的模式。虽然数据仓库的用户不能修改数据，但并不是说数据仓库的数据是永远不变的。分析的结果只能反映过去的情况，当业务变化后，挖掘出的模式会失去时效性。因此数据仓库的数据需要定时更新，以适应决策的需要。</p>
<h3 id="1-3-数据库与数据仓库的区别"><a href="#1-3-数据库与数据仓库的区别" class="headerlink" title="1.3. 数据库与数据仓库的区别"></a>1.3. 数据库与数据仓库的区别</h3><p>数据库与数据仓库的区别实际讲的是 <code>OLTP</code> 与 <code>OLAP</code> 的区别。</p>
<p>操作型处理，叫联机事务处理 OLTP（On-Line Transaction Processing，），也可以称面向交易的处理系统，它是针对具体业务在数据库联机的日常操作，通常对少数记录进行查询、修改。用户较为关心操作的响应时间、数据的安全性、完整性和并发支持的用户数等问题。传统的数据库系统作为数据管理的主要手段，主要用于操作型处理。</p>
<p>分析型处理，叫联机分析处理 OLAP（On-Line Analytical Processing）一般针对某些主题的历史数据进行分析，支持 管理决策。</p>
<p>首先要明白，数据仓库的出现，并不是要取代数据库。</p>
<ul>
<li>数据库是面向事务的设计，数据仓库是面向主题设计的。</li>
<li>数据库一般存储业务数据，数据仓库存储的一般是历史数据。</li>
<li>数据库设计是尽量避免冗余，一般针对某一业务应用进行设计，比如一张简单的User表，记录用户名、密码等简单数据即可，符合业务应用，但是不符合分析。数据仓库在设计是有意引入冗余，依照分析需求，分析维度、分析指标进行设计。</li>
<li>数据库是为捕获数据而设计，数据仓库是为分析数据而设计。</li>
</ul>
<p><strong>数据仓库，是在数据库已经大量存在的情况下，为了进一步挖掘数据资源、为了决策需要而产生的，它决不是所谓的“大型数据库”。</strong></p>
<h3 id="1-4-数仓的分层架构"><a href="#1-4-数仓的分层架构" class="headerlink" title="1.4. 数仓的分层架构"></a>1.4. 数仓的分层架构</h3><p>按照数据流入流出的过程，数据仓库架构可分为三层——源数据、数据仓库、数据应用。</p>
<p><img src="/images/hive/1561705874041.png" alt="img">    </p>
<p>数据仓库的数据来源于不同的源数据，并提供多样的数据应用，数据自下而上流入数据仓库后向上层开放应用，而数据仓库只是中间集成化数据管理的一个平台。</p>
<ul>
<li><code>源数据层（ODS）</code>：此层数据无任何更改，直接沿用外围系统数据结构和数据，不对外开放；为临时存储层，是接口数据的临时存储区域，为后一步的数据处理做准备。</li>
<li><code>数据仓库层（DW）</code>：也称为细节层，DW层的数据应该是一致的、准确的、干净的数据，即对源系统数据进行了清洗（去除了杂质）后的数据。</li>
<li><code>数据应用层（DA或APP）</code>：前端应用直接读取的数据源；根据报表、专题分析需求而计算生成的数据。</li>
</ul>
<p>数据仓库从各数据源获取数据及在数据仓库内的数据转换和流动都可以认为是ETL（抽取Extra, 转化Transfer, 装载Load）的过程，ETL是数据仓库的流水线，也可以认为是数据仓库的血液，它维系着数据仓库中数据的新陈代谢，而数据仓库日常的管理和维护工作的大部分精力就是保持ETL的正常和稳定。</p>
<h5 id="为什么要对数据仓库分层？"><a href="#为什么要对数据仓库分层？" class="headerlink" title="为什么要对数据仓库分层？"></a>为什么要对数据仓库分层？</h5><p>用空间换时间，通过大量的预处理来提升应用系统的用户体验（效率），因此数据仓库会存在大量冗余的数据；不分层的话，如果源业务系统的业务规则发生变化将会影响整个数据清洗过程，工作量巨大。</p>
<p>通过数据分层管理可以简化数据清洗的过程，因为把原来一步的工作分到了多个步骤去完成，相当于把一个复杂的工作拆成了多个简单的工作，把一个大的黑盒变成了一个白盒，每一层的处理逻辑都相对简单和容易理解，这样我们比较容易保证每一个步骤的正确性，当数据发生错误的时候，往往我们只需要局部调整某个步骤即可。</p>
<h3 id="1-5-数仓的元数据管理"><a href="#1-5-数仓的元数据管理" class="headerlink" title="1.5. 数仓的元数据管理"></a>1.5. 数仓的元数据管理</h3><p>元数据（Meta Date），主要记录数据仓库中模型的定义、各层级间的映射关系、监控数据仓库的数据状态及ETL的任务运行状态。一般会通过元数据资料库（Metadata Repository）来统一地存储和管理元数据，其主要目的是使数据仓库的设计、部署、操作和管理能达成协同和一致。</p>
<p>元数据是数据仓库管理系统的重要组成部分，元数据管理是企业级数据仓库中的关键组件，贯穿数据仓库构建的整个过程，直接影响着数据仓库的构建、使用和维护。</p>
<ul>
<li><p>构建数据仓库的主要步骤之一是ETL。这时元数据将发挥重要的作用，它定义了源数据系统到数据仓库的映射、数据转换的规则、数据仓库的逻辑结构、数据更新的规则、数据导入历史记录以及装载周期等相关内容。数据抽取和转换的专家以及数据仓库管理员正是通过元数据高效地构建数据仓库。</p>
</li>
<li><p>用户在使用数据仓库时，通过元数据访问数据，明确数据项的含义以及定制报表。</p>
</li>
<li><p>数据仓库的规模及其复杂性离不开正确的元数据管理，包括增加或移除外部数据源，改变数据清洗方法，控制出错的查询以及安排备份等。</p>
<p><img src="/images/hive/1561705895292.png" alt="img">    </p>
</li>
</ul>
<p>元数据可分为技术元数据和业务元数据。技术元数据为开发和管理数据仓库的IT 人员使用，它描述了与数据仓库开发、管理和维护相关的数据，包括数据源信息、数据转换描述、数据仓库模型、数据清洗与更新规则、数据映射和访问权限等。而业务元数据为管理层和业务分析人员服务，从业务角度描述数据，包括商务术语、数据仓库中有什么数据、数据的位置和数据的可用性等，帮助业务人员更好地理解数据仓库中哪些数据是可用的以及如何使用。</p>
<p>由上可见，元数据不仅定义了数据仓库中数据的模式、来源、抽取和转换规则等，而且是整个数据仓库系统运行的基础，元数据把数据仓库系统中各个松散的组件联系起来，组成了一个有机的整体。</p>
<h2 id="2-Hive-的基本概念"><a href="#2-Hive-的基本概念" class="headerlink" title="2. Hive 的基本概念"></a>2. Hive 的基本概念</h2><h3 id="2-1-Hive-简介"><a href="#2-1-Hive-简介" class="headerlink" title="2.1. Hive 简介"></a>2.1. Hive 简介</h3><h5 id="什么是-Hive"><a href="#什么是-Hive" class="headerlink" title="什么是 Hive"></a>什么是 Hive</h5><p>Hive是基于Hadoop的一个数据仓库工具，可以将<strong>结构化的数据</strong>文件映射为一张数据库表，并提供类SQL查询功能。</p>
<p>其本质是将SQL转换为MapReduce的任务进行运算，底层由HDFS来提供数据的存储，说白了hive可以理解为一个将SQL转换为MapReduce的任务的工具，甚至更进一步可以说hive就是一个MapReduce的客户端</p>
<h5 id="为什么使用-Hive"><a href="#为什么使用-Hive" class="headerlink" title="为什么使用 Hive"></a>为什么使用 Hive</h5><ul>
<li>采用类SQL语法去操作数据，提供快速开发的能力。</li>
<li>避免了去写MapReduce，减少开发人员的学习成本。</li>
<li>功能扩展很方便。</li>
</ul>
<h3 id="2-2-Hive-架构"><a href="#2-2-Hive-架构" class="headerlink" title="2.2. Hive 架构"></a>2.2. Hive 架构</h3><p><img src="/images/hive/1561705918286.png" alt="img">    </p>
<ul>
<li><strong>用户接口：</strong> 包括CLI、JDBC/ODBC、WebGUI。其中，CLI(command line interface)为shell命令行；JDBC/ODBC是Hive的JAVA实现，与传统数据库JDBC类似；WebGUI是通过浏览器访问Hive。</li>
<li><strong>元数据存储：</strong> 通常是存储在关系数据库如mysql/derby中。Hive 将元数据存储在数据库中。Hive 中的元数据包括表的名字，表的列和分区及其属性，表的属性（是否为外部表等），表的数据所在目录等。</li>
<li><strong>解释器、编译器、优化器、执行器:</strong> 完成HQL 查询语句从词法分析、语法分析、编译、优化以及查询计划的生成。生成的查询计划存储在HDFS 中，并在随后有MapReduce 调用执行。</li>
</ul>
<h3 id="2-3-Hive-与-Hadoop-的关系"><a href="#2-3-Hive-与-Hadoop-的关系" class="headerlink" title="2.3. Hive 与 Hadoop 的关系"></a>2.3. Hive 与 Hadoop 的关系</h3><p>Hive利用HDFS存储数据，利用MapReduce查询分析数据</p>
<p><img src="/images/hive/1561705940248.png" alt="img">    </p>
<h3 id="2-4-Hive与传统数据库对比"><a href="#2-4-Hive与传统数据库对比" class="headerlink" title="2.4. Hive与传统数据库对比"></a>2.4. Hive与传统数据库对比</h3><p>hive用于海量数据的离线数据分析</p>
<p><img src="/images/hive/1561705959607.png" alt="img">    </p>
<p>总结：hive具有sql数据库的外表，但应用场景完全不同，hive只适合用来做批量数据统计分析</p>
<h3 id="2-5-Hive-的安装"><a href="#2-5-Hive-的安装" class="headerlink" title="2.5. Hive 的安装"></a>2.5. Hive 的安装</h3><p>这里我们选用hive的版本是2.1.1<br>下载地址为：<br><a href="http://archive.apache.org/dist/hive/hive-2.1.1/apache-hive-2.1.1-bin.tar.gz" target="_blank" rel="noopener">http://archive.apache.org/dist/hive/hive-2.1.1/apache-hive-2.1.1-bin.tar.gz</a></p>
<p>下载之后，将我们的安装包上传到第三台机器的/export/softwares目录下面去</p>
<h5 id="第一步：上传并解压安装包"><a href="#第一步：上传并解压安装包" class="headerlink" title="第一步：上传并解压安装包"></a>第一步：上传并解压安装包</h5><p>将我们的hive的安装包上传到第三台服务器的/export/softwares路径下，然后进行解压</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/softwares/</span><br><span class="line">tar -zxvf apache-hive-2.1.1-bin.tar.gz -C ../servers/</span><br></pre></td></tr></table></figure>
<h5 id="第二步：安装mysql"><a href="#第二步：安装mysql" class="headerlink" title="第二步：安装mysql"></a>第二步：安装mysql</h5><p>第一步：在线安装mysql相关的软件包</p>
<p> <code>yum  install  mysql  mysql-server  mysql-devel</code></p>
<p>第二步：启动mysql的服务</p>
<p><code>/etc/init.d/mysqld start</code></p>
<p>第三步：通过mysql安装自带脚本进行设置</p>
<p><code>/usr/bin/mysql_secure_installation</code></p>
<p>第四步：进入mysql的客户端然后进行授权</p>
<p> <code>grant all privileges on *.* to &#39;root&#39;@&#39;%&#39; identified by &#39;123456&#39; with grant option;</code></p>
<p> <code>flush privileges;</code></p>
<h5 id="第三步：修改hive的配置文件"><a href="#第三步：修改hive的配置文件" class="headerlink" title="第三步：修改hive的配置文件"></a>第三步：修改hive的配置文件</h5><p>修改hive-env.sh</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/apache-hive-2.1.1-bin/conf</span><br><span class="line">cp hive-env.sh.template hive-env.sh</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">HADOOP_HOME=/export/servers/hadoop-2.7.5</span><br><span class="line">export HIVE_CONF_DIR=/export/servers/apache-hive-2.1.1-bin/conf</span><br></pre></td></tr></table></figure>
<p>修改hive-site.xml</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/apache-hive-2.1.1-bin/conf</span><br><span class="line">vim hive-site.xml</span><br></pre></td></tr></table></figure>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0" encoding="UTF-8" standalone="no"?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>123456<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://node03:3306/hive?createDatabaseIfNotExist=true&amp;amp;useSSL=false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.schema.verification<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>datanucleus.schema.autoCreateAll<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.thrift.bind.host<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>node03<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h5 id="第四步：添加mysql的连接驱动包到hive的lib目录下"><a href="#第四步：添加mysql的连接驱动包到hive的lib目录下" class="headerlink" title="第四步：添加mysql的连接驱动包到hive的lib目录下"></a>第四步：添加mysql的连接驱动包到hive的lib目录下</h5><p>hive使用mysql作为元数据存储，必然需要连接mysql数据库，所以我们添加一个mysql的连接驱动包到hive的安装目录下，然后就可以准备启动hive了</p>
<p>将我们准备好的mysql-connector-java-5.1.38.jar  这个jar包直接上传到<br><code>/export/servers/apache-hive-2.1.1-bin/lib</code>  这个目录下即可</p>
<p>至此，hive的安装部署已经完成，接下来我们来看下hive的三种交互方式</p>
<h5 id="第五步：配置hive的环境变量"><a href="#第五步：配置hive的环境变量" class="headerlink" title="第五步：配置hive的环境变量"></a>第五步：配置hive的环境变量</h5><p>node03服务器执行以下命令配置hive的环境变量</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo vim /etc/profile</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export HIVE_HOME=/export/servers/apache-hive-2.1.1-bin</span><br><span class="line">export PATH=:$HIVE_HOME/bin:$PATH</span><br></pre></td></tr></table></figure>
<h3 id="2-6-Hive-的交互方式"><a href="#2-6-Hive-的交互方式" class="headerlink" title="2.6. Hive 的交互方式"></a>2.6. Hive 的交互方式</h3><h5 id="第一种交互方式-bin-hive"><a href="#第一种交互方式-bin-hive" class="headerlink" title="第一种交互方式 bin/hive"></a>第一种交互方式 <code>bin/hive</code></h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/apache-hive-2.1.1-bin/</span><br><span class="line">bin/hive</span><br></pre></td></tr></table></figure>
<p>创建一个数据库</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> mytest;</span><br></pre></td></tr></table></figure>
<h5 id="第二种交互方式：使用sql语句或者sql脚本进行交互"><a href="#第二种交互方式：使用sql语句或者sql脚本进行交互" class="headerlink" title="第二种交互方式：使用sql语句或者sql脚本进行交互"></a>第二种交互方式：<code>使用sql语句或者sql脚本进行交互</code></h5><p>不进入hive的客户端直接执行hive的hql语句</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/apache-hive-2.1.1-bin</span><br><span class="line">bin/hive -e "create database if not exists mytest;"</span><br></pre></td></tr></table></figure>
<p>或者我们可以将我们的hql语句写成一个sql脚本然后执行</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers</span><br><span class="line">vim  hive.sql</span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> mytest;</span><br><span class="line"><span class="keyword">use</span> mytest;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> stu(<span class="keyword">id</span> <span class="built_in">int</span>,<span class="keyword">name</span> <span class="keyword">string</span>);</span><br></pre></td></tr></table></figure>
<p>通过hive -f   来执行我们的sql脚本</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/hive -f /export/servers/hive.sql</span><br></pre></td></tr></table></figure>
<h2 id="3-Hive-的基本操作"><a href="#3-Hive-的基本操作" class="headerlink" title="3. Hive 的基本操作"></a>3. Hive 的基本操作</h2><h3 id="3-1-数据库操作"><a href="#3-1-数据库操作" class="headerlink" title="3.1 数据库操作"></a>3.1 数据库操作</h3><h4 id="3-1-1-创建数据库"><a href="#3-1-1-创建数据库" class="headerlink" title="3.1.1 创建数据库"></a>3.1.1 创建数据库</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> myhive;</span><br><span class="line"><span class="keyword">use</span>  myhive;</span><br></pre></td></tr></table></figure>
<p>说明：hive的表存放位置模式是由hive-site.xml当中的一个属性指定的</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/user/hive/warehouse<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h4 id="3-1-2-创建数据库并指定位置"><a href="#3-1-2-创建数据库并指定位置" class="headerlink" title="3.1.2 创建数据库并指定位置"></a>3.1.2 创建数据库并指定位置</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> myhive2 location <span class="string">'/myhive2'</span>;</span><br></pre></td></tr></table></figure>
<h4 id="3-1-3-设置数据库键值对信息"><a href="#3-1-3-设置数据库键值对信息" class="headerlink" title="3.1.3 设置数据库键值对信息"></a>3.1.3 设置数据库键值对信息</h4><p>数据库可以有一些描述性的键值对信息，在创建时添加：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> foo <span class="keyword">with</span> dbproperties (<span class="string">'owner'</span>=<span class="string">'itcast'</span>, <span class="string">'date'</span>=<span class="string">'20190120'</span>);</span><br></pre></td></tr></table></figure>
<p>查看数据库的键值对信息： </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">describe</span> <span class="keyword">database</span> <span class="keyword">extended</span> foo;</span><br></pre></td></tr></table></figure>
<p>修改数据库的键值对信息： </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">database</span> foo <span class="keyword">set</span> dbproperties (<span class="string">'owner'</span>=<span class="string">'itheima'</span>);</span><br></pre></td></tr></table></figure>
<h4 id="3-1-4-查看数据库更多详细信息"><a href="#3-1-4-查看数据库更多详细信息" class="headerlink" title="3.1.4 查看数据库更多详细信息"></a>3.1.4 查看数据库更多详细信息</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">desc database extended  myhive2;</span><br></pre></td></tr></table></figure>
<h4 id="3-1-5-删除数据库"><a href="#3-1-5-删除数据库" class="headerlink" title="3.1.5 删除数据库"></a>3.1.5 删除数据库</h4><p>删除一个空数据库，如果数据库下面有数据表，那么就会报错</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">drop</span>  <span class="keyword">database</span>  myhive2;</span><br></pre></td></tr></table></figure>
<p>强制删除数据库，包含数据库下面的表一起删除</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">drop</span>  <span class="keyword">database</span>  myhive  <span class="keyword">cascade</span>;</span><br></pre></td></tr></table></figure>
<h3 id="3-2-数据库表操作"><a href="#3-2-数据库表操作" class="headerlink" title="3.2 数据库表操作"></a>3.2 数据库表操作</h3><h4 id="3-2-1-创建表的语法"><a href="#3-2-1-创建表的语法" class="headerlink" title="3.2.1  创建表的语法:"></a>3.2.1  创建表的语法:</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> [<span class="keyword">external</span>] <span class="keyword">table</span> [<span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span>] table_name (</span><br><span class="line">col_name data_type [<span class="keyword">comment</span> <span class="string">'字段描述信息'</span>]</span><br><span class="line">col_name data_type [<span class="keyword">comment</span> <span class="string">'字段描述信息'</span>])</span><br><span class="line">[<span class="keyword">comment</span> <span class="string">'表的描述信息'</span>]</span><br><span class="line">[partitioned <span class="keyword">by</span> (col_name data_type,...)]</span><br><span class="line">[clustered <span class="keyword">by</span> (col_name,col_name,...)]</span><br><span class="line">[sorted <span class="keyword">by</span> (col_name [<span class="keyword">asc</span>|<span class="keyword">desc</span>],...) <span class="keyword">into</span> num_buckets buckets]</span><br><span class="line">[<span class="keyword">row</span> <span class="keyword">format</span> row_format]</span><br><span class="line">[storted <span class="keyword">as</span> ....]</span><br><span class="line">[location <span class="string">'指定表的路径'</span>]</span><br></pre></td></tr></table></figure>
<p>说明：</p>
<ol>
<li><p><a href>create  table</a></p>
<p>创建一个指定名字的表。如果相同名字的表已经存在，则抛出异常；用户可以用 IF NOT EXISTS 选项来忽略这个异常。</p>
</li>
<li><p><a href>external</a></p>
<p>可以让用户创建一个外部表，在建表的同时指定一个指向实际数据的路径（LOCATION），Hive 创建内部表时，会将数据移动到数据仓库指向的路径；若创建外部表，仅记录数据所在的路径，不对数据的位置做任何改变。在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据。</p>
</li>
<li><p><a href>comment</a></p>
<p>表示注释,默认不能使用中文 </p>
</li>
<li><p><a href>partitioned by</a>  </p>
<p>表示使用表分区,一个表可以拥有一个或者多个分区，每一个分区单独存在一个目录下 .</p>
</li>
<li><p><a href>clustered by</a><br>对于每一个表分文件， Hive可以进一步组织成桶，也就是说桶是更为细粒度的数据范围划分。Hive也是 针对某一列进行桶的组织。</p>
</li>
<li><p><a href>sorted by</a></p>
<p>指定排序字段和排序规则</p>
</li>
<li><p><a href>row format</a> </p>
<p>​    指定表文件字段分隔符</p>
</li>
<li><p><a href>storted as</a>指定表文件的存储格式,   常用格式:SEQUENCEFILE, TEXTFILE, RCFILE,如果文件数据是纯文本，可以使用 STORED AS TEXTFILE。如果数据需要压缩，使用 storted  as SEQUENCEFILE。</p>
</li>
<li><p><a href><strong>location</strong></a> </p>
<p>指定表文件的存储路径</p>
</li>
</ol>
<h4 id="3-2-2-内部表的操作"><a href="#3-2-2-内部表的操作" class="headerlink" title="3.2.2 内部表的操作"></a>3.2.2 内部表的操作</h4><p>创建表时,如果没有使用external关键字,则该表是内部表（managed table） </p>
<p><a href>Hive建表字段类型</a></p>
<table>
<thead>
<tr>
<th>分类</th>
<th>类型</th>
<th>描述</th>
<th>字面量示例</th>
</tr>
</thead>
<tbody>
<tr>
<td>原始类型</td>
<td>BOOLEAN</td>
<td>true/false</td>
<td>TRUE</td>
</tr>
<tr>
<td></td>
<td>TINYINT</td>
<td>1字节的有符号整数, -128~127</td>
<td>1Y</td>
</tr>
<tr>
<td></td>
<td>SMALLINT</td>
<td>2个字节的有符号整数，-32768~32767</td>
<td>1S</td>
</tr>
<tr>
<td></td>
<td>INT</td>
<td>4个字节的带符号整数</td>
<td>1</td>
</tr>
<tr>
<td></td>
<td>BIGINT</td>
<td>8字节带符号整数</td>
<td>1L</td>
</tr>
<tr>
<td></td>
<td>FLOAT</td>
<td>4字节单精度浮点数</td>
<td>1.0</td>
</tr>
<tr>
<td></td>
<td>DOUBLE</td>
<td>8字节双精度浮点数</td>
<td>1.0</td>
</tr>
<tr>
<td></td>
<td>DEICIMAL</td>
<td>任意精度的带符号小数</td>
<td>1.0</td>
</tr>
<tr>
<td></td>
<td>STRING</td>
<td>字符串，变长</td>
<td>“a”,’b’</td>
</tr>
<tr>
<td></td>
<td>VARCHAR</td>
<td>变长字符串</td>
<td>“a”,’b’</td>
</tr>
<tr>
<td></td>
<td>CHAR</td>
<td>固定长度字符串</td>
<td>“a”,’b’</td>
</tr>
<tr>
<td></td>
<td>BINARY</td>
<td>字节数组</td>
<td>无法表示</td>
</tr>
<tr>
<td></td>
<td>TIMESTAMP</td>
<td>时间戳，毫秒值精度</td>
<td>122327493795</td>
</tr>
<tr>
<td></td>
<td>DATE</td>
<td>日期</td>
<td>‘2016-03-29’</td>
</tr>
<tr>
<td></td>
<td>INTERVAL</td>
<td>时间频率间隔</td>
<td></td>
</tr>
<tr>
<td>复杂类型</td>
<td>ARRAY</td>
<td>有序的的同类型的集合</td>
<td>array(1,2)</td>
</tr>
<tr>
<td></td>
<td>MAP</td>
<td>key-value,key必须为原始类型，value可以任意类型</td>
<td>map(‘a’,1,’b’,2)</td>
</tr>
<tr>
<td></td>
<td>STRUCT</td>
<td>字段集合,类型可以不同</td>
<td>struct(‘1’,1,1.0), named_stract(‘col1’,’1’,’col2’,1,’clo3’,1.0)</td>
</tr>
<tr>
<td></td>
<td>UNION</td>
<td>在有限取值范围内的一个值</td>
<td>create_union(1,’a’,63)</td>
</tr>
</tbody>
</table>
<p> <a href>建表入门:</a></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">use</span> myhive;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> stu(<span class="keyword">id</span> <span class="built_in">int</span>,<span class="keyword">name</span> <span class="keyword">string</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> stu <span class="keyword">values</span> (<span class="number">1</span>,<span class="string">"zhangsan"</span>);  <span class="comment">#插入数据</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> stu;</span><br></pre></td></tr></table></figure>
<p><a href>创建表并指定字段之间的分隔符</a></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span>  <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> stu2(<span class="keyword">id</span> <span class="built_in">int</span> ,<span class="keyword">name</span> <span class="keyword">string</span>) <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br></pre></td></tr></table></figure>
<p><a href>创建表并指定表文件的存放路径</a></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span>  <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> stu2(<span class="keyword">id</span> <span class="built_in">int</span> ,<span class="keyword">name</span> <span class="keyword">string</span>) <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span> location <span class="string">'/user/stu2'</span>;</span><br></pre></td></tr></table></figure>
<p><a href>根据查询结果创建表</a></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> stu3 <span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> stu2; <span class="comment"># 通过复制表结构和表内容创建新表</span></span><br></pre></td></tr></table></figure>
<p><a href>根据已经存在的表结构创建表</a></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> stu4 <span class="keyword">like</span> stu;</span><br></pre></td></tr></table></figure>
<p><a href>查询表的详细信息</a></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">desc formatted  stu2;</span><br></pre></td></tr></table></figure>
<p>. <a href>删除表</a></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> stu4;</span><br></pre></td></tr></table></figure>
<h4 id="3-2-3-外部表的操作"><a href="#3-2-3-外部表的操作" class="headerlink" title="3.2.3 外部表的操作"></a>3.2.3 外部表的操作</h4><p><a href>外部表说明</a></p>
<p>外部表因为是指定其他的hdfs路径的数据加载到表当中来，所以hive表会认为自己不完全独占这份数据，所以删除hive表的时候，数据仍然存放在hdfs当中，不会删掉.</p>
<p><a href>内部表和外部表的使用场景</a></p>
<p>每天将收集到的网站日志定期流入HDFS文本文件。在外部表（原始日志表）的基础上做大量的统计分析，用到的中间表、结果表使用内部表存储，数据通过SELECT+INSERT进入内部表。</p>
<p><a href>操作案例</a></p>
<p>分别创建老师与学生表外部表，并向表中加载数据</p>
<p><a href>创建老师表</a></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> teacher (t_id <span class="keyword">string</span>,t_name <span class="keyword">string</span>) <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br></pre></td></tr></table></figure>
<p><a href>创建学生表</a></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> student (s_id <span class="keyword">string</span>,s_name <span class="keyword">string</span>,s_birth <span class="keyword">string</span> , s_sex <span class="keyword">string</span> ) <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br></pre></td></tr></table></figure>
<p><a href>加载数据</a></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/export/servers/hivedatas/student.csv'</span> <span class="keyword">into</span> <span class="keyword">table</span> student;</span><br></pre></td></tr></table></figure>
<p><a href>加载数据并覆盖已有数据</a></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/export/servers/hivedatas/student.csv'</span> overwrite  <span class="keyword">into</span> <span class="keyword">table</span> student;</span><br></pre></td></tr></table></figure>
<p><a href>从hdfs文件系统向表中加载数据（需要提前将数据上传到hdfs文件系统）</a></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/hivedatas</span><br><span class="line">hdfs dfs -mkdir -p /hivedatas</span><br><span class="line">hdfs dfs -put techer.csv /hivedatas/</span><br><span class="line">load data inpath '/hivedatas/techer.csv' into table teacher;</span><br></pre></td></tr></table></figure>
<h4 id="3-2-4-分区表的操作"><a href="#3-2-4-分区表的操作" class="headerlink" title="3.2.4 分区表的操作"></a>3.2.4 分区表的操作</h4><p>在大数据中，最常用的一种思想就是分治，我们可以把大的文件切割划分成一个个的小的文件，这样每次操作一个小的文件就会很容易了，同样的道理，在hive当中也是支持这种思想的，就是我们可以把大的数据，按照每月，或者天进行切分成一个个的小的文件,存放在不同的文件夹中.</p>
<p><a href="/data/20180101/1.log">创建分区表语法</a></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> score(s_id <span class="keyword">string</span>,c_id <span class="keyword">string</span>, s_score <span class="built_in">int</span>) partitioned <span class="keyword">by</span> (<span class="keyword">month</span> <span class="keyword">string</span>) <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br></pre></td></tr></table></figure>
<p><a href="/data/20180101/1.log">创建一个表带多个分区</a></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> score2 (s_id <span class="keyword">string</span>,c_id <span class="keyword">string</span>, s_score <span class="built_in">int</span>) partitioned <span class="keyword">by</span> (<span class="keyword">year</span> <span class="keyword">string</span>,<span class="keyword">month</span> <span class="keyword">string</span>,<span class="keyword">day</span> <span class="keyword">string</span>) <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br></pre></td></tr></table></figure>
<p><a href="/data/20180101/1.log">加载数据到分区表中</a></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/export/servers/hivedatas/score.csv'</span> <span class="keyword">into</span> <span class="keyword">table</span> score <span class="keyword">partition</span> (<span class="keyword">month</span>=<span class="string">'201806'</span>);</span><br></pre></td></tr></table></figure>
<p><a href="/data/20180101/1.log">加载数据到多分区表中</a></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/export/servers/hivedatas/score.csv'</span> <span class="keyword">into</span> <span class="keyword">table</span> score2 <span class="keyword">partition</span>(<span class="keyword">year</span>=<span class="string">'2018'</span>,<span class="keyword">month</span>=<span class="string">'06'</span>,<span class="keyword">day</span>=<span class="string">'01'</span>);</span><br></pre></td></tr></table></figure>
<p><a href="/data/20180101/1.log">多分区表联合查询(使用 <code>union all</code>)</a></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> score <span class="keyword">where</span> <span class="keyword">month</span> = <span class="string">'201806'</span> <span class="keyword">union</span> <span class="keyword">all</span> <span class="keyword">select</span> * <span class="keyword">from</span> score <span class="keyword">where</span> <span class="keyword">month</span> = <span class="string">'201806'</span>;</span><br></pre></td></tr></table></figure>
<p><a href="/data/20180101/1.log">查看分区</a></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">show</span>  <span class="keyword">partitions</span>  score;</span><br></pre></td></tr></table></figure>
<p><a href="/data/20180101/1.log">添加一个分区</a></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> score <span class="keyword">add</span> <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201805'</span>);</span><br></pre></td></tr></table></figure>
<p><a href="/data/20180101/1.log">删除分区</a></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> score <span class="keyword">drop</span> <span class="keyword">partition</span>(<span class="keyword">month</span> = <span class="string">'201806'</span>);</span><br></pre></td></tr></table></figure>
<h4 id="3-2-5-分区表综合练习"><a href="#3-2-5-分区表综合练习" class="headerlink" title="3.2.5 分区表综合练习"></a>3.2.5 分区表综合练习</h4><p><code>需求描述</code>：</p>
<p>现在有一个文件score.csv文件，存放在集群的这个目录下/scoredatas/month=201806，这个文件每天都会生成，存放到对应的日期文件夹下面去，文件别人也需要公用，不能移动。需求，创建hive对应的表，并将数据加载到表中，进行数据统计分析，且删除表之后，数据不能删除</p>
<p><code>数据准备</code>：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -mkdir -p /scoredatas/month=201806</span><br><span class="line">hdfs dfs -put score.csv /scoredatas/month=201806/</span><br></pre></td></tr></table></figure>
<p><code>创建外部分区表，并指定文件数据存放目录</code></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create external table score4(s_id string, c_id string,s_score int) partitioned by (month string) row format delimited fields terminated by '\t' location '/scoredatas';</span><br></pre></td></tr></table></figure>
<p><code>进行表的修复</code>(<a href>建立表与数据文件之间的一个关系映射</a>)</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">msck  repair   table  score4;</span><br></pre></td></tr></table></figure>
<h4 id="3-2-6-分桶表操作"><a href="#3-2-6-分桶表操作" class="headerlink" title="3.2.6 分桶表操作"></a>3.2.6 分桶表操作</h4><p> 分桶，就是将数据按照指定的字段进行划分到多个文件当中去,<a href>分桶就是MapReduce中的分区</a>.</p>
<p><code>开启 Hive 的分桶功能</code></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.enforce.bucketing=<span class="literal">true</span>;</span><br></pre></td></tr></table></figure>
<p><code>设置 Reduce 个数</code></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> mapreduce.job.reduces=<span class="number">3</span>;</span><br></pre></td></tr></table></figure>
<p><code>创建分桶表</code></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> course (c_id <span class="keyword">string</span>,c_name <span class="keyword">string</span>,t_id <span class="keyword">string</span>) clustered <span class="keyword">by</span>(c_id) <span class="keyword">into</span> <span class="number">3</span> buckets <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br></pre></td></tr></table></figure>
<p>桶表的数据加载，由于通标的数据加载通过hdfs  dfs  -put文件或者通过load  data均不好使，只能通过insert  overwrite</p>
<p>创建普通表，并通过insert  overwriter的方式将普通表的数据通过查询的方式加载到桶表当中去</p>
<p><code>创建普通表</code></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> course_common (c_id <span class="keyword">string</span>,c_name <span class="keyword">string</span>,t_id <span class="keyword">string</span>) <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br></pre></td></tr></table></figure>
<p><code>普通表中加载数据</code></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/export/servers/hivedatas/course.csv'</span> <span class="keyword">into</span> <span class="keyword">table</span> course_common;</span><br></pre></td></tr></table></figure>
<p><code>通过insert  overwrite给桶表中加载数据</code></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> course <span class="keyword">select</span> * <span class="keyword">from</span> course_common cluster <span class="keyword">by</span>(c_id);</span><br></pre></td></tr></table></figure>
<h3 id="3-3-修改表结构"><a href="#3-3-修改表结构" class="headerlink" title="3.3 修改表结构"></a>3.3 修改表结构</h3><p><a href>重命名:</a></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span>  <span class="keyword">table</span>  old_table_name  <span class="keyword">rename</span>  <span class="keyword">to</span>  new_table_name;</span><br></pre></td></tr></table></figure>
<p>把表score4修改成score5</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> score4 <span class="keyword">rename</span> <span class="keyword">to</span> score5;</span><br></pre></td></tr></table></figure>
<p><a href>增加/修改列信息:</a></p>
<ul>
<li>查询表结构</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">desc score5;</span><br></pre></td></tr></table></figure>
<ul>
<li>添加列</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> score5 <span class="keyword">add</span> <span class="keyword">columns</span> (mycol <span class="keyword">string</span>, mysco <span class="built_in">int</span>);</span><br></pre></td></tr></table></figure>
<ul>
<li>更新列</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> score5 <span class="keyword">change</span> <span class="keyword">column</span> mysco mysconew <span class="built_in">int</span>;</span><br></pre></td></tr></table></figure>
<ul>
<li>删除表</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> score5;</span><br></pre></td></tr></table></figure>
<p>1.8. hive表中加载数据</p>
<p>直接向分区表中插入数据</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> score3 <span class="keyword">like</span> score;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> score3 <span class="keyword">partition</span>(<span class="keyword">month</span> =<span class="string">'201807'</span>) <span class="keyword">values</span> (<span class="string">'001'</span>,<span class="string">'002'</span>,<span class="string">'100'</span>);</span><br></pre></td></tr></table></figure>
<p>通过查询插入数据</p>
<p>通过load方式加载数据</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/export/servers/hivedatas/score.csv'</span> overwrite <span class="keyword">into</span> <span class="keyword">table</span> score <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201806'</span>);</span><br></pre></td></tr></table></figure>
<p>通过查询方式加载数据</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> score4 <span class="keyword">like</span> score;</span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> score4 <span class="keyword">partition</span>(<span class="keyword">month</span> = <span class="string">'201806'</span>) <span class="keyword">select</span> s_id,c_id,s_score <span class="keyword">from</span> score;</span><br></pre></td></tr></table></figure>
<h2 id="4-Hive-查询语法"><a href="#4-Hive-查询语法" class="headerlink" title="4. Hive 查询语法"></a>4. Hive 查询语法</h2><h3 id="4-1-SELECT"><a href="#4-1-SELECT" class="headerlink" title="4.1. SELECT"></a>4.1. SELECT</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> [<span class="keyword">ALL</span> | <span class="keyword">DISTINCT</span>] select_expr, select_expr, ...</span><br><span class="line"><span class="keyword">FROM</span> table_reference</span><br><span class="line">[<span class="keyword">WHERE</span> where_condition]</span><br><span class="line">[<span class="keyword">GROUP</span> <span class="keyword">BY</span> col_list [<span class="keyword">HAVING</span> condition]]</span><br><span class="line">[CLUSTER <span class="keyword">BY</span> col_list</span><br><span class="line">| [<span class="keyword">DISTRIBUTE</span> <span class="keyword">BY</span> col_list] [<span class="keyword">SORT</span> <span class="keyword">BY</span>| <span class="keyword">ORDER</span> <span class="keyword">BY</span> col_list]</span><br><span class="line">]</span><br><span class="line">[<span class="keyword">LIMIT</span> <span class="built_in">number</span>]</span><br></pre></td></tr></table></figure>
<ol>
<li>order by 会对输入做全局排序，因此只有一个reducer，会导致当输入规模较大时，需要较长的计算时间。</li>
<li>sort by不是全局排序，其在数据进入reducer前完成排序。因此，如果用sort by进行排序，并且设置mapred.reduce.tasks&gt;1，则sort by只保证每个reducer的输出有序，不保证全局有序。</li>
<li>distribute by(字段)根据指定的字段将数据分到不同的reducer，且分发算法是hash散列。</li>
<li>cluster by(字段) 除了具有distribute by的功能外，还会对该字段进行排序.</li>
</ol>
<p>因此，如果distribute 和sort字段是同一个时，此时，<code>cluster by = distribute by + sort by</code></p>
<h3 id="4-2-查询语法"><a href="#4-2-查询语法" class="headerlink" title="4.2. 查询语法"></a>4.2. 查询语法</h3><p><a href>全表查询</a></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> score;</span><br></pre></td></tr></table></figure>
<p><a href>选择特定列</a></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> s_id ,c_id <span class="keyword">from</span> score;</span><br></pre></td></tr></table></figure>
<p><a href>列别名</a></p>
<p>1）重命名一个列。<br>2）便于计算。<br>3）紧跟列名，也可以在列名和别名之间加入关键字‘AS’</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> s_id <span class="keyword">as</span> myid ,c_id <span class="keyword">from</span> score;</span><br></pre></td></tr></table></figure>
<h3 id="4-3-常用函数"><a href="#4-3-常用函数" class="headerlink" title="4.3. 常用函数"></a>4.3. 常用函数</h3><ul>
<li>求总行数（count）</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">count</span>(<span class="number">1</span>) <span class="keyword">from</span> score;</span><br></pre></td></tr></table></figure>
<ul>
<li>求分数的最大值（max）</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">max</span>(s_score) <span class="keyword">from</span> score;</span><br></pre></td></tr></table></figure>
<ul>
<li>求分数的最小值（min）</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">min</span>(s_score) <span class="keyword">from</span> score;</span><br></pre></td></tr></table></figure>
<ul>
<li>求分数的总和（sum）</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">sum</span>(s_score) <span class="keyword">from</span> score;</span><br></pre></td></tr></table></figure>
<ul>
<li>求分数的平均值（avg）</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">avg</span>(s_score) <span class="keyword">from</span> score;</span><br></pre></td></tr></table></figure>
<h3 id="4-4-LIMIT语句"><a href="#4-4-LIMIT语句" class="headerlink" title="4.4. LIMIT语句"></a>4.4. LIMIT语句</h3><p>典型的查询会返回多行数据。LIMIT子句用于限制返回的行数。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> score <span class="keyword">limit</span> <span class="number">3</span>;</span><br></pre></td></tr></table></figure>
<h3 id="4-5-WHERE语句"><a href="#4-5-WHERE语句" class="headerlink" title="4.5. WHERE语句"></a>4.5. WHERE语句</h3><ol>
<li>使用WHERE 子句，将不满足条件的行过滤掉。</li>
<li>WHERE 子句紧随 FROM 子句。</li>
<li>案例实操</li>
</ol>
<p>查询出分数大于60的数据</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> score <span class="keyword">where</span> s_score &gt; <span class="number">60</span>;</span><br></pre></td></tr></table></figure>
<p><a href>比较运算符</a></p>
<table>
<thead>
<tr>
<th>操作符</th>
<th>支持的数据类型</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>A=B</td>
<td>基本数据类型</td>
<td>如果A等于B则返回TRUE，反之返回FALSE</td>
</tr>
<tr>
<td>A&lt;=&gt;B</td>
<td>基本数据类型</td>
<td>如果A和B都为NULL，则返回TRUE，其他的和等号（=）操作符的结果一致，如果任一为NULL则结果为NULL</td>
</tr>
<tr>
<td>A&lt;&gt;B, A!=B</td>
<td>基本数据类型</td>
<td>A或者B为NULL则返回NULL；如果A不等于B，则返回TRUE，反之返回FALSE</td>
</tr>
<tr>
<td>A&lt;B</td>
<td>基本数据类型</td>
<td>A或者B为NULL，则返回NULL；如果A小于B，则返回TRUE，反之返回FALSE</td>
</tr>
<tr>
<td>A&lt;=B</td>
<td>基本数据类型</td>
<td>A或者B为NULL，则返回NULL；如果A小于等于B，则返回TRUE，反之返回FALSE</td>
</tr>
<tr>
<td>A&gt;B</td>
<td>基本数据类型</td>
<td>A或者B为NULL，则返回NULL；如果A大于B，则返回TRUE，反之返回FALSE</td>
</tr>
<tr>
<td>A&gt;=B</td>
<td>基本数据类型</td>
<td>A或者B为NULL，则返回NULL；如果A大于等于B，则返回TRUE，反之返回FALSE</td>
</tr>
<tr>
<td>A [NOT] BETWEEN B AND C</td>
<td>基本数据类型</td>
<td>如果A，B或者C任一为NULL，则结果为NULL。如果A的值大于等于B而且小于或等于C，则结果为TRUE，反之为FALSE。如果使用NOT关键字则可达到相反的效果。</td>
</tr>
<tr>
<td>A IS NULL</td>
<td>所有数据类型</td>
<td>如果A等于NULL，则返回TRUE，反之返回FALSE</td>
</tr>
<tr>
<td>A IS NOT NULL</td>
<td>所有数据类型</td>
<td>如果A不等于NULL，则返回TRUE，反之返回FALSE</td>
</tr>
<tr>
<td>IN(数值1, 数值2)</td>
<td>所有数据类型</td>
<td>使用 IN运算显示列表中的值</td>
</tr>
<tr>
<td>A [NOT] LIKE B</td>
<td>STRING 类型</td>
<td>B是一个SQL下的简单正则表达式，如果A与其匹配的话，则返回TRUE；反之返回FALSE。B的表达式说明如下：‘x%’表示A必须以字母‘x’开头，‘%x’表示A必须以字母’x’结尾，而‘%x%’表示A包含有字母’x’,可以位于开头，结尾或者字符串中间。如果使用NOT关键字则可达到相反的效果。</td>
</tr>
<tr>
<td>A RLIKE B, A REGEXP B</td>
<td>STRING</td>
<td>类型    B是一个正则表达式，如果A与其匹配，则返回TRUE；反之返回FALSE。匹配使用的是JDK中的正则表达式接口实现的，因为正则也依据其中的规则。例如，正则表达式必须和整个字符串A相匹配，而不是只需与其字符串匹配。</td>
</tr>
</tbody>
</table>
<ul>
<li>查询分数等于80的所有的数据</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> score <span class="keyword">where</span> s_score = <span class="number">80</span>;</span><br></pre></td></tr></table></figure>
<ul>
<li>查询分数在80到100的所有数据</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> score <span class="keyword">where</span> s_score <span class="keyword">between</span> <span class="number">80</span> <span class="keyword">and</span> <span class="number">100</span>;</span><br></pre></td></tr></table></figure>
<ul>
<li>查询成绩为空的所有数据</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> score <span class="keyword">where</span> s_score <span class="keyword">is</span> <span class="literal">null</span>;</span><br></pre></td></tr></table></figure>
<ul>
<li>查询成绩是80和90的数据</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> score <span class="keyword">where</span> s_score <span class="keyword">in</span>(<span class="number">80</span>,<span class="number">90</span>);</span><br></pre></td></tr></table></figure>
<h3 id="4-6-LIKE-和-RLIKE"><a href="#4-6-LIKE-和-RLIKE" class="headerlink" title="4.6. LIKE 和 RLIKE"></a>4.6. LIKE 和 RLIKE</h3><ol>
<li>使用LIKE运算选择类似的值</li>
<li>选择条件可以包含字符或数字:</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">% 代表零个或多个字符(任意个字符)。</span><br><span class="line">_ 代表一个字符。</span><br></pre></td></tr></table></figure>
<ol>
<li><p>RLIKE子句是Hive中这个功能的一个扩展，其可以通过Java的正则表达式这个更强大的语言来指定匹配条件。</p>
</li>
<li><p>案例实操</p>
<ol>
<li>查找以8开头的所有成绩</li>
</ol>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> score <span class="keyword">where</span> s_score <span class="keyword">like</span> <span class="string">'8%'</span>;</span><br></pre></td></tr></table></figure>
<ol>
<li>查找第二个数值为9的所有成绩数据</li>
</ol>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> score <span class="keyword">where</span> s_score <span class="keyword">like</span> <span class="string">'_9%'</span>;</span><br></pre></td></tr></table></figure>
<ol>
<li>查找s_id中含1的数据</li>
</ol>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> score <span class="keyword">where</span> s_id <span class="keyword">rlike</span> <span class="string">'[1]'</span>;  <span class="comment">#  like '%1%'</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="4-7-逻辑运算符"><a href="#4-7-逻辑运算符" class="headerlink" title="4.7. 逻辑运算符"></a>4.7. 逻辑运算符</h3><table>
<thead>
<tr>
<th>操作符</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr>
<td>AND</td>
<td>逻辑并</td>
</tr>
<tr>
<td>OR</td>
<td>逻辑或</td>
</tr>
<tr>
<td>NOT</td>
<td>逻辑否</td>
</tr>
</tbody>
</table>
<ul>
<li>查询成绩大于80，并且s_id是01的数据</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> score <span class="keyword">where</span> s_score &gt;<span class="number">80</span> <span class="keyword">and</span> s_id = <span class="string">'01'</span>;</span><br></pre></td></tr></table></figure>
<ul>
<li>查询成绩大于80，或者s_id  是01的数</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> score <span class="keyword">where</span> s_score &gt; <span class="number">80</span> <span class="keyword">or</span> s_id = <span class="string">'01'</span>;</span><br></pre></td></tr></table></figure>
<ul>
<li>查询s_id  不是 01和02的学生</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> score <span class="keyword">where</span> s_id <span class="keyword">not</span> <span class="keyword">in</span> (<span class="string">'01'</span>,<span class="string">'02'</span>);</span><br></pre></td></tr></table></figure>
<h3 id="4-8-分组"><a href="#4-8-分组" class="headerlink" title="4.8. 分组"></a>4.8. 分组</h3><h4 id="GROUP-BY-语句"><a href="#GROUP-BY-语句" class="headerlink" title="GROUP BY 语句"></a>GROUP BY 语句</h4><p>GROUP BY语句通常会和聚合函数一起使用，按照一个或者多个列队结果进行分组，然后对每个组执行聚合操作。<br>案例实操：</p>
<ul>
<li>计算每个学生的平均分数</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> s_id ,<span class="keyword">avg</span>(s_score) <span class="keyword">from</span> score <span class="keyword">group</span> <span class="keyword">by</span> s_id;</span><br></pre></td></tr></table></figure>
<ul>
<li>计算每个学生最高成绩</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> s_id ,<span class="keyword">max</span>(s_score) <span class="keyword">from</span> score <span class="keyword">group</span> <span class="keyword">by</span> s_id;</span><br></pre></td></tr></table></figure>
<h4 id="HAVING-语句"><a href="#HAVING-语句" class="headerlink" title="HAVING 语句"></a>HAVING 语句</h4><ol>
<li><p>having与where不同点</p>
<ol>
<li>where针对表中的列发挥作用，查询数据；having针对查询结果中的列发挥作用，筛选数据。</li>
<li>where后面不能写分组函数，而having后面可以使用分组函数。</li>
<li>having只用于group by分组统计语句。</li>
</ol>
</li>
<li><p>案例实操：</p>
<ul>
<li>求每个学生的平均分数</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> s_id ,<span class="keyword">avg</span>(s_score) <span class="keyword">from</span> score <span class="keyword">group</span> <span class="keyword">by</span> s_id;</span><br></pre></td></tr></table></figure>
<ul>
<li>求每个学生平均分数大于85的人</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> s_id ,<span class="keyword">avg</span>(s_score) avgscore <span class="keyword">from</span> score <span class="keyword">group</span> <span class="keyword">by</span> s_id <span class="keyword">having</span> avgscore &gt; <span class="number">85</span>;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="4-9-JOIN-语句"><a href="#4-9-JOIN-语句" class="headerlink" title="4.9. JOIN 语句"></a>4.9. JOIN 语句</h3><h4 id="4-9-1-等值-JOIN"><a href="#4-9-1-等值-JOIN" class="headerlink" title="4.9.1. 等值 JOIN"></a>4.9.1. 等值 JOIN</h4><p>Hive支持通常的SQL JOIN语句，但是只支持等值连接，不支持非等值连接。</p>
<p>案例操作: 查询分数对应的姓名</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> s.s_id,s.s_score,stu.s_name,stu.s_birth  <span class="keyword">from</span> score s  <span class="keyword">join</span> student stu <span class="keyword">on</span> s.s_id = stu.s_id;</span><br></pre></td></tr></table></figure>
<h4 id="4-9-2-表的别名"><a href="#4-9-2-表的别名" class="headerlink" title="4.9.2. 表的别名"></a>4.9.2. 表的别名</h4><ul>
<li><p>好处</p>
<ul>
<li>使用别名可以简化查询。</li>
<li>使用表名前缀可以提高执行效率。</li>
</ul>
</li>
<li><p>案例实操</p>
<ul>
<li>合并老师与课程表</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> techer t <span class="keyword">join</span> course c <span class="keyword">on</span> t.t_id = c.t_id;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="4-9-3-内连接"><a href="#4-9-3-内连接" class="headerlink" title="4.9.3. 内连接"></a>4.9.3. 内连接</h4><p>内连接：只有进行连接的两个表中都存在与连接条件相匹配的数据才会被保留下来。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> techer t <span class="keyword">inner</span> <span class="keyword">join</span> course c <span class="keyword">on</span> t.t_id = c.t_id;</span><br></pre></td></tr></table></figure>
<h4 id="4-9-4-左外连接"><a href="#4-9-4-左外连接" class="headerlink" title="4.9.4. 左外连接"></a>4.9.4. 左外连接</h4><p>左外连接：JOIN操作符左边表中符合WHERE子句的所有记录将会被返回。<br>查询老师对应的课程</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> techer t <span class="keyword">left</span> <span class="keyword">join</span> course c <span class="keyword">on</span> t.t_id = c.t_id;</span><br></pre></td></tr></table></figure>
<h4 id="4-9-5-右外连接"><a href="#4-9-5-右外连接" class="headerlink" title="4.9.5. 右外连接"></a>4.9.5. 右外连接</h4><p>右外连接：JOIN操作符右边表中符合WHERE子句的所有记录将会被返回。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> teacher t <span class="keyword">right</span> <span class="keyword">join</span> course c <span class="keyword">on</span> t.t_id = c.t_id;</span><br></pre></td></tr></table></figure>
<h4 id="4-9-6-多表连接"><a href="#4-9-6-多表连接" class="headerlink" title="4.9.6. 多表连接"></a>4.9.6. 多表连接</h4><p>注意：连接 n个表，至少需要n-1个连接条件。例如：连接三个表，至少需要两个连接条件。</p>
<p>多表连接查询，查询老师对应的课程，以及对应的分数，对应的学生</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> teacher t</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> course c</span><br><span class="line"><span class="keyword">on</span> t.t_id = c.t_id</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> score s</span><br><span class="line"><span class="keyword">on</span> s.c_id = c.c_id</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> student stu</span><br><span class="line"><span class="keyword">on</span> s.s_id = stu.s_id;</span><br></pre></td></tr></table></figure>
<p>大多数情况下，Hive会对每对JOIN连接对象启动一个MapReduce任务。本例中会首先启动一个MapReduce job对表techer和表course进行连接操作，然后会再启动一个MapReduce job将第一个MapReduce job的输出和表score;进行连接操作。</p>
<h3 id="4-10-排序"><a href="#4-10-排序" class="headerlink" title="4.10. 排序"></a>4.10. 排序</h3><h4 id="4-10-1-全局排序"><a href="#4-10-1-全局排序" class="headerlink" title="4.10.1. 全局排序"></a>4.10.1. 全局排序</h4><p>Order By：全局排序，一个reduce</p>
<ol>
<li><p>使用 ORDER BY 子句排序<br>ASC（ascend）: 升序（默认）<br>DESC（descend）: 降序</p>
</li>
<li><p>ORDER BY 子句在SELECT语句的结尾。</p>
</li>
<li><p>案例实操</p>
<ol>
<li>查询学生的成绩，并按照分数降序排列</li>
</ol>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> student s <span class="keyword">LEFT</span> <span class="keyword">JOIN</span> score sco <span class="keyword">ON</span> s.s_id = sco.s_id <span class="keyword">ORDER</span> <span class="keyword">BY</span> sco.s_score <span class="keyword">DESC</span>;</span><br></pre></td></tr></table></figure>
<ol>
<li>查询学生的成绩，并按照分数升序排列</li>
</ol>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> student s <span class="keyword">LEFT</span> <span class="keyword">JOIN</span> score sco <span class="keyword">ON</span> s.s_id = sco.s_id <span class="keyword">ORDER</span> <span class="keyword">BY</span> sco.s_score <span class="keyword">asc</span>;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h4 id="4-10-2-按照别名排序"><a href="#4-10-2-按照别名排序" class="headerlink" title="4.10.2. 按照别名排序"></a>4.10.2. 按照别名排序</h4><p>按照分数的平均值排序</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> s_id ,<span class="keyword">avg</span>(s_score) <span class="keyword">avg</span> <span class="keyword">from</span> score <span class="keyword">group</span> <span class="keyword">by</span> s_id <span class="keyword">order</span> <span class="keyword">by</span> <span class="keyword">avg</span>;</span><br></pre></td></tr></table></figure>
<h4 id="4-10-3-多个列排序"><a href="#4-10-3-多个列排序" class="headerlink" title="4.10.3. 多个列排序"></a>4.10.3. 多个列排序</h4><p>按照学生id和平均成绩进行排序</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> s_id ,<span class="keyword">avg</span>(s_score) <span class="keyword">avg</span> <span class="keyword">from</span> score <span class="keyword">group</span> <span class="keyword">by</span> s_id <span class="keyword">order</span> <span class="keyword">by</span> s_id,<span class="keyword">avg</span>;</span><br></pre></td></tr></table></figure>
<h4 id="4-10-4-每个MapReduce内部排序（Sort-By）局部排序"><a href="#4-10-4-每个MapReduce内部排序（Sort-By）局部排序" class="headerlink" title="4.10.4. 每个MapReduce内部排序（Sort By）局部排序"></a>4.10.4. 每个MapReduce内部排序（Sort By）局部排序</h4><p>Sort By：每个MapReduce内部进行排序，对全局结果集来说不是排序。</p>
<ol>
<li>设置reduce个数</li>
</ol>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> mapreduce.job.reduces=<span class="number">3</span>;</span><br></pre></td></tr></table></figure>
<ol>
<li>查看设置reduce个数</li>
</ol>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> mapreduce.job.reduces;</span><br></pre></td></tr></table></figure>
<ol>
<li>查询成绩按照成绩降序排列</li>
</ol>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> score <span class="keyword">sort</span> <span class="keyword">by</span> s_score;</span><br></pre></td></tr></table></figure>
<ol>
<li>将查询结果导入到文件中（按照成绩降序排列）</li>
</ol>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> <span class="keyword">directory</span> <span class="string">'/export/servers/hivedatas/sort'</span> <span class="keyword">select</span> * <span class="keyword">from</span> score <span class="keyword">sort</span> <span class="keyword">by</span> s_score;</span><br></pre></td></tr></table></figure>
<h4 id="4-10-5-分区排序（DISTRIBUTE-BY）"><a href="#4-10-5-分区排序（DISTRIBUTE-BY）" class="headerlink" title="4.10.5. 分区排序（DISTRIBUTE BY）"></a>4.10.5. 分区排序（DISTRIBUTE BY）</h4><p>Distribute By：类似MR中partition，进行分区，结合sort by使用。</p>
<p>注意，Hive要求DISTRIBUTE BY语句要写在SORT BY语句之前。</p>
<p>对于distribute by进行测试，一定要分配多reduce进行处理，否则无法看到distribute by的效果。</p>
<p>案例实操：先按照学生id进行分区，再按照学生成绩进行排序。</p>
<ol>
<li>设置reduce的个数，将我们对应的s_id划分到对应的reduce当中去</li>
</ol>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> mapreduce.job.reduces=<span class="number">7</span>;</span><br></pre></td></tr></table></figure>
<ol>
<li>通过distribute by  进行数据的分区</li>
</ol>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> <span class="keyword">directory</span> <span class="string">'/export/servers/hivedatas/sort'</span> <span class="keyword">select</span> * <span class="keyword">from</span> score <span class="keyword">distribute</span> <span class="keyword">by</span> s_id <span class="keyword">sort</span> <span class="keyword">by</span> s_score;</span><br></pre></td></tr></table></figure>
<h4 id="4-10-6-CLUSTER-BY"><a href="#4-10-6-CLUSTER-BY" class="headerlink" title="4.10.6. CLUSTER BY"></a>4.10.6. CLUSTER BY</h4><p>当distribute by和sort by字段相同时，可以使用cluster by方式。</p>
<p>cluster by除了具有distribute by的功能外还兼具sort by的功能。但是排序只能是倒序排序，不能指定排序规则为ASC或者DESC。</p>
<p>以下两种写法等价</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> score cluster <span class="keyword">by</span> s_id;</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> score <span class="keyword">distribute</span> <span class="keyword">by</span> s_id <span class="keyword">sort</span> <span class="keyword">by</span> s_id;</span><br></pre></td></tr></table></figure>
<h2 id="5-Hive-Shell参数"><a href="#5-Hive-Shell参数" class="headerlink" title="5.Hive Shell参数"></a>5.Hive Shell参数</h2><h3 id="5-1-Hive命令行"><a href="#5-1-Hive命令行" class="headerlink" title="5.1 Hive命令行"></a><strong>5.1 Hive命令行</strong></h3><p><a href>语法结构</a></p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/hive [-hiveconf x=y]* [&lt;-i filename&gt;]* [&lt;-f filename&gt;|&lt;-e query-string&gt;] [-S]</span><br></pre></td></tr></table></figure>
<p><a href>说明：</a></p>
<p>1、 -i 从文件初始化HQL。</p>
<p>2、 <code>-e从命令行执行指定的HQL</code> </p>
<p>3、 <code>-f 执行HQL脚本</code> </p>
<p>4、 -v 输出执行的HQL语句到控制台 </p>
<p>5、 -p <port> connect to Hive Server on port number </port></p>
<p>6、 -hiveconf x=y Use this to set hive/hadoop configuration variables.  设置hive运行时候的参数配置</p>
<h3 id="5-2-Hive参数配置方式"><a href="#5-2-Hive参数配置方式" class="headerlink" title="5.2 Hive参数配置方式"></a>5.2 Hive参数配置方式</h3><p>开发Hive应用时，不可避免地需要设定Hive的参数。设定Hive的参数可以调优HQL代码的执行效率，或帮助定位问题。</p>
<p><strong>对于一般参数，有以下三种设定方式：</strong></p>
<ul>
<li>配置文件 </li>
<li>命令行参数 </li>
<li><p>参数声明 </p>
<p><code>配置文件</code>：Hive的配置文件包括</p>
</li>
<li><p>用户自定义配置文件：<a href>$HIVE_CONF_DIR/hive-site.xml</a> </p>
</li>
<li><p>默认配置文件：            <a href>$HIVE_CONF_DIR/hive-default.xml</a> </p>
<p><strong>用户自定义配置会覆盖默认配置。</strong></p>
</li>
</ul>
<p>另外，Hive也会读入Hadoop的配置，因为Hive是作为Hadoop的客户端启动的，Hive的配置会覆盖Hadoop的配置。</p>
<p>配置文件的设定对本机启动的所有Hive进程都有效。</p>
<p><code>命令行参数：</code>启动Hive（客户端或Server方式）时，可以在命令行添加-hiveconf param=value来设定参数，例如：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/hive -hiveconf hive.root.logger=INFO,console</span><br></pre></td></tr></table></figure>
<p> 这一设定对本次启动的Session（对于Server方式启动，则是所有请求的Sessions）有效。</p>
<p><code>参数声明</code>：可以在HQL中使用SET关键字设定参数，例如：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> mapred.reduce.tasks=<span class="number">100</span>;</span><br></pre></td></tr></table></figure>
<p>这一设定的作用域也是session级的。</p>
<p>上述三种设定方式的优先级依次递增。即参数声明覆盖命令行参数，命令行参数覆盖配置文件设定。注意某些系统级的参数，例如log4j相关的设定，必须用前两种方式设定，因为那些参数的读取在Session建立以前已经完成了。</p>
<p><a href>参数声明  &gt;   命令行参数   &gt;  配置文件参数（hive）</a></p>
<h2 id="6-Hive-函数"><a href="#6-Hive-函数" class="headerlink" title="6. Hive 函数"></a>6. Hive 函数</h2><h3 id="6-1-内置函数"><a href="#6-1-内置函数" class="headerlink" title="6.1. 内置函数"></a>6.1. 内置函数</h3><p>内容较多，见《Hive官方文档》</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF</span><br></pre></td></tr></table></figure>
<ol>
<li><p>查看系统自带的函数</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"> show <span class="built_in">functions</span>;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>显示自带的函数的用法</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"> desc <span class="keyword">function</span> upper;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>详细显示自带的函数的用法</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"> desc <span class="keyword">function</span> extended upper;</span></span><br></pre></td></tr></table></figure>
<p>4:常用内置函数</p>
</li>
</ol>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#字符串连接函数： concat </span></span><br><span class="line">  <span class="keyword">select</span> <span class="keyword">concat</span>(<span class="string">'abc'</span>,<span class="string">'def’,'</span>gh<span class="string">');</span></span><br><span class="line"><span class="string">#带分隔符字符串连接函数： concat_ws </span></span><br><span class="line"><span class="string">  select concat_ws('</span>,<span class="string">','</span>abc<span class="string">','</span><span class="keyword">def</span><span class="string">','</span>gh<span class="string">');</span></span><br><span class="line"><span class="string">#cast类型转换</span></span><br><span class="line"><span class="string">  select cast(1.5 as int);</span></span><br><span class="line"><span class="string">#get_json_object(json 解析函数，用来处理json，必须是json格式)</span></span><br><span class="line"><span class="string">   select get_json_object('</span>&#123;<span class="string">"name"</span>:<span class="string">"jack"</span>,<span class="string">"age"</span>:<span class="string">"20"</span>&#125;<span class="string">','</span>$.name<span class="string">');</span></span><br><span class="line"><span class="string">#URL解析函数</span></span><br><span class="line"><span class="string">   select parse_url('</span><span class="keyword">http</span>://facebook.com/path1/p.php?k1=v1&amp;k2=v2<span class="comment">#Ref1', 'HOST');</span></span><br><span class="line"><span class="comment">#explode：把map集合中每个键值对或数组中的每个元素都单独生成一行的形式</span></span><br></pre></td></tr></table></figure>
<h3 id="6-2-自定义函数"><a href="#6-2-自定义函数" class="headerlink" title="6.2. 自定义函数"></a>6.2. 自定义函数</h3><h4 id="6-2-1-概述"><a href="#6-2-1-概述" class="headerlink" title="6.2.1 概述:"></a>6.2.1 概述:</h4><ol>
<li>Hive 自带了一些函数，比如：max/min等，当Hive提供的内置函数无法满足你的业务处理需要时，此时就可以考虑使用用户自定义函数(UDF).</li>
<li>根据用户自定义函数类别分为以下三种：<ol>
<li>UDF（User-Defined-Function）<ul>
<li>一进一出  </li>
</ul>
</li>
<li>UDAF（User-Defined Aggregation Function）<ul>
<li>聚集函数，多进一出</li>
<li>类似于：<code>count</code>/<code>max</code>/<code>min</code></li>
</ul>
</li>
<li>UDTF（User-Defined Table-Generating Functions）<ul>
<li>一进多出</li>
<li>如 <code>lateral</code> <code>view</code> <code>explore()</code></li>
</ul>
</li>
</ol>
</li>
<li>编程步骤：<ol>
<li>继承org.apache.hadoop.hive.ql.UDF</li>
<li>需要实现evaluate函数；evaluate函数支持重载；</li>
</ol>
</li>
<li>注意事项<ol>
<li>UDF必须要有返回类型，可以返回null，但是返回类型不能为void；</li>
<li>UDF中常用Text/LongWritable等类型，不推荐使用java类型；</li>
</ol>
</li>
</ol>
<h4 id="6-2-2-UDF-开发实例"><a href="#6-2-2-UDF-开发实例" class="headerlink" title="6.2.2 UDF 开发实例"></a>6.2.2 UDF 开发实例</h4><h5 id="Step-1-创建-Maven-工程"><a href="#Step-1-创建-Maven-工程" class="headerlink" title="Step 1 创建 Maven 工程"></a>Step 1 创建 Maven 工程</h5><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.hive/hive-exec --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-exec<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.5<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-common --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.5<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-compiler-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">source</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">source</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">target</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">target</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">encoding</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">encoding</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h5 id="Step-2-开发-Java-类集成-UDF"><a href="#Step-2-开发-Java-类集成-UDF" class="headerlink" title="Step 2 开发 Java 类集成 UDF"></a>Step 2 开发 Java 类集成 UDF</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyUDF</span>  <span class="keyword">extends</span> <span class="title">UDF</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> Text <span class="title">evaluate</span><span class="params">(<span class="keyword">final</span> Text str)</span></span>&#123;</span><br><span class="line">        String tmp_str = str.toString();</span><br><span class="line">        <span class="keyword">if</span>(str != <span class="keyword">null</span> &amp;&amp; !tmp_str.equals(<span class="string">""</span>))&#123;</span><br><span class="line">          String str_ret =   tmp_str.substring(<span class="number">0</span>, <span class="number">1</span>).toUpperCase() + tmp_str.substring(<span class="number">1</span>);</span><br><span class="line">          <span class="keyword">return</span>  <span class="keyword">new</span> Text(str_ret);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span>  <span class="keyword">new</span> Text(<span class="string">""</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="Step-3-项目打包，并上传到hive的lib目录下"><a href="#Step-3-项目打包，并上传到hive的lib目录下" class="headerlink" title="Step 3 项目打包，并上传到hive的lib目录下"></a>Step 3 项目打包，并上传到hive的lib目录下</h5><p><img src="http://ppw6n93dt.bkt.clouddn.com/8dda7bfdfab0655e99a3c3b17afc422e.png" alt></p>
<h5 id="Step-4-添加jar包"><a href="#Step-4-添加jar包" class="headerlink" title="Step 4 添加jar包"></a>Step 4 添加jar包</h5><p>重命名我们的jar包名称</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/apache-hive-2.7.5-bin/lib</span><br><span class="line">mv original-day_10_hive_udf-1.0-SNAPSHOT.jar my_upper.jar</span><br></pre></td></tr></table></figure>
<p>hive的客户端添加我们的jar包</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">add jar /export/servers/apache-hive-2.7.5-bin/lib/my_upper.jar;</span><br></pre></td></tr></table></figure>
<h5 id="Step-5-设置函数与我们的自定义函数关联"><a href="#Step-5-设置函数与我们的自定义函数关联" class="headerlink" title="Step 5 设置函数与我们的自定义函数关联"></a>Step 5 设置函数与我们的自定义函数关联</h5><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">temporary</span> <span class="keyword">function</span> my_upper <span class="keyword">as</span> <span class="string">'cn.itcast.udf.ItcastUDF'</span>;</span><br></pre></td></tr></table></figure>
<h5 id="Step-6-使用自定义函数"><a href="#Step-6-使用自定义函数" class="headerlink" title="Step 6 使用自定义函数"></a>Step 6 使用自定义函数</h5><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> my_upper(<span class="string">'abc'</span>);</span><br></pre></td></tr></table></figure>
<h2 id="7-hive的数据压缩"><a href="#7-hive的数据压缩" class="headerlink" title="7.hive的数据压缩"></a>7.hive的数据压缩</h2><p>在实际工作当中，hive当中处理的数据，一般都需要经过压缩，前期我们在学习hadoop的时候，已经配置过hadoop的压缩，我们这里的hive也是一样的可以使用压缩来节省我们的MR处理的网络带宽</p>
<h3 id="7-1-MR支持的压缩编码"><a href="#7-1-MR支持的压缩编码" class="headerlink" title="7.1 MR支持的压缩编码"></a>7.1 MR支持的压缩编码</h3><table>
<thead>
<tr>
<th>压缩格式</th>
<th>工具</th>
<th>算法</th>
<th>文件扩展名</th>
<th>是否可切分</th>
</tr>
</thead>
<tbody>
<tr>
<td>DEFAULT</td>
<td>无</td>
<td>DEFAULT</td>
<td>.deflate</td>
<td>否</td>
</tr>
<tr>
<td>Gzip</td>
<td>gzip</td>
<td>DEFAULT</td>
<td>.gz</td>
<td>否</td>
</tr>
<tr>
<td>bzip2</td>
<td>bzip2</td>
<td>bzip2</td>
<td>.bz2</td>
<td>是</td>
</tr>
<tr>
<td>LZO</td>
<td>lzop</td>
<td>LZO</td>
<td>.lzo</td>
<td>否</td>
</tr>
<tr>
<td>LZ4</td>
<td>无</td>
<td>LZ4</td>
<td>.lz4</td>
<td>否</td>
</tr>
<tr>
<td>Snappy</td>
<td>无</td>
<td>Snappy</td>
<td>.snappy</td>
<td>否</td>
</tr>
</tbody>
</table>
<p>为了支持多种压缩/解压缩算法，Hadoop引入了编码/解码器，如下表所示</p>
<table>
<thead>
<tr>
<th>压缩格式</th>
<th>对应的编码/解码器</th>
</tr>
</thead>
<tbody>
<tr>
<td>DEFLATE</td>
<td>org.apache.hadoop.io.compress.DefaultCodec</td>
</tr>
<tr>
<td>gzip</td>
<td>org.apache.hadoop.io.compress.GzipCodec</td>
</tr>
<tr>
<td>bzip2</td>
<td>org.apache.hadoop.io.compress.BZip2Codec</td>
</tr>
<tr>
<td>LZO</td>
<td>com.hadoop.compression.lzo.LzopCodec</td>
</tr>
<tr>
<td>LZ4</td>
<td>org.apache.hadoop.io.compress.Lz4Codec</td>
</tr>
<tr>
<td>Snappy</td>
<td>org.apache.hadoop.io.compress.SnappyCodec</td>
</tr>
</tbody>
</table>
<p>压缩性能的比较</p>
<table>
<thead>
<tr>
<th>压缩算法</th>
<th>原始文件大小</th>
<th>压缩文件大小</th>
<th>压缩速度</th>
<th>解压速度</th>
</tr>
</thead>
<tbody>
<tr>
<td>gzip</td>
<td>8.3GB</td>
<td>1.8GB</td>
<td>17.5MB/s</td>
<td>58MB/s</td>
</tr>
<tr>
<td>bzip2</td>
<td>8.3GB</td>
<td>1.1GB</td>
<td>2.4MB/s</td>
<td>9.5MB/s</td>
</tr>
<tr>
<td>LZO</td>
<td>8.3GB</td>
<td>2.9GB</td>
<td>49.3MB/s</td>
<td>74.6MB/s</td>
</tr>
</tbody>
</table>
<p><a href="http://google.github.io/snappy/" target="_blank" rel="noopener">http://google.github.io/snappy/</a></p>
<p>On a single core of a Core i7 processor in 64-bit mode, Snappy compresses at about <code>250 MB/sec</code> or more and decompresses at about <code>500 MB/se</code>c or more.</p>
<h3 id="7-2-压缩配置参数"><a href="#7-2-压缩配置参数" class="headerlink" title="7.2 压缩配置参数"></a><strong>7.2 压缩配置</strong>参数</h3><p>要在Hadoop中启用压缩，可以配置如下参数（mapred-site.xml文件中）：</p>
<table>
<thead>
<tr>
<th>参数</th>
<th>默认值</th>
<th>阶段</th>
<th>建议</th>
</tr>
</thead>
<tbody>
<tr>
<td>io.compression.codecs   （在core-site.xml中配置）</td>
<td>org.apache.hadoop.io.compress.DefaultCodec, org.apache.hadoop.io.compress.GzipCodec, org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.Lz4Codec</td>
<td>输入压缩</td>
<td>Hadoop使用文件扩展名判断是否支持某种编解码器</td>
</tr>
<tr>
<td>mapreduce.map.output.compress</td>
<td>false</td>
<td>mapper输出</td>
<td>这个参数设为true启用压缩</td>
</tr>
<tr>
<td>mapreduce.map.output.compress.codec</td>
<td>org.apache.hadoop.io.compress.DefaultCodec</td>
<td>mapper输出</td>
<td>使用LZO、LZ4或snappy编解码器在此阶段压缩数据</td>
</tr>
<tr>
<td>mapreduce.output.fileoutputformat.compress</td>
<td>false</td>
<td>reducer输出</td>
<td>这个参数设为true启用压缩</td>
</tr>
<tr>
<td>mapreduce.output.fileoutputformat.compress.codec</td>
<td>org.apache.hadoop.io.compress. DefaultCodec</td>
<td>reducer输出</td>
<td>使用标准工具或者编解码器，如gzip和bzip2</td>
</tr>
<tr>
<td>mapreduce.output.fileoutputformat.compress.type</td>
<td>RECORD</td>
<td>reducer输出</td>
<td>SequenceFile输出使用的压缩类型：NONE和BLOCK</td>
</tr>
</tbody>
</table>
<h3 id="7-3-开启Map输出阶段压缩"><a href="#7-3-开启Map输出阶段压缩" class="headerlink" title="7.3 开启Map输出阶段压缩"></a><strong>7.3 开启Map</strong>输出阶段压缩</h3><p>开启map输出阶段压缩可以减少job中map和Reduce task间数据传输量。具体配置如下：</p>
<p><strong>案例实操：</strong></p>
<p>1）开启hive中间传输数据压缩功能</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.compress.intermediate=<span class="literal">true</span>;</span><br></pre></td></tr></table></figure>
<p>2）开启mapreduce中map输出压缩功能</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> mapreduce.map.output.compress=<span class="literal">true</span>;</span><br></pre></td></tr></table></figure>
<p>3）设置mapreduce中map输出数据的压缩方式</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> mapreduce.map.output.compress.codec= org.apache.hadoop.io.compress.SnappyCodec;</span><br></pre></td></tr></table></figure>
<p>4）执行查询语句</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">count</span>(<span class="number">1</span>) <span class="keyword">from</span> score;</span><br></pre></td></tr></table></figure>
<h3 id="7-4-开启Reduce输出阶段压缩"><a href="#7-4-开启Reduce输出阶段压缩" class="headerlink" title="7.4 开启Reduce输出阶段压缩"></a><strong>7.4</strong> 开启Reduce输出阶段压缩</h3><p>当Hive将输出写入到表中时，输出内容同样可以进行压缩。属性hive.exec.compress.output控制着这个功能。用户可能需要保持默认设置文件中的默认值false，这样默认的输出就是非压缩的纯文本文件了。用户可以通过在查询语句或执行脚本中设置这个值为true，来开启输出结果压缩功能。</p>
<p><strong>案例实操</strong>：</p>
<p>1）开启hive最终输出数据压缩功能</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.compress.output=<span class="literal">true</span>;</span><br></pre></td></tr></table></figure>
<p>2）开启mapreduce最终输出数据压缩</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> mapreduce.output.fileoutputformat.compress=<span class="literal">true</span>;</span><br></pre></td></tr></table></figure>
<p>3）设置mapreduce最终数据输出压缩方式</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> mapreduce.output.fileoutputformat.compress.codec = org.apache.hadoop.io.compress.SnappyCodec;</span><br></pre></td></tr></table></figure>
<p>4）设置mapreduce最终数据输出压缩为块压缩</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> mapreduce.output.fileoutputformat.compress.type=<span class="keyword">BLOCK</span>;</span><br></pre></td></tr></table></figure>
<p>5）测试一下输出结果是否是压缩文件</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite local directory <span class="string">'/export/servers/snappy'</span> select * from score distribute by s_id sort by s_id desc;</span><br></pre></td></tr></table></figure>
<h2 id="8-hive的数据存储格式"><a href="#8-hive的数据存储格式" class="headerlink" title="8.hive的数据存储格式"></a><strong>8.hive的数据存储格式</strong></h2><p>Hive支持的存储数的格式主要有：TEXTFILE（行式存储） 、SEQUENCEFILE(行式存储)、ORC（列式存储）、PARQUET（列式存储）。 </p>
<h3 id="8-1-列式存储和行式存储"><a href="#8-1-列式存储和行式存储" class="headerlink" title="8.1 列式存储和行式存储"></a><strong>8.1</strong> <strong>列式存储</strong>和行式存储</h3><p><img src="/images/hive/wps1.jpg" alt="img"> </p>
<p>上图左边为逻辑表，右边第一个为行式存储，第二个为列式存储。</p>
<p><strong>行存储的特点：</strong> 查询满足条件的一整行数据的时候，列存储则需要去每个聚集的字段找到对应的每个列的值，行存储只需要找到其中一个值，其余的值都在相邻地方，所以此时行存储查询的速度更快。</p>
<p><strong>列存储的特点：</strong> 因为每个字段的数据聚集存储，在查询只需要少数几个字段的时候，能大大减少读取的数据量；每个字段的数据类型一定是相同的，列式存储可以针对性的设计更好的设计压缩算法。</p>
<p><code>TEXTFILE和SEQUENCEFILE的存储格式都是基于行存储的；</code></p>
<p><code>ORC和PARQUET是基于列式存储的。</code></p>
<h3 id="8-2-常用数据存储格式"><a href="#8-2-常用数据存储格式" class="headerlink" title="8.2 常用数据存储格式"></a>8.2 常用数据存储格式</h3><p><code>TEXTFILE格式</code></p>
<p>默认格式，数据不做压缩，磁盘开销大，数据解析开销大。可结合Gzip、Bzip2使用.</p>
<p><code>ORC格式</code></p>
<p>Orc (Optimized Row Columnar)是hive 0.11版里引入的新的存储格式。</p>
<p>可以看到每个Orc文件由1个或多个stripe组成，每个stripe250MB大小，每个Stripe里有三部分组成，分别是Index Data,Row Data,Stripe Footer：</p>
<ul>
<li><a href>indexData</a>：某些列的索引数据</li>
<li><a href>rowData</a> :真正的数据存储</li>
<li><a href>StripFooter</a>：stripe的元数据信息</li>
</ul>
<p><img src="/images/hive/wps2.jpg" alt="img"> </p>
<p><code>PARQUET格式</code></p>
<p>Parquet是面向分析型业务的列式存储格式，由Twitter和Cloudera合作开发，</p>
<p>Parquet文件是以二进制方式存储的，所以是不可以直接读取的，文件中包括该文件的数据和元数据，因此Parquet格式文件是自解析的。</p>
<p>通常情况下，在存储Parquet数据的时候会按照Block大小设置行组的大小，由于一般情况下每一个Mapper任务处理数据的最小单位是一个Block，这样可以把每一个行组由一个Mapper任务处理，增大任务执行并行度。Parquet文件的格式如下图所示。</p>
<p><img src="/images/hive/wps3.jpg" alt="img">    </p>
<h2 id="9-文件存储格式与数据压缩结合"><a href="#9-文件存储格式与数据压缩结合" class="headerlink" title="9. 文件存储格式与数据压缩结合"></a>9. 文件存储格式与数据压缩结合</h2><h3 id="9-1-压缩比和查询速度对比"><a href="#9-1-压缩比和查询速度对比" class="headerlink" title="9.1 压缩比和查询速度对比"></a>9.1 压缩比和查询速度对比</h3><p><code>1）TextFile</code></p>
<p>（1）创建表，存储数据格式为TEXTFILE</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> log_text (</span><br><span class="line">track_time <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">url</span> <span class="keyword">string</span>,</span><br><span class="line">session_id <span class="keyword">string</span>,</span><br><span class="line">referer <span class="keyword">string</span>,</span><br><span class="line">ip <span class="keyword">string</span>,</span><br><span class="line">end_user_id <span class="keyword">string</span>,</span><br><span class="line">city_id <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'\t'</span></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> TEXTFILE ;</span><br></pre></td></tr></table></figure>
<p>（2）向表中加载数据</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/export/servers/hivedatas/log.data'</span> <span class="keyword">into</span> <span class="keyword">table</span> log_text ;</span><br></pre></td></tr></table></figure>
<p>（3）查看表中数据大小</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dfs -du -h /user/hive/warehouse/myhive.db/log_text;</span><br></pre></td></tr></table></figure>
<p><code>2）ORC</code></p>
<p>（1）创建表，存储数据格式为ORC</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> log_orc(</span><br><span class="line">track_time <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">url</span> <span class="keyword">string</span>,</span><br><span class="line">session_id <span class="keyword">string</span>,</span><br><span class="line">referer <span class="keyword">string</span>,</span><br><span class="line">ip <span class="keyword">string</span>,</span><br><span class="line">end_user_id <span class="keyword">string</span>,</span><br><span class="line">city_id <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'\t'</span></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> orc ;</span><br></pre></td></tr></table></figure>
<p>（2）向表中加载数据</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> log_orc <span class="keyword">select</span> * <span class="keyword">from</span> log_text ;</span><br></pre></td></tr></table></figure>
<p>（3）查看表中数据大小</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dfs -du -h /user/hive/warehouse/myhive.db/log_orc;</span><br></pre></td></tr></table></figure>
<p><code>3）Parquet</code></p>
<p>（1）创建表，存储数据格式为parquet</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> log_parquet(</span><br><span class="line">track_time <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">url</span> <span class="keyword">string</span>,</span><br><span class="line">session_id <span class="keyword">string</span>,</span><br><span class="line">referer <span class="keyword">string</span>,</span><br><span class="line">ip <span class="keyword">string</span>,</span><br><span class="line">end_user_id <span class="keyword">string</span>,</span><br><span class="line">city_id <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'\t'</span></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> PARQUET ;</span><br></pre></td></tr></table></figure>
<p>（2）向表中加载数据</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> log_parquet <span class="keyword">select</span> * <span class="keyword">from</span> log_text ;</span><br></pre></td></tr></table></figure>
<p>（3）查看表中数据大小</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dfs -du -h /user/hive/warehouse/myhive.db/log_parquet;</span><br></pre></td></tr></table></figure>
<p><a href>存储文件的压缩比总结：</a></p>
<p><strong>ORC &gt;  Parquet &gt;  textFile</strong></p>
<p><code>4)存储文件的查询速度测试：</code></p>
<p>1）TextFile</p>
<p>hive (default)&gt; select count(1) from log_text;</p>
<p>Time taken: 21.54 seconds, Fetched: 1 row(s)</p>
<p>2）ORC</p>
<p>hive (default)&gt; select count(1) from log_orc;</p>
<p>Time taken: 20.867 seconds, Fetched: 1 row(s)</p>
<p>3）Parquet</p>
<p>hive (default)&gt; select count(1) from log_parquet;</p>
<p>Time taken: 22.922 seconds, Fetched: 1 row(s)</p>
<p><a href>存储文件的查询速度总结：</a></p>
<p><strong>ORC &gt; TextFile &gt; Parquet</strong></p>
<h3 id="9-2-ORC存储指定压缩方式"><a href="#9-2-ORC存储指定压缩方式" class="headerlink" title="9.2 ORC存储指定压缩方式"></a>9.2 ORC存储指定压缩方式</h3><p>官网：<a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC</a></p>
<p>ORC存储方式的压缩：</p>
<table>
<thead>
<tr>
<th>Key</th>
<th>Default</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>orc.compress</td>
<td><code>ZLIB</code></td>
<td>high level compression (one of NONE, ZLIB, SNAPPY)</td>
</tr>
<tr>
<td>orc.compress.size</td>
<td>262,144</td>
<td>number of bytes in each compression chunk</td>
</tr>
<tr>
<td>orc.stripe.size</td>
<td>67,108,864</td>
<td>number of bytes in each stripe</td>
</tr>
<tr>
<td>orc.row.index.stride</td>
<td>10,000</td>
<td>number of rows between index entries (must be &gt;= 1000)</td>
</tr>
<tr>
<td>orc.create.index</td>
<td>true</td>
<td>whether to create row indexes</td>
</tr>
<tr>
<td>orc.bloom.filter.columns</td>
<td>“”</td>
<td>comma separated list of column names for which bloom filter should be created</td>
</tr>
<tr>
<td>orc.bloom.filter.fpp</td>
<td>0.05</td>
<td>false positive probability for bloom filter (must &gt;0.0 and &lt;1.0)</td>
</tr>
</tbody>
</table>
<p><code>1）创建一个非压缩的的ORC存储方式</code></p>
<p>（1）建表语句</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> log_orc_none(</span><br><span class="line">track_time <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">url</span> <span class="keyword">string</span>,</span><br><span class="line">session_id <span class="keyword">string</span>,</span><br><span class="line">referer <span class="keyword">string</span>,</span><br><span class="line">ip <span class="keyword">string</span>,</span><br><span class="line">end_user_id <span class="keyword">string</span>,</span><br><span class="line">city_id <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'\t'</span></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> orc tblproperties (<span class="string">"orc.compress"</span>=<span class="string">"NONE"</span>);</span><br></pre></td></tr></table></figure>
<p>（2）插入数据</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> log_orc_none <span class="keyword">select</span> * <span class="keyword">from</span> log_text ;</span><br></pre></td></tr></table></figure>
<p>（3）查看插入后数据</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dfs -du -h /user/hive/warehouse/myhive.db/log_orc_none;</span><br></pre></td></tr></table></figure>
<p><code>2）创建一个SNAPPY压缩的ORC存储方式</code></p>
<p>（1）建表语句</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> log_orc_snappy(</span><br><span class="line">track_time <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">url</span> <span class="keyword">string</span>,</span><br><span class="line">session_id <span class="keyword">string</span>,</span><br><span class="line">referer <span class="keyword">string</span>,</span><br><span class="line">ip <span class="keyword">string</span>,</span><br><span class="line">end_user_id <span class="keyword">string</span>,</span><br><span class="line">city_id <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'\t'</span></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> orc tblproperties (<span class="string">"orc.compress"</span>=<span class="string">"SNAPPY"</span>);</span><br></pre></td></tr></table></figure>
<p>（2）插入数据</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> log_orc_snappy <span class="keyword">select</span> * <span class="keyword">from</span> log_text ;</span><br></pre></td></tr></table></figure>
<p>（3）查看插入后数据</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dfs -du -h /user/hive/warehouse/myhive.db/log_orc_snappy ;</span><br></pre></td></tr></table></figure>
<h3 id="9-3-存储方式和压缩总结："><a href="#9-3-存储方式和压缩总结：" class="headerlink" title="9.3 存储方式和压缩总结："></a>9.3 存储方式和压缩总结：</h3><p>​    在实际的项目开发当中，hive表的数据存储格式一般选择：<a href>orc或parquet。压缩方式一般选择snappy</a>。</p>
<h2 id="10-hive调优"><a href="#10-hive调优" class="headerlink" title="10.hive调优"></a>10.hive调优</h2><h3 id="10-1-Fetch抓取"><a href="#10-1-Fetch抓取" class="headerlink" title="10.1 Fetch抓取"></a><strong>10.1</strong> Fetch抓取</h3><p>Hive中对某些情况的查询可以不必使用MapReduce计算。例如：SELECT * FROM score;在这种情况下，Hive可以简单地读取score对应的存储目录下的文件，然后输出查询结果到控制台。通过设置hive.fetch.task.conversion参数,可以控制查询语句是否走MapReduce.</p>
<p>案例实操：</p>
<p>1）把hive.fetch.task.conversion设置成none，然后执行查询语句，都会执行mapreduce程序。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.fetch.task.conversion=<span class="keyword">none</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> score;</span><br><span class="line"><span class="keyword">select</span> s_score <span class="keyword">from</span> score;</span><br><span class="line"><span class="keyword">select</span> s_score <span class="keyword">from</span> score <span class="keyword">limit</span> <span class="number">3</span>;</span><br></pre></td></tr></table></figure>
<p>2）把hive.fetch.task.conversion设置成more，然后执行查询语句，如下查询方式都不会执行mapreduce程序。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.fetch.task.conversion=more;</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> score;</span><br><span class="line"><span class="keyword">select</span> s_score <span class="keyword">from</span> score;</span><br><span class="line"><span class="keyword">select</span> s_score <span class="keyword">from</span> score <span class="keyword">limit</span> <span class="number">3</span>;</span><br></pre></td></tr></table></figure>
<h3 id="10-2-本地模式"><a href="#10-2-本地模式" class="headerlink" title="10.2 本地模式"></a>10.2 本地模式</h3><p>大多数的Hadoop Job是需要Hadoop提供的完整的可扩展性来处理大数据集的。不过，有时Hive的输入数据量是非常小的。在这种情况下，为查询触发执行任务时消耗可能会比实际job的执行时间要多的多。对于大多数这种情况，Hive可以通过本地模式在单台机器上处理所有的任务。对于小数据集，执行时间可以明显被缩短。</p>
<p>用户可以通过设置hive.exec.mode.local.auto的值为true，来让Hive在适当的时候自动启动这个优化。</p>
<p>案例实操：</p>
<p>1）开启本地模式，并执行查询语句</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.mode.local.auto=<span class="literal">true</span>; </span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> score cluster <span class="keyword">by</span> s_id;</span><br></pre></td></tr></table></figure>
<p>2）关闭本地模式，并执行查询语句</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.mode.local.auto=<span class="literal">false</span>; </span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> score cluster <span class="keyword">by</span> s_id;</span><br></pre></td></tr></table></figure>
<h3 id="10-3-MapJoin"><a href="#10-3-MapJoin" class="headerlink" title="10.3 MapJoin"></a>10.3 MapJoin</h3><p>如果不指定MapJoin或者不符合MapJoin的条件，那么Hive解析器会在Reduce阶段完成join,容易发生数据倾斜。可以用MapJoin把小表全部加载到内存在map端进行join，避免reducer处理。</p>
<p>1）开启MapJoin参数设置：</p>
<p>（1）设置自动选择Mapjoin</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.auto.convert.join = <span class="literal">true</span>;</span><br></pre></td></tr></table></figure>
<p>（2）大表小表的阈值设置（默认25M以下认为是小表）：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.mapjoin.smalltable.filesize=<span class="number">25123456</span>;</span><br></pre></td></tr></table></figure>
<h3 id="10-4-Group-By"><a href="#10-4-Group-By" class="headerlink" title="10.4 Group By"></a>10.4 Group By</h3><p> 默认情况下，Map阶段同一Key数据分发给一个reduce，当一个key数据过大时就倾斜了。并不是所有的聚合操作都需要在Reduce端完成，很多聚合操作都可以先在Map端进行部分聚合，最后在Reduce端得出最终结果。</p>
<p>开启Map端聚合参数设置</p>
<p>（1）是否在Map端进行聚合，默认为True</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.map.aggr = <span class="literal">true</span>;</span><br></pre></td></tr></table></figure>
<p>（2）在Map端进行聚合操作的条目数目</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.groupby.mapaggr.checkinterval = <span class="number">100000</span>;</span><br></pre></td></tr></table></figure>
<p>（3）有数据倾斜的时候进行负载均衡（默认是false）</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.groupby.skewindata = <span class="literal">true</span>;</span><br></pre></td></tr></table></figure>
<p> 当选项设定为 true，生成的查询计划会有两个MR Job。</p>
<p>第一个MR Job中，Map的输出结果会随机分布到Reduce中，每个Reduce做部分聚合操作，并输出结果，这样处理的结果是相同的Group By Key有可能被分发到不同的Reduce中，从而达到负载均衡的目的；</p>
<p>第二个MR Job再根据预处理的数据结果按照Group By Key分布到Reduce中（这个过程可以保证相同的Group By Key被分布到同一个Reduce中），最后完成最终的聚合操作。</p>
<h3 id="10-5-Count-distinct"><a href="#10-5-Count-distinct" class="headerlink" title="10.5 Count(distinct)"></a>10.5 Count(distinct)</h3><p>数据量小的时候无所谓，数据量大的情况下，由于COUNT DISTINCT操作需要用一个Reduce Task来完成，这一个Reduce需要处理的数据量太大，就会导致整个Job很难完成，一般COUNT DISTINCT使用先GROUP BY再COUNT的方式替换：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">count</span>(<span class="keyword">distinct</span> s_id) <span class="keyword">from</span> score;</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">count</span>(s_id) <span class="keyword">from</span> (<span class="keyword">select</span> <span class="keyword">id</span> <span class="keyword">from</span> score <span class="keyword">group</span> <span class="keyword">by</span> s_id) a;</span><br></pre></td></tr></table></figure>
<p>虽然会多用一个Job来完成，但在数据量大的情况下，这个绝对是值得的。</p>
<h3 id="10-6-笛卡尔积"><a href="#10-6-笛卡尔积" class="headerlink" title="10.6 笛卡尔积"></a>10.6 笛卡尔积</h3><p>尽量避免笛卡尔积，即避免join的时候不加on条件，或者无效的on条件，Hive只能使用1个reducer来完成笛卡尔积。</p>
<h3 id="10-7-动态分区调整"><a href="#10-7-动态分区调整" class="headerlink" title="10.7 动态分区调整"></a>10.7 <strong>动态分区</strong>调整</h3><p>往hive分区表中插入数据时，hive提供了一个动态分区功能，其可以基于查询参数的位置去推断分区的名称，从而建立分区。使用Hive的动态分区，需要进行相应的配置。</p>
<p>Hive的动态分区是以第一个表的分区规则，来对应第二个表的分区规则，将第一个表的所有分区，全部拷贝到第二个表中来，第二个表在加载数据的时候，不需要指定分区了，直接用第一个表的分区即可</p>
<h4 id="10-7-1-开启动态分区参数设置"><a href="#10-7-1-开启动态分区参数设置" class="headerlink" title="10.7.1 开启动态分区参数设置"></a>10.7.1 开启动态分区参数设置</h4><p>（1）开启动态分区功能（默认true，开启）</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.dynamic.partition=<span class="literal">true</span>;</span><br></pre></td></tr></table></figure>
<p>（2）设置为非严格模式（动态分区的模式，默认strict，表示必须指定至少一个分区为静态分区，nonstrict模式表示允许所有的分区字段都可以使用动态分区。）</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.dynamic.partition.mode=nonstrict;</span><br></pre></td></tr></table></figure>
<p>（3）在所有执行MR的节点上，最大一共可以创建多少个动态分区。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span>  hive.exec.max.dynamic.partitions=<span class="number">1000</span>;</span><br></pre></td></tr></table></figure>
<p>（4）在每个执行MR的节点上，最大可以创建多少个动态分区。该参数需要根据实际的数据来设定。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.max.dynamic.partitions.pernode=<span class="number">100</span></span><br></pre></td></tr></table></figure>
<p>（5）整个MR Job中，最大可以创建多少个HDFS文件。</p>
<p>​    在linux系统当中，每个linux用户最多可以开启1024个进程，每一个进程最多可以打开2048个文件，即持有2048个文件句柄，下面这个值越大，就可以打开文件句柄越大</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.max.created.files=<span class="number">100000</span>;</span><br></pre></td></tr></table></figure>
<p>（6）当有空分区生成时，是否抛出异常。一般不需要设置。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.error.on.empty.partition=<span class="literal">false</span>;</span><br></pre></td></tr></table></figure>
<h4 id="10-7-2-案例操作"><a href="#10-7-2-案例操作" class="headerlink" title="10.7.2 案例操作"></a>10.7.2 案例操作</h4><p>需求：将ori中的数据按照时间(如：20111231234568)，插入到目标表ori_partitioned的相应分区中。</p>
<p>（1）准备数据原表</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> ori_partitioned(<span class="keyword">id</span> <span class="built_in">bigint</span>, <span class="built_in">time</span> <span class="built_in">bigint</span>, uid <span class="keyword">string</span>, keyword <span class="keyword">string</span>, url_rank <span class="built_in">int</span>, click_num <span class="built_in">int</span>, click_url <span class="keyword">string</span>) </span><br><span class="line">PARTITIONED <span class="keyword">BY</span> (p_time <span class="built_in">bigint</span>) </span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/export/servers/hivedatas/small_data'</span> <span class="keyword">into</span>  <span class="keyword">table</span> ori_partitioned <span class="keyword">partition</span> (p_time=<span class="string">'20111230000010'</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/export/servers/hivedatas/small_data'</span> <span class="keyword">into</span>  <span class="keyword">table</span> ori_partitioned <span class="keyword">partition</span> (p_time=<span class="string">'20111230000011'</span>);</span><br></pre></td></tr></table></figure>
<p>（2）创建目标分区表</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> ori_partitioned_target(<span class="keyword">id</span> <span class="built_in">bigint</span>, <span class="built_in">time</span> <span class="built_in">bigint</span>, uid <span class="keyword">string</span>, keyword <span class="keyword">string</span>, url_rank <span class="built_in">int</span>, click_num <span class="built_in">int</span>, click_url <span class="keyword">string</span>) PARTITIONED <span class="keyword">BY</span> (p_time <span class="keyword">STRING</span>) <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br></pre></td></tr></table></figure>
<p>（3）向目标分区表加载数据</p>
<p>如果按照之前介绍的往指定一个分区中Insert数据，那么这个需求很不容易实现。这时候就需要使用动态分区来实现。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> overwrite <span class="keyword">TABLE</span> ori_partitioned_target <span class="keyword">PARTITION</span> (p_time)</span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">id</span>, <span class="built_in">time</span>, uid, keyword, url_rank, click_num, click_url, p_time</span><br><span class="line"><span class="keyword">FROM</span> ori_partitioned;</span><br></pre></td></tr></table></figure>
<p>注意：在SELECT子句的最后几个字段，必须对应前面<strong>PARTITION (p_time)</strong>中指定的分区字段，包括顺序。</p>
<p>(4)查看分区</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">show</span> <span class="keyword">partitions</span> ori_partitioned_target;</span><br></pre></td></tr></table></figure>
<h3 id="10-8-并行执行"><a href="#10-8-并行执行" class="headerlink" title="10.8 并行执行"></a>10.8 并行执行</h3><p>Hive会将一个查询转化成一个或者多个阶段。这样的阶段可以是MapReduce阶段、抽样阶段、合并阶段、limit阶段。或者Hive执行过程中可能需要的其他阶段。默认情况下，Hive一次只会执行一个阶段。不过，某个特定的job可能包含众多的阶段，而这些阶段可能并非完全互相依赖的，也就是说有些阶段是可以并行执行的，这样可能使得整个job的执行时间缩短。不过，如果有更多的阶段可以并行执行，那么job可能就越快完成。</p>
<p>​    通过设置参数hive.exec.parallel值为true，就可以开启并发执行。不过，在共享集群中，需要注意下，如果job中并行阶段增多，那么集群利用率就会增加。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.parallel = <span class="literal">true</span>;</span><br></pre></td></tr></table></figure>
<p>当然，得是在系统资源比较空闲的时候才有优势，否则，没资源，并行也起不来。</p>
<h3 id="10-9-严格模式"><a href="#10-9-严格模式" class="headerlink" title="10.9 严格模式"></a>10.9 严格模式</h3><p>Hive提供了一个严格模式，可以防止用户执行那些可能意向不到的不好的影响的查询。</p>
<p>​    通过设置属性hive.mapred.mode值为默认是非严格模式nonstrict 。开启严格模式需要修改hive.mapred.mode值为strict，开启严格模式可以禁止3种类型的查询。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.mapred.mode = <span class="keyword">strict</span>; <span class="comment">#开启严格模式</span></span><br><span class="line"><span class="keyword">set</span> hive.mapred.mode = nostrict; <span class="comment">#开启非严格模式</span></span><br></pre></td></tr></table></figure>
<p>1）<code>对于分区表，在where语句中必须含有分区字段作为过滤条件来限制范围，否则不允许执行</code>。换句话说，就是用户不允许扫描所有分区。进行这个限制的原因是，通常分区表都拥有非常大的数据集，而且数据增加迅速。没有进行分区限制的查询可能会消耗令人不可接受的巨大资源来处理这个表。</p>
<p>2）<code>对于使用了order by语句的查询，要求必须使用limit语句</code>。因为order by为了执行排序过程会将所有的结果数据分发到同一个Reducer中进行处理，强制要求用户增加这个LIMIT语句可以防止Reducer额外执行很长一段时间。</p>
<p>3）<code>限制笛卡尔积的查询</code>。对关系型数据库非常了解的用户可能期望在执行JOIN查询的时候不使用ON语句而是使用where语句，这样关系数据库的执行优化器就可以高效地将WHERE语句转化成那个ON语句。不幸的是，Hive并不会执行这种优化，因此，如果表足够大，那么这个查询就会出现不可控的情况。</p>
<h3 id="10-10-JVM重用"><a href="#10-10-JVM重用" class="headerlink" title="10.10 JVM重用"></a>10.10 <strong>JVM重用</strong></h3><p>JVM重用是Hadoop调优参数的内容，其对Hive的性能具有非常大的影响，特别是对于很难避免小文件的场景或task特别多的场景，这类场景大多数执行时间都很短。</p>
<p>Hadoop的默认配置通常是使用派生JVM来执行map和Reduce任务的。这时JVM的启动过程可能会造成相当大的开销，尤其是执行的job包含有成百上千task任务的情况。JVM重用可以使得JVM实例在同一个job中重新使用N次。N的值可以在Hadoop的mapred-site.xml文件中进行配置。通常在10-20之间，具体多少需要根据具体业务场景测试得出。</p>
<p>我们也可以在hive当中通过</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span>  mapred.job.reuse.jvm.num.tasks=<span class="number">10</span>;</span><br></pre></td></tr></table></figure>
<p>这个设置来设置我们的jvm重用</p>
<p>这个功能的缺点是，开启JVM重用将一直占用使用到的task插槽，以便进行重用，直到任务完成后才能释放。如果某个“不平衡的”job中有某几个reduce task执行的时间要比其他Reduce task消耗的时间多的多的话，那么保留的插槽就会一直空闲着却无法被其他的job使用，直到所有的task都结束了才会释放。</p>
<h3 id="10-11-推测执行"><a href="#10-11-推测执行" class="headerlink" title="10.11 推测执行"></a>10.11 <strong>推测执行</strong></h3><p>在分布式集群环境下，因为程序Bug（包括Hadoop本身的bug），负载不均衡或者资源分布不均等原因，会造成同一个作业的多个任务之间运行速度不一致，有些任务的运行速度可能明显慢于其他任务（比如一个作业的某个任务进度只有50%，而其他所有任务已经运行完毕），则这些任务会拖慢作业的整体执行进度。为了避免这种情况发生，Hadoop采用了推测执行（Speculative Execution）机制<code>，它根据一定的法则推测出“拖后腿”的任务，并为这样的任务启动一个备份任务，让该任务与原始任务同时处理同一份数据，并最终选用最先成功运行完成任务的计算结果作为最终结果。</code></p>
<p>设置开启推测执行参数：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> mapred.map.tasks.speculative.execution=<span class="literal">true</span></span><br><span class="line"><span class="keyword">set</span> mapred.reduce.tasks.speculative.execution=<span class="literal">true</span></span><br><span class="line"><span class="keyword">set</span> hive.mapred.reduce.tasks.speculative.execution=<span class="literal">true</span>;</span><br></pre></td></tr></table></figure>
<p>关于调优这些推测执行变量，还很难给一个具体的建议。如果用户对于运行时的偏差非常敏感的话，那么可以将这些功能关闭掉。如果用户因为输入数据量很大而需要执行长时间的map或者Reduce task的话，那么启动推测执行造成的浪费是非常巨大大。</p>
<h3 id><a href="#" class="headerlink" title=" "></a> </h3>
      
    </div>

    

    
    
    

    

    
      
    
    
      <div>
        <div id="reward-container">
  <div>Thank you for your accept. mua！</div>
  <button id="reward-button" disable="enable" onclick="var qr = document.getElementById(&quot;qr&quot;); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">

    
      
      
        
      
      <div style="display: inline-block">
        <img src="/images/reward/wechatpay.jpg" alt="丨HF丨 微信支付">
        <p>微信支付</p>
      </div>
    
      
      
        
      
      <div style="display: inline-block">
        <img src="/images/reward/alipay.jpg" alt="丨HF丨 支付宝">
        <p>支付宝</p>
      </div>
    

  </div>
</div>

      </div>
    

    
      <div>
        




  



<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>丨HF丨</li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    
    <a href="https://manzhong.github.io/2017/07/07/Hive.html" title="Hive">https://manzhong.github.io/2017/07/07/Hive.html</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li>
</ul>

      </div>
    
    <div>
      
        <div>
    
        <div style="text-align:center;color: #ccc;font-size:16px;">-------------本文结束<i class="fa fa-heart"></i>感谢您的阅读-------------</div>
    
</div>
      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/BigData/" rel="tag"><i class="fa fa-tag"></i> BigData</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/07/06/Yarn资源调度.html" rel="next" title="Yarn资源调度">
                <i class="fa fa-chevron-left"></i> Yarn资源调度
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/07/08/flume.html" rel="prev" title="flume">
                flume <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  
    <div class="comments" id="comments">
      <div id="lv-container" data-id="city" data-uid="MTAyMC81MDA2Mi8yNjU1Mw=="></div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <div class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/hexo.jpg" alt="丨HF丨">
            
              <p class="site-author-name" itemprop="name">丨HF丨</p>
              <div class="site-description motion-element" itemprop="description">第二名就是头号输家!!!</div>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">54</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  
                    
                      <a href="/categories/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">6</span>
                    <span class="site-state-item-name">分类</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                      <a href="/tags/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">13</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://github.com/ipyker" title="GitHub &rarr; https://github.com/ipyker" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="mailto:pyker@qq.com" title="E-Mail &rarr; mailto:pyker@qq.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://weibo.com/viszhang" title="Weibo &rarr; https://weibo.com/viszhang" rel="noopener" target="_blank"><i class="fa fa-fw fa-weibo"></i>Weibo</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="tencent://message/?uin=123435796&Site=&menu=yes" title="QQ &rarr; tencent://message/?uin=123435796&Site=&menu=yes" rel="noopener" target="_blank"><i class="fa fa-fw fa-qq"></i>QQ</a>
                </span>
              
            </div>
          

          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-inline">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-book"></i>
                推荐阅读
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="http://www.54tianzhisheng.cn/tags/Flink/" title="http://www.54tianzhisheng.cn/tags/Flink/" rel="noopener" target="_blank">Flink</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://nginxconfig.io/" title="https://nginxconfig.io/" rel="noopener" target="_blank">Nginxconfig</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="http://linux.51yip.com/" title="http://linux.51yip.com/" rel="noopener" target="_blank">Linux命令手册</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://echarts.baidu.com/index.html" title="https://echarts.baidu.com/index.html" rel="noopener" target="_blank">echarts可视化库</a>
                  </li>
                
              </ul>
            </div>
          

          
            
          
          
        </div>
      </div>

      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
            <span id="scrollpercent"><span>0</span>%</span>
          
        </div>
      

    </div>
  </aside>
  


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" style=" text-align:center;">&copy; 2017 – <span itemprop="copyrightYear">2020</span>
  <span class="with-love" id="animate">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">HF</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"> 站点字数合计:</i>
    </span>
    
    <span title="站点总字数">961k</span>
  
  
  <span class="post-meta-divider">|</span>
  <a href="http://www.beian.miit.gov.cn" rel="noopener" target="_blank">粤ICP备19028706号 </a>

</div>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>







<div class="run_time" style=" text-align:center;">
  <span id="timeDate">载入天数...</span><span id="times">载入时分秒...</span>
  <script>
    var now = new Date(); 
    function createtime() { 
        var grt= new Date("07/23/2017 10:00:00");//此处修改你的建站时间或者网站上线时间 
        now.setTime(now.getTime()+250); 
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days); 
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours); 
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum); 
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;} 
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum); 
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;} 
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 "; 
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒"; 
    } 
    setInterval("createtime()",250);
  </script>
</div>
        
<div class="busuanzi-count" style=" text-align:center;">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      我的第 <span class="busuanzi-value" id="busuanzi_value_site_uv"></span> 位朋友，
    </span>
  



  
    <span class="site-pv">
      历经 <span class="busuanzi-value" id="busuanzi_value_site_pv"></span> 次回眸才与你相遇
    </span>
  
</div>










        
      </div>
    </footer>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>





  



  






  



  
    
    
      
    
  
  <script color="255,0,255" opacity="0.7" zindex="-1" count="140" src="//cdn.jsdelivr.net/gh/theme-next/theme-next-canvas-nest@1.0.0/canvas-nest.min.js"></script>













  
  <script src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script src="/lib/fancybox/source/jquery.fancybox.pack.js"></script>


  


  <script src="/js/utils.js?v=7.1.1"></script>

  <script src="/js/motion.js?v=7.1.1"></script>



  
  


  <script src="/js/schemes/muse.js?v=7.1.1"></script>




  
  <script src="/js/scrollspy.js?v=7.1.1"></script>
<script src="/js/post-details.js?v=7.1.1"></script>



  


  <script src="/js/next-boot.js?v=7.1.1"></script>


  

  

  
  
  

  

  
  
  


  


  
    <script>
  window.livereOptions = {
    refer: '2017/07/07/Hive.html'
  };
  (function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
  })(document, 'script');
</script>

  


  
  <script>
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url).replace(/\/{2,}/g, '/');
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x"></i></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x"></i></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  

  

  
  <script>
    (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      bp.src = (curProtocol === 'https') ? 'https://zz.bdstatic.com/linksubmit/push.js' : 'http://push.zhanzhang.baidu.com/push.js';
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>


  

  

  

  

  
  
  
  <script src="/lib/bookmark/bookmark.min.js?v=1.0"></script>
  <script>
  
    bookmark.scrollToMark('auto', "#更多");
  
  </script>


  

  

  

</body>
</html>
<!-- 页面点击小红心 -->
<script type="text/javascript" src="/js/src/clicklove.js"></script>
