<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-bounce.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":false,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="Hbase增强 一 Hbase与MapReduce的集成 HBase当中的数据最终都是存储在HDFS上面的，HBase天生的支持MR的操作，我们可以通过MR直接处理HBase当中的数据，并且MR可以将处理后的结果直接存储到HBase当中去  需求一 读取myuser这张表当中的数据写入到HBase的另外一张表当中去 1 创建myuser2 表 其中列簇名与myuser中列簇名一致  依赖:  1">
<meta property="og:type" content="article">
<meta property="og:title" content="Hbase增强">
<meta property="og:url" content="http://example.com/2017/07/18/Hbase%E5%A2%9E%E5%BC%BA/index.html">
<meta property="og:site_name" content="春雨里洗过的太阳">
<meta property="og:description" content="Hbase增强 一 Hbase与MapReduce的集成 HBase当中的数据最终都是存储在HDFS上面的，HBase天生的支持MR的操作，我们可以通过MR直接处理HBase当中的数据，并且MR可以将处理后的结果直接存储到HBase当中去  需求一 读取myuser这张表当中的数据写入到HBase的另外一张表当中去 1 创建myuser2 表 其中列簇名与myuser中列簇名一致  依赖:  1">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/images/Hbase/xcl.png">
<meta property="og:image" content="http://example.com/images/Hbase/xcl2.png">
<meta property="og:image" content="http://example.com/images/Hbase/r.jpg">
<meta property="article:published_time" content="2017-07-18T01:07:02.000Z">
<meta property="article:modified_time" content="2020-12-13T13:20:33.541Z">
<meta property="article:author" content="HF">
<meta property="article:tag" content="Hbase">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/images/Hbase/xcl.png">

<link rel="canonical" href="http://example.com/2017/07/18/Hbase%E5%A2%9E%E5%BC%BA/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>
<link rel="stylesheet" type="text/css" href="/css/injector.css" />
  <title>Hbase增强 | 春雨里洗过的太阳</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">春雨里洗过的太阳</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">世间所有的相遇，都是久别重逢</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2017/07/18/Hbase%E5%A2%9E%E5%BC%BA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/hexo.jpg">
      <meta itemprop="name" content="HF">
      <meta itemprop="description" content="第二名就是头号输家">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="春雨里洗过的太阳">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Hbase增强
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2017-07-18 09:07:02" itemprop="dateCreated datePublished" datetime="2017-07-18T09:07:02+08:00">2017-07-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-12-13 21:20:33" itemprop="dateModified" datetime="2020-12-13T21:20:33+08:00">2020-12-13</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Hbase/" itemprop="url" rel="index"><span itemprop="name">Hbase</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>37k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>34 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="Hbase增强"><a href="#Hbase增强" class="headerlink" title="Hbase增强"></a>Hbase增强</h1><h2 id="一-Hbase与MapReduce的集成"><a href="#一-Hbase与MapReduce的集成" class="headerlink" title="一 Hbase与MapReduce的集成"></a>一 Hbase与MapReduce的集成</h2><p>HBase当中的数据最终都是存储在HDFS上面的，HBase天生的支持MR的操作，我们可以通过MR直接处理HBase当中的数据，并且MR可以将处理后的结果直接存储到HBase当中去</p>
<h3 id="需求一-读取myuser这张表当中的数据写入到HBase的另外一张表当中去"><a href="#需求一-读取myuser这张表当中的数据写入到HBase的另外一张表当中去" class="headerlink" title="需求一   读取myuser这张表当中的数据写入到HBase的另外一张表当中去"></a>需求一   读取myuser这张表当中的数据写入到HBase的另外一张表当中去</h3><h4 id="1-创建myuser2-表"><a href="#1-创建myuser2-表" class="headerlink" title="1 创建myuser2 表"></a>1 创建myuser2 表</h4><p>其中列簇名与myuser中列簇名一致</p>
<p>依赖:</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.hbase/hbase-client --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hbase<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hbase-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.hbase/hbase-server --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hbase<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hbase-server<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.12<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">scope</span>&gt;</span>test<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.testng<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>testng<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>6.14.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">scope</span>&gt;</span>test<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.hbase/hbase-mapreduce --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hbase<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hbase-mapreduce<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.5<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-hdfs<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span> 2.7.5<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.5<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-compiler-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">source</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">source</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">target</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">target</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">encoding</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">encoding</span>&gt;</span></span><br><span class="line">                    <span class="comment">&lt;!--    &lt;verbal&gt;true&lt;/verbal&gt;--&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line"></span><br><span class="line">            <span class="comment">&lt;!--将我们其他用到的一些jar包全部都打包进来  --&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-shade-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">goal</span>&gt;</span>shade<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">minimizeJar</span>&gt;</span>false<span class="tag">&lt;/<span class="name">minimizeJar</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h4 id="定义mapper类"><a href="#定义mapper类" class="headerlink" title="定义mapper类"></a>定义mapper类</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 负责读取myuser表当中的数据</span></span><br><span class="line"><span class="comment"> * 如果mapper类需要读取hbase表数据，那么我们mapper类需要继承TableMapper这样的一个类</span></span><br><span class="line"><span class="comment"> * 将key2   value2定义成 text  和put类型</span></span><br><span class="line"><span class="comment"> * text里面装rowkey</span></span><br><span class="line"><span class="comment"> * put装我们需要插入的数据</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HBaseSourceMapper</span> <span class="keyword">extends</span> <span class="title">TableMapper</span>&lt;<span class="title">Text</span>,<span class="title">Put</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> key  rowkey</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> value  result对象，封装了我们一条条的数据</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> context  上下文对象</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> InterruptedException</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * 需求：读取myuser表当中f1列族下面的name和age列</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     ImmutableBytesWritable 封装了rowkey</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(ImmutableBytesWritable key, Result value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">       <span class="comment">//获取到rowkey的字节数组</span></span><br><span class="line">        <span class="keyword">byte</span>[] bytes = key.get();</span><br><span class="line">        String rowkey = Bytes.toString(bytes);</span><br><span class="line"></span><br><span class="line">        Put put = <span class="keyword">new</span> Put(bytes);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//获取到所有的cell</span></span><br><span class="line">        List&lt;Cell&gt; cells = value.listCells();</span><br><span class="line">        <span class="keyword">for</span> (Cell cell : cells) &#123;</span><br><span class="line">            <span class="comment">//获取cell对应的列族</span></span><br><span class="line">            <span class="keyword">byte</span>[] familyBytes = CellUtil.cloneFamily(cell);</span><br><span class="line">            <span class="comment">//获取对应的列</span></span><br><span class="line">            <span class="keyword">byte</span>[] qualifierBytes = CellUtil.cloneQualifier(cell);</span><br><span class="line">            <span class="comment">//这里判断我们只需要f1列族，下面的name和age列</span></span><br><span class="line">            <span class="keyword">if</span>(Bytes.toString(familyBytes).equals(<span class="string">&quot;f1&quot;</span>) &amp;&amp; Bytes.toString(qualifierBytes).equals(<span class="string">&quot;name&quot;</span>) ||  Bytes.toString(qualifierBytes).equals(<span class="string">&quot;age&quot;</span>))&#123;</span><br><span class="line">                put.add(cell);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//将数据写出去</span></span><br><span class="line">        <span class="keyword">if</span>(!put.isEmpty())&#123;</span><br><span class="line">            context.write(<span class="keyword">new</span> Text(rowkey),put);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="定义-reduce"><a href="#定义-reduce" class="headerlink" title="定义 reduce"></a>定义 reduce</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.Put;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.io.ImmutableBytesWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.mapreduce.TableReducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 负责将数据写入到myuser2</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HBaseSinkReducer</span> <span class="keyword">extends</span> <span class="title">TableReducer</span>&lt;<span class="title">Text</span>,<span class="title">Put</span>,<span class="title">ImmutableBytesWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;Put&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (Put put : values) &#123;</span><br><span class="line">            context.write(<span class="keyword">new</span> ImmutableBytesWritable(key.toString().getBytes()),put);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="定义主类"><a href="#定义主类" class="headerlink" title="定义主类"></a>定义主类</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configured;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.HBaseConfiguration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.Put;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.Scan;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.Tool;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.ToolRunner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> javax.swing.plaf.nimbus.AbstractRegionPainter;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HBaseMain</span> <span class="keyword">extends</span> <span class="title">Configured</span> <span class="keyword">implements</span> <span class="title">Tool</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">run</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Job job = Job.getInstance(<span class="keyword">super</span>.getConf(), <span class="string">&quot;hbaseMR&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//打包运行，必须设置main方法所在的主类</span></span><br><span class="line">        job.setJarByClass(HBaseMain.class);</span><br><span class="line"></span><br><span class="line">        Scan scan = <span class="keyword">new</span> Scan();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//定义我们的mapper类和reducer类</span></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * String table, Scan scan,</span></span><br><span class="line"><span class="comment">         Class&lt;? extends TableMapper&gt; mapper,</span></span><br><span class="line"><span class="comment">         Class&lt;?&gt; outputKeyClass,</span></span><br><span class="line"><span class="comment">         Class&lt;?&gt; outputValueClass, Job job,</span></span><br><span class="line"><span class="comment">         boolean addDependencyJars</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        TableMapReduceUtil.initTableMapperJob(<span class="string">&quot;myuser&quot;</span>,scan,HBaseSourceMapper.class, Text.class, Put.class,job,<span class="keyword">false</span>);</span><br><span class="line">        <span class="comment">//使用工具类初始化reducer类</span></span><br><span class="line">        TableMapReduceUtil.initTableReducerJob(<span class="string">&quot;myuser2&quot;</span>,HBaseSinkReducer.class,job);</span><br><span class="line">        <span class="keyword">boolean</span> b = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">        <span class="keyword">return</span> b?<span class="number">0</span>:<span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//程序入口类</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//Configuration conf, Tool tool, String[] args</span></span><br><span class="line">        Configuration configuration = HBaseConfiguration.create();</span><br><span class="line">        configuration.set(<span class="string">&quot;hbase.zookeeper.quorum&quot;</span>,<span class="string">&quot;node01:2181,node02:2181,node03:2181&quot;</span>);</span><br><span class="line">        <span class="keyword">int</span> run = ToolRunner.run(configuration, <span class="keyword">new</span> HBaseMain(), args);</span><br><span class="line">        System.exit(run);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="运行"><a href="#运行" class="headerlink" title="运行"></a>运行</h4><p>1  本地运行</p>
<p>直接选中main方法所在的类，运行即可</p>
<p>2 打包集群运行</p>
<p>注意，我们需要使用打包插件，将HBase的依赖jar包都打入到工程jar包里面去</p>
<p>pom.xml当中添加打包插件</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-shade-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goal</span>&gt;</span>shade<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">minimizeJar</span>&gt;</span>true<span class="tag">&lt;/<span class="name">minimizeJar</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>代码中添加:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">job.setJarByClass(HBaseMain.class);</span><br></pre></td></tr></table></figure>

<p>使用maven打包</p>
<p>将jar包上传服务器:运行</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yarn jar hbaseStudy-1.0-SNAPSHOT.jar  cn.baidu.hbasemr.HBaseMR</span><br></pre></td></tr></table></figure>

<p>或者我们也可以自己设置我们的环境变量，然后运行original那个比较小的jar包</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_HOME&#x3D;&#x2F;export&#x2F;servers&#x2F;hadoop-2.7.5&#x2F;</span><br><span class="line">export HBASE_HOME&#x3D;&#x2F;export&#x2F;servers&#x2F;hbase-2.0.0&#x2F;</span><br><span class="line">export HADOOP_CLASSPATH&#x3D;$&#123;HBASE_HOME&#125;&#x2F;bin&#x2F;hbase mapredcp</span><br><span class="line">yarn jar original-hbaseStudy-1.0-SNAPSHOT.jar  cn.baidu.hbasemr.HbaseMR</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="需求2-读取HDFS文件，写入到HBase表当中去"><a href="#需求2-读取HDFS文件，写入到HBase表当中去" class="headerlink" title="需求2 读取HDFS文件，写入到HBase表当中去"></a>需求2 读取HDFS文件，写入到HBase表当中去</h4><p>读取hdfs路径/hbase/input/user.txt，然后将数据写入到myuser2这张表当中去</p>
<p>准备数据:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -mkdir -p /hbase/input</span><br><span class="line">cd /export/servers/</span><br><span class="line">vim user.txt</span><br><span class="line"> </span><br><span class="line"> 0007    zhangsan        18</span><br><span class="line">0008    lisi    25</span><br><span class="line">0009    wangwu  20</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>上传hdfs:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -put user.txt &#x2F;hbase&#x2F;input</span><br></pre></td></tr></table></figure>

<h4 id="定义mapper"><a href="#定义mapper" class="headerlink" title="定义mapper"></a>定义mapper</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 通过这个mapper读取hdfs上面的文件，然后进行处理</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HDFSMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>,<span class="title">Text</span>,<span class="title">Text</span>,<span class="title">NullWritable</span>&gt;</span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//读取到数据之后不做任何处理，直接将数据写入到reduce里面去进行处理</span></span><br><span class="line">        context.write(value,NullWritable.get());</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="定义reduce"><a href="#定义reduce" class="headerlink" title="定义reduce"></a>定义reduce</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.Put;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.io.ImmutableBytesWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.mapreduce.TableReducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HBaseWriteReducer</span> <span class="keyword">extends</span> <span class="title">TableReducer</span>&lt;<span class="title">Text</span>,<span class="title">NullWritable</span>,<span class="title">ImmutableBytesWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 0007    zhangsan        18</span></span><br><span class="line"><span class="comment">     0008    lisi    25</span></span><br><span class="line"><span class="comment">     0009    wangwu  20</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> key</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> values</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> context</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> InterruptedException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;NullWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        String[] split = key.toString().split(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line"></span><br><span class="line">        Put put = <span class="keyword">new</span> Put(split[<span class="number">0</span>].getBytes());</span><br><span class="line">        put.addColumn(<span class="string">&quot;f1&quot;</span>.getBytes(),<span class="string">&quot;name&quot;</span>.getBytes(),split[<span class="number">1</span>].getBytes());</span><br><span class="line">        put.addColumn(<span class="string">&quot;f1&quot;</span>.getBytes(),<span class="string">&quot;age&quot;</span>.getBytes(),split[<span class="number">2</span>].getBytes());</span><br><span class="line">        <span class="comment">//将我们的数据写出去，key3是ImmutableBytesWritable，这个里面装的是rowkey</span></span><br><span class="line">        <span class="comment">//然后将写出去的数据封装到put对象里面去了</span></span><br><span class="line">        context.write(<span class="keyword">new</span> ImmutableBytesWritable(split[<span class="number">0</span>].getBytes()),put);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="定义主类-1"><a href="#定义主类-1" class="headerlink" title="定义主类"></a>定义主类</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configured;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.HBaseConfiguration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.TextInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.Tool;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.ToolRunner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> javax.swing.plaf.nimbus.AbstractRegionPainter;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HdfsHBaseMain</span> <span class="keyword">extends</span> <span class="title">Configured</span> <span class="keyword">implements</span> <span class="title">Tool</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">run</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//获取job对象</span></span><br><span class="line">        Job job = Job.getInstance(<span class="keyword">super</span>.getConf(), <span class="string">&quot;hdfs2Hbase&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//第一步：读取文件，解析成key，value对</span></span><br><span class="line">        job.setInputFormatClass(TextInputFormat.class);</span><br><span class="line">        TextInputFormat.addInputPath(job,<span class="keyword">new</span> Path(<span class="string">&quot;hdfs://node01:8020/hbase/input&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//第二步：自定义map逻辑，接受k1,v1，转换成为k2  v2进行输出</span></span><br><span class="line">        job.setMapperClass(HDFSMapper.class);</span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(NullWritable.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//分区，排序，规约，分组</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//第七步：设置reduce类</span></span><br><span class="line">        TableMapReduceUtil.initTableReducerJob(<span class="string">&quot;myuser2&quot;</span>,HBaseWriteReducer.class,job);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">boolean</span> b = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> b?<span class="number">0</span>:<span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Configuration configuration = HBaseConfiguration.create();</span><br><span class="line">        configuration.set(<span class="string">&quot;hbase.zookeeper.quorum&quot;</span>,<span class="string">&quot;node01:2181,node02:2181,node03:2181&quot;</span>);</span><br><span class="line">        <span class="keyword">int</span> run = ToolRunner.run(configuration, <span class="keyword">new</span> HdfsHBaseMain(), args);</span><br><span class="line">        System.exit(run);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="需求三-通过bulkload的方式批量加载数据到HBase当中去"><a href="#需求三-通过bulkload的方式批量加载数据到HBase当中去" class="headerlink" title="需求三 通过bulkload的方式批量加载数据到HBase当中去"></a>需求三 通过bulkload的方式批量加载数据到HBase当中去</h4><p>加载数据到HBase当中去的方式多种多样，我们可以使用HBase的javaAPI或者使用sqoop将我们的数据写入或者导入到HBase当中去，但是这些方式不是慢就是在导入的过程的占用Region资源导致效率低下，我们也可以通过MR的程序，将我们的数据直接转换成HBase的最终存储格式HFile，然后直接load数据到HBase当中去即可</p>
<p>HBase中每张Table在根目录（/HBase）下用一个文件夹存储，Table名为文件夹名，在Table文件夹下每个Region同样用一个文件夹存储，每个Region文件夹下的每个列族也用文件夹存储，而每个列族下存储的就是一些HFile文件，HFile就是HBase数据在HFDS下存储格式，所以HBase存储文件最终在hdfs上面的表现形式就是HFile，如果我们可以直接将数据转换为HFile的格式，那么我们的HBase就可以直接读取加载HFile格式的文件，就可以直接读取了</p>
<p>优点：</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1.导入过程不占用Region资源 </span><br><span class="line">2.能快速导入海量的数据</span><br><span class="line">3.节省内存</span><br></pre></td></tr></table></figure>

<p>使用bulkload的方式将我们的数据直接生成HFile格式，然后直接加载到HBase的表当中去,不走hlog和hRegionServer.</p>
<p>例如:</p>
<p>将我们hdfs上面的这个路径/hbase/input/user.txt的数据文件，转换成HFile格式，然后load到myuser2这张表里面去</p>
<h4 id="定义mapper-1"><a href="#定义mapper-1" class="headerlink" title="定义mapper"></a>定义mapper</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.Put;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.io.ImmutableBytesWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HDFSReadMapper</span>  <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>,<span class="title">Text</span>,<span class="title">ImmutableBytesWritable</span>,<span class="title">Put</span>&gt;</span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 0007    zhangsan        18</span></span><br><span class="line"><span class="comment">     0008    lisi    25</span></span><br><span class="line"><span class="comment">     0009    wangwu  20</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> key</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> value</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> context</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> InterruptedException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        String[] split = value.toString().split(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line"></span><br><span class="line">        Put put = <span class="keyword">new</span> Put(split[<span class="number">0</span>].getBytes());</span><br><span class="line">        put.addColumn(<span class="string">&quot;f1&quot;</span>.getBytes(),<span class="string">&quot;name&quot;</span>.getBytes(),split[<span class="number">1</span>].getBytes());</span><br><span class="line">        put.addColumn(<span class="string">&quot;f1&quot;</span>.getBytes(),<span class="string">&quot;age&quot;</span>.getBytes(),split[<span class="number">2</span>].getBytes());</span><br><span class="line"></span><br><span class="line">        context.write(<span class="keyword">new</span> ImmutableBytesWritable(split[<span class="number">0</span>].getBytes()),put);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="主类-程序入口"><a href="#主类-程序入口" class="headerlink" title="主类 程序入口"></a>主类 程序入口</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configured;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.HBaseConfiguration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.TableName;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.Connection;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.ConnectionFactory;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.Put;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.Table;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.io.ImmutableBytesWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.mapreduce.HFileOutputFormat2;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hdfs.DFSUtil;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.TextInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.Tool;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.ToolRunner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BulkLoadMain</span> <span class="keyword">extends</span> <span class="title">Configured</span> <span class="keyword">implements</span> <span class="title">Tool</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">run</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Configuration conf = <span class="keyword">super</span>.getConf();</span><br><span class="line">        <span class="comment">//获取job对象</span></span><br><span class="line">        Job job = Job.getInstance(conf, <span class="string">&quot;bulkLoad&quot;</span>);</span><br><span class="line">        Connection connection = ConnectionFactory.createConnection(conf);</span><br><span class="line">        Table table = connection.getTable(TableName.valueOf(<span class="string">&quot;myuser2&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//读取文件</span></span><br><span class="line">        job.setInputFormatClass(TextInputFormat.class);</span><br><span class="line">        TextInputFormat.addInputPath(job,<span class="keyword">new</span> Path(<span class="string">&quot;hdfs://node01:8020/hbase/input&quot;</span>));</span><br><span class="line"></span><br><span class="line">        job.setMapperClass(HDFSReadMapper.class);</span><br><span class="line">        job.setMapOutputKeyClass(ImmutableBytesWritable.class);</span><br><span class="line">        job.setMapOutputValueClass(Put.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//将数据输出成为HFile格式</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//Job job, Table table, RegionLocator regionLocator</span></span><br><span class="line">        <span class="comment">//配置增量的添加数据</span></span><br><span class="line">        HFileOutputFormat2.configureIncrementalLoad(job,table,connection.getRegionLocator(TableName.valueOf(<span class="string">&quot;myuser2&quot;</span>)));</span><br><span class="line">        <span class="comment">//设置输出classs类，决定了我们输出数据格式</span></span><br><span class="line">        job.setOutputFormatClass(HFileOutputFormat2.class);</span><br><span class="line">        <span class="comment">//设置输出路径</span></span><br><span class="line">        HFileOutputFormat2.setOutputPath(job,<span class="keyword">new</span> Path(<span class="string">&quot;hdfs://node01:8020/hbase/hfile_out&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="keyword">boolean</span> b = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> b?<span class="number">0</span>:<span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Configuration configuration = HBaseConfiguration.create();</span><br><span class="line">        configuration.set(<span class="string">&quot;hbase.zookeeper.quorum&quot;</span>,<span class="string">&quot;node01:2181,node02:2181,node03:2181&quot;</span>);</span><br><span class="line">        <span class="keyword">int</span> run = ToolRunner.run(configuration, <span class="keyword">new</span> BulkLoadMain(), args);</span><br><span class="line">        System.exit(run);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>打jar包上传运行:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yarn jar original-hbaseStudy-1.0-SNAPSHOT.jar  cn.baidu.hbasemr.HBaseLoad</span><br></pre></td></tr></table></figure>

<h4 id="开发代码-加载数据"><a href="#开发代码-加载数据" class="headerlink" title="开发代码  加载数据"></a>开发代码  加载数据</h4><p>将我们的输出路径下面的HFile文件，加载到我们的hbase表当中去</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.HBaseConfiguration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.TableName;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.Admin;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.Connection;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.ConnectionFactory;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.Table;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LoadData</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Configuration configuration = HBaseConfiguration.create();</span><br><span class="line">        configuration.set(<span class="string">&quot;hbase.zookeeper.property.clientPort&quot;</span>, <span class="string">&quot;2181&quot;</span>);</span><br><span class="line">        configuration.set(<span class="string">&quot;hbase.zookeeper.quorum&quot;</span>, <span class="string">&quot;node01,node02,node03&quot;</span>);</span><br><span class="line">        Connection connection =  ConnectionFactory.createConnection(configuration);</span><br><span class="line">        Admin admin = connection.getAdmin();</span><br><span class="line">        Table table = connection.getTable(TableName.valueOf(<span class="string">&quot;myuser2&quot;</span>));</span><br><span class="line">        LoadIncrementalHFiles load = <span class="keyword">new</span> LoadIncrementalHFiles(configuration);</span><br><span class="line">        load.doBulkLoad(<span class="keyword">new</span> Path(<span class="string">&quot;hdfs://node01:8020/hbase/hfile_out&quot;</span>), admin,table,connection.getRegionLocator(TableName.valueOf(<span class="string">&quot;myuser2&quot;</span>)));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>或者我们也可以通过命令行来进行加载数据</p>
<p>先将hbase的jar包添加到hadoop的classpath路径下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export HBASE_HOME&#x3D;&#x2F;export&#x2F;servers&#x2F;hbase-2.0.0&#x2F;</span><br><span class="line">export HADOOP_HOME&#x3D;&#x2F;export&#x2F;servers&#x2F;hadoop-2.7.5&#x2F;</span><br><span class="line">export HADOOP_CLASSPATH&#x3D;$&#123;HBASE_HOME&#125;&#x2F;bin&#x2F;hbase mapredcp</span><br></pre></td></tr></table></figure>

<p>然后执行以下命令，将hbase的HFile直接导入到表myuser2当中来</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yarn jar &#x2F;export&#x2F;servers&#x2F;hbase-2.0.0&#x2F;lib&#x2F;hbase-server-1.2.0-cdh5.14.0.jar completebulkload &#x2F;hbase&#x2F;hfile_out myuser2</span><br></pre></td></tr></table></figure>

<p>##二 hive 与Hbase的对比</p>
<h2 id="Hive"><a href="#Hive" class="headerlink" title="Hive"></a>Hive</h2><h3 id="数据仓库工具"><a href="#数据仓库工具" class="headerlink" title="数据仓库工具"></a>数据仓库工具</h3><p>Hive的本质其实就相当于将HDFS中已经存储的文件在Mysql中做了一个双射关系，以方便使用HQL去管理查询。</p>
<h3 id="用于数据分析、清洗"><a href="#用于数据分析、清洗" class="headerlink" title="用于数据分析、清洗"></a>用于数据分析、清洗</h3><p>Hive适用于离线的数据分析和清洗，延迟较高</p>
<h3 id="基于HDFS、MapReduce"><a href="#基于HDFS、MapReduce" class="headerlink" title="基于HDFS、MapReduce"></a>基于HDFS、MapReduce</h3><p>Hive存储的数据依旧在DataNode上，编写的HQL语句终将是转换为MapReduce代码执行。</p>
<h2 id="HBase"><a href="#HBase" class="headerlink" title="HBase"></a>HBase</h2><h3 id="nosql数据库"><a href="#nosql数据库" class="headerlink" title="nosql数据库"></a>nosql数据库</h3><p>是一种面向列存储的非关系型数据库。</p>
<h3 id="用于存储结构化和非结构话的数据"><a href="#用于存储结构化和非结构话的数据" class="headerlink" title="用于存储结构化和非结构话的数据"></a>用于存储结构化和非结构话的数据</h3><p>适用于单表非关系型数据的存储，不适合做关联查询，类似JOIN等操作。</p>
<h3 id="基于HDFS"><a href="#基于HDFS" class="headerlink" title="基于HDFS"></a>基于HDFS</h3><p>数据持久化存储的体现形式是Hfile，存放于DataNode中，被ResionServer以region的形式进行管理。</p>
<h3 id="延迟较低，接入在线业务使用"><a href="#延迟较低，接入在线业务使用" class="headerlink" title="延迟较低，接入在线业务使用"></a>延迟较低，接入在线业务使用</h3><p>面对大量的企业数据，HBase可以直线单表大量数据的存储，同时提供了高效的数据访问速度。</p>
<h3 id="总结：Hive与HBase"><a href="#总结：Hive与HBase" class="headerlink" title="总结：Hive与HBase"></a>总结：Hive与HBase</h3><p>Hive和Hbase是两种基于Hadoop的不同技术，Hive是一种类SQL的引擎，并且运行MapReduce任务，Hbase是一种在Hadoop之上的NoSQL 的Key/vale数据库。这两种工具是可以同时使用的。就像用Google来搜索，用FaceBook进行社交一样，Hive可以用来进行统计查询，HBase可以用来进行实时查询，数据也可以从Hive写到HBase，或者从HBase写回Hive。</p>
<p>##三 hive 与hbase的整合</p>
<p>hive与我们的HBase各有千秋，各自有着不同的功能，但是归根接地，hive与hbase的数据最终都是存储在hdfs上面的，一般的我们为了存储磁盘的空间，不会将一份数据存储到多个地方，导致磁盘空间的浪费，我们可以直接将数据存入hbase，然后通过hive整合hbase直接使用sql语句分析hbase里面的数据即可，非常方便</p>
<h3 id="需求一将hive分析结果的数据，保存到HBase当中去"><a href="#需求一将hive分析结果的数据，保存到HBase当中去" class="headerlink" title="需求一将hive分析结果的数据，保存到HBase当中去"></a>需求一将hive分析结果的数据，保存到HBase当中去</h3><h4 id="1-拷贝hbase的五个依赖jar包到hive的lib目录下"><a href="#1-拷贝hbase的五个依赖jar包到hive的lib目录下" class="headerlink" title="1 拷贝hbase的五个依赖jar包到hive的lib目录下"></a>1 拷贝hbase的五个依赖jar包到hive的lib目录下</h4><p>将我们HBase的五个jar包拷贝到hive的lib目录下</p>
<p>hbase的jar包都在/export/servers/hbase-2.0.0/lib</p>
<p>我们需要拷贝五个jar包名字如下</p>
<p>hbase-client-2.0.0.jar               </p>
<p>hbase-hadoop2-compat-2.0.0.jar</p>
<p>hbase-hadoop-compat-2.0.0.jar</p>
<p>hbase-it-2.0.0.jar    </p>
<p>hbase-server-2.0.0.jar</p>
<p>我们直接在node03执行以下命令，通过创建软连接的方式来进行jar包的依赖</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ln -s /export/servers/hbase-2.0.0/lib/hbase-client-2.0.0.jar /export/servers/apache-hive-2.1.0-bin/lib/hbase-client-2.0.0.jar</span><br><span class="line">ln -s /export/servers/hbase-2.0.0/lib/hbase-hadoop2-compat-2.0.0.jar /export/servers/apache-hive-2.1.0-bin/lib/hbase-hadoop2-compat-2.0.0.jar</span><br><span class="line">ln -s /export/servers/hbase-2.0.0/lib/hbase-hadoop-compat-2.0.0.jar /export/servers/apache-hive-2.1.0-bin/lib/hbase-hadoop-compat-2.0.0.jar</span><br><span class="line">ln -s /export/servers/hbase-2.0.0/lib/hbase-it-2.0.0.jar /export/servers/apache-hive-2.1.0-bin/lib/hbase-it-2.0.0.jar</span><br><span class="line">ln -s /export/servers/hbase-2.0.0/lib/hbase-server-2.0.0.jar /export/servers/apache-hive-2.1.0-bin/lib/hbase-server-2.0.0.jar    </span><br></pre></td></tr></table></figure>

<h4 id="2-修改hive的配置文件"><a href="#2-修改hive的配置文件" class="headerlink" title="2 修改hive的配置文件"></a>2 修改hive的配置文件</h4><p>编辑node03服务器上面的hive的配置文件hive-site.xml添加以下两行配置</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>node01,node02,node03<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>node01,node02,node03<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="3-修改hive-env-sh配置文件添加以下配置"><a href="#3-修改hive-env-sh配置文件添加以下配置" class="headerlink" title="3 修改hive-env.sh配置文件添加以下配置"></a>3 修改hive-env.sh配置文件添加以下配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> HADOOP_HOME=/<span class="built_in">export</span>/servers/hadoop-2.7.5</span><br><span class="line"><span class="built_in">export</span> HBASE_HOME=/<span class="built_in">export</span>/servers/hbase-2.0.0</span><br><span class="line"><span class="built_in">export</span> HIVE_CONF_DIR=/<span class="built_in">export</span>/servers/apache-hive-2.1.0-bin/conf</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="4-hive当中建表并加载以下数据"><a href="#4-hive当中建表并加载以下数据" class="headerlink" title="4 hive当中建表并加载以下数据"></a>4 hive当中建表并加载以下数据</h4><h4 id="hive当中建表"><a href="#hive当中建表" class="headerlink" title="hive当中建表"></a>hive当中建表</h4><p>进入hive客户端</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin&#x2F;hive</span><br></pre></td></tr></table></figure>

<p>创建hive数据库与hive对应的数据库表</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">create database course;</span><br><span class="line">use course;</span><br><span class="line">create external table if not exists course.score(id int,cname string,score int) row format delimited fields terminated by &#39;\t&#39; stored as textfile;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="准备数据内容如下"><a href="#准备数据内容如下" class="headerlink" title="准备数据内容如下"></a>准备数据内容如下</h4><p>node03执行以下命令，准备数据文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">vim hive-hbase.txt</span><br><span class="line"></span><br><span class="line">1       zhangsan        80</span><br><span class="line">2       lisi    60</span><br><span class="line">3       wangwu  30</span><br><span class="line">4       zhaoliu 70</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="进行加载数据"><a href="#进行加载数据" class="headerlink" title="进行加载数据"></a>进行加载数据</h4><p>进入hive客户端进行加载数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (course)&gt; load data local inpath &#39;&#x2F;export&#x2F;hive-hbase.txt&#39; into table score;</span><br><span class="line">hive (course)&gt; select * from score;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="5-创建hive管理表与HBase进行映射"><a href="#5-创建hive管理表与HBase进行映射" class="headerlink" title="5 创建hive管理表与HBase进行映射"></a>5 创建hive管理表与HBase进行映射</h4><p>我们可以创建一个hive的管理表与hbase当中的表进行映射，hive管理表当中的数据，都会存储到hbase上面去</p>
<p>hive当中创建内部表</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">create table course.hbase_score(id int,cname string,score int)  </span><br><span class="line">stored by &#x27;org.apache.hadoop.hive.hbase.HBaseStorageHandler&#x27;  </span><br><span class="line">with serdeproperties(&quot;hbase.columns.mapping&quot; = &quot;cf:name,cf:score&quot;) </span><br><span class="line">tblproperties(&quot;hbase.table.name&quot; = &quot;hbase_score&quot;);</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>通过insert  overwrite select  插入数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite table course.hbase_score select id,cname,score from course.score;</span><br></pre></td></tr></table></figure>

<h4 id="6-hbase当中查看表hbase-score"><a href="#6-hbase当中查看表hbase-score" class="headerlink" title="6 hbase当中查看表hbase_score"></a>6 hbase当中查看表hbase_score</h4><p>进入hbase的客户端查看表hbase_score，并查看当中的数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):023:0&gt; list</span><br><span class="line">TABLE                                                                                       </span><br><span class="line">hbase_score                                                                                 </span><br><span class="line">myuser                                                                                      </span><br><span class="line">myuser2                                                                                     </span><br><span class="line">student                                                                                     </span><br><span class="line">user                                                                                        </span><br><span class="line">5 row(s) in 0.0210 seconds</span><br><span class="line"></span><br><span class="line">&#x3D;&gt; [&quot;hbase_score&quot;, &quot;myuser&quot;, &quot;myuser2&quot;, &quot;student&quot;, &quot;user&quot;]</span><br><span class="line">hbase(main):024:0&gt; scan &#39;hbase_score&#39;</span><br><span class="line">ROW                      COLUMN+CELL                                                        </span><br><span class="line"> 1                       column&#x3D;cf:name, timestamp&#x3D;1550628395266, value&#x3D;zhangsan            </span><br><span class="line"> 1                       column&#x3D;cf:score, timestamp&#x3D;1550628395266, value&#x3D;80                 </span><br><span class="line"> 2                       column&#x3D;cf:name, timestamp&#x3D;1550628395266, value&#x3D;lisi                </span><br><span class="line"> 2                       column&#x3D;cf:score, timestamp&#x3D;1550628395266, value&#x3D;60                 </span><br><span class="line"> 3                       column&#x3D;cf:name, timestamp&#x3D;1550628395266, value&#x3D;wangwu              </span><br><span class="line"> 3                       column&#x3D;cf:score, timestamp&#x3D;1550628395266, value&#x3D;30                 </span><br><span class="line"> 4                       column&#x3D;cf:name, timestamp&#x3D;1550628395266, value&#x3D;zhaoliu             </span><br><span class="line"> 4                       column&#x3D;cf:score, timestamp&#x3D;1550628395266, value&#x3D;70                 </span><br><span class="line">4 row(s) in 0.0360 seconds</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="需求二创建hive外部表，映射HBase当中已有的表模型，"><a href="#需求二创建hive外部表，映射HBase当中已有的表模型，" class="headerlink" title="需求二创建hive外部表，映射HBase当中已有的表模型，"></a>需求二创建hive外部表，映射HBase当中已有的表模型，</h3><h3 id="第一步：HBase当中创建表并手动插入加载一些数据"><a href="#第一步：HBase当中创建表并手动插入加载一些数据" class="headerlink" title="第一步：HBase当中创建表并手动插入加载一些数据"></a>第一步：HBase当中创建表并手动插入加载一些数据</h3><p>进入HBase的shell客户端，手动创建一张表，并插入加载一些数据进去</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">create &#39;hbase_hive_score&#39;,&#123; NAME &#x3D;&gt;&#39;cf&#39;&#125;</span><br><span class="line">put &#39;hbase_hive_score&#39;,&#39;1&#39;,&#39;cf:name&#39;,&#39;zhangsan&#39;</span><br><span class="line">put &#39;hbase_hive_score&#39;,&#39;1&#39;,&#39;cf:score&#39;, &#39;95&#39;</span><br><span class="line">put &#39;hbase_hive_score&#39;,&#39;2&#39;,&#39;cf:name&#39;,&#39;lisi&#39;</span><br><span class="line">put &#39;hbase_hive_score&#39;,&#39;2&#39;,&#39;cf:score&#39;, &#39;96&#39;</span><br><span class="line">put &#39;hbase_hive_score&#39;,&#39;3&#39;,&#39;cf:name&#39;,&#39;wangwu&#39;</span><br><span class="line">put &#39;hbase_hive_score&#39;,&#39;3&#39;,&#39;cf:score&#39;, &#39;97&#39;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>操作成功结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):049:0&gt; create &#39;hbase_hive_score&#39;,&#123; NAME &#x3D;&gt;&#39;cf&#39;&#125;</span><br><span class="line">0 row(s) in 1.2970 seconds</span><br><span class="line"></span><br><span class="line">&#x3D;&gt; Hbase::Table - hbase_hive_score</span><br><span class="line">hbase(main):050:0&gt; put &#39;hbase_hive_score&#39;,&#39;1&#39;,&#39;cf:name&#39;,&#39;zhangsan&#39;</span><br><span class="line">0 row(s) in 0.0600 seconds</span><br><span class="line"></span><br><span class="line">hbase(main):051:0&gt; put &#39;hbase_hive_score&#39;,&#39;1&#39;,&#39;cf:score&#39;, &#39;95&#39;</span><br><span class="line">0 row(s) in 0.0310 seconds</span><br><span class="line"></span><br><span class="line">hbase(main):052:0&gt; put &#39;hbase_hive_score&#39;,&#39;2&#39;,&#39;cf:name&#39;,&#39;lisi&#39;</span><br><span class="line">0 row(s) in 0.0230 seconds</span><br><span class="line"></span><br><span class="line">hbase(main):053:0&gt; put &#39;hbase_hive_score&#39;,&#39;2&#39;,&#39;cf:score&#39;, &#39;96&#39;</span><br><span class="line">0 row(s) in 0.0220 seconds</span><br><span class="line"></span><br><span class="line">hbase(main):054:0&gt; put &#39;hbase_hive_score&#39;,&#39;3&#39;,&#39;cf:name&#39;,&#39;wangwu&#39;</span><br><span class="line">0 row(s) in 0.0200 seconds</span><br><span class="line"></span><br><span class="line">hbase(main):055:0&gt; put &#39;hbase_hive_score&#39;,&#39;3&#39;,&#39;cf:score&#39;, &#39;97&#39;</span><br><span class="line">0 row(s) in 0.0250 seconds</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="第二步：建立hive的外部表，映射HBase当中的表以及字段"><a href="#第二步：建立hive的外部表，映射HBase当中的表以及字段" class="headerlink" title="第二步：建立hive的外部表，映射HBase当中的表以及字段"></a>第二步：建立hive的外部表，映射HBase当中的表以及字段</h3><p>在hive当中建立外部表，</p>
<p>进入hive客户端，然后执行以下命令进行创建hive外部表，就可以实现映射HBase当中的表数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CREATE external TABLE course.hbase2hive(id int, name string, score int) STORED BY &#39;org.apache.hadoop.hive.hbase.HBaseStorageHandler&#39; WITH SERDEPROPERTIES (&quot;hbase.columns.mapping&quot; &#x3D; &quot;:key,cf:name,cf:score&quot;) TBLPROPERTIES(&quot;hbase.table.name&quot; &#x3D;&quot;hbase_hive_score&quot;);</span><br></pre></td></tr></table></figure>

<h2 id="四-hbase预分区"><a href="#四-hbase预分区" class="headerlink" title="四 hbase预分区"></a>四 hbase预分区</h2><h2 id="1、为何要预分区？"><a href="#1、为何要预分区？" class="headerlink" title="1、为何要预分区？"></a>1、为何要预分区？</h2><p>* 增加数据读写效率</p>
<p>* 负载均衡，防止数据倾斜</p>
<p>* 方便集群容灾调度region</p>
<p>* 优化Map数量</p>
<h2 id="2、如何预分区？"><a href="#2、如何预分区？" class="headerlink" title="2、如何预分区？"></a>2、如何预分区？</h2><p>每一个region维护着startRow与endRowKey，如果加入的数据符合某个region维护的rowKey范围，则该数据交给这个region维护。</p>
<h2 id="3、如何设定预分区？"><a href="#3、如何设定预分区？" class="headerlink" title="3、如何设定预分区？"></a>3、如何设定预分区？</h2><h3 id="1、手动指定预分区"><a href="#1、手动指定预分区" class="headerlink" title="1、手动指定预分区"></a>1、手动指定预分区</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):001:0&gt; create &#39;staff&#39;,&#39;info&#39;,&#39;partition1&#39;,SPLITS &#x3D;&gt; [&#39;1000&#39;,&#39;2000&#39;,&#39;3000&#39;,&#39;4000&#39;]</span><br></pre></td></tr></table></figure>

<h3 id="2、使用16进制算法生成预分区"><a href="#2、使用16进制算法生成预分区" class="headerlink" title="2、使用16进制算法生成预分区"></a>2、使用16进制算法生成预分区</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):003:0&gt; create &#39;staff2&#39;,&#39;info&#39;,&#39;partition2&#39;,&#123;NUMREGIONS &#x3D;&gt; 15, SPLITALGO &#x3D;&gt; &#39;HexStringSplit&#39;&#125;</span><br></pre></td></tr></table></figure>

<h3 id="3、使用JavaAPI创建预分区"><a href="#3、使用JavaAPI创建预分区" class="headerlink" title="3、使用JavaAPI创建预分区"></a>3、使用JavaAPI创建预分区</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">同 hbase上篇</span><br></pre></td></tr></table></figure>

<h2 id="五-HBase的rowKey设计技巧"><a href="#五-HBase的rowKey设计技巧" class="headerlink" title="五 HBase的rowKey设计技巧"></a>五 HBase的rowKey设计技巧</h2><p>HBase是三维有序存储的，通过rowkey（行键），column key（column family和qualifier）和TimeStamp（时间戳）这个三个维度可以对HBase中的数据进行快速定位。</p>
<p>HBase中rowkey可以唯一标识一行记录，在HBase查询的时候，有以下几种方式：</p>
<ol>
<li><p>通过get方式，指定rowkey获取唯一一条记录</p>
</li>
<li><p>通过scan方式，设置startRow和stopRow参数进行范围匹配</p>
</li>
<li><p>全表扫描，即直接扫描整张表中所有行记录</p>
</li>
</ol>
<h3 id="1-rowkey长度原则"><a href="#1-rowkey长度原则" class="headerlink" title="1 rowkey长度原则"></a>1 rowkey长度原则</h3><p>rowkey是一个二进制码流，可以是任意字符串，最大长度64kb，实际应用中一般为10-100bytes，以byte[]形式保存，一般设计成定长。</p>
<p>建议越短越好，不要超过16个字节，原因如下：</p>
<p>v  数据的持久化文件HFile中是按照KeyValue存储的，如果rowkey过长，比如超过100字节，1000w行数据，光rowkey就要占用100*<br>1000w=10亿个字节，将近1G数据，这样会极大影响HFile的存储效率；</p>
<p>v  MemStore将缓存部分数据到内存，如果rowkey字段过长，内存的有效利用率就会降低，系统不能缓存更多的数据，这样会降低检索效率。</p>
<h3 id="2-rowkey散列原则"><a href="#2-rowkey散列原则" class="headerlink" title="2 rowkey散列原则"></a>2 rowkey散列原则</h3><p>如果rowkey按照时间戳的方式递增，不要将时间放在二进制码的前面，建议将rowkey的高位作为散列字段，由程序随机生成，低位放时间字段，这样将提高数据均衡分布在每个RegionServer，以实现负载均衡的几率。如果没有散列字段，首字段直接是时间信息，所有的数据都会集中在一个RegionServer上，这样在数据检索的时候负载会集中在个别的RegionServer上，造成热点问题，会降低查询效率。</p>
<h3 id="3-rowkey唯一原则"><a href="#3-rowkey唯一原则" class="headerlink" title="3 rowkey唯一原则"></a>3 rowkey唯一原则</h3><p>必须在设计上保证其唯一性，rowkey是按照字典顺序排序存储的，因此，设计rowkey的时候，要充分利用这个排序的特点，将经常读取的数据存储到一块，将最近可能会被访问的数据放到一块。</p>
<h3 id="4什么是热点"><a href="#4什么是热点" class="headerlink" title="4什么是热点"></a>4什么是热点</h3><p>HBase中的行是按照rowkey的字典顺序排序的，这种设计优化了scan操作，可以将相关的行以及会被一起读取的行存取在临近位置，便于scan。然而糟糕的rowkey设计是热点的源头。 </p>
<p>热点发生在大量的client直接访问集群的一个或极少数个节点（访问可能是读，写或者其他操作）。大量访问会使热点region所在的单个机器超出自身承受能力，引起性能下降甚至region不可用，这也会影响同一个RegionServer上的其他region，由于主机无法服务其他region的请求。 </p>
<p>设计良好的数据访问模式以使集群被充分，均衡的利用。为了避免写热点，设计rowkey使得不同行在同一个region，但是在更多数据情况下，数据应该被写入集群的多个region，而不是一个。下面是一些常见的避免热点的方法以及它们的优缺点：</p>
<h4 id="1加盐"><a href="#1加盐" class="headerlink" title="1加盐"></a>1加盐</h4><p>这里所说的加盐不是密码学中的加盐，而是在rowkey的前面增加随机数，具体就是给rowkey分配一个随机前缀以使得它和之前的rowkey的开头不同。分配的前缀种类数量应该和你想使用数据分散到不同的region的数量一致。加盐之后的rowkey就会根据随机生成的前缀分散到各个region上，以避免热点。</p>
<h4 id="2哈希"><a href="#2哈希" class="headerlink" title="2哈希"></a>2哈希</h4><p>哈希会使同一行永远用一个前缀加盐。哈希也可以使负载分散到整个集群，但是读却是可以预测的。使用确定的哈希可以让客户端重构完整的rowkey，可以使用get操作准确获取某一个行数据。</p>
<h4 id="3反转"><a href="#3反转" class="headerlink" title="3反转"></a>3反转</h4><p>第三种防止热点的方法时反转固定长度或者数字格式的rowkey。这样可以使得rowkey中经常改变的部分（最没有意义的部分）放在前面。这样可以有效的随机rowkey，但是牺牲了rowkey的有序性。</p>
<p>反转rowkey的例子以手机号为rowkey，可以将手机号反转后的字符串作为rowkey，这样的就避免了以手机号那样比较固定开头导致热点问题</p>
<h4 id="3时间戳反转"><a href="#3时间戳反转" class="headerlink" title="3时间戳反转"></a>3时间戳反转</h4><p>一个常见的数据处理问题是快速获取数据的最近版本，使用反转的时间戳作为rowkey的一部分对这个问题十分有用，可以用 Long.Max_Value - timestamp 追加到key的末尾，例如 [key][reverse_timestamp] , [key] 的最新值可以通过scan [key]获得[key]的第一条记录，因为HBase中rowkey是有序的，第一条记录是最后录入的数据。</p>
<p>其他一些建议：</p>
<p>尽量减少行键和列族的大小在HBase中，value永远和它的key一起传输的。当具体的值在系统间传输时，它的rowkey，列名，时间戳也会一起传输。如果你的rowkey和列名很大，这个时候它们将会占用大量的存储空间。</p>
<p>列族尽可能越短越好，最好是一个字符。</p>
<p>冗长的属性名虽然可读性好，但是更短的属性名存储在HBase中会更好。</p>
<h2 id="六-Hbase的协处理器"><a href="#六-Hbase的协处理器" class="headerlink" title="六 Hbase的协处理器"></a>六 Hbase的协处理器</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http:&#x2F;&#x2F;hbase.apache.org&#x2F;book.html#cp</span><br></pre></td></tr></table></figure>

<p>1、 起源  Hbase 作为列族数据库最经常被人诟病的特性包括：无法轻易建立“二级索引”，难以执 行求和、计数、排序等操作。比如，在旧版本的(&lt;0.92)Hbase 中，统计数据表的总行数，需 要使用 Counter 方法，执行一次 MapReduce Job 才能得到。虽然 HBase 在数据存储层中集成  了 MapReduce，能够有效用于数据表的分布式计算。然而在很多情况下，做一些简单的相 加或者聚合计算的时候， 如果直接将计算过程放置在 server 端，能够减少通讯开销，从而获 得很好的性能提升。于是， HBase 在 0.92 之后引入了协处理器(coprocessors)，实现一些激动  人心的新特性：能够轻易建立二次索引、复杂过滤器(谓词下推)以及访问控制等。</p>
<h2 id="2、协处理器有两种：-observer-和-endpoint"><a href="#2、协处理器有两种：-observer-和-endpoint" class="headerlink" title="2、协处理器有两种： observer 和 endpoint"></a>2、协处理器有两种： observer 和 endpoint</h2><p>  (1) Observer 类似于传统数据库中的触发器，当发生某些事件的时候这类协处理器会被 Server 端调用。Observer Coprocessor 就是一些散布在 HBase Server 端代码中的 hook 钩子， 在固定的事件发生时被调用。比如： put 操作之前有钩子函数 prePut，该函数在 put 操作<br> 执行前会被 Region Server 调用；在 put 操作之后则有 postPut 钩子函数</p>
<p>以 Hbase2.0.0 版本为例，它提供了三种观察者接口：<br> ● RegionObserver：提供客户端的数据操纵事件钩子： Get、 Put、 Delete、 Scan 等。<br> ● WALObserver：提供 WAL 相关操作钩子。<br> ● MasterObserver：提供 DDL-类型的操作钩子。如创建、删除、修改数据表等。<br> 到 0.96 版本又新增一个 RegionServerObserver</p>
<p>下图是以 RegionObserver 为例子讲解 Observer 这种协处理器的原理：</p>
<p><img src="/images/Hbase/xcl.png" alt="img"></p>
<p> (2) Endpoint 协处理器类似传统数据库中的存储过程，客户端可以调用这些 Endpoint 协处 理器执行一段 Server 端代码，并将 Server 端代码的结果返回给客户端进一步处理，最常 见的用法就是进行聚集操作。如果没有协处理器，当用户需要找出一张表中的最大数据，即</p>
<p>max 聚合操作，就必须进行全表扫描，在客户端代码内遍历扫描结果，并执行求最大值的 操作。这样的方法无法利用底层集群的并发能力，而将所有计算都集中到 Client 端统一执 行，势必效率低下。利用 Coprocessor，用户可以将求最大值的代码部署到 HBase Server 端，<br> HBase 将利用底层 cluster 的多个节点并发执行求最大值的操作。即在每个 Region 范围内 执行求最大值的代码，将每个 Region 的最大值在 Region Server 端计算出，仅仅将该 max 值返回给客户端。在客户端进一步将多个 Region 的最大值进一步处理而找到其中的最大值。<br> 这样整体的执行效率就会提高很多<br> 下图是 EndPoint 的工作原理：</p>
<p><img src="/images/Hbase/xcl2.png" alt="img"></p>
<p>(3)总结</p>
<p>Observer 允许集群在正常的客户端操作过程中可以有不同的行为表现<br> Endpoint 允许扩展集群的能力，对客户端应用开放新的运算命令<br> observer 类似于 RDBMS 中的触发器，主要在服务端工作<br> endpoint 类似于 RDBMS 中的存储过程，主要在 client 端工作<br> observer 可以实现权限管理、优先级设置、监控、 ddl 控制、 二级索引等功能<br> endpoint 可以实现 min、 max、 avg、 sum、 distinct、 group by 等功能</p>
<h2 id="3、协处理器加载方式"><a href="#3、协处理器加载方式" class="headerlink" title="3、协处理器加载方式"></a>3、协处理器加载方式</h2><p>​     协处理器的加载方式有两种，我们称之为静态加载方式（ Static Load） 和动态加载方式 （ Dynamic Load）。 静态加载的协处理器称之为 System Coprocessor，动态加载的协处理器称 之为 Table Coprocessor<br>​      1、静态加载 </p>
<p>通过修改 hbase-site.xml 这个文件来实现， 启动全局 aggregation，能过操纵所有的表上 的数据。只需要添加如下代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hbase.coprocessor.user.region.classes&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;org.apache.hadoop.hbase.coprocessor.AggregateImplementation&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>为所有 table 加载了一个 cp class，可以用” ,”分割加载多个 class</p>
<p> 2、动态加载</p>
<p>启用表 aggregation，只对特定的表生效。通过 HBase Shell 来实现。<br> disable 指定表。 hbase&gt; disable ‘mytable’<br> 添加 aggregation<br> hbase&gt; alter ‘mytable’, METHOD =&gt; ‘table_att’,’coprocessor’=&gt;<br> ‘|org.apache.Hadoop.hbase.coprocessor.AggregateImplementation||’<br> 重启指定表 hbase&gt; enable ‘mytable’</p>
<p>协处理器卸载</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">三步</span><br><span class="line">disable &#39;test&#39;</span><br><span class="line">alter &#39;test&#39;,METHOD&#x3D;&gt;&#39;table_att_unset&#39;,NAME&#x3D;&gt;&#39;coprocessor$1&#39;</span><br><span class="line">enable &#39;test&#39;</span><br></pre></td></tr></table></figure>

<h2 id="4、协处理器Observer应用实战"><a href="#4、协处理器Observer应用实战" class="headerlink" title="4、协处理器Observer应用实战"></a>4、协处理器Observer应用实战</h2><p>通过协处理器Observer实现hbase当中一张表插入数据，然后通过协处理器，将数据复制一份保存到另外一张表当中去，但是只取当第一张表当中的部分列数据保存到第二张表当中去</p>
<h3 id="第一步：HBase当中创建第一张表proc1"><a href="#第一步：HBase当中创建第一张表proc1" class="headerlink" title="第一步：HBase当中创建第一张表proc1"></a>第一步：HBase当中创建第一张表proc1</h3><p>在HBase当中创建一张表，表名user2，并只有一个列族info</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd &#x2F;export&#x2F;servers&#x2F;hbase-2.0.0&#x2F;</span><br><span class="line">bin&#x2F;hbase shell</span><br><span class="line">hbase(main):053:0&gt; create &#39;proc1&#39;,&#39;info&#39;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="第二步：Hbase当中创建第二张表proc2"><a href="#第二步：Hbase当中创建第二张表proc2" class="headerlink" title="第二步：Hbase当中创建第二张表proc2"></a>第二步：Hbase当中创建第二张表proc2</h3><p>创建第二张表’proc2，作为目标表，将第一张表当中插入数据的部分列，使用协处理器，复制到’proc2表当中来</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):054:0&gt; create &#39;proc2&#39;,&#39;info&#39;</span><br></pre></td></tr></table></figure>

<h3 id="第三步：开发HBase的协处理器"><a href="#第三步：开发HBase的协处理器" class="headerlink" title="第三步：开发HBase的协处理器"></a>第三步：开发HBase的协处理器</h3><p>开发HBase的协处理器Copo</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.coprocessor.ObserverContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.coprocessor.RegionCoprocessor;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.coprocessor.RegionObserver;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.util.Bytes;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.wal.WALEdit;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"><span class="keyword">import</span> java.util.Optional;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyProcessor</span> <span class="keyword">implements</span> <span class="title">RegionObserver</span>,<span class="title">RegionCoprocessor</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">static</span> Connection connection = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">static</span> Table table = <span class="keyword">null</span>;</span><br><span class="line">    <span class="comment">//使用静态代码块来创建连接对象，避免频繁的创建连接对象</span></span><br><span class="line">    <span class="keyword">static</span>&#123;</span><br><span class="line">        Configuration conf = HBaseConfiguration.create();</span><br><span class="line">        conf.set(<span class="string">&quot;hbase.zookeeper.quorum&quot;</span>,<span class="string">&quot;node01:2181&quot;</span>);</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            connection = ConnectionFactory.createConnection(conf);</span><br><span class="line">            table = connection.getTable(TableName.valueOf(<span class="string">&quot;proc2&quot;</span>));</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">private</span> RegionCoprocessorEnvironment env = <span class="keyword">null</span>;</span><br><span class="line">    <span class="comment">//定义列族名</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String FAMAILLY_NAME = <span class="string">&quot;info&quot;</span>;</span><br><span class="line">    <span class="comment">//定义列名</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String QUALIFIER_NAME = <span class="string">&quot;name&quot;</span>;</span><br><span class="line">    <span class="comment">//2.0加入该方法，否则无法生效</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Optional&lt;RegionObserver&gt; <span class="title">getRegionObserver</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// Extremely important to be sure that the coprocessor is invoked as a RegionObserver</span></span><br><span class="line">        <span class="keyword">return</span> Optional.of(<span class="keyword">this</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 初始化协处理器环境</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> e</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">start</span><span class="params">(CoprocessorEnvironment e)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        env = (RegionCoprocessorEnvironment) e;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">stop</span><span class="params">(CoprocessorEnvironment e)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        <span class="comment">// nothing to do here</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 覆写prePut方法，在我们数据插入之前进行拦截，</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> e</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> put  put对象里面封装了我们需要插入到目标表的数据</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> edit</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> durability</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">prePut</span><span class="params">(<span class="keyword">final</span> ObserverContext&lt;RegionCoprocessorEnvironment&gt; e,</span></span></span><br><span class="line"><span class="function"><span class="params">                       <span class="keyword">final</span> Put put, <span class="keyword">final</span> WALEdit edit, <span class="keyword">final</span> Durability durability)</span></span></span><br><span class="line"><span class="function">            <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">//通过put对象获取插入数据的rowkey</span></span><br><span class="line">            <span class="keyword">byte</span>[] rowBytes = put.getRow();</span><br><span class="line">            String rowkey = Bytes.toString(rowBytes);</span><br><span class="line">            <span class="comment">//获取我们插入数据的name字段的值</span></span><br><span class="line"></span><br><span class="line">            List&lt;Cell&gt; list = put.get(Bytes.toBytes(FAMAILLY_NAME), Bytes.toBytes(QUALIFIER_NAME));</span><br><span class="line">            <span class="comment">//判断如果没有获取到info列族，和name列，直接返回即可</span></span><br><span class="line">            <span class="keyword">if</span> (list == <span class="keyword">null</span> || list.size() == <span class="number">0</span>) &#123;</span><br><span class="line">                <span class="keyword">return</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">//获取到info列族，name列对应的cell</span></span><br><span class="line">            Cell cell2 = list.get(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">            <span class="comment">//通过cell获取数据值</span></span><br><span class="line">            String nameValue = Bytes.toString(CellUtil.cloneValue(cell2));</span><br><span class="line">            <span class="comment">//创建put对象，将数据插入到proc2表里面去</span></span><br><span class="line">            Put put2 = <span class="keyword">new</span> Put(rowkey.getBytes());</span><br><span class="line">            put2.addColumn(Bytes.toBytes(FAMAILLY_NAME), Bytes.toBytes(QUALIFIER_NAME),  nameValue.getBytes());</span><br><span class="line">            table.put(put2);</span><br><span class="line">            table.close();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e1) &#123;</span><br><span class="line">            <span class="keyword">return</span> ;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="第四步：将项目打成jar包，并上传到HDFS上面"><a href="#第四步：将项目打成jar包，并上传到HDFS上面" class="headerlink" title="第四步：将项目打成jar包，并上传到HDFS上面"></a>第四步：将项目打成jar包，并上传到HDFS上面</h3><p>将我们的协处理器打成一个jar包，此处不需要用任何的打包插件即可，然后上传到hdfs</p>
<p>将打好的jar包上传到linux的/export/servers路径下</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers</span><br><span class="line">mv original-hbase-1.0-SNAPSHOT.jar  processor.jar</span><br><span class="line">hdfs dfs -mkdir -p /processor</span><br><span class="line">hdfs dfs -put processor.jar /processor</span><br></pre></td></tr></table></figure>

<h3 id="第五步：将打好的jar包挂载到proc1表当中去"><a href="#第五步：将打好的jar包挂载到proc1表当中去" class="headerlink" title="第五步：将打好的jar包挂载到proc1表当中去"></a>第五步：将打好的jar包挂载到proc1表当中去</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):056:0&gt; describe &#39;proc1&#39;</span><br><span class="line">hbase(main):055:0&gt; alter &#39;proc1&#39;,METHOD &#x3D;&gt; &#39;table_att&#39;,&#39;Coprocessor&#39;&#x3D;&gt;&#39;hdfs:&#x2F;&#x2F;node01:8020&#x2F;processor&#x2F;processor.jar|cn.itcast.hbasemr.demo4.MyProcessor|1001|&#39;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>再次查看’proc1’表，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):043:0&gt; describe &#39;proc1&#39;</span><br></pre></td></tr></table></figure>

<p>可以查看到我们的卸载器已经加载了</p>
<h3 id="第六步：proc1表当中添加数据"><a href="#第六步：proc1表当中添加数据" class="headerlink" title="第六步：proc1表当中添加数据"></a>第六步：proc1表当中添加数据</h3><p>进入hbase-shell客户端，然后直接执行以下命令向proc1表当中添加数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">put &#39;proc1&#39;,&#39;0001&#39;,&#39;info:name&#39;,&#39;zhangsan&#39;</span><br><span class="line">put &#39;proc1&#39;,&#39;0001&#39;,&#39;info:age&#39;,&#39;28&#39;</span><br><span class="line">put &#39;proc1&#39;,&#39;0002&#39;,&#39;info:name&#39;,&#39;lisi&#39;</span><br><span class="line">put &#39;proc1&#39;,&#39;0002&#39;,&#39;info:age&#39;,&#39;25&#39;</span><br></pre></td></tr></table></figure>

<p>向proc1表当中添加数据，然后通过</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scan  &#39;proc2&#39;</span><br></pre></td></tr></table></figure>

<p>我们会发现，proc2表当中也插入了数据，并且只有info列族，name列</p>
<p>​    注意：如果需要卸载我们的协处理器，那么进入hbase的shell命令行，执行以下命令即可</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">disable &#39;proc1&#39;</span><br><span class="line">alter &#39;proc1&#39;,METHOD&#x3D;&gt;&#39;table_att_unset&#39;,NAME&#x3D;&gt;&#39;coprocessor$1&#39;</span><br><span class="line">enable &#39;proc1&#39;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="七-HBase当中的二级索引的基本介绍"><a href="#七-HBase当中的二级索引的基本介绍" class="headerlink" title="七 HBase当中的二级索引的基本介绍"></a>七 HBase当中的二级索引的基本介绍</h2><p>由于HBase的查询比较弱，如果需要实现类似于  select  name,salary,count(1),max(salary) from user  group  by name,salary order  by  salary 等这样的复杂性的统计需求，基本上不可能，或者说比较困难，所以我们在使用HBase的时候，一般都会借助二级索引的方案来进行实现</p>
<p>HBase的一级索引就是rowkey，我们只能通过rowkey进行检索。如果我们相对hbase里面列族的列列进行一些组合查询，就需要采用HBase的二级索引方案来进行多条件的查询。 </p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">\1.</span> <span class="string">MapReduce方案 </span></span><br><span class="line"><span class="meta">\2.</span> <span class="string">ITHBASE（Indexed-Transanctional HBase）方案 </span></span><br><span class="line"><span class="meta">\3.</span> <span class="string">IHBASE（Index HBase）方案 </span></span><br><span class="line"><span class="meta">\4.</span> <span class="string">Hbase Coprocessor(协处理器)方案 </span></span><br><span class="line"><span class="meta">\5.</span> <span class="string">Solr+hbase方案</span></span><br><span class="line"><span class="meta">\6.</span> <span class="string">CCIndex（complementalclustering index）方案</span></span><br><span class="line"><span class="meta">还有</span> <span class="string">MySQL 等数据库</span></span><br><span class="line"><span class="attr">常见的二级索引我们一般可以借助各种其他的方式来实现，例如Phoenix或者solr或者ES等</span></span><br></pre></td></tr></table></figure>



<h2 id="八-HBase调优"><a href="#八-HBase调优" class="headerlink" title="八 HBase调优"></a>八 HBase调优</h2><h2 id="1、通用优化"><a href="#1、通用优化" class="headerlink" title="1、通用优化"></a>1、通用优化</h2><h3 id="1、NameNode的元数据备份使用SSD"><a href="#1、NameNode的元数据备份使用SSD" class="headerlink" title="1、NameNode的元数据备份使用SSD"></a>1、NameNode的元数据备份使用SSD</h3><h3 id="2、定时备份NameNode上的元数据，每小时或者每天备份，如果数据极其重要，可以5"><a href="#2、定时备份NameNode上的元数据，每小时或者每天备份，如果数据极其重要，可以5" class="headerlink" title="2、定时备份NameNode上的元数据，每小时或者每天备份，如果数据极其重要，可以5~"></a>2、定时备份NameNode上的元数据，每小时或者每天备份，如果数据极其重要，可以5~</h3><p>10分钟备份一次。备份可以通过定时任务复制元数据目录即可。</p>
<h3 id="3、为NameNode指定多个元数据目录，使用dfs-name-dir或者dfs-namenode-name-dir指定。一个指定本地磁盘，一个指定网络磁盘。这样可以提供元数据的冗余和健壮性，以免发生故障。"><a href="#3、为NameNode指定多个元数据目录，使用dfs-name-dir或者dfs-namenode-name-dir指定。一个指定本地磁盘，一个指定网络磁盘。这样可以提供元数据的冗余和健壮性，以免发生故障。" class="headerlink" title="3、为NameNode指定多个元数据目录，使用dfs.name.dir或者dfs.namenode.name.dir指定。一个指定本地磁盘，一个指定网络磁盘。这样可以提供元数据的冗余和健壮性，以免发生故障。"></a>3、为NameNode指定多个元数据目录，使用dfs.name.dir或者dfs.namenode.name.dir指定。一个指定本地磁盘，一个指定网络磁盘。这样可以提供元数据的冗余和健壮性，以免发生故障。</h3><h3 id="4、设置dfs-namenode-name-dir-restore为true，允许尝试恢复之前失败的dfs-namenode-name-dir目录，在创建checkpoint时做此尝试，如果设置了多个磁盘，建议允许。"><a href="#4、设置dfs-namenode-name-dir-restore为true，允许尝试恢复之前失败的dfs-namenode-name-dir目录，在创建checkpoint时做此尝试，如果设置了多个磁盘，建议允许。" class="headerlink" title="4、设置dfs.namenode.name.dir.restore为true，允许尝试恢复之前失败的dfs.namenode.name.dir目录，在创建checkpoint时做此尝试，如果设置了多个磁盘，建议允许。"></a>4、设置dfs.namenode.name.dir.restore为true，允许尝试恢复之前失败的dfs.namenode.name.dir目录，在创建checkpoint时做此尝试，如果设置了多个磁盘，建议允许。</h3><h3 id="5、NameNode节点必须配置为RAID1（镜像盘）结构。"><a href="#5、NameNode节点必须配置为RAID1（镜像盘）结构。" class="headerlink" title="5、NameNode节点必须配置为RAID1（镜像盘）结构。"></a>5、NameNode节点必须配置为RAID1（镜像盘）结构。</h3><h3 id="6、补充：什么是Raid0、Raid0-1、Raid1、Raid5"><a href="#6、补充：什么是Raid0、Raid0-1、Raid1、Raid5" class="headerlink" title="6、补充：什么是Raid0、Raid0+1、Raid1、Raid5"></a>6、补充：什么是Raid0、Raid0+1、Raid1、Raid5</h3><p><img src="/images/Hbase/r.jpg" alt="img"></p>
<p><strong>Standalone</strong></p>
<p>最普遍的单磁盘储存方式。</p>
<p><strong>Cluster</strong></p>
<p>集群储存是通过将数据分布到集群中各节点的存储方式,提供单一的使用接口与界面,使用户可以方便地对所有数据进行统一使用与管理。</p>
<p><strong>Hot swap</strong></p>
<p>用户可以再不关闭系统,不切断电源的情况下取出和更换硬盘,提高系统的恢复能力、拓展性和灵活性。</p>
<p><strong>Raid0</strong></p>
<p>Raid0是所有raid中存储性能最强的阵列形式。其工作原理就是在多个磁盘上分散存取连续的数据,这样,当需要存取数据是多个磁盘可以并排执行,每个磁盘执行属于它自己的那部分数据请求,显著提高磁盘整体存取性能。但是不具备容错能力,适用于低成本、低可靠性的台式系统。</p>
<p><strong>Raid1</strong></p>
<p>又称镜像盘,把一个磁盘的数据镜像到另一个磁盘上,采用镜像容错来提高可靠性,具有raid中最高的数据冗余能力。存数据时会将数据同时写入镜像盘内,读取数据则只从工作盘读出。发生故障时,系统将从镜像盘读取数据,然后再恢复工作盘正确数据。这种阵列方式可靠性极高,但是其容量会减去一半。广泛用于数据要求极严的应用场合,如商业金融、档案管理等领域。只允许一颗硬盘出故障。</p>
<p><strong>Raid0+1</strong></p>
<p>将Raid0和Raid1技术结合在一起,兼顾两者的优势。在数据得到保障的同时,还能提供较强的存储性能。不过至少要求4个或以上的硬盘，但也只允许一个磁盘出错。是一种三高技术。 </p>
<p><strong>Raid5</strong></p>
<p>Raid5可以看成是Raid0+1的低成本方案。采用循环偶校验独立存取的阵列方式。将数据和相对应的奇偶校验信息分布存储到组成RAID5的各个磁盘上。当其中一个磁盘数据发生损坏后,利用剩下的磁盘和相应的奇偶校验信息 重新恢复/生成丢失的数据而不影响数据的可用性。至少需要3个或以上的硬盘。适用于大数据量的操作。成本稍高、储存性强、可靠性强的阵列方式。</p>
<p>RAID还有其他方式，请自行查阅。</p>
<h3 id="7、保持NameNode日志目录有足够的空间，这些日志有助于帮助你发现问题。"><a href="#7、保持NameNode日志目录有足够的空间，这些日志有助于帮助你发现问题。" class="headerlink" title="7、保持NameNode日志目录有足够的空间，这些日志有助于帮助你发现问题。"></a>7、保持NameNode日志目录有足够的空间，这些日志有助于帮助你发现问题。</h3><h3 id="8、因为Hadoop是IO密集型框架，所以尽量提升存储的速度和吞吐量（类似位宽）。"><a href="#8、因为Hadoop是IO密集型框架，所以尽量提升存储的速度和吞吐量（类似位宽）。" class="headerlink" title="8、因为Hadoop是IO密集型框架，所以尽量提升存储的速度和吞吐量（类似位宽）。"></a>8、因为Hadoop是IO密集型框架，所以尽量提升存储的速度和吞吐量（类似位宽）。</h3><h2 id="2-、Linux优化"><a href="#2-、Linux优化" class="headerlink" title="2   、Linux优化"></a>2   、Linux优化</h2><h3 id="1、开启文件系统的预读缓存可以提高读取速度"><a href="#1、开启文件系统的预读缓存可以提高读取速度" class="headerlink" title="1、开启文件系统的预读缓存可以提高读取速度"></a>1、开启文件系统的预读缓存可以提高读取速度</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ sudo blockdev --setra 32768 &#x2F;dev&#x2F;sda</span><br><span class="line"></span><br><span class="line">（注意：ra是readahead的缩写）</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="2、关闭进程睡眠池"><a href="#2、关闭进程睡眠池" class="headerlink" title="2、关闭进程睡眠池"></a>2、关闭进程睡眠池</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo sysctl -w vm.swappiness&#x3D;0</span><br></pre></td></tr></table></figure>

<h3 id="3、调整ulimit上限，默认值为比较小的数字"><a href="#3、调整ulimit上限，默认值为比较小的数字" class="headerlink" title="3、调整ulimit上限，默认值为比较小的数字"></a>3、调整ulimit上限，默认值为比较小的数字</h3><p>$ ulimit -n 查看允许最大进程数</p>
<p>$ ulimit -u 查看允许打开最大文件数</p>
<p>修改:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ sudo vi &#x2F;etc&#x2F;security&#x2F;limits.conf 修改打开文件数限制</span><br><span class="line">末尾添加：</span><br><span class="line">*                soft    nofile          1024000</span><br><span class="line">*                hard    nofile          1024000</span><br><span class="line">Hive             -       nofile          1024000</span><br><span class="line">hive             -       nproc           1024000 </span><br><span class="line">$ sudo vi &#x2F;etc&#x2F;security&#x2F;limits.d&#x2F;20-nproc.conf 修改用户打开进程数限制</span><br><span class="line">修改为：</span><br><span class="line">#*          soft    nproc     4096</span><br><span class="line">#root       soft    nproc     unlimited</span><br><span class="line">*          soft    nproc     40960</span><br><span class="line">root       soft    nproc     unlimited</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="4、开启集群的时间同步NTP，请参看之前文档"><a href="#4、开启集群的时间同步NTP，请参看之前文档" class="headerlink" title="4、开启集群的时间同步NTP，请参看之前文档"></a>4、开启集群的时间同步NTP，请参看之前文档</h3><h3 id="5、更新系统补丁（注意：更新补丁前，请先测试新版本补丁对集群节点的兼容性）"><a href="#5、更新系统补丁（注意：更新补丁前，请先测试新版本补丁对集群节点的兼容性）" class="headerlink" title="5、更新系统补丁（注意：更新补丁前，请先测试新版本补丁对集群节点的兼容性）"></a>5、更新系统补丁（注意：更新补丁前，请先测试新版本补丁对集群节点的兼容性）</h3><h2 id="3、HDFS优化（hdfs-site-xml）"><a href="#3、HDFS优化（hdfs-site-xml）" class="headerlink" title="3、HDFS优化（hdfs-site.xml）"></a>3、HDFS优化（hdfs-site.xml）</h2><h3 id="1、保证RPC调用会有较多的线程数"><a href="#1、保证RPC调用会有较多的线程数" class="headerlink" title="1、保证RPC调用会有较多的线程数"></a>1、保证RPC调用会有较多的线程数</h3><p>属性：dfs.namenode.handler.count</p>
<p>解释：该属性是NameNode服务默认线程数，的默认值是10，根据机器的可用内存可以调整为50~<br>100</p>
<p>属性：dfs.datanode.handler.count</p>
<p>解释：该属性默认值为10，是DataNode的处理线程数，如果HDFS客户端程序读写请求比较多，可以调高到15<del>20，设置的值越大，内存消耗越多，不要调整的过高，一般业务中，5</del>10即可。</p>
<h3 id="2、副本数的调整"><a href="#2、副本数的调整" class="headerlink" title="2、副本数的调整"></a>2、副本数的调整</h3><p>属性：dfs.replication</p>
<p>解释：如果数据量巨大，且不是非常之重要，可以调整为2<del>3，如果数据非常之重要，可以调整为3</del>5。</p>
<h3 id="3-、文件块大小的调整"><a href="#3-、文件块大小的调整" class="headerlink" title="3.、文件块大小的调整"></a>3.、文件块大小的调整</h3><p>属性：dfs.blocksize</p>
<p>解释：块大小定义，该属性应该根据存储的大量的单个文件大小来设置，如果大量的单个文件都小于100M，建议设置成64M块大小，对于大于100M或者达到GB的这种情况，建议设置成256M，一般设<br>置范围波动在64m<del>256m之间。<br>置范围波动在64M</del>256M之间。</p>
<h2 id="4、MapReduce优化（mapred-site-xml）"><a href="#4、MapReduce优化（mapred-site-xml）" class="headerlink" title="4、MapReduce优化（mapred-site.xml）"></a>4、MapReduce优化（mapred-site.xml）</h2><h3 id="1、Job任务服务线程数调整"><a href="#1、Job任务服务线程数调整" class="headerlink" title="1、Job任务服务线程数调整"></a>1、Job任务服务线程数调整</h3><p>mapreduce.jobtracker.handler.count</p>
<p>该属性是job任务线程数，默认值10，根据机器的可用内存可调整为50-100</p>
<h3 id="2、Http服务器工作线程数"><a href="#2、Http服务器工作线程数" class="headerlink" title="2、Http服务器工作线程数"></a>2、Http服务器工作线程数</h3><p>属性：mapreduce.tasktracker.http.threads<br>解释：定义HTTP服务器工作线程数，默认值40，对于大集群可调整为80-100</p>
<h3 id="3、文件排序合并优化"><a href="#3、文件排序合并优化" class="headerlink" title="3、文件排序合并优化"></a>3、文件排序合并优化</h3><p>属性：mapreduce.task.io.sort.factor</p>
<p>解释：文件排序时同时合并的数据流的数量，这也定义了同时打开文件的个数，默认值为10，如果调高该参数，可以明显减少磁盘IO，即减少文件读取的次数。</p>
<h3 id="4、设置任务并发"><a href="#4、设置任务并发" class="headerlink" title="4、设置任务并发"></a>4、设置任务并发</h3><p>属性：mapreduce.map.speculative</p>
<p>解释：该属性可以设置任务是否可以并发执行，如果任务多而小，该属性设置为true可以明显加快任务执行效率，但是对于延迟非常高的任务，建议改为false，这就类似于迅雷下载。</p>
<h3 id="5、MR输出数据的压缩"><a href="#5、MR输出数据的压缩" class="headerlink" title="5、MR输出数据的压缩"></a>5、MR输出数据的压缩</h3><p>属性：mapreduce.map.output.compress、mapreduce.output.fileoutputformat.compress</p>
<p>解释：对于大集群而言，建议设置Map-Reduce的输出为压缩的数据，而对于小集群，则不需要。</p>
<h3 id="6、优化Mapper和Reducer的个数"><a href="#6、优化Mapper和Reducer的个数" class="headerlink" title="6、优化Mapper和Reducer的个数"></a>6、优化Mapper和Reducer的个数</h3><p>属性：</p>
<p>mapreduce.tasktracker.map.tasks.maximum</p>
<p>mapreduce.tasktracker.reduce.tasks.maximum</p>
<p>解释：以上两个属性分别为一个单独的Job任务可以同时运行的Map和Reduce的数量。</p>
<p>设置上面两个参数时，需要考虑CPU核数、磁盘和内存容量。假设一个8核的CPU，业务内容非常消耗CPU，那么可以设置map数量为4，如果该业务不是特别消耗CPU类型的，那么可以设置map数量为40，reduce数量为20。这些参数的值修改完成之后，一定要观察是否有较长等待的任务，如果有的话，可以减少数量以加快任务执行，如果设置一个很大的值，会引起大量的上下文切换，以及内存与磁盘之间的数据交换，这里没有标准的配置数值，需要根据业务和硬件配置以及经验来做出选择。</p>
<p>在同一时刻，不要同时运行太多的MapReduce，这样会消耗过多的内存，任务会执行的非常缓慢，我们需要根据CPU核数，内存容量设置一个MR任务并发的最大值，使固定数据量的任务完全加载到内存中，避免频繁的内存和磁盘数据交换，从而降低磁盘IO，提高性能。</p>
<p>大概配比：</p>
<table>
<thead>
<tr>
<th>CPU   CORE</th>
<th>MEM（GB）</th>
<th>Map</th>
<th>Reduce</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>1</td>
<td>5</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>4</td>
<td>5</td>
<td>1~4</td>
<td>2</td>
</tr>
<tr>
<td>16</td>
<td>32</td>
<td>16</td>
<td>8</td>
</tr>
<tr>
<td>16</td>
<td>64</td>
<td>16</td>
<td>8</td>
</tr>
<tr>
<td>24</td>
<td>64</td>
<td>24</td>
<td>12</td>
</tr>
<tr>
<td>24</td>
<td>128</td>
<td>24</td>
<td>12</td>
</tr>
</tbody></table>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">大概估算公式：</span><br><span class="line">map &#x3D; 2 + ⅔cpu_core</span><br><span class="line">reduce &#x3D; 2 + ⅓cpu_core</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="5、HBase优化"><a href="#5、HBase优化" class="headerlink" title="5、HBase优化"></a>5、HBase优化</h2><h3 id="1、在HDFS的文件中追加内容"><a href="#1、在HDFS的文件中追加内容" class="headerlink" title="1、在HDFS的文件中追加内容"></a>1、在HDFS的文件中追加内容</h3><p>不是不允许追加内容么？没错，请看背景故事：</p>
<p>属性：dfs.support.append</p>
<p>文件：hdfs-site.xml、hbase-site.xml</p>
<p>解释：开启HDFS追加同步，可以优秀的配合HBase的数据同步和持久化。默认值为true。</p>
<h3 id="2、优化DataNode允许的最大文件打开数"><a href="#2、优化DataNode允许的最大文件打开数" class="headerlink" title="2、优化DataNode允许的最大文件打开数"></a>2、优化DataNode允许的最大文件打开数</h3><p>属性：dfs.datanode.max.transfer.threads</p>
<p>文件：hdfs-site.xml</p>
<p>解释：HBase一般都会同一时间操作大量的文件，根据集群的数量和规模以及数据动作，设置为4096或者更高。默认值：4096</p>
<h3 id="3、优化延迟高的数据操作的等待时间"><a href="#3、优化延迟高的数据操作的等待时间" class="headerlink" title="3、优化延迟高的数据操作的等待时间"></a>3、优化延迟高的数据操作的等待时间</h3><p>属性：dfs.image.transfer.timeout</p>
<p>文件：hdfs-site.xml</p>
<p>解释：如果对于某一次数据操作来讲，延迟非常高，socket需要等待更长的时间，建议把该值设置为更大的值（默认60000毫秒），以确保socket不会被timeout掉。</p>
<h3 id="4、优化数据的写入效率"><a href="#4、优化数据的写入效率" class="headerlink" title="4、优化数据的写入效率"></a>4、优化数据的写入效率</h3><p>属性：</p>
<p>mapreduce.map.output.compress</p>
<p>mapreduce.map.output.compress.codec</p>
<p>文件：mapred-site.xml</p>
<p>解释：开启这两个数据可以大大提高文件的写入效率，减少写入时间。第一个属性值修改为true，第二个属性值修改为：org.apache.hadoop.io.compress.GzipCodec</p>
<h3 id="5、优化DataNode存储"><a href="#5、优化DataNode存储" class="headerlink" title="5、优化DataNode存储"></a>5、优化DataNode存储</h3><p>属性：dfs.datanode.failed.volumes.tolerated</p>
<p>文件：hdfs-site.xml</p>
<p>解释：默认为0，意思是当DataNode中有一个磁盘出现故障，则会认为该DataNode shutdown了。如果修改为1，则一个磁盘出现故障时，数据会被复制到其他正常的DataNode上，当前的DataNode继续工作。</p>
<h3 id="6、设置RPC监听数量"><a href="#6、设置RPC监听数量" class="headerlink" title="6、设置RPC监听数量"></a>6、设置RPC监听数量</h3><p>属性：hbase.regionserver.handler.count</p>
<p>文件：hbase-site.xml</p>
<p>解释：默认值为30，用于指定RPC监听的数量，可以根据客户端的请求数进行调整，读写请求较多时，增加此值。 </p>
<h3 id="7、优化HStore文件大小"><a href="#7、优化HStore文件大小" class="headerlink" title="7、优化HStore文件大小"></a>7、优化HStore文件大小</h3><p>属性：hbase.hregion.max.filesize</p>
<p>文件：hbase-site.xml</p>
<p>解释：默认值10737418240（10GB），如果需要运行HBase的MR任务，可以减小此值，因为一个region对应一个map任务，如果单个region过大，会导致map任务执行时间过长。该值的意思就是，如果HFile的大小达到这个数值，则这个region会被切分为两个Hfile。</p>
<h3 id="8、优化hbase客户端缓存"><a href="#8、优化hbase客户端缓存" class="headerlink" title="8、优化hbase客户端缓存"></a>8、优化hbase客户端缓存</h3><p>属性：hbase.client.write.buffer</p>
<p>文件：hbase-site.xml</p>
<p>解释：用于指定HBase客户端缓存，增大该值可以减少RPC调用次数，但是会消耗更多内存，反之则反之。一般我们需要设定一定的缓存大小，以达到减少RPC次数的目的。</p>
<h3 id="9、指定scan-next扫描HBase所获取的行数"><a href="#9、指定scan-next扫描HBase所获取的行数" class="headerlink" title="9、指定scan.next扫描HBase所获取的行数"></a>9、指定scan.next扫描HBase所获取的行数</h3><p>属性：hbase.client.scanner.caching</p>
<p>文件：hbase-site.xml</p>
<p>解释：用于指定scan.next方法获取的默认行数，值越大，消耗内存越大。</p>
<h2 id="6、内存优化"><a href="#6、内存优化" class="headerlink" title="6、内存优化"></a>6、内存优化</h2><p>HBase操作过程中需要大量的内存开销，毕竟Table是可以缓存在内存中的，一般会分配整个可用内存的70%给HBase的Java堆。但是不建议分配非常大的堆内存，因为GC过程持续太久会导致RegionServer处于长期不可用状态，一般16~<br>48G内存就可以了，如果因为框架占用内存过高导致系统内存不足，框架一样会被系统服务拖死。</p>
<h2 id="7、JVM优化"><a href="#7、JVM优化" class="headerlink" title="7、JVM优化"></a>7、JVM优化</h2><p>涉及文件：hbase-env.sh</p>
<h3 id="1、并行GC"><a href="#1、并行GC" class="headerlink" title="1、并行GC"></a>1、并行GC</h3><p>参数：-XX:+UseParallelGC</p>
<p>解释：开启并行GC</p>
<h3 id="2、同时处理垃圾回收的线程数"><a href="#2、同时处理垃圾回收的线程数" class="headerlink" title="2、同时处理垃圾回收的线程数"></a>2、同时处理垃圾回收的线程数</h3><p>参数：-XX:ParallelGCThreads=cpu_core – 1</p>
<p>解释：该属性设置了同时处理垃圾回收的线程数。</p>
<h3 id="3、禁用手动GC"><a href="#3、禁用手动GC" class="headerlink" title="3、禁用手动GC"></a>3、禁用手动GC</h3><p>参数：-XX:DisableExplicitGC</p>
<p>解释：防止开发人员手动调用GC</p>
<h2 id="8、Zookeeper优化"><a href="#8、Zookeeper优化" class="headerlink" title="8、Zookeeper优化"></a>8、Zookeeper优化</h2><h3 id="1、优化Zookeeper会话超时时间"><a href="#1、优化Zookeeper会话超时时间" class="headerlink" title="1、优化Zookeeper会话超时时间"></a>1、优化Zookeeper会话超时时间</h3><p>参数：zookeeper.session.timeout</p>
<p>文件：hbase-site.xml</p>
<p>解释：In hbase-site.xml, set zookeeper.session.timeout to 30 seconds or less to bound failure detection (20-30 seconds is a good start).该值会直接关系到master发现服务器宕机的最大周期，默认值为30秒，如果该值过小，会在HBase在写入大量数据发生而GC时，导致RegionServer短暂的不可用，从而没有向ZK发送心跳包，最终导致认为从节点shutdown。一般20台左右的集群需要配置5台zookeeper。</p>

    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>HF
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="http://example.com/2017/07/18/Hbase%E5%A2%9E%E5%BC%BA/" title="Hbase增强">http://example.com/2017/07/18/Hbase增强/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Hbase/" rel="tag"># Hbase</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2017/07/17/Hbase/" rel="prev" title="Hbase">
      <i class="fa fa-chevron-left"></i> Hbase
    </a></div>
      <div class="post-nav-item">
    <a href="/2017/08/10/scala%E5%85%A5%E9%97%A8/" rel="next" title="scala入门">
      scala入门 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Hbase%E5%A2%9E%E5%BC%BA"><span class="nav-number">1.</span> <span class="nav-text">Hbase增强</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80-Hbase%E4%B8%8EMapReduce%E7%9A%84%E9%9B%86%E6%88%90"><span class="nav-number">1.1.</span> <span class="nav-text">一 Hbase与MapReduce的集成</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%9C%80%E6%B1%82%E4%B8%80-%E8%AF%BB%E5%8F%96myuser%E8%BF%99%E5%BC%A0%E8%A1%A8%E5%BD%93%E4%B8%AD%E7%9A%84%E6%95%B0%E6%8D%AE%E5%86%99%E5%85%A5%E5%88%B0HBase%E7%9A%84%E5%8F%A6%E5%A4%96%E4%B8%80%E5%BC%A0%E8%A1%A8%E5%BD%93%E4%B8%AD%E5%8E%BB"><span class="nav-number">1.1.1.</span> <span class="nav-text">需求一   读取myuser这张表当中的数据写入到HBase的另外一张表当中去</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E5%88%9B%E5%BB%BAmyuser2-%E8%A1%A8"><span class="nav-number">1.1.1.1.</span> <span class="nav-text">1 创建myuser2 表</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%9A%E4%B9%89mapper%E7%B1%BB"><span class="nav-number">1.1.1.2.</span> <span class="nav-text">定义mapper类</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%9A%E4%B9%89-reduce"><span class="nav-number">1.1.1.3.</span> <span class="nav-text">定义 reduce</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%9A%E4%B9%89%E4%B8%BB%E7%B1%BB"><span class="nav-number">1.1.1.4.</span> <span class="nav-text">定义主类</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%BF%90%E8%A1%8C"><span class="nav-number">1.1.1.5.</span> <span class="nav-text">运行</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%9C%80%E6%B1%822-%E8%AF%BB%E5%8F%96HDFS%E6%96%87%E4%BB%B6%EF%BC%8C%E5%86%99%E5%85%A5%E5%88%B0HBase%E8%A1%A8%E5%BD%93%E4%B8%AD%E5%8E%BB"><span class="nav-number">1.1.1.6.</span> <span class="nav-text">需求2 读取HDFS文件，写入到HBase表当中去</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%9A%E4%B9%89mapper"><span class="nav-number">1.1.1.7.</span> <span class="nav-text">定义mapper</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%9A%E4%B9%89reduce"><span class="nav-number">1.1.1.8.</span> <span class="nav-text">定义reduce</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%9A%E4%B9%89%E4%B8%BB%E7%B1%BB-1"><span class="nav-number">1.1.1.9.</span> <span class="nav-text">定义主类</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%9C%80%E6%B1%82%E4%B8%89-%E9%80%9A%E8%BF%87bulkload%E7%9A%84%E6%96%B9%E5%BC%8F%E6%89%B9%E9%87%8F%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE%E5%88%B0HBase%E5%BD%93%E4%B8%AD%E5%8E%BB"><span class="nav-number">1.1.1.10.</span> <span class="nav-text">需求三 通过bulkload的方式批量加载数据到HBase当中去</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%9A%E4%B9%89mapper-1"><span class="nav-number">1.1.1.11.</span> <span class="nav-text">定义mapper</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%BB%E7%B1%BB-%E7%A8%8B%E5%BA%8F%E5%85%A5%E5%8F%A3"><span class="nav-number">1.1.1.12.</span> <span class="nav-text">主类 程序入口</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BC%80%E5%8F%91%E4%BB%A3%E7%A0%81-%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE"><span class="nav-number">1.1.1.13.</span> <span class="nav-text">开发代码  加载数据</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Hive"><span class="nav-number">1.2.</span> <span class="nav-text">Hive</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E5%B7%A5%E5%85%B7"><span class="nav-number">1.2.1.</span> <span class="nav-text">数据仓库工具</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%94%A8%E4%BA%8E%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E3%80%81%E6%B8%85%E6%B4%97"><span class="nav-number">1.2.2.</span> <span class="nav-text">用于数据分析、清洗</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8EHDFS%E3%80%81MapReduce"><span class="nav-number">1.2.3.</span> <span class="nav-text">基于HDFS、MapReduce</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#HBase"><span class="nav-number">1.3.</span> <span class="nav-text">HBase</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#nosql%E6%95%B0%E6%8D%AE%E5%BA%93"><span class="nav-number">1.3.1.</span> <span class="nav-text">nosql数据库</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%94%A8%E4%BA%8E%E5%AD%98%E5%82%A8%E7%BB%93%E6%9E%84%E5%8C%96%E5%92%8C%E9%9D%9E%E7%BB%93%E6%9E%84%E8%AF%9D%E7%9A%84%E6%95%B0%E6%8D%AE"><span class="nav-number">1.3.2.</span> <span class="nav-text">用于存储结构化和非结构话的数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8EHDFS"><span class="nav-number">1.3.3.</span> <span class="nav-text">基于HDFS</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BB%B6%E8%BF%9F%E8%BE%83%E4%BD%8E%EF%BC%8C%E6%8E%A5%E5%85%A5%E5%9C%A8%E7%BA%BF%E4%B8%9A%E5%8A%A1%E4%BD%BF%E7%94%A8"><span class="nav-number">1.3.4.</span> <span class="nav-text">延迟较低，接入在线业务使用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%BB%E7%BB%93%EF%BC%9AHive%E4%B8%8EHBase"><span class="nav-number">1.3.5.</span> <span class="nav-text">总结：Hive与HBase</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%9C%80%E6%B1%82%E4%B8%80%E5%B0%86hive%E5%88%86%E6%9E%90%E7%BB%93%E6%9E%9C%E7%9A%84%E6%95%B0%E6%8D%AE%EF%BC%8C%E4%BF%9D%E5%AD%98%E5%88%B0HBase%E5%BD%93%E4%B8%AD%E5%8E%BB"><span class="nav-number">1.3.6.</span> <span class="nav-text">需求一将hive分析结果的数据，保存到HBase当中去</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E6%8B%B7%E8%B4%9Dhbase%E7%9A%84%E4%BA%94%E4%B8%AA%E4%BE%9D%E8%B5%96jar%E5%8C%85%E5%88%B0hive%E7%9A%84lib%E7%9B%AE%E5%BD%95%E4%B8%8B"><span class="nav-number">1.3.6.1.</span> <span class="nav-text">1 拷贝hbase的五个依赖jar包到hive的lib目录下</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E4%BF%AE%E6%94%B9hive%E7%9A%84%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6"><span class="nav-number">1.3.6.2.</span> <span class="nav-text">2 修改hive的配置文件</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-%E4%BF%AE%E6%94%B9hive-env-sh%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E6%B7%BB%E5%8A%A0%E4%BB%A5%E4%B8%8B%E9%85%8D%E7%BD%AE"><span class="nav-number">1.3.6.3.</span> <span class="nav-text">3 修改hive-env.sh配置文件添加以下配置</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-hive%E5%BD%93%E4%B8%AD%E5%BB%BA%E8%A1%A8%E5%B9%B6%E5%8A%A0%E8%BD%BD%E4%BB%A5%E4%B8%8B%E6%95%B0%E6%8D%AE"><span class="nav-number">1.3.6.4.</span> <span class="nav-text">4 hive当中建表并加载以下数据</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#hive%E5%BD%93%E4%B8%AD%E5%BB%BA%E8%A1%A8"><span class="nav-number">1.3.6.5.</span> <span class="nav-text">hive当中建表</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%87%86%E5%A4%87%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%A6%82%E4%B8%8B"><span class="nav-number">1.3.6.6.</span> <span class="nav-text">准备数据内容如下</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%BF%9B%E8%A1%8C%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE"><span class="nav-number">1.3.6.7.</span> <span class="nav-text">进行加载数据</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-%E5%88%9B%E5%BB%BAhive%E7%AE%A1%E7%90%86%E8%A1%A8%E4%B8%8EHBase%E8%BF%9B%E8%A1%8C%E6%98%A0%E5%B0%84"><span class="nav-number">1.3.6.8.</span> <span class="nav-text">5 创建hive管理表与HBase进行映射</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-hbase%E5%BD%93%E4%B8%AD%E6%9F%A5%E7%9C%8B%E8%A1%A8hbase-score"><span class="nav-number">1.3.6.9.</span> <span class="nav-text">6 hbase当中查看表hbase_score</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%9C%80%E6%B1%82%E4%BA%8C%E5%88%9B%E5%BB%BAhive%E5%A4%96%E9%83%A8%E8%A1%A8%EF%BC%8C%E6%98%A0%E5%B0%84HBase%E5%BD%93%E4%B8%AD%E5%B7%B2%E6%9C%89%E7%9A%84%E8%A1%A8%E6%A8%A1%E5%9E%8B%EF%BC%8C"><span class="nav-number">1.3.7.</span> <span class="nav-text">需求二创建hive外部表，映射HBase当中已有的表模型，</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E4%B8%80%E6%AD%A5%EF%BC%9AHBase%E5%BD%93%E4%B8%AD%E5%88%9B%E5%BB%BA%E8%A1%A8%E5%B9%B6%E6%89%8B%E5%8A%A8%E6%8F%92%E5%85%A5%E5%8A%A0%E8%BD%BD%E4%B8%80%E4%BA%9B%E6%95%B0%E6%8D%AE"><span class="nav-number">1.3.8.</span> <span class="nav-text">第一步：HBase当中创建表并手动插入加载一些数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E4%BA%8C%E6%AD%A5%EF%BC%9A%E5%BB%BA%E7%AB%8Bhive%E7%9A%84%E5%A4%96%E9%83%A8%E8%A1%A8%EF%BC%8C%E6%98%A0%E5%B0%84HBase%E5%BD%93%E4%B8%AD%E7%9A%84%E8%A1%A8%E4%BB%A5%E5%8F%8A%E5%AD%97%E6%AE%B5"><span class="nav-number">1.3.9.</span> <span class="nav-text">第二步：建立hive的外部表，映射HBase当中的表以及字段</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%9B-hbase%E9%A2%84%E5%88%86%E5%8C%BA"><span class="nav-number">1.4.</span> <span class="nav-text">四 hbase预分区</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1%E3%80%81%E4%B8%BA%E4%BD%95%E8%A6%81%E9%A2%84%E5%88%86%E5%8C%BA%EF%BC%9F"><span class="nav-number">1.5.</span> <span class="nav-text">1、为何要预分区？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2%E3%80%81%E5%A6%82%E4%BD%95%E9%A2%84%E5%88%86%E5%8C%BA%EF%BC%9F"><span class="nav-number">1.6.</span> <span class="nav-text">2、如何预分区？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3%E3%80%81%E5%A6%82%E4%BD%95%E8%AE%BE%E5%AE%9A%E9%A2%84%E5%88%86%E5%8C%BA%EF%BC%9F"><span class="nav-number">1.7.</span> <span class="nav-text">3、如何设定预分区？</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1%E3%80%81%E6%89%8B%E5%8A%A8%E6%8C%87%E5%AE%9A%E9%A2%84%E5%88%86%E5%8C%BA"><span class="nav-number">1.7.1.</span> <span class="nav-text">1、手动指定预分区</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2%E3%80%81%E4%BD%BF%E7%94%A816%E8%BF%9B%E5%88%B6%E7%AE%97%E6%B3%95%E7%94%9F%E6%88%90%E9%A2%84%E5%88%86%E5%8C%BA"><span class="nav-number">1.7.2.</span> <span class="nav-text">2、使用16进制算法生成预分区</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3%E3%80%81%E4%BD%BF%E7%94%A8JavaAPI%E5%88%9B%E5%BB%BA%E9%A2%84%E5%88%86%E5%8C%BA"><span class="nav-number">1.7.3.</span> <span class="nav-text">3、使用JavaAPI创建预分区</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%94-HBase%E7%9A%84rowKey%E8%AE%BE%E8%AE%A1%E6%8A%80%E5%B7%A7"><span class="nav-number">1.8.</span> <span class="nav-text">五 HBase的rowKey设计技巧</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-rowkey%E9%95%BF%E5%BA%A6%E5%8E%9F%E5%88%99"><span class="nav-number">1.8.1.</span> <span class="nav-text">1 rowkey长度原则</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-rowkey%E6%95%A3%E5%88%97%E5%8E%9F%E5%88%99"><span class="nav-number">1.8.2.</span> <span class="nav-text">2 rowkey散列原则</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-rowkey%E5%94%AF%E4%B8%80%E5%8E%9F%E5%88%99"><span class="nav-number">1.8.3.</span> <span class="nav-text">3 rowkey唯一原则</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4%E4%BB%80%E4%B9%88%E6%98%AF%E7%83%AD%E7%82%B9"><span class="nav-number">1.8.4.</span> <span class="nav-text">4什么是热点</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1%E5%8A%A0%E7%9B%90"><span class="nav-number">1.8.4.1.</span> <span class="nav-text">1加盐</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2%E5%93%88%E5%B8%8C"><span class="nav-number">1.8.4.2.</span> <span class="nav-text">2哈希</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3%E5%8F%8D%E8%BD%AC"><span class="nav-number">1.8.4.3.</span> <span class="nav-text">3反转</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3%E6%97%B6%E9%97%B4%E6%88%B3%E5%8F%8D%E8%BD%AC"><span class="nav-number">1.8.4.4.</span> <span class="nav-text">3时间戳反转</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%AD-Hbase%E7%9A%84%E5%8D%8F%E5%A4%84%E7%90%86%E5%99%A8"><span class="nav-number">1.9.</span> <span class="nav-text">六 Hbase的协处理器</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2%E3%80%81%E5%8D%8F%E5%A4%84%E7%90%86%E5%99%A8%E6%9C%89%E4%B8%A4%E7%A7%8D%EF%BC%9A-observer-%E5%92%8C-endpoint"><span class="nav-number">1.10.</span> <span class="nav-text">2、协处理器有两种： observer 和 endpoint</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3%E3%80%81%E5%8D%8F%E5%A4%84%E7%90%86%E5%99%A8%E5%8A%A0%E8%BD%BD%E6%96%B9%E5%BC%8F"><span class="nav-number">1.11.</span> <span class="nav-text">3、协处理器加载方式</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4%E3%80%81%E5%8D%8F%E5%A4%84%E7%90%86%E5%99%A8Observer%E5%BA%94%E7%94%A8%E5%AE%9E%E6%88%98"><span class="nav-number">1.12.</span> <span class="nav-text">4、协处理器Observer应用实战</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E4%B8%80%E6%AD%A5%EF%BC%9AHBase%E5%BD%93%E4%B8%AD%E5%88%9B%E5%BB%BA%E7%AC%AC%E4%B8%80%E5%BC%A0%E8%A1%A8proc1"><span class="nav-number">1.12.1.</span> <span class="nav-text">第一步：HBase当中创建第一张表proc1</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E4%BA%8C%E6%AD%A5%EF%BC%9AHbase%E5%BD%93%E4%B8%AD%E5%88%9B%E5%BB%BA%E7%AC%AC%E4%BA%8C%E5%BC%A0%E8%A1%A8proc2"><span class="nav-number">1.12.2.</span> <span class="nav-text">第二步：Hbase当中创建第二张表proc2</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E4%B8%89%E6%AD%A5%EF%BC%9A%E5%BC%80%E5%8F%91HBase%E7%9A%84%E5%8D%8F%E5%A4%84%E7%90%86%E5%99%A8"><span class="nav-number">1.12.3.</span> <span class="nav-text">第三步：开发HBase的协处理器</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E5%9B%9B%E6%AD%A5%EF%BC%9A%E5%B0%86%E9%A1%B9%E7%9B%AE%E6%89%93%E6%88%90jar%E5%8C%85%EF%BC%8C%E5%B9%B6%E4%B8%8A%E4%BC%A0%E5%88%B0HDFS%E4%B8%8A%E9%9D%A2"><span class="nav-number">1.12.4.</span> <span class="nav-text">第四步：将项目打成jar包，并上传到HDFS上面</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E4%BA%94%E6%AD%A5%EF%BC%9A%E5%B0%86%E6%89%93%E5%A5%BD%E7%9A%84jar%E5%8C%85%E6%8C%82%E8%BD%BD%E5%88%B0proc1%E8%A1%A8%E5%BD%93%E4%B8%AD%E5%8E%BB"><span class="nav-number">1.12.5.</span> <span class="nav-text">第五步：将打好的jar包挂载到proc1表当中去</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E5%85%AD%E6%AD%A5%EF%BC%9Aproc1%E8%A1%A8%E5%BD%93%E4%B8%AD%E6%B7%BB%E5%8A%A0%E6%95%B0%E6%8D%AE"><span class="nav-number">1.12.6.</span> <span class="nav-text">第六步：proc1表当中添加数据</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%83-HBase%E5%BD%93%E4%B8%AD%E7%9A%84%E4%BA%8C%E7%BA%A7%E7%B4%A2%E5%BC%95%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BB%8B%E7%BB%8D"><span class="nav-number">1.13.</span> <span class="nav-text">七 HBase当中的二级索引的基本介绍</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%AB-HBase%E8%B0%83%E4%BC%98"><span class="nav-number">1.14.</span> <span class="nav-text">八 HBase调优</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1%E3%80%81%E9%80%9A%E7%94%A8%E4%BC%98%E5%8C%96"><span class="nav-number">1.15.</span> <span class="nav-text">1、通用优化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1%E3%80%81NameNode%E7%9A%84%E5%85%83%E6%95%B0%E6%8D%AE%E5%A4%87%E4%BB%BD%E4%BD%BF%E7%94%A8SSD"><span class="nav-number">1.15.1.</span> <span class="nav-text">1、NameNode的元数据备份使用SSD</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2%E3%80%81%E5%AE%9A%E6%97%B6%E5%A4%87%E4%BB%BDNameNode%E4%B8%8A%E7%9A%84%E5%85%83%E6%95%B0%E6%8D%AE%EF%BC%8C%E6%AF%8F%E5%B0%8F%E6%97%B6%E6%88%96%E8%80%85%E6%AF%8F%E5%A4%A9%E5%A4%87%E4%BB%BD%EF%BC%8C%E5%A6%82%E6%9E%9C%E6%95%B0%E6%8D%AE%E6%9E%81%E5%85%B6%E9%87%8D%E8%A6%81%EF%BC%8C%E5%8F%AF%E4%BB%A55"><span class="nav-number">1.15.2.</span> <span class="nav-text">2、定时备份NameNode上的元数据，每小时或者每天备份，如果数据极其重要，可以5~</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3%E3%80%81%E4%B8%BANameNode%E6%8C%87%E5%AE%9A%E5%A4%9A%E4%B8%AA%E5%85%83%E6%95%B0%E6%8D%AE%E7%9B%AE%E5%BD%95%EF%BC%8C%E4%BD%BF%E7%94%A8dfs-name-dir%E6%88%96%E8%80%85dfs-namenode-name-dir%E6%8C%87%E5%AE%9A%E3%80%82%E4%B8%80%E4%B8%AA%E6%8C%87%E5%AE%9A%E6%9C%AC%E5%9C%B0%E7%A3%81%E7%9B%98%EF%BC%8C%E4%B8%80%E4%B8%AA%E6%8C%87%E5%AE%9A%E7%BD%91%E7%BB%9C%E7%A3%81%E7%9B%98%E3%80%82%E8%BF%99%E6%A0%B7%E5%8F%AF%E4%BB%A5%E6%8F%90%E4%BE%9B%E5%85%83%E6%95%B0%E6%8D%AE%E7%9A%84%E5%86%97%E4%BD%99%E5%92%8C%E5%81%A5%E5%A3%AE%E6%80%A7%EF%BC%8C%E4%BB%A5%E5%85%8D%E5%8F%91%E7%94%9F%E6%95%85%E9%9A%9C%E3%80%82"><span class="nav-number">1.15.3.</span> <span class="nav-text">3、为NameNode指定多个元数据目录，使用dfs.name.dir或者dfs.namenode.name.dir指定。一个指定本地磁盘，一个指定网络磁盘。这样可以提供元数据的冗余和健壮性，以免发生故障。</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4%E3%80%81%E8%AE%BE%E7%BD%AEdfs-namenode-name-dir-restore%E4%B8%BAtrue%EF%BC%8C%E5%85%81%E8%AE%B8%E5%B0%9D%E8%AF%95%E6%81%A2%E5%A4%8D%E4%B9%8B%E5%89%8D%E5%A4%B1%E8%B4%A5%E7%9A%84dfs-namenode-name-dir%E7%9B%AE%E5%BD%95%EF%BC%8C%E5%9C%A8%E5%88%9B%E5%BB%BAcheckpoint%E6%97%B6%E5%81%9A%E6%AD%A4%E5%B0%9D%E8%AF%95%EF%BC%8C%E5%A6%82%E6%9E%9C%E8%AE%BE%E7%BD%AE%E4%BA%86%E5%A4%9A%E4%B8%AA%E7%A3%81%E7%9B%98%EF%BC%8C%E5%BB%BA%E8%AE%AE%E5%85%81%E8%AE%B8%E3%80%82"><span class="nav-number">1.15.4.</span> <span class="nav-text">4、设置dfs.namenode.name.dir.restore为true，允许尝试恢复之前失败的dfs.namenode.name.dir目录，在创建checkpoint时做此尝试，如果设置了多个磁盘，建议允许。</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5%E3%80%81NameNode%E8%8A%82%E7%82%B9%E5%BF%85%E9%A1%BB%E9%85%8D%E7%BD%AE%E4%B8%BARAID1%EF%BC%88%E9%95%9C%E5%83%8F%E7%9B%98%EF%BC%89%E7%BB%93%E6%9E%84%E3%80%82"><span class="nav-number">1.15.5.</span> <span class="nav-text">5、NameNode节点必须配置为RAID1（镜像盘）结构。</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6%E3%80%81%E8%A1%A5%E5%85%85%EF%BC%9A%E4%BB%80%E4%B9%88%E6%98%AFRaid0%E3%80%81Raid0-1%E3%80%81Raid1%E3%80%81Raid5"><span class="nav-number">1.15.6.</span> <span class="nav-text">6、补充：什么是Raid0、Raid0+1、Raid1、Raid5</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7%E3%80%81%E4%BF%9D%E6%8C%81NameNode%E6%97%A5%E5%BF%97%E7%9B%AE%E5%BD%95%E6%9C%89%E8%B6%B3%E5%A4%9F%E7%9A%84%E7%A9%BA%E9%97%B4%EF%BC%8C%E8%BF%99%E4%BA%9B%E6%97%A5%E5%BF%97%E6%9C%89%E5%8A%A9%E4%BA%8E%E5%B8%AE%E5%8A%A9%E4%BD%A0%E5%8F%91%E7%8E%B0%E9%97%AE%E9%A2%98%E3%80%82"><span class="nav-number">1.15.7.</span> <span class="nav-text">7、保持NameNode日志目录有足够的空间，这些日志有助于帮助你发现问题。</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8%E3%80%81%E5%9B%A0%E4%B8%BAHadoop%E6%98%AFIO%E5%AF%86%E9%9B%86%E5%9E%8B%E6%A1%86%E6%9E%B6%EF%BC%8C%E6%89%80%E4%BB%A5%E5%B0%BD%E9%87%8F%E6%8F%90%E5%8D%87%E5%AD%98%E5%82%A8%E7%9A%84%E9%80%9F%E5%BA%A6%E5%92%8C%E5%90%9E%E5%90%90%E9%87%8F%EF%BC%88%E7%B1%BB%E4%BC%BC%E4%BD%8D%E5%AE%BD%EF%BC%89%E3%80%82"><span class="nav-number">1.15.8.</span> <span class="nav-text">8、因为Hadoop是IO密集型框架，所以尽量提升存储的速度和吞吐量（类似位宽）。</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-%E3%80%81Linux%E4%BC%98%E5%8C%96"><span class="nav-number">1.16.</span> <span class="nav-text">2   、Linux优化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1%E3%80%81%E5%BC%80%E5%90%AF%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E7%9A%84%E9%A2%84%E8%AF%BB%E7%BC%93%E5%AD%98%E5%8F%AF%E4%BB%A5%E6%8F%90%E9%AB%98%E8%AF%BB%E5%8F%96%E9%80%9F%E5%BA%A6"><span class="nav-number">1.16.1.</span> <span class="nav-text">1、开启文件系统的预读缓存可以提高读取速度</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2%E3%80%81%E5%85%B3%E9%97%AD%E8%BF%9B%E7%A8%8B%E7%9D%A1%E7%9C%A0%E6%B1%A0"><span class="nav-number">1.16.2.</span> <span class="nav-text">2、关闭进程睡眠池</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3%E3%80%81%E8%B0%83%E6%95%B4ulimit%E4%B8%8A%E9%99%90%EF%BC%8C%E9%BB%98%E8%AE%A4%E5%80%BC%E4%B8%BA%E6%AF%94%E8%BE%83%E5%B0%8F%E7%9A%84%E6%95%B0%E5%AD%97"><span class="nav-number">1.16.3.</span> <span class="nav-text">3、调整ulimit上限，默认值为比较小的数字</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4%E3%80%81%E5%BC%80%E5%90%AF%E9%9B%86%E7%BE%A4%E7%9A%84%E6%97%B6%E9%97%B4%E5%90%8C%E6%AD%A5NTP%EF%BC%8C%E8%AF%B7%E5%8F%82%E7%9C%8B%E4%B9%8B%E5%89%8D%E6%96%87%E6%A1%A3"><span class="nav-number">1.16.4.</span> <span class="nav-text">4、开启集群的时间同步NTP，请参看之前文档</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5%E3%80%81%E6%9B%B4%E6%96%B0%E7%B3%BB%E7%BB%9F%E8%A1%A5%E4%B8%81%EF%BC%88%E6%B3%A8%E6%84%8F%EF%BC%9A%E6%9B%B4%E6%96%B0%E8%A1%A5%E4%B8%81%E5%89%8D%EF%BC%8C%E8%AF%B7%E5%85%88%E6%B5%8B%E8%AF%95%E6%96%B0%E7%89%88%E6%9C%AC%E8%A1%A5%E4%B8%81%E5%AF%B9%E9%9B%86%E7%BE%A4%E8%8A%82%E7%82%B9%E7%9A%84%E5%85%BC%E5%AE%B9%E6%80%A7%EF%BC%89"><span class="nav-number">1.16.5.</span> <span class="nav-text">5、更新系统补丁（注意：更新补丁前，请先测试新版本补丁对集群节点的兼容性）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3%E3%80%81HDFS%E4%BC%98%E5%8C%96%EF%BC%88hdfs-site-xml%EF%BC%89"><span class="nav-number">1.17.</span> <span class="nav-text">3、HDFS优化（hdfs-site.xml）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1%E3%80%81%E4%BF%9D%E8%AF%81RPC%E8%B0%83%E7%94%A8%E4%BC%9A%E6%9C%89%E8%BE%83%E5%A4%9A%E7%9A%84%E7%BA%BF%E7%A8%8B%E6%95%B0"><span class="nav-number">1.17.1.</span> <span class="nav-text">1、保证RPC调用会有较多的线程数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2%E3%80%81%E5%89%AF%E6%9C%AC%E6%95%B0%E7%9A%84%E8%B0%83%E6%95%B4"><span class="nav-number">1.17.2.</span> <span class="nav-text">2、副本数的调整</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E3%80%81%E6%96%87%E4%BB%B6%E5%9D%97%E5%A4%A7%E5%B0%8F%E7%9A%84%E8%B0%83%E6%95%B4"><span class="nav-number">1.17.3.</span> <span class="nav-text">3.、文件块大小的调整</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4%E3%80%81MapReduce%E4%BC%98%E5%8C%96%EF%BC%88mapred-site-xml%EF%BC%89"><span class="nav-number">1.18.</span> <span class="nav-text">4、MapReduce优化（mapred-site.xml）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1%E3%80%81Job%E4%BB%BB%E5%8A%A1%E6%9C%8D%E5%8A%A1%E7%BA%BF%E7%A8%8B%E6%95%B0%E8%B0%83%E6%95%B4"><span class="nav-number">1.18.1.</span> <span class="nav-text">1、Job任务服务线程数调整</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2%E3%80%81Http%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%B7%A5%E4%BD%9C%E7%BA%BF%E7%A8%8B%E6%95%B0"><span class="nav-number">1.18.2.</span> <span class="nav-text">2、Http服务器工作线程数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3%E3%80%81%E6%96%87%E4%BB%B6%E6%8E%92%E5%BA%8F%E5%90%88%E5%B9%B6%E4%BC%98%E5%8C%96"><span class="nav-number">1.18.3.</span> <span class="nav-text">3、文件排序合并优化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4%E3%80%81%E8%AE%BE%E7%BD%AE%E4%BB%BB%E5%8A%A1%E5%B9%B6%E5%8F%91"><span class="nav-number">1.18.4.</span> <span class="nav-text">4、设置任务并发</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5%E3%80%81MR%E8%BE%93%E5%87%BA%E6%95%B0%E6%8D%AE%E7%9A%84%E5%8E%8B%E7%BC%A9"><span class="nav-number">1.18.5.</span> <span class="nav-text">5、MR输出数据的压缩</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6%E3%80%81%E4%BC%98%E5%8C%96Mapper%E5%92%8CReducer%E7%9A%84%E4%B8%AA%E6%95%B0"><span class="nav-number">1.18.6.</span> <span class="nav-text">6、优化Mapper和Reducer的个数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5%E3%80%81HBase%E4%BC%98%E5%8C%96"><span class="nav-number">1.19.</span> <span class="nav-text">5、HBase优化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1%E3%80%81%E5%9C%A8HDFS%E7%9A%84%E6%96%87%E4%BB%B6%E4%B8%AD%E8%BF%BD%E5%8A%A0%E5%86%85%E5%AE%B9"><span class="nav-number">1.19.1.</span> <span class="nav-text">1、在HDFS的文件中追加内容</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2%E3%80%81%E4%BC%98%E5%8C%96DataNode%E5%85%81%E8%AE%B8%E7%9A%84%E6%9C%80%E5%A4%A7%E6%96%87%E4%BB%B6%E6%89%93%E5%BC%80%E6%95%B0"><span class="nav-number">1.19.2.</span> <span class="nav-text">2、优化DataNode允许的最大文件打开数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3%E3%80%81%E4%BC%98%E5%8C%96%E5%BB%B6%E8%BF%9F%E9%AB%98%E7%9A%84%E6%95%B0%E6%8D%AE%E6%93%8D%E4%BD%9C%E7%9A%84%E7%AD%89%E5%BE%85%E6%97%B6%E9%97%B4"><span class="nav-number">1.19.3.</span> <span class="nav-text">3、优化延迟高的数据操作的等待时间</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4%E3%80%81%E4%BC%98%E5%8C%96%E6%95%B0%E6%8D%AE%E7%9A%84%E5%86%99%E5%85%A5%E6%95%88%E7%8E%87"><span class="nav-number">1.19.4.</span> <span class="nav-text">4、优化数据的写入效率</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5%E3%80%81%E4%BC%98%E5%8C%96DataNode%E5%AD%98%E5%82%A8"><span class="nav-number">1.19.5.</span> <span class="nav-text">5、优化DataNode存储</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6%E3%80%81%E8%AE%BE%E7%BD%AERPC%E7%9B%91%E5%90%AC%E6%95%B0%E9%87%8F"><span class="nav-number">1.19.6.</span> <span class="nav-text">6、设置RPC监听数量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7%E3%80%81%E4%BC%98%E5%8C%96HStore%E6%96%87%E4%BB%B6%E5%A4%A7%E5%B0%8F"><span class="nav-number">1.19.7.</span> <span class="nav-text">7、优化HStore文件大小</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8%E3%80%81%E4%BC%98%E5%8C%96hbase%E5%AE%A2%E6%88%B7%E7%AB%AF%E7%BC%93%E5%AD%98"><span class="nav-number">1.19.8.</span> <span class="nav-text">8、优化hbase客户端缓存</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9%E3%80%81%E6%8C%87%E5%AE%9Ascan-next%E6%89%AB%E6%8F%8FHBase%E6%89%80%E8%8E%B7%E5%8F%96%E7%9A%84%E8%A1%8C%E6%95%B0"><span class="nav-number">1.19.9.</span> <span class="nav-text">9、指定scan.next扫描HBase所获取的行数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6%E3%80%81%E5%86%85%E5%AD%98%E4%BC%98%E5%8C%96"><span class="nav-number">1.20.</span> <span class="nav-text">6、内存优化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7%E3%80%81JVM%E4%BC%98%E5%8C%96"><span class="nav-number">1.21.</span> <span class="nav-text">7、JVM优化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1%E3%80%81%E5%B9%B6%E8%A1%8CGC"><span class="nav-number">1.21.1.</span> <span class="nav-text">1、并行GC</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2%E3%80%81%E5%90%8C%E6%97%B6%E5%A4%84%E7%90%86%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E7%9A%84%E7%BA%BF%E7%A8%8B%E6%95%B0"><span class="nav-number">1.21.2.</span> <span class="nav-text">2、同时处理垃圾回收的线程数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3%E3%80%81%E7%A6%81%E7%94%A8%E6%89%8B%E5%8A%A8GC"><span class="nav-number">1.21.3.</span> <span class="nav-text">3、禁用手动GC</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8%E3%80%81Zookeeper%E4%BC%98%E5%8C%96"><span class="nav-number">1.22.</span> <span class="nav-text">8、Zookeeper优化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1%E3%80%81%E4%BC%98%E5%8C%96Zookeeper%E4%BC%9A%E8%AF%9D%E8%B6%85%E6%97%B6%E6%97%B6%E9%97%B4"><span class="nav-number">1.22.1.</span> <span class="nav-text">1、优化Zookeeper会话超时时间</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="HF"
      src="/images/hexo.jpg">
  <p class="site-author-name" itemprop="name">HF</p>
  <div class="site-description" itemprop="description">第二名就是头号输家</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">84</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">38</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">39</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      推荐阅读
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="http://www.54tianzhisheng.cn/tags/Flink/" title="http:&#x2F;&#x2F;www.54tianzhisheng.cn&#x2F;tags&#x2F;Flink&#x2F;" rel="noopener" target="_blank">Flink</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://nginxconfig.io/" title="https:&#x2F;&#x2F;nginxconfig.io&#x2F;" rel="noopener" target="_blank">Nginxconfig</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://linux.51yip.com/" title="http:&#x2F;&#x2F;linux.51yip.com&#x2F;" rel="noopener" target="_blank">Linux命令手册</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://echarts.baidu.com/index.html" title="https:&#x2F;&#x2F;echarts.baidu.com&#x2F;index.html" rel="noopener" target="_blank">echarts可视化库</a>
        </li>
    </ul>
  </div>
<div style="">
  <canvas id="canvas" style="width:60%;">当前浏览器不支持canvas，请更换浏览器后再试</canvas>
</div>
<script>
(function(){

   var digit=
    [
        [
            [0,0,1,1,1,0,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,0,1,1,0],
            [0,0,1,1,1,0,0]
        ],//0
        [
            [0,0,0,1,1,0,0],
            [0,1,1,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [1,1,1,1,1,1,1]
        ],//1
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,1,1],
            [1,1,1,1,1,1,1]
        ],//2
        [
            [1,1,1,1,1,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//3
        [
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,0],
            [0,0,1,1,1,1,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,1,1,0],
            [1,1,1,1,1,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,1]
        ],//4
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,1,1,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//5
        [
            [0,0,0,0,1,1,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//6
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0]
        ],//7
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//8
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,1,1,0,0,0,0]
        ],//9
        [
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0],
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0]
        ]//:
    ];

var canvas = document.getElementById('canvas');

if(canvas.getContext){
    var cxt = canvas.getContext('2d');
    //声明canvas的宽高
    var H = 100,W = 700;
    canvas.height = H;
    canvas.width = W;
    cxt.fillStyle = '#f00';
    cxt.fillRect(10,10,50,50);

    //存储时间数据
    var data = [];
    //存储运动的小球
    var balls = [];
    //设置粒子半径
    var R = canvas.height/20-1;
    (function(){
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        //存储时间数字，由十位小时、个位小时、冒号、十位分钟、个位分钟、冒号、十位秒钟、个位秒钟这7个数字组成
        data.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
    })();

    /*生成点阵数字*/
    function renderDigit(index,num){
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    cxt.beginPath();
                    cxt.arc(14*(R+2)*index + j*2*(R+1)+(R+1),i*2*(R+1)+(R+1),R,0,2*Math.PI);
                    cxt.closePath();
                    cxt.fill();
                }
            }
        }
    }

    /*更新时钟*/
    function updateDigitTime(){
        var changeNumArray = [];
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        var NewData = [];
        NewData.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
        for(var i = data.length-1; i >=0 ; i--){
            //时间发生变化
            if(NewData[i] !== data[i]){
                //将变化的数字值和在data数组中的索引存储在changeNumArray数组中
                changeNumArray.push(i+'_'+(Number(data[i])+1)%10);
            }
        }
        //增加小球
        for(var i = 0; i< changeNumArray.length; i++){
            addBalls.apply(this,changeNumArray[i].split('_'));
        }
        data = NewData.concat();
    }

    /*更新小球状态*/
    function updateBalls(){
        for(var i = 0; i < balls.length; i++){
            balls[i].stepY += balls[i].disY;
            balls[i].x += balls[i].stepX;
            balls[i].y += balls[i].stepY;
            if(balls[i].x > W + R || balls[i].y > H + R){
                balls.splice(i,1);
                i--;
            }
        }
    }

    /*增加要运动的小球*/
    function addBalls(index,num){
        var numArray = [1,2,3];
        var colorArray =  ["#3BE","#09C","#A6C","#93C","#9C0","#690","#FB3","#F80","#F44","#C00"];
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    var ball = {
                        x:14*(R+2)*index + j*2*(R+1)+(R+1),
                        y:i*2*(R+1)+(R+1),
                        stepX:Math.floor(Math.random() * 4 -2),
                        stepY:-2*numArray[Math.floor(Math.random()*numArray.length)],
                        color:colorArray[Math.floor(Math.random()*colorArray.length)],
                        disY:1
                    };
                    balls.push(ball);
                }
            }
        }
    }

    /*渲染*/
    function render(){
        //重置画布宽度，达到清空画布的效果
        canvas.height = 100;
        //渲染时钟
        for(var i = 0; i < data.length; i++){
            renderDigit(i,data[i]);
        }
        //渲染小球
        for(var i = 0; i < balls.length; i++){
            cxt.beginPath();
            cxt.arc(balls[i].x,balls[i].y,R,0,2*Math.PI);
            cxt.fillStyle = balls[i].color;
            cxt.closePath();
            cxt.fill();
        }
    }

    clearInterval(oTimer);
    var oTimer = setInterval(function(){
        //更新时钟
        updateDigitTime();
        //更新小球状态
        updateBalls();
        //渲染
        render();
    },50);
}

})();
</script>
      </div>

    
          <script type="text/javascript" charset="utf-8" src="/js/tagcloud.js"></script>
         <script type="text/javascript" charset="utf-8" src="/js/tagcanvas.js"></script>
           <div class="widget-wrap">
        <h3 class="widget-title">Tag Cloud</h3>
        <div id="myCanvasContainer" class="widget tagcloud">
            <canvas width="250" height="250" id="resCanvas" style="width=100%">
                <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Ai/" rel="tag">Ai</a><span class="tag-list-count">7</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Azkaban/" rel="tag">Azkaban</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Blog/" rel="tag">Blog</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ClouderaManager/" rel="tag">ClouderaManager</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ElSearch/" rel="tag">ElSearch</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Flink/" rel="tag">Flink</a><span class="tag-list-count">9</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Flume/" rel="tag">Flume</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Git/" rel="tag">Git</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hadoop/" rel="tag">Hadoop</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hbase/" rel="tag">Hbase</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hdfs/" rel="tag">Hdfs</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hexo/" rel="tag">Hexo</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hive/" rel="tag">Hive</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hue/" rel="tag">Hue</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Impala/" rel="tag">Impala</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Java/" rel="tag">Java</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Jvm/" rel="tag">Jvm</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Kafka/" rel="tag">Kafka</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Kettle/" rel="tag">Kettle</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Kudu/" rel="tag">Kudu</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linux/" rel="tag">Linux</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Livy/" rel="tag">Livy</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/MapReduce/" rel="tag">MapReduce</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Mysql/" rel="tag">Mysql</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Oozie/" rel="tag">Oozie</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python/" rel="tag">Python</a><span class="tag-list-count">8</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Redis/" rel="tag">Redis</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Scala/" rel="tag">Scala</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Shell/" rel="tag">Shell</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Spark/" rel="tag">Spark</a><span class="tag-list-count">14</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Sqoop/" rel="tag">Sqoop</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Web/" rel="tag">Web</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Yarn/" rel="tag">Yarn</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ZK/" rel="tag">ZK</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E5%8F%AF%E8%A7%86%E5%8C%96/" rel="tag">数据分析与可视化</a><span class="tag-list-count">8</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/" rel="tag">数据库</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E4%B8%8E%E5%88%86%E6%9E%90/" rel="tag">数据挖掘与分析</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/" rel="tag">数据结构与算法</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习与深度学习</a><span class="tag-list-count">2</span></li></ul>
            </canvas>
        </div>
    </div>
    

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright" style=" text-align:center;">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">HF</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">1.3m</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">19:11</span>
</div>

  <!-- 网站运行时间的设置 -->
<div class="run_time" style=" text-align:center;">
  <span id="timeDate">载入天数...</span><span id="times">载入时分秒...</span>
  <script>
    var now = new Date(); 
    function createtime() { 
        var grt= new Date("07/23/2017 10:00:00");//此处修改你的建站时间或者网站上线时间 
        now.setTime(now.getTime()+250); 
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days); 
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours); 
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum); 
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;} 
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum); 
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;} 
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 "; 
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒"; 
    } 
    setInterval("createtime()",250);
  </script>
</div>
        
<div class="busuanzi-count" style=" text-align:center;">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      我的第 <span class="busuanzi-value" id="busuanzi_value_site_uv"></span> 位朋友，
    </span>
  



  
    <span class="site-pv">
      历经 <span class="busuanzi-value" id="busuanzi_value_site_pv"></span> 次回眸才与你相遇
    </span>
  
</div>










      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>

<script src="/js/bookmark.js"></script>


  <script defer src="/lib/three/three.min.js"></script>
    <script defer src="/lib/three/canvas_lines.min.js"></script>


  




  
<script src="/js/local-search.js"></script>













  

  
<div class="moon-menu">
  <div class="moon-menu-items">
    
    <div class="moon-menu-item" onclick="back2bottom()">
      <i class='fas fa-chevron-down'></i>    </div>
    
    <div class="moon-menu-item" onclick="back2top()">
      <i class='fas fa-chevron-up'></i>    </div>
    
  </div>
  <div class="moon-menu-button">
    <svg class="moon-menu-bg">
      <circle class="moon-menu-cricle" cx="50%" cy="50%" r="44%"></circle>
      <circle class="moon-menu-border" cx="50%" cy="50%" r="48%"></circle>
    </svg>
    <div class="moon-menu-content">
      <div class="moon-menu-icon"><i class='fas fa-ellipsis-v'></i></div>
      <div class="moon-menu-text"></div>
    </div>
  </div>
</div><script src="/js/injector.js"></script>
</body>
</html>
<!-- 页面点击小红心 -->
<script type="text/javascript" src="/js/clicklove.js"></script>
<!-- 雪花特效 -->
<script type="text/javascript" src="/js/jquery.js"></script>
<script type="text/javascript" src="/js/jquery.min.js"></script>
<script type="text/javascript" src="/js/snow.js"></script>