<!DOCTYPE html>












  


<html class="theme-next mist use-motion" lang="zh-CN">

<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">




  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link rel="stylesheet" href="/lib/pace/pace-theme-bounce.min.css?v=1.0.2">













  <meta name="baidu-site-verification" content="true">



  
  
  <link rel="stylesheet" href="/lib/fancybox/source/jquery.fancybox.css">







<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2">

<link rel="stylesheet" href="/css/main.css?v=7.1.1">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.1.1">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.1.1">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.1.1">


  <link rel="mask-icon" href="/images/logo.svg?v=7.1.1" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '7.1.1',
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":true,"dimmer":false},
    back2top: true,
    back2top_sidebar: true,
    fancybox: true,
    fastclick: true,
    lazyload: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"shrinkIn","post_header":"slideLeftIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideDownIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="#一 Spark 概述 ##1 什么是Spark Apache Spark 是一个快速的, 多用途的集群计算系统, 相对于 Hadoop MapReduce 将中间结果保存在磁盘中, Spark 使用了内存保存中间结果, 能在数据尚未写入硬盘时在内存中进行运算. Spark 只是一个计算框架, 不像 Hadoop 一样包含了分布式文件系统和完备的调度系统, 如果要使用 Spark, 需要搭载其它的">
<meta name="keywords" content="BigData">
<meta property="og:type" content="article">
<meta property="og:title" content="spark入门">
<meta property="og:url" content="https://manzhong.github.io/2017/08/15/spark入门.html">
<meta property="og:site_name" content="春雨里洗过的太阳">
<meta property="og:description" content="#一 Spark 概述 ##1 什么是Spark Apache Spark 是一个快速的, 多用途的集群计算系统, 相对于 Hadoop MapReduce 将中间结果保存在磁盘中, Spark 使用了内存保存中间结果, 能在数据尚未写入硬盘时在内存中进行运算. Spark 只是一个计算框架, 不像 Hadoop 一样包含了分布式文件系统和完备的调度系统, 如果要使用 Spark, 需要搭载其它的">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://manzhong.github.io/images/spark/zj.jpg">
<meta property="og:image" content="https://manzhong.github.io/images/spark/j.png">
<meta property="og:image" content="https://manzhong.github.io/images/spark/s.png">
<meta property="og:image" content="https://manzhong.github.io/images/spark/ya.png">
<meta property="og:image" content="https://manzhong.github.io/images/spark/sf.png">
<meta property="og:image" content="https://manzhong.github.io/images/spark/yxlc.png">
<meta property="og:image" content="https://manzhong.github.io/images/spark/rddf.png">
<meta property="og:updated_time" content="2019-08-08T00:58:32.045Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="spark入门">
<meta name="twitter:description" content="#一 Spark 概述 ##1 什么是Spark Apache Spark 是一个快速的, 多用途的集群计算系统, 相对于 Hadoop MapReduce 将中间结果保存在磁盘中, Spark 使用了内存保存中间结果, 能在数据尚未写入硬盘时在内存中进行运算. Spark 只是一个计算框架, 不像 Hadoop 一样包含了分布式文件系统和完备的调度系统, 如果要使用 Spark, 需要搭载其它的">
<meta name="twitter:image" content="https://manzhong.github.io/images/spark/zj.jpg">



  <link rel="alternate" href="/atom.xml" title="春雨里洗过的太阳" type="application/atom+xml">



  
  
  <link rel="canonical" href="https://manzhong.github.io/2017/08/15/spark入门">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>spark入门 | 春雨里洗过的太阳</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>
 
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">春雨里洗过的太阳</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-bookmark"></i> <br>分类</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-about">

    
    
    
      
    

    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>关于</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-互动">

    
    
    
      
    

    

    <a href="/guestbook/" rel="section"><i class="menu-item-icon fa fa-fw fa-comments"></i> <br>互动</a>

  </li>

      
      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>搜索</a>
        </li>
      
    </ul>
  

  
    

  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://manzhong.github.io/2017/08/15/spark入门.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="丨HF丨">
      <meta itemprop="description" content="第二名就是头号输家!!!">
      <meta itemprop="image" content="/images/hexo.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="春雨里洗过的太阳">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">spark入门

              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2017-08-15 15:59:52" itemprop="dateCreated datePublished" datetime="2017-08-15T15:59:52+08:00">2017-08-15</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-08-08 08:58:32" itemprop="dateModified" datetime="2019-08-08T08:58:32+08:00">2019-08-08</time>
              
            
          </span>

          

          
            
            
          

          
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">本文字数：</span>
                
                <span title="本文字数">25k</span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">22 分钟</span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>#一 Spark 概述</p>
<p>##1 什么是Spark</p>
<p>Apache Spark 是一个快速的, 多用途的集群计算系统, 相对于 Hadoop MapReduce 将中间结果保存在磁盘中, Spark 使用了内存保存中间结果, 能在数据尚未写入硬盘时在内存中进行运算.</p>
<p>Spark 只是一个计算框架, 不像 Hadoop 一样包含了分布式文件系统和完备的调度系统, 如果要使用 Spark, 需要搭载其它的文件系统和更成熟的调度系统</p>
<p>Spark 产生之前, 已经有非常成熟的计算系统存在了, 例如 MapReduce, 这些计算系统提供了高层次的API, 把计算运行在集群中并提供容错能力, 从而实现分布式计算.</p>
<p>虽然这些框架提供了大量的对访问利用计算资源的抽象, 但是它们缺少了对利用分布式内存的抽象, 这些框架多个计算之间的数据复用就是将中间数据写到一个稳定的文件系统中(例如HDFS), 所以会产生数据的复制备份, 磁盘的I/O以及数据的序列化, 所以这些框架在遇到需要在多个计算之间复用中间结果的操作时会非常的不高效.</p>
<p>而这类操作是非常常见的, 例如迭代式计算, 交互式数据挖掘, 图计算等.</p>
<p>认识到这个问题后, 学术界的 AMPLab 提出了一个新的模型, 叫做 <code>RDDs</code>.</p>
<p><code>RDDs</code> 是一个可以容错且并行的数据结构, 它可以让用户显式的将中间结果数据集保存在内中, 并且通过控制数据集的分区来达到数据存放处理最优化.</p>
<p>同时 <code>RDDs</code> 也提供了丰富的 API 来操作数据集.</p>
<p>后来 RDDs 被 AMPLab 在一个叫做 Spark 的框架中提供并开源.</p>
<p><strong>mr 的问题</strong></p>
<p>1 计算过程比较缓慢,不适应与交互式计算,和迭代计算</p>
<p>2 不是所有的计算都由Map和Reduce两个阶段组成</p>
<p><strong>spark解决问题</strong></p>
<p>1  第一个问题  解决  中间结果存在内存中</p>
<p>2 提供了更好的API 函数式</p>
<p>##2 spark特点</p>
<ul>
<li><p>速度快</p>
<p>Spark 的在内存时的运行速度是 Hadoop MapReduce 的100倍基于硬盘的运算速度大概是 Hadoop MapReduce 的10倍Spark 实现了一种叫做 RDDs 的 DAG 执行引擎, 其数据缓存在内存中可以进行迭代处理</p>
</li>
<li><p>易用</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">df = spark.read.json(<span class="string">"logs.json"</span>)</span><br><span class="line">df.where(<span class="string">"age &gt; 21"</span>) \</span><br><span class="line">  .select(<span class="string">"name.first"</span>) \</span><br><span class="line">  .show()</span><br></pre></td></tr></table></figure>
<ul>
<li>Spark 支持 Java, Scala, Python, R, SQL 等多种语言的API.</li>
<li>Spark 支持超过80个高级运算符使得用户非常轻易的构建并行计算程序</li>
<li>Spark 可以使用基于 Scala, Python, R, SQL的 Shell 交互式查询.</li>
</ul>
</li>
<li><p>通用</p>
<p>Spark 提供一个完整的技术栈, 包括 SQL执行, Dataset命令式API, 机器学习库MLlib, 图计算框架GraphX, 流计算SparkStreaming用户可以在同一个应用中同时使用这些工具, 这一点是划时代的</p>
</li>
<li><p>兼容</p>
<p>Spark 可以运行在 Hadoop Yarn, Apache Mesos, Kubernets, Spark Standalone等集群中Spark 可以访问 HBase, HDFS, Hive, Cassandra 在内的多种数据库</p>
</li>
</ul>
<p>总结</p>
<ul>
<li>支持 Java, Scala, Python 和 R 的 API</li>
<li>可扩展至超过 8K 个节点</li>
<li>能够在内存中缓存数据集, 以实现交互式数据分析</li>
<li>提供命令行窗口, 减少探索式的数据分析的反应时间</li>
</ul>
<h2 id="3-spark组件"><a href="#3-spark组件" class="headerlink" title="3 spark组件"></a>3 spark组件</h2><p>Spark 最核心的功能是 RDDs, RDDs 存在于 <code>spark-core</code> 这个包内, 这个包也是 Spark 最核心的包.</p>
<p>同时 Spark 在 <code>spark-core</code> 的上层提供了很多工具, 以便于适应不用类型的计算.</p>
<ul>
<li><p>Spark-Core 和 弹性分布式数据集(RDDs)</p>
<p>Spark-Core 是整个 Spark 的基础, 提供了分布式任务调度和基本的 I/O 功能Spark 的基础的程序抽象是弹性分布式数据集(RDDs), 是一个可以并行操作, 有容错的数据集合RDDs 可以通过引用外部存储系统的数据集创建(如HDFS, HBase), 或者通过现有的 RDDs 转换得到RDDs 抽象提供了 Java, Scala, Python 等语言的APIRDDs 简化了编程复杂性, 操作 RDDs 类似通过 Scala 或者 Java8 的 Streaming 操作本地数据集合</p>
</li>
<li><p>Spark SQL</p>
<p>Spark SQL 在 <code>spark-core</code> 基础之上带出了一个名为 DataSet 和 DataFrame 的数据抽象化的概念Spark SQL 提供了在 Dataset 和 DataFrame 之上执行 SQL 的能力Spark SQL 提供了 DSL, 可以通过 Scala, Java, Python 等语言操作 DataSet 和 DataFrame它还支持使用 JDBC/ODBC 服务器操作 SQL 语言</p>
</li>
<li><p>Spark Streaming</p>
<p>Spark Streaming 充分利用 <code>spark-core</code> 的快速调度能力来运行流分析它截取小批量的数据并可以对之运行 RDD Transformation它提供了在同一个程序中同时使用流分析和批量分析的能力</p>
</li>
<li><p>MLlib</p>
<p>MLlib 是 Spark 上分布式机器学习的框架. Spark分布式内存的架构 比 Hadoop磁盘式 的 Apache Mahout 快上 10 倍, 扩展性也非常优良MLlib 可以使用许多常见的机器学习和统计算法, 简化大规模机器学习汇总统计, 相关性, 分层抽样, 假设检定, 随即数据生成支持向量机, 回归, 线性回归, 逻辑回归, 决策树, 朴素贝叶斯协同过滤, ALSK-meansSVD奇异值分解, PCA主成分分析TF-IDF, Word2Vec, StandardScalerSGD随机梯度下降, L-BFGS</p>
</li>
<li><p>GraphX</p>
<p>GraphX 是分布式图计算框架, 提供了一组可以表达图计算的 API, GraphX 还对这种抽象化提供了优化运行</p>
</li>
</ul>
<p>总结</p>
<ul>
<li>Spark 提供了 批处理(RDDs), 结构化查询(DataFrame), 流计算(SparkStreaming), 机器学习(MLlib), 图计算(GraphX) 等组件</li>
<li>这些组件均是依托于通用的计算引擎 RDDs 而构建出的, 所以 <code>spark-core</code> 的 RDDs 是整个 Spark 的基础</li>
</ul>
<p><img src="/images/spark/zj.jpg" alt="img"></p>
<h3 id="Spark和Hadoop的异同"><a href="#Spark和Hadoop的异同" class="headerlink" title="Spark和Hadoop的异同"></a>Spark和Hadoop的异同</h3><table>
<thead>
<tr>
<th></th>
<th>Hadoop</th>
<th>Spark</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>类型</strong></td>
<td>基础平台, 包含计算, 存储, 调度</td>
<td>分布式计算工具</td>
</tr>
<tr>
<td><strong>场景</strong></td>
<td>大规模数据集上的批处理</td>
<td>迭代计算, 交互式计算, 流计算</td>
</tr>
<tr>
<td><strong>延迟</strong></td>
<td>大</td>
<td>小</td>
</tr>
<tr>
<td><strong>易用性</strong></td>
<td>API 较为底层, 算法适应性差</td>
<td>API 较为顶层, 方便使用</td>
</tr>
<tr>
<td><strong>价格</strong></td>
<td>对机器要求低, 便宜</td>
<td>对内存有要求, 相对较贵</td>
</tr>
</tbody>
</table>
<h1 id="二-集群搭建"><a href="#二-集群搭建" class="headerlink" title="二 集群搭建"></a>二 集群搭建</h1><h2 id="1-spark集群结构"><a href="#1-spark集群结构" class="headerlink" title="1 spark集群结构"></a>1 spark集群结构</h2><p>Spark 自身是没有集群管理工具的, 但是如果想要管理数以千计台机器的集群, 没有一个集群管理工具还不太现实, 所以 Spark 可以借助外部的集群工具来进行管理</p>
<p>整个流程就是使用 Spark 的 Client 提交任务, 找到集群管理工具申请资源, 后将计算任务分发到集群中运行</p>
<p><img src="/images/spark/j.png" alt="img"></p>
<p>名词解释</p>
<ul>
<li><p><code>Driver</code></p>
<p>该进程调用 Spark 程序的 main 方法, 并且启动 SparkContext</p>
</li>
<li><p><code>Cluster Manager</code></p>
<p>该进程负责和外部集群工具打交道, 申请或释放集群资源</p>
</li>
<li><p><code>Worker</code></p>
<p>该进程是一个守护进程, 负责启动和管理 Executor</p>
</li>
<li><p><code>Executor</code></p>
<p>该进程是一个JVM虚拟机, 负责运行 Spark Task</p>
</li>
</ul>
<p>运行一个 Spark 程序大致经历如下几个步骤</p>
<ol>
<li>启动 Drive, 创建 SparkContext</li>
<li>Client 提交程序给 Drive, Drive <strong>向 Cluster Manager 申请集群资源</strong></li>
<li>资源申请完毕, <strong>在 Worker 中启动 Executor</strong></li>
<li>Driver 将程序转化为 Tasks, 分发给 Executor 执行</li>
</ol>
<p><strong>Spark 程序可以运行在什么地方?</strong></p>
<ul>
<li><strong>集群:</strong> 一组协同工作的计算机, 通常表现的好像是一台计算机一样, <strong>所运行的任务由软件来控制和调度</strong></li>
<li><strong>集群管理工具:</strong> 调度任务到集群的软件</li>
<li><strong>常见的集群管理工具:</strong> Hadoop Yarn, Apache Mesos, Kubernetes</li>
</ul>
<p>Spark 可以将任务运行在两种模式下:</p>
<ul>
<li><strong>单机,</strong> 使用线程模拟并行来运行程序</li>
<li><strong>集群,</strong> 使用集群管理器来和不同类型的集群交互, 将任务运行在集群中</li>
</ul>
<p>Spark 可以使用的集群管理工具有:</p>
<ul>
<li>Spark Standalone</li>
<li>Hadoop Yarn</li>
<li>Apache Mesos</li>
<li>Kubernetes</li>
</ul>
<p><strong>Driver 和 Worker 什么时候被启动?</strong></p>
<p><img src="/images/spark/s.png" alt="img"></p>
<ul>
<li>Standalone 集群中, 分为两个角色: Master 和 Slave, 而 Slave 就是 Worker, 所以在 Standalone 集群中, 启动之初就会创建固定数量的 Worker</li>
<li>Driver 的启动分为两种模式: Client 和 Cluster. 在 Client 模式下, Driver 运行在 Client 端, 在 Client 启动的时候被启动. 在 Cluster 模式下, Driver 运行在某个 Worker 中, 随着应用的提交而启动</li>
</ul>
<p><img src="/images/spark/ya.png" alt="img"></p>
<ul>
<li>在 Yarn 集群模式下, 也依然分为 Client 模式和 Cluster 模式, 较新的版本中已经逐渐在废弃 Client 模式了, 所以上图所示为 Cluster 模式</li>
<li>如果要在 Yarn 中运行 Spark 程序, 首先会和 RM 交互, 开启 ApplicationMaster, 其中运行了 Driver, Driver创建基础环境后, 会由 RM 提供对应的容器, 运行 Executor, Executor会反向向 Driver 反向注册自己, 并申请 Tasks 执行</li>
</ul>
<p><strong>总结</strong></p>
<ul>
<li><code>Master</code> 负责总控, 调度, 管理和协调 Worker, 保留资源状况等</li>
<li><code>Slave</code> 对应 Worker 节点, 用于启动 Executor 执行 Tasks, 定期向 Master汇报</li>
<li><code>Driver</code> 运行在 Client 或者 Slave(Worker) 中, 默认运行在 Slave(Worker) 中</li>
</ul>
<h2 id="2-集群搭建"><a href="#2-集群搭建" class="headerlink" title="2 集群搭建"></a>2 集群搭建</h2><p><strong>集群规划</strong></p>
<table>
<thead>
<tr>
<th style="text-align:left">Node01</th>
<th style="text-align:left">Node02</th>
<th style="text-align:left">Node03</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Master</td>
<td style="text-align:left">Slave</td>
<td style="text-align:left">Slave</td>
</tr>
<tr>
<td style="text-align:left">History Server</td>
<td style="text-align:left"></td>
</tr>
</tbody>
</table>
<h3 id="2-1-下载解压"><a href="#2-1-下载解压" class="headerlink" title="2.1 下载解压"></a>2.1 下载解压</h3><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">网址</span><br><span class="line">https:<span class="regexp">//</span>archive.apache.org<span class="regexp">/dist/</span>spark<span class="regexp">/spark-2.2.0/</span>spark-<span class="number">2.2</span>.<span class="number">0</span>-bin-hadoop2.<span class="number">7</span>.tgz</span><br></pre></td></tr></table></figure>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">上传解压</span><br><span class="line">tar -xzvf spark<span class="number">-2.2</span><span class="number">.0</span>-bin-hadoop2<span class="number">.7</span>.tgz -C /path</span><br></pre></td></tr></table></figure>
<p>解压后改名为spark</p>
<p>修改配置文件<code>spark-env.sh</code>, 以指定运行参数</p>
<ul>
<li><p>进入配置目录, 并复制一份新的配置文件, 以供在此基础之上进行修改</p>
<figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">cd</span> /export/servers/spark/<span class="keyword">conf</span></span><br><span class="line"><span class="keyword">cp</span> spark-env.<span class="keyword">sh</span>.template spark-env.<span class="keyword">sh</span></span><br><span class="line"><span class="keyword">vi</span> spark-env.<span class="keyword">sh</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>将以下内容复制进配置文件末尾</p>
<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 指定 Java Home</span></span><br><span class="line"><span class="builtin-name">export</span> <span class="attribute">JAVA_HOME</span>=/export/servers/jdk1.8.0_141</span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定 Spark Master 地址</span></span><br><span class="line"><span class="builtin-name">export</span> <span class="attribute">SPARK_MASTER_HOST</span>=node01</span><br><span class="line"><span class="builtin-name">export</span> <span class="attribute">SPARK_MASTER_PORT</span>=7077</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="2-2-配置"><a href="#2-2-配置" class="headerlink" title="2.2 配置"></a>2.2 配置</h3><ol>
<li><p>修改配置文件 <code>slaves</code>, 以指定从节点为止, 从在使用 <code>sbin/start-all.sh</code> 启动集群的时候, 可以一键启动整个集群所有的 Worker</p>
<ul>
<li><p>进入配置目录, 并复制一份新的配置文件, 以供在此基础之上进行修改</p>
<figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">cd</span> /export/servers/spark/<span class="keyword">conf</span></span><br><span class="line"><span class="keyword">cp</span> slaves.template slaves</span><br><span class="line"><span class="keyword">vi</span> slaves</span><br></pre></td></tr></table></figure>
</li>
<li><p>配置所有从节点的地址</p>
<figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">node02</span></span><br><span class="line"><span class="symbol">node03</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>配置 <code>HistoryServer</code></p>
<ol>
<li><p>默认情况下, Spark 程序运行完毕后, 就无法再查看运行记录的 Web UI 了, 通过 HistoryServer 可以提供一个服务, 通过读取日志文件, 使得我们可以在程序运行结束后, 依然能够查看运行过程</p>
</li>
<li><p>复制 <code>spark-defaults.conf</code>, 以供修改</p>
<figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">cd</span> /export/servers/spark/<span class="keyword">conf</span></span><br><span class="line"><span class="keyword">cp</span> spark-defaults.<span class="keyword">conf</span>.template spark-defaults.<span class="keyword">conf</span></span><br><span class="line"><span class="keyword">vi</span> spark-defaults.<span class="keyword">conf</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>将以下内容复制到<code>spark-defaults.conf</code>末尾处, 通过这段配置, 可以指定 Spark 将日志输入到 HDFS 中</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">spark<span class="selector-class">.eventLog</span><span class="selector-class">.enabled</span>  true</span><br><span class="line">spark<span class="selector-class">.eventLog</span><span class="selector-class">.dir</span>      hdfs:<span class="comment">//node01:8020/spark_log</span></span><br><span class="line">spark<span class="selector-class">.eventLog</span><span class="selector-class">.compress</span> true</span><br></pre></td></tr></table></figure>
</li>
<li><p>将以下内容复制到<code>spark-env.sh</code>的<strong>末尾</strong>, 配置 HistoryServer 启动参数, 使得 HistoryServer 在启动的时候读取 HDFS 中写入的 Spark 日志</p>
<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 指定 Spark History 运行参数</span></span><br><span class="line"><span class="builtin-name">export</span> <span class="attribute">SPARK_HISTORY_OPTS</span>=<span class="string">"-Dspark.history.ui.port=4000 -Dspark.history.retainedApplications=3 -Dspark.history.fs.logDirectory=hdfs://node01:8020/spark_log"</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>为 Spark 创建 HDFS 中的日志目录</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -mkdir -<span class="selector-tag">p</span> /spark_log</span><br></pre></td></tr></table></figure>
</li>
</ol>
</li>
</ol>
<h3 id="2-3-分发运行"><a href="#2-3-分发运行" class="headerlink" title="2.3 分发运行"></a>2.3 分发运行</h3><ol>
<li><p>将 Spark 安装包分发给集群中其它机器</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /<span class="built_in">export</span>/servers</span><br><span class="line">scp -r spark/ node03:<span class="variable">$PWD</span></span><br><span class="line">scp -r spark/ node02:<span class="variable">$PWD</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>启动 Spark Master 和 Slaves, 以及 HistoryServer</p>
<figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">cd</span> /export/servers/spark</span><br><span class="line">sbin/start-<span class="keyword">all</span>.<span class="keyword">sh</span></span><br><span class="line">sbin/start-<span class="keyword">history</span>-server.<span class="keyword">sh</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<h2 id="3-spark集群高可用搭建"><a href="#3-spark集群高可用搭建" class="headerlink" title="3 spark集群高可用搭建"></a>3 spark集群高可用搭建</h2><p>了解如何使用 Zookeeper 帮助 Spark Standalone 高可用</p>
<p>对于 Spark Standalone 集群来说, 当 Worker 调度出现问题的时候, 会自动的弹性容错, 将出错的 Task 调度到其它 Worker 执行</p>
<p>但是对于 Master 来说, 是会出现单点失败的, 为了避免可能出现的单点失败问题, Spark 提供了两种方式满足高可用</p>
<ul>
<li>使用 Zookeeper 实现 Masters 的主备切换</li>
<li>使用文件系统做主备切换</li>
</ul>
<p>使用文件系统做主备切换的场景实在太小, 所以此处不做探讨</p>
<h3 id="3-1-停止spark集群"><a href="#3-1-停止spark集群" class="headerlink" title="3.1 停止spark集群"></a>3.1 停止spark集群</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/spark</span><br><span class="line">sbin/stop-all.sh</span><br></pre></td></tr></table></figure>
<h3 id="3-2-修改配置文件-增加-Spark-运行时参数-从而指定-Zookeeper-的位置"><a href="#3-2-修改配置文件-增加-Spark-运行时参数-从而指定-Zookeeper-的位置" class="headerlink" title="3.2 修改配置文件, 增加 Spark 运行时参数, 从而指定 Zookeeper 的位置"></a>3.2 <strong>修改配置文件, 增加 Spark 运行时参数, 从而指定 Zookeeper 的位置</strong></h3><ol>
<li><p>进入 <code>spark-env.sh</code> 所在目录, 打开 vi 编辑</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/spark/conf</span><br><span class="line">vi spark-env.sh</span><br></pre></td></tr></table></figure>
</li>
<li><p>编辑 <code>spark-env.sh</code>, 添加 Spark 启动参数, 并去掉 SPARK_MASTER_HOST 地址</p>
</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># 指定 Java Home</span><br><span class="line">export JAVA_HOME=/export/servers/jdk1.8.0_141</span><br><span class="line"></span><br><span class="line"># 指定 Spark Master 地址</span><br><span class="line"># export SPARK_MASTER_HOST=node01            //这个注释掉  修改1</span><br><span class="line">export SPARK_MASTER_PORT=7077</span><br><span class="line"></span><br><span class="line"># 指定 Spark History 运行参数</span><br><span class="line">export SPARK_HISTORY_OPTS=&quot;-Dspark.history.ui.port=4000 -Dspark.history.retainedApplications=3 -Dspark.history.fs.logDirectory=hdfs://node01:8020/spark_log&quot;</span><br><span class="line"></span><br><span class="line"># 指定 Spark 运行时参数                      //添加  修改2</span><br><span class="line">export SPARK_DAEMON_JAVA_OPTS=&quot;-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=node01:2181,node02:2181,node03:2181 -Dspark.deploy.zookeeper.dir=/spark&quot;</span><br></pre></td></tr></table></figure>
<h3 id="3-3-分发配置文件到整个集群"><a href="#3-3-分发配置文件到整个集群" class="headerlink" title="3.3 分发配置文件到整个集群"></a>3.3 <strong>分发配置文件到整个集群</strong></h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/spark/conf</span><br><span class="line">scp spark-env.sh node02:$PWD</span><br><span class="line">scp spark-env.sh node03:$PWD</span><br></pre></td></tr></table></figure>
<h3 id="3-4-启动"><a href="#3-4-启动" class="headerlink" title="3.4 启动"></a>3.4 启动</h3><ol>
<li><p>在 <code>node01</code> 上启动整个集群</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/spark</span><br><span class="line">sbin/start-all.sh</span><br><span class="line">sbin/start-history-server.sh</span><br></pre></td></tr></table></figure>
</li>
<li><p>在 <code>node02</code> 上单独再启动一个 Master</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/spark</span><br><span class="line">sbin/start-master.sh</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="3-5-查看-node01-master-和-node02-master-的-WebUI"><a href="#3-5-查看-node01-master-和-node02-master-的-WebUI" class="headerlink" title="3.5 查看 node01 master 和 node02 master 的 WebUI"></a>3.5 <strong>查看</strong> <code>node01 master</code> <strong>和</strong> <code>node02 master</code> <strong>的 WebUI</strong></h3><p>你会发现一个是 <code>ALIVE(主)</code>, 另外一个是 <code>STANDBY(备)</code></p>
<p>如果关闭一个, 则另外一个成为<code>ALIVE</code>, 但是这个过程可能要持续两分钟左右, 需要耐心等待</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 在 Node01 中执行如下指令</span></span><br><span class="line">cd /export/servers/spark/</span><br><span class="line">sbin/stop-master.sh</span><br></pre></td></tr></table></figure>
<p>Spark HA 选举</p>
<p>Spark HA 的 Leader 选举使用了一个叫做 Curator 的 Zookeeper 客户端来进行</p>
<p>Zookeeper 是一个分布式强一致性的协调服务, Zookeeper 最基本的一个保证是: 如果多个节点同时创建一个 ZNode, 只有一个能够成功创建. 这个做法的本质使用的是 Zookeeper 的 ZAB 协议, 能够在分布式环境下达成一致.</p>
<p><strong>spark各服务端口</strong></p>
<table>
<thead>
<tr>
<th style="text-align:left">Service</th>
<th style="text-align:left">port</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Master WebUI</td>
<td style="text-align:left">node01:8080</td>
</tr>
<tr>
<td style="text-align:left">Worker WebUI</td>
<td style="text-align:left">node01:8081</td>
</tr>
<tr>
<td style="text-align:left">History Server</td>
<td style="text-align:left">node01:4000</td>
</tr>
</tbody>
</table>
<h2 id="4-第一个应用程序"><a href="#4-第一个应用程序" class="headerlink" title="4 第一个应用程序"></a>4 第一个应用程序</h2><p>流程:</p>
<p>Step 1 进入 Spark 安装目录中</p>
<figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd <span class="regexp">/export/</span>servers<span class="regexp">/spark/</span></span><br></pre></td></tr></table></figure>
<p>Step 2 运行 Spark 示例任务</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \                                //提交spark程序</span><br><span class="line">--master spark://node01:7077,node02:7077,node03:7077 \                   //master地址</span><br><span class="line">--executor-memory 1G \                                                      //这两行为参数</span><br><span class="line">--total-executor-cores 2 \</span><br><span class="line">/export/servers/spark/examples/jars/spark-examples_2.11-2.2.0.jar \                 //指定jar包位置</span><br><span class="line">100                                                                      //会提交到spark程序中</span><br></pre></td></tr></table></figure>
<p>Step 3 运行结果</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Pi is roughly 3.1424627142462715</span><br></pre></td></tr></table></figure>
<p>刚才所运行的程序是 Spark 的一个示例程序, 使用 Spark 编写了一个以蒙特卡洛算法来计算圆周率的任务</p>
<ul>
<li>蒙特卡洛算法概述</li>
</ul>
<p><img src="/images/spark/sf.png" alt="img"></p>
<p>通过迭代循环投点的方式实现蒙特卡洛算法求圆周率</p>
<ul>
<li><p>计算过程</p>
<p>1不断的生成随机的点, 根据点距离圆心是否超过半径来判断是否落入园内 </p>
<p>2通过  来计算圆周率   </p>
<p>3不断的迭代</p>
</li>
</ul>
<h1 id="三-spark入门"><a href="#三-spark入门" class="headerlink" title="三 spark入门"></a>三 spark入门</h1><p>Spark 官方提供了两种方式编写代码, 都比较重要, 分别如下</p>
<ul>
<li><p><code>spark-shell</code><br>Spark shell 是 Spark 提供的一个基于 Scala 语言的交互式解释器, 类似于 Scala 提供的交互式解释器, Spark shell 也可以直接在 Shell 中编写代码执行<br>这种方式也比较重要, 因为一般的数据分析任务可能需要探索着进行, 不是一蹴而就的, 使用 Spark shell 先进行探索, 当代码稳定以后, 使用独立应用的方式来提交任务, 这样是一个比较常见的流程</p>
</li>
<li><p><code>spark-submit</code><br>Spark submit 是一个命令, 用于提交 Scala 编写的基于 Spark 框架, 这种提交方式常用作于在集群中运行任务</p>
<p>上面的计算pi就是一个spark-submit</p>
</li>
</ul>
<h2 id="1-Spark-shell-的方式编写-WordCount-本地文件"><a href="#1-Spark-shell-的方式编写-WordCount-本地文件" class="headerlink" title="1 Spark shell 的方式编写 WordCount (本地文件)"></a>1 Spark shell 的方式编写 WordCount (本地文件)</h2><p>Spark shell 简介</p>
<ul>
<li>启动 Spark shell<br>进入 Spark 安装目录后执行 <code>spark-shell --master master</code> 就可以提交Spark 任务</li>
<li>Spark shell 的原理是把每一行 Scala 代码编译成类, 最终交由 Spark 执行</li>
</ul>
<p><strong>Master地址的设置</strong></p>
<table>
<thead>
<tr>
<th style="text-align:left">地址</th>
<th style="text-align:left">解释</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>local[N]</code></td>
<td style="text-align:left">使用 N 条 Worker 线程在本地运行</td>
</tr>
<tr>
<td style="text-align:left"><code>spark://host:port</code></td>
<td style="text-align:left">在 Spark standalone 中运行, 指定 Spark 集群的 Master 地址, 端口默认为 7077</td>
</tr>
<tr>
<td style="text-align:left"><code>mesos://host:port</code></td>
<td style="text-align:left">在 Apache Mesos 中运行, 指定 Mesos 的地址</td>
</tr>
<tr>
<td style="text-align:left"><code>yarn</code></td>
<td style="text-align:left">在 Yarn 中运行, Yarn 的地址由环境变量 <code>HADOOP_CONF_DIR</code> 来指定</td>
</tr>
</tbody>
</table>
<p>接下来使用 Spark shell 的方式编写一个 WordCount</p>
<h3 id="1-1-准备文件"><a href="#1-1-准备文件" class="headerlink" title="1.1 准备文件"></a>1.1 准备文件</h3><p>在 Node01 中创建文件 <code>/export/data/wordcount.txt</code></p>
<figure class="highlight autohotkey"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">word,</span>hello,world,hadoop</span><br><span class="line"><span class="built_in">hive,</span>sqoop,flume,hello</span><br><span class="line"><span class="built_in">kitty,</span>tom,jerry,world</span><br><span class="line">hadoop</span><br></pre></td></tr></table></figure>
<h3 id="2-2-启动spark-shell-本地线程模式"><a href="#2-2-启动spark-shell-本地线程模式" class="headerlink" title="2.2 启动spark-shell 本地线程模式"></a>2.2 启动spark-shell 本地线程模式</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /<span class="built_in">export</span>/servers/spark</span><br><span class="line">bin/spark-shell --master <span class="built_in">local</span>[2]</span><br></pre></td></tr></table></figure>
<h3 id="2-3-执行代码"><a href="#2-3-执行代码" class="headerlink" title="2.3 执行代码"></a>2.3 执行代码</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//获取文件</span></span><br><span class="line"><span class="keyword">val</span> rdd1=sc.textFile(<span class="string">"file:///export/test/wordcount.txt"</span>)</span><br><span class="line"><span class="comment">//获取单个单词</span></span><br><span class="line"><span class="keyword">val</span> rdd2=rdd1.flatMap(_.split(<span class="string">","</span>))   相当于 (item =&gt; item.split(<span class="string">","</span>))</span><br><span class="line"><span class="comment">//给每个单词 符频次  组成元组</span></span><br><span class="line"><span class="keyword">val</span> rdd3=rdd2.map((_,<span class="number">1</span>))              相当于 (item =&gt;(item,<span class="number">1</span>))</span><br><span class="line"><span class="comment">//聚合得到最终结果</span></span><br><span class="line"><span class="keyword">val</span> rdd4=rdd3.reduceByKey(_ + _)      相当于 ((curr,age) =&gt; curr + age)   这一步并未求值</span><br><span class="line"><span class="comment">//打印解结果   collect为收集数据</span></span><br><span class="line"><span class="keyword">val</span> res=rdd4.collect                  在这一步才求值</span><br></pre></td></tr></table></figure>
<p>上述代码中 <code>sc</code> 变量指的是 SparkContext, 是 Spark 程序的上下文和入口</p>
<p>正常情况下我们需要自己创建, 但是如果使用 Spark shell 的话, Spark shell 会帮助我们创建, 并且以变量 <code>sc</code> 的形式提供给我们调用</p>
<p><strong>运行流程</strong></p>
<p><img src="/images/spark/yxlc.png" alt="img"></p>
<ol>
<li><code>flatMap(_.split(&quot; &quot;))</code> 将数据转为数组的形式, 并展平为多个数据</li>
<li><code>map_, 1</code> 将数据转换为元组的形式</li>
<li><code>reduceByKey(_ + _)</code> 计算每个 Key 出现的次数</li>
</ol>
<p><strong>总结</strong></p>
<ol>
<li>使用 Spark shell 可以快速验证想法</li>
<li>Spark 框架下的代码非常类似 Scala 的函数式调用</li>
</ol>
<h2 id="2-读取hdfs上的文件"><a href="#2-读取hdfs上的文件" class="headerlink" title="2 读取hdfs上的文件"></a>2 读取hdfs上的文件</h2><h3 id="2-1-上传文件呢到hdfs"><a href="#2-1-上传文件呢到hdfs" class="headerlink" title="2.1 上传文件呢到hdfs"></a>2.1 上传文件呢到hdfs</h3><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="title">hdfs</span> dfs -put wordcount.txt /<span class="class"><span class="keyword">data</span></span></span><br></pre></td></tr></table></figure>
<h3 id="2-2-在spark-shell中访问hdfs"><a href="#2-2-在spark-shell中访问hdfs" class="headerlink" title="2.2 在spark-shell中访问hdfs"></a>2.2 在spark-shell中访问hdfs</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//获取文件</span></span><br><span class="line"><span class="keyword">val</span> rdd1=sc.textFile(<span class="string">"hdfs://node01:8020/data"</span>)  <span class="comment">//或者("/data") spark默认的文件地址就是hdfs</span></span><br><span class="line"><span class="comment">//获取单个单词</span></span><br><span class="line"><span class="keyword">val</span> rdd2=rdd1.flatMap(_.split(<span class="string">","</span>))   相当于 (item =&gt; item.split(<span class="string">","</span>))</span><br><span class="line"><span class="comment">//给每个单词 符频次  组成元组</span></span><br><span class="line"><span class="keyword">val</span> rdd3=rdd2.map((_,<span class="number">1</span>))              相当于 (item =&gt;(item,<span class="number">1</span>))</span><br><span class="line"><span class="comment">//聚合得到最终结果</span></span><br><span class="line"><span class="keyword">val</span> rdd4=rdd3.reduceByKey(_ + _)      相当于 ((curr,age) =&gt; curr + age)   这一步并未求值</span><br><span class="line"><span class="comment">//打印解结果   collect为收集数据</span></span><br><span class="line"><span class="keyword">val</span> res=rdd4.collect                  在这一步才求值</span><br></pre></td></tr></table></figure>
<p>也可以通过向 Spark 配置 Hadoop 的路径, 来通过路径直接访问</p>
<ul>
<li><p>1.在 <code>spark-env.sh</code> 中添加 Hadoop 的配置路径</p>
<p><code>export HADOOP_CONF_DIR=&quot;/etc/hadoop/conf&quot;</code></p>
</li>
<li><p>2.在配置过后, 可以直接使用 <code>hdfs:///路径</code> 的形式直接访问</p>
</li>
</ul>
<p><strong>在配置过后, 也可以直接使用路径访问</strong></p>
<p>(“/data”)</p>
<h2 id="3-编写独立应用提交spark任务"><a href="#3-编写独立应用提交spark任务" class="headerlink" title="3 编写独立应用提交spark任务"></a>3 编写独立应用提交spark任务</h2><p>创建maven工程</p>
<h3 id="3-1-添加依赖"><a href="#3-1-添加依赖" class="headerlink" title="3.1 添加依赖"></a>3.1 添加依赖</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">properties</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">scala.version</span>&gt;</span>2.11.8<span class="tag">&lt;/<span class="name">scala.version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">spark.version</span>&gt;</span>2.2.0<span class="tag">&lt;/<span class="name">spark.version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">slf4j.version</span>&gt;</span>1.7.16<span class="tag">&lt;/<span class="name">slf4j.version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">log4j.version</span>&gt;</span>1.2.17<span class="tag">&lt;/<span class="name">log4j.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.scala-lang<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>scala-library<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;scala.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-core_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;spark.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.6.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.slf4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>jcl-over-slf4j<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;slf4j.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.slf4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>slf4j-api<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;slf4j.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.slf4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>slf4j-log4j12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;slf4j.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>log4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>log4j<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;log4j.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.10<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">         <span class="comment">&lt;!--scala文件夹需要自己创建--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">sourceDirectory</span>&gt;</span>src/main/scala<span class="tag">&lt;/<span class="name">sourceDirectory</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">testSourceDirectory</span>&gt;</span>src/test/scala<span class="tag">&lt;/<span class="name">testSourceDirectory</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line"></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-compiler-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">source</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">source</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">target</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">target</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">encoding</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">encoding</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line"></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>net.alchim31.maven<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>scala-maven-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.2.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">goal</span>&gt;</span>compile<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">goal</span>&gt;</span>testCompile<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">args</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">arg</span>&gt;</span>-dependencyfile<span class="tag">&lt;/<span class="name">arg</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">arg</span>&gt;</span>$&#123;project.build.directory&#125;/.scala_dependencies<span class="tag">&lt;/<span class="name">arg</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">args</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">			 <span class="comment">&lt;!--打包插件  因为默认的打包不包含maven的依赖--&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-shade-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.1.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">goal</span>&gt;</span>shade<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">filters</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">filter</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">artifact</span>&gt;</span>*:*<span class="tag">&lt;/<span class="name">artifact</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">excludes</span>&gt;</span></span><br><span class="line">                                        <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.SF<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                        <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.DSA<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                        <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.RSA<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;/<span class="name">excludes</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">filter</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">filters</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">transformers</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">transformer</span> <span class="attr">implementation</span>=<span class="string">"org.apache.maven.plugins.shade.resource.ManifestResourceTransformer"</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">mainClass</span>&gt;</span><span class="tag">&lt;/<span class="name">mainClass</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">transformer</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">transformers</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>编写代码</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.nicai.sparkwordcount</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordCount</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//获取sparkContext上下文对象</span></span><br><span class="line">     <span class="keyword">val</span> conf: <span class="type">SparkConf</span> =<span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"word_count"</span>)</span><br><span class="line">     <span class="keyword">val</span> sc: <span class="type">SparkContext</span> = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//读取文件并计算频次</span></span><br><span class="line">    <span class="comment">//加载文件</span></span><br><span class="line">    <span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">"day24Spark/data/wc.txt"</span>)</span><br><span class="line">    <span class="keyword">val</span> rdd2 = rdd1.flatMap(item =&gt; item.split(<span class="string">","</span>))</span><br><span class="line">    <span class="keyword">val</span> rdd3 = rdd2.map(item =&gt; (item,<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">val</span> rdd4 = rdd3.reduceByKey((curr,age) =&gt; curr + age)</span><br><span class="line">    <span class="keyword">val</span> result = rdd4.collect()</span><br><span class="line">    result.foreach(println(_))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-2-运行"><a href="#3-2-运行" class="headerlink" title="3.2 运行"></a>3.2 运行</h3><p>####1以上代码可以直接在本地运行</p>
<p>若是本地运行有错:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Error:scalac: error while loading &lt;root&gt;, Error accessing G:\apache-maven-3.3.9\apache-maven-3.3.9\repository_pinyougou\javax\ws\rs\javax.ws.rs-api\2.0.1\javax.ws.rs-api-2.0.1.jar</span><br><span class="line">或者</span><br><span class="line">Error:scalac: error while loading &lt;root&gt;, Error accessing G:\apache-maven-3.3.9\apache-maven-3.3.9\repository_pinyougou\org\spache\commons\commons-crypto\1.0.0\commons-crypto-1.0.0.jar</span><br><span class="line"></span><br><span class="line">则这两个jar包又问题 请自己下载</span><br></pre></td></tr></table></figure>
<h4 id="2-打包在集群运行spark-submit"><a href="#2-打包在集群运行spark-submit" class="headerlink" title="2 打包在集群运行spark-submit"></a>2 打包在集群运行spark-submit</h4><p>修改代码</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf: <span class="type">SparkConf</span> =<span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"word_count"</span>)</span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">"hdfs://node01:8020/data/wc.txt"</span>)  <span class="comment">//改为hdfs路径</span></span><br></pre></td></tr></table></figure>
<p>使用maven打包</p>
<p><em>spark-submit 命令</em></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">spark-submit [options] &lt;app jar&gt; &lt;app options&gt;</span><br><span class="line"></span><br><span class="line">app jar 程序 Jar 包</span><br><span class="line">app options 程序 Main 方法传入的参数</span><br><span class="line">options 提交应用的参数, 可以有如下选项</span><br></pre></td></tr></table></figure>
<p>options 参数</p>
<table>
<thead>
<tr>
<th>参数</th>
<th>解释</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>--master &lt;url&gt;</code></td>
<td>同 Spark shell 的 Master, 可以是spark, yarn, mesos, kubernetes等 URL</td>
</tr>
<tr>
<td><code>--deploy-mode &lt;client or cluster&gt;</code></td>
<td>Driver 运行位置, 可选 Client 和 Cluster, 分别对应运行在本地和集群(Worker)中</td>
</tr>
<tr>
<td><code>--class &lt;class full name&gt;</code></td>
<td>Jar 中的 Class, 程序入口</td>
</tr>
<tr>
<td><code>--jars &lt;dependencies path&gt;</code></td>
<td>依赖 Jar 包的位置</td>
</tr>
<tr>
<td><code>--driver-memory &lt;memory size&gt;</code></td>
<td>Driver 程序运行所需要的内存, 默认 512M</td>
</tr>
<tr>
<td><code>--executor-memory &lt;memory size&gt;</code></td>
<td>Executor 的内存大小, 默认 1G</td>
</tr>
</tbody>
</table>
<p>打包后有两个jar包  一个带有依赖 (大) 一个(小)  因为集群中的spark自带有Hadoop和spark的jar包  所以不需要上传大的,只需上传小的即可</p>
<p>在上传的jar包所在目录执行:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">spark-submit --master spark://node01:7077 \</span><br><span class="line">--class cn.itcast.spark.WordCounts \</span><br><span class="line">original-spark-0.1.0.jar</span><br></pre></td></tr></table></figure>
<p>若想在任意目录执行 shell-submit  配置一下:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">vim  /etc/profile</span><br><span class="line"></span><br><span class="line">export SPARK_BIN=/export/servers/spark/bin</span><br><span class="line">export PATH=$PATH:$SPARK_BIN</span><br><span class="line">//生效</span><br><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure>
<h2 id="4-三种方式总结"><a href="#4-三种方式总结" class="headerlink" title="4 三种方式总结"></a>4 三种方式总结</h2><p>Spark shell</p>
<ul>
<li>作用<ul>
<li>一般用作于探索阶段, 通过 Spark shell 快速的探索数据规律</li>
<li>当探索阶段结束后, 代码确定以后, 通过独立应用的形式上线运行</li>
</ul>
</li>
<li>功能<ul>
<li>Spark shell 可以选择在集群模式下运行, 还是在线程模式下运行</li>
<li>Spark shell 是一个交互式的运行环境, 已经内置好了 SparkContext 和 SparkSession 对象, 可以直接使用</li>
<li>Spark shell 一般运行在集群中安装有 Spark client 的服务器中, 所以可以自有的访问 HDFS</li>
</ul>
</li>
</ul>
<p>本地运行</p>
<ul>
<li>作用<ul>
<li>在编写独立应用的时候, 每次都要提交到集群中还是不方便, 另外很多时候需要调试程序, 所以在 IDEA 中直接运行会比较方便, 无需打包上传了</li>
</ul>
</li>
<li>功能<ul>
<li>因为本地运行一般是在开发者的机器中运行, 而不是集群中, 所以很难直接使用 HDFS 等集群服务, 需要做一些本地配置, 用的比较少</li>
<li>需要手动创建 SparkContext</li>
</ul>
</li>
</ul>
<p>集群运行</p>
<ul>
<li>作用<ul>
<li>正式环境下比较多见, 独立应用编写好以后, 打包上传到集群中, 使用<code>spark-submit</code>来运行, 可以完整的使用集群资源</li>
</ul>
</li>
<li>功能<ul>
<li>同时在集群中通过<code>spark-submit</code>来运行程序也可以选择是用线程模式还是集群模式</li>
<li>集群中运行是全功能的, HDFS 的访问, Hive 的访问都比较方便</li>
<li>需要手动创建 SparkContext</li>
</ul>
</li>
</ul>
<h1 id="四-RDD简介"><a href="#四-RDD简介" class="headerlink" title="四 RDD简介"></a>四 RDD简介</h1><p>在idea中的wc代码中</p>
<ol>
<li>使用 <code>sc.textFile()</code> 方法读取 HDFS 中的文件, 并生成一个 <code>RDD</code></li>
<li>使用 <code>flatMap</code> 算子将读取到的每一行字符串打散成单词, 并把每个单词变成新的行</li>
<li>使用 <code>map</code> 算子将每个单词转换成 <code>(word, 1)</code> 这种元组形式</li>
<li>使用 <code>reduceByKey</code> 统计单词对应的频率</li>
</ol>
<p>其中所使用到的算子有如下几个:</p>
<ul>
<li><code>flatMap</code> 是一对多</li>
<li><code>map</code> 是一对一</li>
<li><code>reduceByKey</code> 是按照 Key 聚合, 类似 MapReduce 中的 Shuffled</li>
</ul>
<h2 id="1-rdd"><a href="#1-rdd" class="headerlink" title="1 rdd"></a>1 rdd</h2><p>定义</p>
<p>RDD, 全称为 Resilient Distributed Datasets, 是一个容错的, 并行的数据结构, 可以让用户显式地将数据存储到磁盘和内存中, 并能控制数据的分区.</p>
<p>同时, RDD 还提供了一组丰富的操作来操作这些数据. 在这些操作中, 诸如 map, flatMap, filter 等转换操作实现了 Monad 模式, 很好地契合了 Scala 的集合操作. 除此之外, RDD 还提供了诸如 join, groupBy, reduceByKey 等更为方便的操作, 以支持常见的数据运算.</p>
<p>通常来讲, 针对数据处理有几种常见模型, 包括: Iterative Algorithms, Relational Queries, MapReduce, Stream Processing. 例如 Hadoop MapReduce 采用了 MapReduce 模型, Storm 则采用了 Stream Processing 模型. RDD 混合了这四种模型, 使得 Spark 可以应用于各种大数据处理场景.</p>
<p>RDD 作为数据结构, 本质上是一个只读的分区记录集合. 一个 RDD 可以包含多个分区, 每个分区就是一个 DataSet 片段.</p>
<p>RDD 之间可以相互依赖, 如果 RDD 的每个分区最多只能被一个子 RDD 的一个分区使用，则称之为窄依赖, 若被多个子 RDD 的分区依赖，则称之为宽依赖. 不同的操作依据其特性, 可能会产生不同的依赖. 例如 map 操作会产生窄依赖, 而 join 操作则产生宽依赖.</p>
<p>特点</p>
<ol>
<li>RDD 是一个编程模型<ol>
<li>RDD 允许用户显式的指定数据存放在内存或者磁盘</li>
<li>RDD 是分布式的, 用户可以控制 RDD 的分区</li>
</ol>
</li>
<li>RDD 是一个编程模型<ol>
<li>RDD 提供了丰富的操作</li>
<li>RDD 提供了 map, flatMap, filter 等操作符, 用以实现 Monad 模式</li>
<li>RDD 提供了 reduceByKey, groupByKey 等操作符, 用以操作 Key-Value 型数据</li>
<li>RDD 提供了 max, min, mean 等操作符, 用以操作数字型的数据</li>
</ol>
</li>
<li>RDD 是混合型的编程模型, 可以支持迭代计算, 关系查询, MapReduce, 流计算</li>
<li>RDD 是只读的</li>
<li>RDD 之间有依赖关系, 根据执行操作的操作符的不同, 依赖关系可以分为宽依赖和窄依赖</li>
</ol>
<h2 id="2-rdd-分区"><a href="#2-rdd-分区" class="headerlink" title="2 rdd 分区"></a>2 rdd 分区</h2><p><img src="/images/spark/rddf.png" alt="img"></p>
<p>整个 WordCount 案例的程序从结构上可以用上图表示, 分为两个大部分</p>
<p>存储</p>
<p>文件如果存放在 HDFS 上, 是分块的, 类似上图所示, 这个 <code>wordcount.txt</code> 分了三块</p>
<p>计算</p>
<p>Spark 不止可以读取 HDFS, Spark 还可以读取很多其它的数据集, Spark 可以从数据集中创建出 RDD</p>
<p>例如上图中, 使用了一个 RDD 表示 HDFS 上的某一个文件, 这个文件在 HDFS 中是分三块, 那么 RDD 在读取的时候就也有三个分区, 每个 RDD 的分区对应了一个 HDFS 的分块</p>
<p>后续 RDD 在计算的时候, 可以更改分区, 也可以保持三个分区, 每个分区之间有依赖关系, 例如说 RDD2 的分区一依赖了 RDD1 的分区一</p>
<p>RDD 之所以要设计为有分区的, 是因为要进行分布式计算, 每个不同的分区可以在不同的线程, 或者进程, 甚至节点中, 从而做到并行计算</p>
<p><strong>总结</strong></p>
<ol>
<li>RDD 是弹性分布式数据集</li>
<li>RDD 一个非常重要的前提和基础是 RDD 运行在分布式环境下, 其可以分区</li>
</ol>
<h2 id="3-创建rdd"><a href="#3-创建rdd" class="headerlink" title="3 创建rdd"></a>3 创建rdd</h2><p>程序入口 SparkContext</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val conf = <span class="keyword">new</span> SparkConf().setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">val sc: SparkContext = <span class="keyword">new</span> SparkContext(conf)</span><br></pre></td></tr></table></figure>
<p><code>SparkContext</code> 是 spark-core 的入口组件, 是一个 Spark 程序的入口, 在 Spark 0.x 版本就已经存在 <code>SparkContext</code> 了, 是一个元老级的 API</p>
<p>如果把一个 Spark 程序分为前后端, 那么服务端就是可以运行 Spark 程序的集群, 而 <code>Driver</code> 就是 Spark 的前端, 在 <code>Driver</code>中 <code>SparkContext</code> 是最主要的组件, 也是 <code>Driver</code> 在运行时首先会创建的组件, 是 <code>Driver</code> 的核心</p>
<p><code>SparkContext</code> 从提供的 API 来看, 主要作用是连接集群, 创建 RDD, 累加器, 广播变量等</p>
<p>简略的说, RDD 有三种创建方式</p>
<ul>
<li>RDD 可以通过本地集合直接创建</li>
<li>RDD 也可以通过读取外部数据集来创建</li>
<li>RDD 也可以通过其它的 RDD 衍生而来</li>
</ul>
<p>###1 通过本地集合直接创建</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">val conf = <span class="keyword">new</span> SparkConf().setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">val sc = <span class="keyword">new</span> SparkContext(conf)</span><br><span class="line"></span><br><span class="line">val list = List(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>)</span><br><span class="line">val rddParallelize = sc.parallelize(list, <span class="number">2</span>)</span><br><span class="line">val rddMake = sc.makeRDD(list, <span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>通过 <code>parallelize</code> 和 <code>makeRDD</code> 这两个 API 可以通过本地集合创建 RDD</p>
<p>这两个 API 本质上是一样的, 在 <code>makeRDD</code> 这个方法的内部, 最终也是调用了 <code>parallelize</code></p>
<p>因为不是从外部直接读取数据集的, 所以没有外部的分区可以借鉴, 于是在这两个方法都都有两个参数, 第一个参数是本地集合, 第二个参数是分区数</p>
<h3 id="2-通过读取外部文件创建-RDD"><a href="#2-通过读取外部文件创建-RDD" class="headerlink" title="2 通过读取外部文件创建 RDD"></a>2 <strong>通过读取外部文件创建 RDD</strong></h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val conf = <span class="keyword">new</span> SparkConf().setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">val sc = <span class="keyword">new</span> SparkContext(conf)</span><br><span class="line"></span><br><span class="line">val source: RDD[String] = sc.textFile(<span class="string">"hdfs://node01:8020/dataset/wordcount.txt"</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>访问方式<ul>
<li>支持访问文件夹, 例如 <code>sc.textFile(&quot;hdfs:///dataset&quot;)</code></li>
<li>支持访问压缩文件, 例如 <code>sc.textFile(&quot;hdfs:///dataset/words.gz&quot;)</code></li>
<li>支持通过通配符访问, 例如 <code>sc.textFile(&quot;hdfs:///dataset/*.txt&quot;)</code></li>
</ul>
</li>
</ul>
<table>
<thead>
<tr>
<th></th>
<th>如果把 Spark 应用跑在集群上, 则 Worker 有可能在任何一个节点运行所以如果使用 <code>file:///…;</code> 形式访问本地文件的话, 要确保所有的 Worker 中对应路径上有这个文件, 否则可能会报错无法找到文件</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<ul>
<li><strong>分区</strong><ul>
<li>默认情况下读取 HDFS 中文件的时候, 每个 HDFS 的 <code>block</code> 对应一个 RDD 的 <code>partition</code>, <code>block</code> 的默认是128M</li>
<li>通过第二个参数, 可以指定分区数量, 例如 <code>sc.textFile(&quot;hdfs://node01:8020/dataset/wordcount.txt&quot;, 20)</code></li>
<li>如果通过第二个参数指定了分区, 这个分区数量一定不能小于<code>block</code>数</li>
</ul>
</li>
</ul>
<table>
<thead>
<tr>
<th></th>
<th>通常每个 CPU core 对应 2 - 4 个分区是合理的值</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<ul>
<li>支持的平台<ul>
<li>支持 Hadoop 的几乎所有数据格式, 支持 HDFS 的访问</li>
<li>通过第三方的支持, 可以访问AWS和阿里云中的文件, 详情查看对应平台的 API</li>
</ul>
</li>
</ul>
<h3 id="3-通过其它的-RDD-衍生新的-RDD"><a href="#3-通过其它的-RDD-衍生新的-RDD" class="headerlink" title="3 通过其它的 RDD 衍生新的 RDD"></a>3 <strong>通过其它的 RDD 衍生新的 RDD</strong></h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val conf = <span class="keyword">new</span> SparkConf().setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">val sc = <span class="keyword">new</span> SparkContext(conf)</span><br><span class="line"></span><br><span class="line">val source: RDD[String] = sc.textFile(<span class="string">"hdfs://node01:8020/dataset/wordcount.txt"</span>, <span class="number">20</span>)</span><br><span class="line">val words = source.flatMap &#123; line =&gt; line.split(<span class="string">" "</span>) &#125;</span><br></pre></td></tr></table></figure>
<ul>
<li><code>source</code> 是通过读取 HDFS 中的文件所创建的</li>
<li><code>words</code> 是通过 <code>source</code> 调用算子 <code>map</code> 生成的新 RDD</li>
</ul>
<p><strong>总结</strong></p>
<p>RDD 的可以通过三种方式创建, 通过本地集合创建, 通过外部数据集创建, 通过其它的 RDD 衍生</p>
<h2 id="4-rdd算子"><a href="#4-rdd算子" class="headerlink" title="4 rdd算子"></a>4 rdd算子</h2><h3 id="1-Map-算子"><a href="#1-Map-算子" class="headerlink" title="1 Map 算子"></a>1 Map 算子</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sc.parallelize(<span class="type">Seq</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line">  .map( num =&gt; num * <span class="number">10</span> )</span><br><span class="line">  .collect()</span><br></pre></td></tr></table></figure>
<p>作用</p>
<p>把 RDD 中的数据 一对一 的转为另一种形式</p>
<p>调用</p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">def map[<span class="string">U: ClassTag</span>](<span class="link">f: T ⇒ U</span>): RDD[U]</span><br></pre></td></tr></table></figure>
<p>参数</p>
<p><code>f</code> → Map 算子是 <code>原RDD → 新RDD</code> 的过程, 这个函数的参数是原 RDD 数据, 返回值是经过函数转换的新 RDD 的数据</p>
<p>注意点</p>
<p>Map 是一对一, 如果函数是 <code>String → Array[String]</code> 则新的 RDD 中每条数据就是一个数组</p>
<h3 id="2-FlatMap"><a href="#2-FlatMap" class="headerlink" title="2 FlatMap"></a>2 FlatMap</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sc.parallelize(<span class="type">Seq</span>(<span class="string">"Hello lily"</span>, <span class="string">"Hello lucy"</span>, <span class="string">"Hello tim"</span>))</span><br><span class="line">  .flatMap( line =&gt; line.split(<span class="string">" "</span>) )</span><br><span class="line">  .collect()</span><br></pre></td></tr></table></figure>
<p>作用</p>
<p>FlatMap 算子和 Map 算子类似, 但是 FlatMap 是一对多</p>
<p>调用</p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">def flatMap[<span class="string">U: ClassTag</span>](<span class="link">f: T ⇒ List[U]</span>): RDD[U]</span><br></pre></td></tr></table></figure>
<p>参数</p>
<p><code>f</code> → 参数是原 RDD 数据, 返回值是经过函数转换的新 RDD 的数据, 需要注意的是返回值是一个集合, 集合中的数据会被展平后再放入新的 RDD</p>
<p>注意点</p>
<p>flatMap 其实是两个操作, 是 <code>map + flatten</code>, 也就是先转换, 后把转换而来的 List 展开</p>
<h3 id="3ReduceByKey"><a href="#3ReduceByKey" class="headerlink" title="3ReduceByKey"></a>3ReduceByKey</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sc.parallelize(<span class="type">Seq</span>((<span class="string">"a"</span>, <span class="number">1</span>), (<span class="string">"a"</span>, <span class="number">1</span>), (<span class="string">"b"</span>, <span class="number">1</span>)))</span><br><span class="line">  .reduceByKey( (curr, agg) =&gt; curr + agg )</span><br><span class="line">  .collect()</span><br></pre></td></tr></table></figure>
<p>作用</p>
<p>首先按照 Key 分组, 接下来把整组的 Value 计算出一个聚合值, 这个操作非常类似于 MapReduce 中的 Reduce</p>
<p>调用</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduceByKey</span><span class="params">(func: <span class="params">(V, V)</span> ⇒ V)</span>:</span> RDD[(K, V)]</span><br></pre></td></tr></table></figure>
<p>参数</p>
<p>func → 执行数据处理的函数, 传入两个参数, 一个是当前值, 一个是局部汇总, 这个函数需要有一个输出, 输出就是这个 Key 的汇总结果</p>
<p>注意点</p>
<ul>
<li>ReduceByKey 只能作用于 Key-Value 型数据, Key-Value 型数据在当前语境中特指 Tuple2</li>
<li>ReduceByKey 是一个需要 Shuffled 的操作</li>
<li>和其它的 Shuffled 相比, ReduceByKey是高效的, 因为类似 MapReduce 的, 在 Map 端有一个 Cominer, 这样 I/O 的数据便会减少</li>
</ul>
<p><strong>总结</strong></p>
<ol>
<li>map 和 flatMap 算子都是转换, 只是 flatMap 在转换过后会再执行展开, 所以 map 是一对一, flatMap 是一对多</li>
<li>reduceByKey 类似 MapReduce 中的 Reduce</li>
</ol>

      
    </div>

    

    
    
    

    

    
      
    
    
      <div>
        <div id="reward-container">
  <div>Thank you for your accept. mua！</div>
  <button id="reward-button" disable="enable" onclick="var qr = document.getElementById(&quot;qr&quot;); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">

    
      
      
        
      
      <div style="display: inline-block">
        <img src="/images/reward/wechatpay.jpg" alt="丨HF丨 微信支付">
        <p>微信支付</p>
      </div>
    
      
      
        
      
      <div style="display: inline-block">
        <img src="/images/reward/alipay.jpg" alt="丨HF丨 支付宝">
        <p>支付宝</p>
      </div>
    

  </div>
</div>

      </div>
    

    
      <div>
        




  



<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>丨HF丨</li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    
    <a href="https://manzhong.github.io/2017/08/15/spark入门.html" title="spark入门">https://manzhong.github.io/2017/08/15/spark入门.html</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li>
</ul>

      </div>
    
    <div>
      
        <div>
    
        <div style="text-align:center;color: #ccc;font-size:16px;">-------------本文结束<i class="fa fa-heart"></i>感谢您的阅读-------------</div>
    
</div>
      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/BigData/" rel="tag"><i class="fa fa-tag"></i> BigData</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/08/14/ElSearch.html" rel="next" title="ElSearch">
                <i class="fa fa-chevron-left"></i> ElSearch
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/08/16/SparkRDD.html" rel="prev" title="SparkRDD">
                SparkRDD <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  
    <div class="comments" id="comments">
      <div id="lv-container" data-id="city" data-uid="MTAyMC81MDA2Mi8yNjU1Mw=="></div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <div class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/hexo.jpg" alt="丨HF丨">
            
              <p class="site-author-name" itemprop="name">丨HF丨</p>
              <div class="site-description motion-element" itemprop="description">第二名就是头号输家!!!</div>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">54</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  
                    
                      <a href="/categories/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">6</span>
                    <span class="site-state-item-name">分类</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                      <a href="/tags/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">13</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://github.com/ipyker" title="GitHub &rarr; https://github.com/ipyker" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="mailto:pyker@qq.com" title="E-Mail &rarr; mailto:pyker@qq.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://weibo.com/viszhang" title="Weibo &rarr; https://weibo.com/viszhang" rel="noopener" target="_blank"><i class="fa fa-fw fa-weibo"></i>Weibo</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="tencent://message/?uin=123435796&Site=&menu=yes" title="QQ &rarr; tencent://message/?uin=123435796&Site=&menu=yes" rel="noopener" target="_blank"><i class="fa fa-fw fa-qq"></i>QQ</a>
                </span>
              
            </div>
          

          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-inline">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-book"></i>
                推荐阅读
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="http://www.54tianzhisheng.cn/tags/Flink/" title="http://www.54tianzhisheng.cn/tags/Flink/" rel="noopener" target="_blank">Flink</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://nginxconfig.io/" title="https://nginxconfig.io/" rel="noopener" target="_blank">Nginxconfig</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="http://linux.51yip.com/" title="http://linux.51yip.com/" rel="noopener" target="_blank">Linux命令手册</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://echarts.baidu.com/index.html" title="https://echarts.baidu.com/index.html" rel="noopener" target="_blank">echarts可视化库</a>
                  </li>
                
              </ul>
            </div>
          

          
            
          
          
        </div>
      </div>

      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
            <span id="scrollpercent"><span>0</span>%</span>
          
        </div>
      

    </div>
  </aside>
  


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" style=" text-align:center;">&copy; 2017 – <span itemprop="copyrightYear">2020</span>
  <span class="with-love" id="animate">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">HF</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"> 站点字数合计:</i>
    </span>
    
    <span title="站点总字数">961k</span>
  
  
  <span class="post-meta-divider">|</span>
  <a href="http://www.beian.miit.gov.cn" rel="noopener" target="_blank">粤ICP备19028706号 </a>

</div>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>







<div class="run_time" style=" text-align:center;">
  <span id="timeDate">载入天数...</span><span id="times">载入时分秒...</span>
  <script>
    var now = new Date(); 
    function createtime() { 
        var grt= new Date("07/23/2017 10:00:00");//此处修改你的建站时间或者网站上线时间 
        now.setTime(now.getTime()+250); 
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days); 
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours); 
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum); 
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;} 
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum); 
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;} 
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 "; 
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒"; 
    } 
    setInterval("createtime()",250);
  </script>
</div>
        
<div class="busuanzi-count" style=" text-align:center;">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      我的第 <span class="busuanzi-value" id="busuanzi_value_site_uv"></span> 位朋友，
    </span>
  



  
    <span class="site-pv">
      历经 <span class="busuanzi-value" id="busuanzi_value_site_pv"></span> 次回眸才与你相遇
    </span>
  
</div>










        
      </div>
    </footer>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>





  



  






  



  
    
    
      
    
  
  <script color="255,0,255" opacity="0.7" zindex="-1" count="140" src="//cdn.jsdelivr.net/gh/theme-next/theme-next-canvas-nest@1.0.0/canvas-nest.min.js"></script>













  
  <script src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script src="/lib/fancybox/source/jquery.fancybox.pack.js"></script>


  


  <script src="/js/utils.js?v=7.1.1"></script>

  <script src="/js/motion.js?v=7.1.1"></script>



  
  


  <script src="/js/schemes/muse.js?v=7.1.1"></script>




  
  <script src="/js/scrollspy.js?v=7.1.1"></script>
<script src="/js/post-details.js?v=7.1.1"></script>



  


  <script src="/js/next-boot.js?v=7.1.1"></script>


  

  

  
  
  

  

  
  
  


  


  
    <script>
  window.livereOptions = {
    refer: '2017/08/15/spark入门.html'
  };
  (function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
  })(document, 'script');
</script>

  


  
  <script>
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url).replace(/\/{2,}/g, '/');
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x"></i></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x"></i></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  

  

  
  <script>
    (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      bp.src = (curProtocol === 'https') ? 'https://zz.bdstatic.com/linksubmit/push.js' : 'http://push.zhanzhang.baidu.com/push.js';
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>


  

  

  

  

  
  
  
  <script src="/lib/bookmark/bookmark.min.js?v=1.0"></script>
  <script>
  
    bookmark.scrollToMark('auto', "#更多");
  
  </script>


  

  

  

</body>
</html>
<!-- 页面点击小红心 -->
<script type="text/javascript" src="/js/src/clicklove.js"></script>
